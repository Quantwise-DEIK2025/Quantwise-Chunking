{
  "reference_answers": [
    "The Gander project had two main components: (1) an empirical component focused on problem conceptualization, conducted as a mixed-method study with practitioners in industry to provide input for tool design, and (2) a development component focused on building an experimental code review platform incorporating eye-tracking to enable gaze-assisted assistance in code reviews.",
    "The framework identifies (1) employees or other stakeholders of software development organizations, (2) students or other beneficiaries of the university, and (3) independent participants. They are distinguished based on legal and ethical concerns regarding the relationship between researchers and participants, such as potential pressure to participate and secrecy conditions.",
    "The four steps are: (1) Data cleaning \u2013 transforming and anonymizing data (quantitative: coding for statistical tools; qualitative: transcription and anonymization). (2) Data exploration and visualization \u2013 descriptive statistics and outlier detection (quantitative) or preliminary coding ideas (qualitative). (3) Model building and analysis \u2013 statistical analysis like prediction models (quantitative) or coding and theory building (qualitative). (4) Findings presentation \u2013 presenting results in publications or reports.",
    "The three steps are: (i) Publication for reproduction \u2013 artifacts in original state (non-editable documents, executable code), ensuring transparency. (ii) Generalization for general use \u2013 artifacts in editable state (editable documents, source code). (iii) Generalization for continued development \u2013 artifacts with licenses, onboarding guidelines, and community support for ongoing evolution. Each step requires progressively more investment to make artifacts openly reusable.",
    "The risks include: disclosure of irrelevant but sensitive company information, exposure of relevant but confidential events (e.g., security incidents), potential harm to interviewees if their opinions about managers are revealed, difficulty of anonymization with small participant groups, and epistemological concerns that transcripts lack meaning without shared context. These factors led the authors not to recommend sharing raw qualitative data.",
    "The authors recommend not to openly publish qualitative research data. Instead, they suggest publishing study and analysis artifacts such as study protocols, interview guides, interviewee descriptions, and code books from thematic analysis to ensure transparency without compromising confidentiality.",
    "The Gander project discovered that one of the projects used for gaze data analysis lacked a license. After contacting the original author, a MIT license was added, which allowed the team to proceed. Ultimately, the Gander platform was released under a BSD license after reviewing all dependencies.",
    "Eye-tracking is used to detect fixation points during code review, which are connected to programming language elements. This enables real-time gaze-based assistance. As a proof-of-concept, the platform implemented a gaze assistant that visualizes use-declaration relationships in code when users fixate on variable names.",
    "Quantitative survey data, being less rich and more standardized, is easier to anonymize and share openly compared to qualitative data. The authors recommend (R4) that quantitative data be shared openly if, and only if, it is sufficiently anonymized to protect the identity of individuals or companies.",
    "They emphasize the FAIR principle: data should be 'as open as possible and as closed as necessary.' This means fostering reuse and accelerating research through openness, while respecting participants\u2019 privacy, ethical constraints, and companies\u2019 legitimate secrecy concerns.",
    "The main motivation is to reduce the high computational cost of traditional neural architecture search, which often requires training numerous candidate architectures. Training-free indicators aim to estimate performance without full training, enabling faster search.",
    "The three types are (1) synaptic diversity, measuring variability in weight initialization and connections, (2) neuron activation strength, capturing output responses of neurons to input data, and (3) path expressivity, reflecting the complexity of information flow across paths in the architecture.",
    "The feature fusion mechanism combines normalized versions of synaptic diversity, neuron activation strength, and path expressivity through weighted integration. This is necessary because each feature captures different aspects of architecture quality, and fusion provides a more robust performance indicator.",
    "The authors used NAS-Bench-101, NAS-Bench-201, and DARTS search space benchmarks. These datasets were chosen because they provide standardized environments with known ground truth performance, enabling reliable comparison of NAS indicators.",
    "FFI consistently outperformed existing indicators like Synflow, GradNorm, and Jacov in terms of correlation with true architecture performance across all benchmarks, demonstrating higher reliability as a predictor.",
    "Kendall\u2019s Tau is a statistical measure of rank correlation between two variables. The paper uses it to assess how well the rankings produced by FFI align with ground truth rankings of neural architectures.",
    "The authors conducted ablation studies by removing one feature type at a time (synaptic diversity, neuron activation strength, path expressivity) and observed performance drops. This showed that all three features contribute significantly to the effectiveness of FFI.",
    "FFI dramatically reduces computational cost because it avoids training architectures altogether. Its evaluation is orders of magnitude faster than training-based NAS, making large-scale architecture search feasible.",
    "The authors found that FFI maintains high correlation with true performance across diverse search spaces (NAS-Bench-101, NAS-Bench-201, DARTS), demonstrating its robustness and generalizability.",
    "The authors suggest exploring additional complementary features, adaptive fusion strategies that adjust weights dynamically, and extensions of FFI to large-scale real-world tasks to further enhance training-free NAS indicators.",
    "The motivation is to overcome the limitations of existing monocular vergence-based and binocular depth-based methods. Vergence-based methods struggle with low accuracy at long distances, while depth-based methods suffer from errors due to noise and hardware limitations. A hybrid approach combines both to achieve more robust and accurate gaze distance estimation.",
    "The two cues are vergence (eye rotation angles) and depth (from RGB-D camera or stereo imaging). Vergence provides reliable estimation at short distances, while depth cues perform better at longer distances. By cross-referencing the two, the hybrid method improves robustness across a wide range of viewing distances.",
    "The authors conducted experiments using both controlled indoor datasets with calibrated RGB-D cameras and real-world settings where subjects fixated on targets at varying distances. These setups enabled evaluation across short, medium, and long gaze distances.",
    "The mechanism compares the confidence levels of the vergence and depth estimates. When vergence accuracy is high (short distance), it is prioritized. When distance increases and vergence degrades, depth-based estimation is weighted more heavily.",
    "The hybrid method achieved lower average estimation error across all tested distances. Specifically, it maintained accuracy comparable to vergence at short distances and outperformed both vergence and depth baselines at medium and long distances.",
    "Calibration aligns the vergence-based eye-tracking system with the depth-sensing camera to ensure accurate fusion of cues. It was performed by asking participants to fixate on predefined calibration targets at known distances while recording both vergence and depth data.",
    "Applications include human-computer interaction, virtual and augmented reality systems, driver monitoring, and assistive technologies that rely on accurate gaze-based distance estimation.",
    "The authors note limitations such as reliance on depth sensors, which can fail under poor lighting or reflective surfaces, and the assumption of stable fixation, which may not hold in dynamic environments. Further improvements are needed for robustness in unconstrained real-world scenarios.",
    "The 'mid-range gap' arises because vergence is accurate only at short distances and depth sensors are reliable mainly at long distances. The hybrid method bridges this gap by adaptively combining the two, achieving accurate estimation in the mid-range where both cues individually perform poorly.",
    "Future work includes integrating learning-based fusion strategies, improving robustness of depth sensing under challenging conditions, and validating the system in more diverse real-world applications such as outdoor environments.",
    "The four attributes are availability, reliability, data integrity, and efficiency. Availability is measured as the ratio of accepted to total requests. Reliability is measured as the ratio of successful to accepted requests. Data integrity is measured as the ratio of successful requests minus failed requests to successful requests. Efficiency is measured as promised execution time divided by the sum of waiting and execution time.",
    "The model formulates a Mixed Integer Linear Programming (MILP) problem where the objective is to maximize overall trust of VM allocations while minimizing communication delay. Trust is computed from the weighted sum of availability, reliability, data integrity, and efficiency, while delay is computed as the ratio of total VM traffic to server capacity. Constraints ensure each VM is allocated once and within resource limits.",
    "The GA provides a heuristic solution to the NP-hard MILP resource allocation problem. It uses selection, crossover, and mutation to generate near-optimal VM allocation sets. Compared to the optimal solver (intlinprog in MATLAB), GA achieves 90% similarity in results but with significantly less execution time and better scalability for large numbers of CSPs and servers.",
    "Availability decreases as load increases. Under small loads, availability is maximum, while under heavy loads it is minimum. The experiments used small, medium, and heavy load scenarios, averaging 10 runs to compute availability values.",
    "Reliability degradation is caused by hardware and software failures. Hardware failures include failures of virtual machines and processing elements, while software failures arise from errors in request processing and workload execution. Reliability decreases with an increase in the number of requests and load.",
    "The Pareto front shows that maximizing trust by allocating resources only to the most trusted CSPs increases communication delay, while distributing resources reduces delay but lowers trust. Thus, achieving an optimal trade-off requires balancing both objectives.",
    "The CloudSim simulator was used because of its wide acceptance in the research community and its ability to simulate real cloud environments, including request arrivals, failures, and resource allocation scenarios. It enables validation of the trust evaluation and resource allocation models without relying on restricted real cloud provider configurations.",
    "When trust is given higher weight, trust values improve by around 10% but delays increase. Conversely, when delay is prioritized, delays reduce by about 10% but trust decreases. Equal weights achieve a balance, showing a clear trade-off depending on service requirements.",
    "Experiments used CloudSim for trust data collection and MATLAB for optimization. An Intel Core i7 machine with a 4 GHz processor was used. Genetic Algorithm experiments had a population size set based on requirements and generation size fixed at 1000. Scenarios varied CSPs from 6 to 12 and servers from 30 to 60.",
    "The authors propose developing a more reliable and secure model for trust management among different cloud service providers, extending beyond resource allocation to broader multi-cloud trust frameworks.",
    "The motivation is that 28Si single crystals enable determination of the Avogadro constant with unprecedented precision. Isotopically enriched 28Si reduces uncertainties due to isotope mass distribution, making it suitable for defining the kilogram based on fundamental constants.",
    "EPR is used to characterize point defects and impurities in 28Si single crystals. These defects, such as oxygen-vacancy complexes and dangling bonds, affect the accuracy of Avogadro constant determination and thus must be quantified.",
    "The study identified P donors, Pb centers (Si dangling bonds at the Si/SiO2 interface), and oxygen-vacancy centers such as the E\u2032 center as the main paramagnetic defects present in the crystal.",
    "Isotopic enrichment in 28Si improves EPR resolution by reducing hyperfine interactions caused by 29Si nuclei. This results in narrower linewidths and more precise defect characterization.",
    "The experiments used an X-band EPR spectrometer operating at ~9.4 GHz. Measurements were conducted at low temperatures using a helium-flow cryostat to enhance defect signal detection.",
    "Oxygen-vacancy defects alter the lattice parameter of silicon crystals. Since determination of the Avogadro constant depends on precise measurement of the silicon lattice parameter, quantifying such defects is essential to minimize systematic errors.",
    "The study detected defect concentrations on the order of 10^13 to 10^14 spins per cm\u00b3, confirming that 28Si crystals used for Avogadro experiments have extremely low impurity levels.",
    "Phosphorus donors act as shallow impurities in silicon, influencing conductivity and possibly EPR signal strength. Their concentration must be minimized for precision lattice parameter measurement but also provides useful calibration signals in EPR analysis.",
    "28Si single crystals provide higher EPR sensitivity and resolution due to the absence of 29Si nuclear spins, enabling clearer identification of defects and lower detection limits compared to natural silicon.",
    "The authors suggested further EPR investigations at varying temperatures and magnetic fields to better characterize weak defect signals, along with complementary spectroscopic techniques to fully quantify remaining impurities in 28Si crystals.",
    "The primary causes are (i) line edge roughness (LER), (ii) random dopant fluctuation (RDF), and (iii) work function variation (WFV). LER is considered most severe because it can affect RDF and WFV by inducing deformation of the device structure, thus degrading performance more significantly.",
    "The parameters are: (i) Amplitude (rms value of surface roughness), (ii) Correlation length X (3x) and (iii) Correlation length Y (3y). A larger correlation length indicates a smoother line. Additionally, a relation term between x and y directions (\u03b1 or 2) may be included.",
    "Two datasets were generated: (1) 130 datasets each with 50 FinFETs (6,500 total), split into 70% training and 30% validation, and (2) 10 datasets each with 250 FinFETs (2,500 total). LER parameter ranges were: amplitude 0.1\u20130.8 nm, correlation length X 10\u2013100 nm, correlation length Y 20\u2013200 nm.",
    "The earth-mover\u2019s distance (EMD) score was used. It measures the minimal amount of work needed to transform one probability distribution into another, calculated by comparing cumulative distribution functions (CDFs) estimated with Gaussian kernel density estimation. An EMD of 0 means identical distributions.",
    "The optimized ANN had 3 neurons in the input layer, 81 in the first hidden layer, 162 in the second, 324 in the third, and 324 in the output layer. This architecture with 11 mixture components minimized validation loss at 7,800 epochs, making it best suited to describe the distribution of performance metrics.",
    "Negative log likelihood (Negloglik) was used as the loss function because the ANN outputs probability distributions (PDFs). Conventional mean squared error cannot be applied since training is equivalent to maximum likelihood estimation of distributions, not single-point values.",
    "The mixture-MVN ANN successfully predicted skewness, kurtosis, and non-linear correlations that the Gaussian-only ANN could not. The EMD score improved significantly (0.0170 with Gaussian-only vs. 0.00928 with mixture-MVN).",
    "Training time was reduced from 1412 seconds for the non-separated ANN to 185 seconds for the separated ANN, achieving about a 6\u00d7 speedup without significant performance degradation.",
    "The proposed ANN predicts 7 metrics: Ioff, Idsat, Idlin, Idlo, Idhi, Vtsat, and Vtlin. The previous ML model predicted only 4 metrics: Ioff, Idsat, Vtsat, and SS.",
    "By accurately predicting non-Gaussian distributions of performance metrics, the ANN enables simulation of electrical behavior of transistors and DC behavior of digital circuit blocks such as SRAM bit cells, improving variation-aware design.",
    "Because the amplitude of LER does not scale down proportionally with device dimensions, its fraction of the physical channel length and width increases, leading to larger variations in IDS\u2013VGS characteristics.",
    "The parameters are RMS amplitude (\u03c3), correlation length (\u03beX, \u03beY), and roughness exponent (\u03b1). \u03c3 is the standard deviation of roughness amplitude, \u03be represents the wavelength of the roughness profile, and \u03b1 describes how high-frequency components diminish in the roughness profile.",
    "The dataset contained 169 sets, each consisting of 50 sample FinFETs with identical LER and device parameters. Eighteen FinFET structures were chosen, and LER parameters were randomly varied within specified ranges.",
    "Target variables: mean (\u00b5IDS) and standard deviation (\u03c3IDS) of log(IDS). Input features: gate voltage (VGS), RMS amplitude (\u03c3), correlation lengths (\u03beX, \u03beY), roughness exponent (\u03b1), gate length (Lg), fin width (Wfin), and fin height (Hfin).",
    "Horseshoe priors introduce shrinkage and sparsity over weights, promoting zeroing of unnecessary weights while allowing important large weights to remain due to heavy tails. This enables automatic model selection, finding compact layer sizes without manual tuning.",
    "HS-BNN improved prediction accuracy significantly. For \u00b5IDS, MAPE improved from 0.81% (BLR) to 0.55% (HS-BNN). For \u03c3IDS, MAPE improved from 19.59% (BLR) to 6.66% (HS-BNN).",
    "The HS-BNN maintained stable prediction performance (\u03c3IDS MAPE ~7%) even when the number of nodes exceeded 200, while Gaussian-BNN predictions varied significantly with node count. This shows HS-BNN\u2019s robustness to over-parameterization.",
    "K-fold cross-validation was used, ensuring all data were used as a test set at least once. This is suitable for limited datasets, producing less biased evaluations and better generalization estimates.",
    "For a FinFET with Lg=20 nm, Wfin=7 nm, Hfin=42 nm, and LER profile (\u03c3=0.5 nm, \u03beX=20 nm, \u03beY=50 nm, \u03b1=1), varying one parameter at a time showed well-matched trends in both mean and standard deviation of log(IOFF) and VTH, consistent with known device behavior.",
    "HS-BNN predicted LER-induced variations within a few seconds, whereas conventional TCAD simulations required weeks for a single LER profile.",
    "The RAFS achieved 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2 stability by using a rubidium spectral lamp with Xe starter gas, applying optical and isotope double-filtering to reduce noise, employing a large slotted tube microwave cavity with a 40 mm absorption cell to enhance the discrimination signal, and enclosing the physics package in a sealed box to mitigate barometric effects.",
    "The barometric effect was shown to cause a frequency shift coefficient of 7 \u00d7 10\u207b\u00b9\u2075/Pa, degrading 100-s stability to 2.2 \u00d7 10\u207b\u00b9\u2074. The solution was to place the physics package in a sealed box, reducing the barometric influence by nearly an order of magnitude.",
    "The slotted tube microwave cavity allowed flexible size design, enabling the use of a large 40 mm absorption cell to increase atomic signal SNR. It also provided a high field orientation factor (\u03be = 0.82), superior to traditional TE111/TE011 cavities, improving excitation of the clock transition.",
    "The optimal temperatures were 68 \u00b0C for the absorption cell, 93 \u00b0C for the filter cell, and 109 \u00b0C for the lamp bulb.",
    "By optimizing the discrimination slope Kd (18.0 nA/Hz) and measuring the shot noise current (211 \u00b5A, corresponding to noise spectral density of 8.2 pA/Hz\u00b9/\u00b2), \u03c3SNR(\u03c4) was estimated to be 4.7 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2.",
    "The interrogation microwave had phase noise of about \u2212110 dBc/Hz at 2fM (272 Hz). This limited stability to \u03c3PN(\u03c4) = 6.0 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2.",
    "The environmental factors examined were absorption cell temperature, pumping light intensity, 6.835-GHz microwave power, magnetic C-field, and barometric pressure. Among these, temperature shift and barometric pressure shift had the largest impact on stability for \u03c4 > 100 s.",
    "With the H-maser, the RAFS showed 9.0 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2 (1\u2013100 s). With the OMG, it showed 9.1 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2 (1\u2013100 s). Both results were consistent and matched the predicted 7.6 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2 stability.",
    "This result surpasses the previous best stability of 1.2 \u00d7 10\u207b\u00b9\u00b3\u03c4\u207b\u00b9/\u00b2 (recently achieved in pulsed laser-pumped RAFS), marking the first lamp-pumped RAFS to achieve 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2 stability, approaching hydrogen maser performance and enabling next-generation space clocks for satellite navigation.",
    "Predicted contributions were \u03c3SNR(\u03c4) = 4.7 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2, \u03c3PN(\u03c4) = 6.0 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2, and environmental effects controlled at 10\u207b\u00b9\u2075\u03c4\u207b\u00b9/\u00b2, leading to a predicted total stability of 7.6 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2.",
    "1) The average within-group Pearson correlation coefficient (PCC) must drastically increase in absolute value, 2) The average between-group PCC (stocks in the group vs. rest of system) must greatly decrease in absolute value, and 3) The average auto-covariance (AC) of stocks belonging to this group must increase in absolute value.",
    "I^LTM_t = (\u27e8|AC^LTM_t|\u27e9\u27e8|PCC^LTM_t|\u27e9) / \u27e8|PCC^\u00acLTM_t|\u27e9, where the first component (auto-covariance) relates to positive feedbacks in the market, and the second component (correlation ratio) reveals the presence of herding behaviors among investors.",
    "Stocks stay continuously in the leading module for about 1.5 months on average. There is a negative Pearson correlation of -0.19 between the LTM stability coefficient and the size of the DFA- group (stocks with significant DFA exponents not included in the leading module).",
    "The strategy compares the most recent I^LTM_t value against its empirical distribution computed over the previous 15 working days. Values larger than the 95th percentile trigger investment decisions. If the average return of LTM stocks is positive, a buy signal is generated; otherwise, a short position is taken.",
    "As \u03bb\u2081 approaches 1: (1) the absolute value of auto-covariance AC(z_i(t), z_i(t-1)) increases greatly if variable z_i is related to y\u2081; (2) |PCC(z_i(t), z_j(t))| approaches 1 if both variables are related to y\u2081; (3) |PCC(z_i(t), z_j(t))| approaches 0 if only one variable is related to y\u2081.",
    "The LTM strategy achieved: true positives 53%, false positives 47%, false negatives 49%, and true negatives 51%. The VaR strategy performed worse with: true positives 49%, false positives 51%, false negatives 52%, and true negatives 48%.",
    "The indicator showed increasing dynamics corresponding to major market events including: banking sector ratings downgrades of 2007, the failure of Lehman Brothers in September 2008, the American Recovery and Reinvestment Act of 2009, the European Debt crisis of 2011, and the Chinese stock market crisis of 2015-2016.",
    "Stocks with Hurst exponents outside the 0.2-0.8 interval show a distribution of correlations that is shifted to the right compared to other stock pairs, indicating that the DFA selects assets with highly correlated returns. This increases their probability of entering the LTM.",
    "The study draws an analogy where variations in asset prices are like nucleation phenomena near stability limits in thermodynamic systems (superheated liquid or supercooled gas). The LTM acts as the nucleus of the new phase for financial markets, and I^LTM_t plays a role similar to compressibility in thermodynamics - a macroscopic quantity indicating increasing instability near spinodal lines.",
    "Over the entire 2006-2017 period, the LTM strategy achieved a cumulative P&L of 114.44% (using MV=10, PRCTILE=95 parameters), significantly outperforming the Buy&Hold strategy which only achieved 13.42%. Even with transaction costs of 10 basis points, the strategy still generated about 5.5% per year.",
    "Tinbergen identified four dimensions: mechanism (causation), development (ontogeny), function (adaptive value), and evolution (phylogeny). For machines: mechanism explains how behavior is triggered and generated; development covers how machines acquire behaviors through engineering, training data, or experience; function describes how behavior fulfills purposes for human stakeholders; and evolution examines how behavioral patterns spread through copying, reverse-engineering, and market forces.",
    "Flash crashes represent clearly unintended consequences of interacting algorithms operating at unprecedented speeds that humans cannot match. These high-frequency trading algorithms can respond to events and each other faster than any human trader, potentially creating market inefficiencies and raising concerns about whether algorithms could interact to create larger market crises when faced with unforeseen scenarios not covered in their training data.",
    "The three motivations are: (1) the unprecedented ubiquity of algorithms in society with ever-increasing roles in daily activities; (2) the complex properties of algorithms and their environments making some attributes difficult or impossible to formalize analytically; and (3) the substantial challenge of predicting the positive or negative effects of intelligent algorithms on humanity due to their ubiquity and complexity.",
    "Traditional algorithm development focuses on maximizing performance against benchmarks using optimization metrics, while machine behavior study requires broader indicators similar to social science research. It needs randomized experiments, observational inference, and population-based statistics to understand how algorithms behave in different environments and affect societal outcomes, rather than just measuring accuracy or speed.",
    "Researchers may need to violate terms of service when reverse-engineering algorithms (e.g., creating fake personas), face potential legal challenges from platform creators if research damages reputations, and risk civil or criminal penalties under laws like the Computer Fraud and Abuse Act. Additionally, experimental interventions in real-world settings could adversely affect normal users, requiring careful ethical oversight.",
    "Machine evolution is much more flexible than animal evolution - while animals have simple inheritance (two parents, one transmission), machines can have instant global propagation through software updates, open-source sharing of code and training data, and human designers with specific objectives. However, machines also face unique constraints like software patents and regulatory privacy laws that don't apply to biological evolution.",
    "The three scales are: (1) Individual machine behavior - studying specific intelligent machines by themselves, focusing on intrinsic properties driven by source code or design; (2) Collective machine behavior - examining interactive and system-wide behaviors of collections of machine agents; and (3) Hybrid human-machine behavior - studying interactions between machines and humans in complex hybrid systems.",
    "The paper documents algorithmic bias in computer vision, word embeddings, advertising, policing, criminal justice, and social services. It notes that practitioners sometimes must make value trade-offs between competing notions of bias or between human versus machine biases, highlighting the complexity of addressing fairness in algorithmic systems.",
    "The paper explains that while the code for specifying AI model architecture and training can be simple, the results are often very complex 'black boxes' where the exact functional processes generating outputs are hard to interpret, even for the scientists who created the algorithms. This is compounded by proprietary source code and training data, where often only inputs and outputs are publicly observable.",
    "The study showed that simple algorithms injected into human gameplay can improve coordination outcomes among humans. Specifically, locally noisy autonomous agents improved global human coordination in network experiments, demonstrating that bots can enhance human collective behavior rather than just replace or compete with humans.",
    "The standard gravity model cannot be directly used to estimate weights of existing connection flights because in airline networks, unlike complete graphs such as international trade networks, most transport involves intermediate stops and no direct connections may exist for large distances, leading to observed flows that differ from expected flows.",
    "The observed passenger flow f_ij consists of two components: (1) f_ij^(g) - the number of passengers traveling directly from origin in country i to final destination in country j, given by the gravity equation, and (2) f_ij^(transit) - the number of passengers who use the connection i \u2192 j as part of their longer journey.",
    "The model correctly predicts more than 98% of the total passenger flow in the world.",
    "The authors use p(i \u2192 j \u2192 k) = C \u00b7 f(r_ij, r_jk), where C is a normalization constant and f(r_ij, r_jk) = 1/(r_ij \u00b7 r_jk), reflecting passengers' tendency to choose the shortest connections.",
    "The distance coefficient \u03b1 was determined to be 1.5 for 1996 and 1.6 for 2004.",
    "The paper identifies three major events: the September 2001 attacks in New York and Washington D.C. (which started a chain including SARS epidemic, additional terrorist attempts, wars, and rising oil prices), the 2008 global financial crisis, and the 2010 eruption of Eyjafjallaj\u00f6kull volcano in Iceland.",
    "The authors used data from the International Civil Aviation Organization (ICAO) containing annual traffic on international scheduled services for the years 1990-2011, along with econometric data from Penn World Table 8.1 and distance data from CEPII.",
    "In 2004, only 2308 connections (10%) out of 22650 possible connections were direct flights, while 12749 connections (56%) were shortest paths with length equal to 2.",
    "The missing globalization puzzle refers to the counter-intuitive finding that despite globalization conceptually reducing effective distance, most econometric studies show the distance coefficient increases over time, meaning distance becomes more important rather than less important in economic flows.",
    "The authors use a root mean square (RMS) formula \u0394(\u03b1) = \u221a(1/N_m \u00d7 \u03a3[P(f_ij) - P(f_ij^mcf)(\u03b1)]\u00b2) to measure agreement between normalized histograms of empirical and modelled flows across 15 logarithmically spaced bins, with the minimum \u0394(\u03b1) indicating the optimal distance coefficient.",
    "The researchers identified several challenges: Chinese does not have explicit word boundary markers and contains no whitespace between words; Chinese words are not clearly marked grammatically; Chinese contains a very large number of homophones in sentences; and Chinese consists of several thousand characters (Hanzi) with words consisting of one or more characters, making recognition, segmentation and analysis more complicated compared to English.",
    "The SLS_L model achieved an AUC of 0.9360, which outperformed both the Lasso-Logistics (L_L) model with AUC of 0.8344 and the MCP-Logistic (MCP_L) model with AUC of 0.8707. The SLS_L model demonstrated the highest prediction accuracy among all tested approaches.",
    "The optimal tuning parameters were \u03bb\u2081 = 0.0251 and \u03bb\u2082 = 0.001. \u03bb\u2081 performs variable selection and controls the level of sparsity, while \u03bb\u2082 controls the degree of coefficient smoothing, representing the similarity between coefficients. The non-zero value of \u03bb\u2082 indicates that network structure information was effectively utilized.",
    "The word 'Short Term' had the highest betweenness centrality at 287.89, followed by 'Pessimistic', 'Input Market', 'Larger' and 'Market'. Betweenness centrality measures the number of times a vertex acts as a link along the shortest path between two other nodes, with high betweenness indicating high probability to control information flow in the network.",
    "The network density was 0.1695, meaning there were 522 links in the 56 \u00d7 56 adjacency matrix. This indicates the textual network is a sparse network, which is significantly affected by the fact that the text network of research reports on stocks is loosely knit instead of densely connected.",
    "A total of 2082 reports were collected from 65 security companies. The number of words in a report was most frequently between 500 and 2000 words, with a mean of 3.8 security reports per day.",
    "The study found that the highest AUC occurred when predicting the next one week (five trading days), yielding up to 0.9360 for the SWS index. This finding aligns with Asquith et al.'s discovery that analyst reports can affect market reactions with a five trade days delay, indicating that time is needed for news to translate into trading activity.",
    "Out of 56 words, 31 were found to be effective predictors (55.4%). Effective keywords were defined as those whose coefficients did not shrink to zero in at least one of the three models (MCP_L, SLS_L, and L_L), while the remaining 25 words had coefficients that shrunk to zero in all three models.",
    "Only 5 out of 25 effective terms had a positive impact on financial markets, with most keywords receiving negative connotations. The strongest positive indicator was 'Imagine' and the strongest negative indicator was 'Concern'. Even the seemingly positive word 'Securitization' received a negative connotation, supporting the asymmetric response theory that negative information has greater impact than positive information.",
    "The process involved four steps: 1) Dictionary building using Sogou cell lexicon (63,320 words), 2) Text segmentation using Jieba package with Hidden Markov model, 3) Words cleaning removing stop words and low-frequency terms, and 4) Keyword vector selection using chi-square statistics. After filtering at the 5% significance level, 56 words remained from an initial set of 3285 keywords.",
    "O(L\u00b3), where L is the number of locations",
    "When \u03b2 > 1, the resulting pruned graph may be redundant by retaining sub-optimal routes. When 0 < \u03b2 < 1, the graph is lossy as not all shortest paths are retained. \u03b2 allows trading between quality (redundancy and path quality) and complexity (edge set size).",
    "Three of the four regions achieved F1-scores exceeding 0.9: Styria Austria (0.95 with \u03b2=0.95), Central African Republic (0.94 with \u03b2=0.95), and South Sudan (0.90 with \u03b2=0.95). The German-Austrian border region achieved a maximum F1-score of 0.75.",
    "The paper uses the Open Source Routing Machine (OSRM) from OpenStreetMap, which implements multilevel Dijkstra's (MLD) and contraction hierarchies (CH) algorithms for routing.",
    "If a route between two locations has a distance similar to the sum of distances between these locations and a common third location, then the considered route is likely indirect. The algorithm compares route distances to detect when d*1,2 + d*2,3 \u2248 d*1,3, indicating location l2 lies between l1 and l3.",
    "The fastest route via Autobahn A8 was 33 km/30 minutes, while the alternative route through Rosenheim was 27.1 km/33 minutes total. The algorithm always removes this route regardless of \u03b2 < 1 because it optimizes for fastest time rather than shortest distance, even though a direct, faster route exists.",
    "For Europe with 18,091 cities/towns, the routing took 5.01 hours and the pruning took 316.36 seconds using 128 cores.",
    "For complete location graphs, the proposed algorithm has O(L\u00b3) complexity compared to O(L\u2074 log L) for the optimal Iterative-Global strategy, providing better computational efficiency without loss of optimality.",
    "Ground truth was created by manually inspecting OpenStreetMap to determine if the fastest route between each location pair is direct. A connection was labeled direct if no other location lies on or nearby the fastest route, though this process involved subjective decisions in ambiguous cases.",
    "Location graphs are needed for route optimization, load optimization in electrical and transportation networks, and agent-based modeling applications including transportation of goods, evacuation models, traffic simulations, disease transmission, movement of people, and migration simulation.",
    "The five time scales are known as the 'R's' of resilience: recon, resist, respond, recover, and restore. These represent different phases from prior to an event to potentially days or weeks after a disturbance.",
    "The adaptive capacity is bounded by the real and reactive power component limits given in equations (4) and (5) and the temporal limits defined in equations (6) and (7). This results in a 'manifold' that represents the adaptive capacity of an asset.",
    "The primary resilience challenge is fuel availability for the diesel generators. Fuel gets delivered by boat on the Yukon River, which is impassable from August through April due to long and very cold winters, potentially creating life-threatening situations during winter due to diesel fuel depletion.",
    "The frequency response is defined by df/dt = f\u2080\u0394P/2H, where \u0394P is the disturbance or difference between generation and load, and H is the inertial constant. Large inertia slows the rate of frequency response, allowing additional time for generation units to ramp up or down before reaching frequency limits.",
    "By January 5, 2019, the Alaska Village Electric Cooperative (AVEC) had installed a 900 kW, 52-meter rotor diameter type IV pitch-controlled wind turbine generator manufactured by Emergya Wind Technologies and started producing power.",
    "When diesel generators are taken offline, there is a large negative impact on short-term resilience due to reduced inertia and generation ramping capability, but a positive effect on long-term resilience because the system burns less fuel to support the load, conserving fuel for extended operation.",
    "The platform treats GridLAB-D and Python federates as message federates, with data exchange configured using JSON config files by defining corresponding endpoints. The HELICS API facilitates communication through specific endpoints for monitoring and dispatching data.",
    "The generators are assumed to have an inertia constant of 2, have ramping capability of reaching full output in 10 seconds, and their rate of burning fuel for this study are constant.",
    "The short-term resilience is based on a frequency limit of 58Hz. It measures the maximum size of disturbance the system can withstand without dropping below this frequency limit before under frequency load shed (UFLS) occurs.",
    "When wind is run at maximum output, diesel generation can be taken offline, improving long-term resilience by conserving fuel. When run below maximum output, the fast ramping capability of inverters allows for increased short-term resilience of the system by providing additional generation capacity for frequency response.",
    "The researchers implemented a 'Google Trends strategy' that sells the DJIA at closing price p(t) if the relative change in search volume \u0394n(t-1, \u0394t) > 0, and buys back at price p(t+1). If \u0394n(t-1, \u0394t) < 0, they buy at p(t) and sell at p(t+1). The strategy uses relative change in search volume: \u0394n(t, \u0394t) = n(t) - N(t-1, \u0394t) where N(t-1, \u0394t) is the average search volume over the previous \u0394t weeks.",
    "The best-performing Google Trends strategy achieved a profit of 326% using the search term 'debt' with \u0394t = 3 weeks during the period from January 2004 to February 2011.",
    "The researchers quantified financial relevance by calculating the frequency of each search term in the online edition of the Financial Times from August 2004 to June 2011, normalized by the number of Google hits for each search term. They found a positive correlation between this financial relevance indicator and trading returns (Kendall's tau = 0.275, z = 4.01, N = 98, p < 0.001).",
    "Strategies based on U.S. search volume data performed better because investors prefer to trade on their domestic market. The researchers found that U.S.-only search data better captured the information gathering behavior of U.S. stock market participants than worldwide data, with mean returns of 0.60 vs 0.43 standard deviations above random strategies (t = 2.69, df = 97, p < 0.01).",
    "The 'buy and hold' strategy yielded 16% profit, equal to the overall increase in the DJIA from January 2004 to February 2011. The 'Dow Jones strategy', which used changes in stock prices instead of search volume data, yielded only 33% profit with \u0394t = 3 weeks, or 0.45 standard deviations above random strategies when averaged over \u0394t = 1 to 6 weeks.",
    "The researchers tested each component separately: a long-only strategy (buying after search volume decreases) achieved mean returns of 0.41 standard deviations above random (t = 11.42, p < 0.001), and a short-only strategy (selling after search volume increases) achieved 0.19 standard deviations above random (t = 5.28, p < 0.001). Both components significantly outperformed random strategies.",
    "The researchers used Herbert Simon's model of decision making, suggesting that Google Trends data and stock market data reflect two subsequent stages in investors' decision-making process. They proposed that trends to sell at lower prices are preceded by periods of concern, during which people gather more information about the market, reflected by increased Google search volumes for financially relevant terms.",
    "The researchers analyzed 98 search terms. They included terms related to the concept of stock markets, with some terms suggested by the Google Sets service, a tool which identifies semantically related keywords. The set was not arbitrarily chosen as they intentionally introduced some financial bias in their selection.",
    "The researchers averaged over three realizations of each search term's time series, based on three independent data requests made on consecutive weeks (April 10, 17, and 24, 2011). They noted that search volume data changes slightly over time due to Google's extraction procedure, so this averaging approach addressed the variability across different access dates.",
    "The Google Trends strategies significantly outperformed random investment strategies with a mean return of 0.60 standard deviations above the random strategy mean (t = 8.65, df = 97, p < 0.001, one sample t-test) for U.S. search data, and 0.43 standard deviations (t = 6.40, df = 97, p < 0.001) for global search data.",
    "2nd January 2007 until 31st December 2012",
    "891,171 different words",
    "0.074",
    "Bank of America (BAC) with a correlation of 0.43",
    "5 am London time",
    "9:30 am to 4 pm New York time (for most of the year, 2:30 pm to 9 pm London time)",
    "Travelers replaced Citigroup on 8th June 2009",
    "One day before the news (lag -1) and on the same day as the news (lag 0)",
    "0.040",
    "No significant correlation was found (median correlation coefficient = 0.000, p = 0.784)",
    "The width D of the stripe at time scale T is defined as D(T) = (L/T - 1)^(-1) * \u03a3|P_T(t_{k+1}) - P_T(t_k)|, which is the average of the absolute value of price increments at time scale T.",
    "The study analyzed 9 stocks from the London Stock Exchange during 2002 (251 trading days). The stocks were: AstraZeNeca (AZN), British Petroleum (BP), GlaxoSmithKline (GSK), Heritage Financial Group (HBOS), Royal Bank of Scotland Group (RBS), Rio Tinto (RIO), Royal Dutch Shell (SHEL), Unilever (ULVR), and Vodafone Group (VOD).",
    "The average Hurst exponent was 0.44, which is less than 0.5, indicating negative correlation and antipersistent behavior in price increments.",
    "The memory effect disappears at time scales larger than 180 seconds for resistances and 150 seconds for supports. The statistical significance (p-value < 0.05) was maintained up to 60-90 seconds for both supports and resistances.",
    "The time between consecutive bounces (t) followed a power law distribution, while the maximum distance (d) was best described by an exponentially truncated power law. For example, at 60 seconds time scale: N \u221d t^(-0.56) for time and N \u221d d^(-0.61) exp(-0.03 d) for distance.",
    "They modeled bounce events as a Bernoulli process and used Bayesian inference to estimate p(b|b_prev). The expected value is E[p(b|b_prev)] = (n_b_prev + 1)/(N + 2) and the variance is Var[p(b|b_prev)] = [(n_b_prev + 1)(N - n_b_prev + 1)]/[(N + 3)(N + 2)\u00b2].",
    "The researchers found no evidence of highly preferred prices in their histograms of local minima/maxima. When comparing histograms of support and resistance levels with overall price level histograms, they observed no anomalies or significant excess around round numbers across all stocks and time scales investigated.",
    "The conditional bounce probabilities for actual stock data were well above 0.5 and increased with the number of previous bounces, while shuffled series showed probabilities near 0.5 that remained nearly constant. This difference was at least one order of magnitude larger than any bias from finite stripe effects, providing evidence for memory effects in the original data.",
    "The researchers used two main statistical tests: (1) A chi-squared test with 3 degrees of freedom to test the independence hypothesis p(b|b_prev) = c, using a significance level \u03b1 = 0.05, and (2) A Kolmogorov-Smirnov test to assess whether bounce frequencies from reshuffled series were compatible with the posterior distribution found for bounce frequency.",
    "Technical analysis assumes that 'history repeats itself' because price trends reflect market psychology, and since the psychology of investors does not change over time, investors will always react in the same way when they encounter the same conditions. This leads to the belief that patterns that anticipated specific trends in the past will do the same in the future.",
    "The three components are economic cost C_F, social cost C_S, and environmental cost C_E.",
    "The environmental cost C_E is directly proportional to capacity c through the relationship C_E = \u03b3ct, where \u03b3 = abc and represents conversion constants linking capacity to pollutant, pollutant to impact, and impact to cost.",
    "The two conditions are strong sustainability (when demand is met with no substitution) and weak sustainability (when demand is met via substitution). In strong sustainability, the base activities alone fulfill the demand, while weak sustainability allows for replacement activities when base activities fail to meet constraints.",
    "In the weak case, the set of all sustainable activities forms a subset of an N-level union of sustainable activities and creates a topological cover.",
    "The three conditions are: (1) C'_i,j,... \u2264 C'_i,j,...^max for all cost components, (2) t'_i,j,... \u2264 t'_i,j,...^max for duration constraints, and (3) either strong sustainability where D = \u03a3c\u00b9_i (base activities meet demand) or weak sustainability where demand is met through substitution via a topological cover.",
    "The five requirements are: (1) satisfying human food and fiber needs, (2) enhancing environmental quality and natural resources, (3) making efficient use of non-renewable and on-farm resources, (4) integrating natural biological cycles and controls, and (5) sustaining economic viability while enhancing quality of life.",
    "The fundamental challenge is that social costs are difficult to quantify and assign monetary values to, making it problematic to connect the social dimension analytically to the other dimensions. The paper notes this is easier to handle at the local level where consensus is more achievable.",
    "The paper addresses higher-order dependencies by acknowledging that activities have cascading effects beyond their immediate scope, similar to Life Cycle Assessment. The bike trip example illustrates this: riding a bike requires eating an apple, which supports an apple farmer, who uses trucks for transport that emit pollution, creating indirect environmental costs.",
    "Zachary acknowledges that while economic values can be determined from markets and environmental costs from measurements and models, social costs will continue to be difficult to quantify. Many theoretical terms like 'balance', 'restrictions', 'maintaining', and 'controlling' are inherently difficult to quantify, and some approaches like Socio-Biological don't lend themselves easily to quantitative models.",
    "The eight classifications are based on combinations of: duration constraint satisfaction (t < t_max: yes/no), cost constraint satisfaction (C < C_max: yes/no), and activity complexity (single/multiple levels). This creates 2\u00b3 = 8 possibilities ranging from unsustainable single-level activities that meet neither constraint to sustainable/potentially sustainable multiple-level activities that meet both constraints.",
    "The proposed system achieves a binary pattern modulation speed of 97 kHz, which is about 5 times faster than the fastest DMD's maximum rate of 20.7 kHz.",
    "The final modulation frequency is given by \u03b4 = (Fg \u00d7 \u0394x)/(b \u00d7 \u03b4), where Fg is the scanning frequency of the galvanic mirrors, \u0394x is the scanning range of the beam on the DMD, b is the binning number of DMD mirrors in one direction, and \u03b4 is the size of each micro-mirror of the DMD.",
    "The authors achieved 42 Hz frame rate at 80 \u00d7 80-pixel resolution for dynamic scene imaging. They used a compressive sensing based algorithm incorporating both spatial and temporal prior constraints with the optimization function min(\u03bb\u03a8(Xt) + \u03a6(Xt - Xt-1)).",
    "Using high-end galvanic mirrors like the CTI 6200H with 1 kHz working frequency, the modulation speed could reach 485 kHz, which would enable 210 frames per second at 80 \u00d7 80-pixel resolution.",
    "As k increases, the imaging quality degrades because successive scanned sub patterns from a high resolution random pattern are not entirely independent from each other, which mathematically degenerates the reconstruction performance. However, even with large k values and noise, the results still restore decent images.",
    "The relationship is yi - yi-1 = \u03a3u,v Pi(u,v)\u0394X(u,v), where \u0394X(u,v) = X(u,v) - X(u-1,v). This shows that the difference between consecutive measurements can reconstruct the horizontal edges of the scene, reducing the required patterns by 50% compared to previous methods.",
    "The DMD is a Texas Instrument DLP Discovery 4100, 7XGA with 1024 \u00d7 768 pixel resolution, 13.6 \u03bcm micro-mirror size, and maximum 20 kHz projection rate. The galvanic mirrors are GVS011 from Thorlabs, single axis scanning devices operating at 200 Hz.",
    "The relationship is x = d sin(2\u03b8), where x is the distance from the beam's hitting position at GM1 to that at the DMD, d is the distance between the two galvanic mirrors (12 cm), and \u03b8 is the rotating angle. For small scanning ranges, this can be approximated as p = p0 - (2dkU)/\u03b40.",
    "Using acoustic optical deflectors (AOD) with 20 kHz scanning frequency could achieve 20 times faster illumination patterning, reaching 9.5 MHz modulation speed.",
    "The main limitations are: (1) the system needs careful mechanical mounting for calibration, though this can be addressed with customized programmable mounts, and (2) the scheme currently only works for random patterns and is inapplicable for other structured patterns like Hadamard and sinusoidal patterns.",
    "The method combines structured event extraction using syntactic parsing, Restricted Boltzmann Machines (RBMs) for pre-training, and sentence2vec framework to achieve effective event embeddings.",
    "The model uses an estimated true label sgn(Pi - P0) shared across all data sources in the hinge loss functions, where Pi is the probability for multi-source information on day i and P0 is a threshold parameter to determine prediction positiveness.",
    "The three levels are: super group level loss (log-likelihood), group level loss (temporal consistency between consecutive days), and instance level loss (hinge losses for each data source).",
    "News events contributed most to the overall prediction, followed by quantitative data in second place, while sentiments had the least impact among the three sources.",
    "M-MI improved F1-score by 6.9% in 2015 and 9.2% in 2016 compared to the nMIL baseline.",
    "The justification is based on the Efficient Market Hypothesis, which suggests that different data sources would keep up-to-date with the latest stock market information and commonly indicate the same sign (index rise or fall), allowing for consensus learning among correlated predictions.",
    "The three-level tree has the core verb as root node, subject and object of the verb as second layer nodes, and their nearest modifiers as child nodes. The core words (verb, subject, object, and their modifiers) are connected together as structure information to represent the event.",
    "F1-scores generally first increase then decrease as history days increase. This is explained by the quick decay of impacts from news, sentiments, and quantitative indices after 2-3 days, making out-of-date information less relevant.",
    "The LDA-S method (an extension of Latent Dirichlet Allocation) is used because it extracts topic-specific sentiments, recognizing that sentiment polarities depend on topics or domains, where the same word can express different sentiments in different contexts.",
    "Pi = \u03b80pm\u2212i + \u03b81pd\u2212i + \u03b82ps\u2212i, where \u03b80, \u03b81, and \u03b82 are source-specific weights for news, quantitative data, and sentiments respectively, with the constraint that \u03b80 + \u03b81 + \u03b82 = 1.",
    "Agricultural land consolidation, rural construction land consolidation, land reclamation, and land development",
    "27 administrative villages covering 28,090 hm\u00b2, representing 32.75% of the total area",
    "RS\u1d62 = \u2211\u2c7c\u2096\u2098 S\u1d62\u2c7cH\u1d62\u2096 X\u2c7c\u2096E\u2096\u2098, where j is the source of risk, k is the habitat type, m is the ecological receptor type, S\u1d62\u2c7c is density of risk sources, H\u1d62\u2096 is habitat abundance, X\u2c7c\u2096 is exposure coefficient, and E\u2096\u2098 is response coefficient",
    "Number of input nodes: 10, number of output nodes: 30, number of iterations: 1000",
    "Landscape pattern (0.01 to 1.62) and soil (0.01 to 1.46) show the highest ecological risk values",
    "D\u1d62\u2c7c = w\u209b \u00b7 D\u02e2\u1d62\u2c7c + w\u2090 \u00b7 \u221a\u2211\u1d30d=1 w\u2090 \u00b7 (a\u1d48\u1d62 - a\u1d48\u2c7c)\u00b2, where D\u1d62\u2c7c is mixed distance, D\u02e2\u1d62\u2c7c is geospatial distance, and w\u209b and w\u2090 are geospatial and attribute space weights respectively",
    "Ecological risk weight of land consolidation: 0.4, time urgency weight: 0.3, spatial suitability weight: 0.3",
    "Soil quality, meteorological conditions, water quality, and farmland conditions",
    "Area C exhibits the lowest ecological risk levels across all aspects, indicating potential advantages for agricultural development and superior water resources",
    "h\u1d62\u2c7c = e^(-D\u1d62\u2c7c/2\u03c3\u00b2), where h\u1d62\u2c7c is the proximity function, D\u1d62\u2c7c represents the distance between neurons, and \u03c3 is the diffusion parameter of the Gaussian function",
    "Graph theory extends to Leonhard Euler in the 18th century.",
    "Relational algebra grew out of a need to efficiently compress data during the 1960s, when storage was both limited and very expensive.",
    "Graphs are expressed in node-arc-node (subject-predicate-object) triples.",
    "The parallel-processer-based graphics processing unit (GPU) offers hardware alternatives to accelerate large-scale graph processing, and some firms, such as Cray, have developed specially configured supercomputers to digest and return rapid results from massively scaled graphs.",
    "When the preponderance of relationships becomes many-to-many, RDBMS performance takes a nosedive. Moreover, the RDBMS schema is typically inflexible, requiring high maintenance to effect the most minute change.",
    "The hierarchical model was created at IBM to represent tree-structured relationships, and the network model of the late 1960s was an early attempt to model objects and their relationships, which would re-emerge in the 1980s with object-oriented databases.",
    "There is little semantic commonality among the various graph languages in use and their rules of syntax. Higher volume graph databases rely on stylized variations of RDF to enumerate their triples, making sharing data between various graph databases dependent on the user's tolerance for expressing the same triples in differing syntactical frameworks.",
    "To alleviate storage consistency concerns, many graph databases do support the Atomic, Consistent, Isolated, and Durable (ACID) consistency model, which is a spin-off storage-locking scheme from RDBMS technology.",
    "Graphs can be intelligently reduced to more salient subgraphs that can be better managed, queried, and understood. This reinforces the practice of persisting data in a relational or appropriate nongraph NoSQL environment, from which subgraphs (database 'views') can be intelligently isolated for further analysis.",
    "Built-in mathematical functions, residing in graph databases, could add a level of depth to truly understanding and quantifying graph relationships. Metric algorithms derived from graph theory could be applied as analytical tools, extending beyond mere networks to networks of networks.",
    "Mars-like pressure was set at 8 mbar while Earth-standard pressure was set at 1000 mbar.",
    "The relative errors were 6.79% for sandy soil, 7.68% for intermediate soil, and 7.76% for bedrock soil.",
    "The maximum difference between surface and subsurface temperature increased by 26.72%, from 11.6\u00b0C at Earth's pressure to 14.7\u00b0C at Mars' pressure.",
    "I_sin = (\u0394G_s/\u0394T_s)\u221a(2\u03c0/P), where \u0394T_s = T_max - T_min, \u0394G_s = G_max - G_min, and P is the diurnal period.",
    "An anti-reflection coated germanium circular optic with 74.9 mm diameter and 5.0 mm thickness was chosen to comply with the minimum thickness required to avoid reaching germanium's fracture strength caused by the pressure differential.",
    "A total of 9,225 radiometric images were collected and saved as plain text 640 \u00d7 480 matrices with each cell containing the temperature in degrees Celsius.",
    "k = k_r + k_c + k_g, where k_r is transfer across pore spaces, k_c is conduction between grain contact areas, and k_g is conduction of gas filling pores between grains. Gas conduction (k_g) dominates at pressures between 0.1 and 1000 mbar.",
    "At Earth's pressure, the relative difference between highest and lowest thermal inertia soils was only 4.20%, but at Martian pressure, this difference increased significantly to 42.84%, indicating soils can be better assessed at Martian pressure.",
    "The PI-640i is a 320-g LWIR camera working in 8\u201314 \u03bcm spectral range, with 640 \u00d7 480 pixel resolution, 60\u00b0 \u00d7 45\u00b0 FOV, germanium optic, measuring temperatures from \u221220\u00b0C to 900\u00b0C with 0.04\u00b0C thermal sensitivity.",
    "G = \u2212I\u221a(\u03c0/P) \u2202T/\u2202Z'|_{Z'=0} = \u03b5\u03c3_B T^4_{heater} \u2212 \u03b5\u03c3_B T^4_{s_i}, where T_{heater} represents MEC heater temperature and T_{s_i} represents mean surface temperature of each soil sample.",
    "The two fundamental problems are: (1) Sign ambiguity - singular vectors with similar information are randomly distributed in two areas due to random variable pk \u2208 {1, -1}, and (2) Manifold features - singular vectors are unit vectors with norm 1 that exist as points on a unit hypersphere (manifold), making them inefficient to learn with generic neural networks based on Euclidean geometry.",
    "SVP achieves approximately 36% better performance than GAP under FGSM adversarial attacks on the CIFAR10 dataset with natural training.",
    "Sign ambiguity is removed by aligning singular vectors based on a center vector that is rotated to {0, 0, ..., 0, 1, 0}. The method verifies which hyper-sphere a singular vector belongs to according to the sign of \u0169HW-1,k and multiplies vectors on the negative half-sphere by -1 to align them.",
    "The arccosine function's first derivative can easily diverge as d/dz arccos(z) = -1/\u221a(1-z\u00b2) for -1 < z < 1. To prevent this instability during learning, the arccosine is approximated as -z + \u03c0/2 using first-order Taylor expansion.",
    "KL-divergence is used to learn the center vector uc. The method assumes that each component of the aligned \u016b features follows a Gaussian distribution, and the center vector is learned so that this assumption holds true by minimizing the KL-divergence between the actual distribution and a Gaussian distribution.",
    "SVP achieves a silhouette score of 0.692, which is 0.444, 0.510, and 0.551 better than GAP (0.248), GMP (0.182), and MPN (0.141) respectively. The SVP silhouette score is more than twice as high as other pooling methods.",
    "Rotation centers the singular vector distribution in the spherical coordinate system by rotating the center vector vc to {0,...,0,1,0} using Rodrigues rotation. This prevents singular vectors from being located near discontinuity boundaries where components are bounded ([0,\u03c0] for \u03c61 to \u03c6N-2 and [-\u03c0,\u03c0] for \u03c6N-1), ensuring the shortest path coincides with the real path during learning.",
    "KD-SVP achieves a performance improvement of 1.69% over the original KD-SVD method on the CIFAR100 dataset (improving from 71.64% to 73.33% accuracy).",
    "When the sign ambiguity removal step (step 2) is omitted, learning becomes impossible. The ablation study shows that this step is indispensable for proper learning of singular vectors, as indicated by the 'NaN' result in the experimental plot.",
    "SVP has higher computational complexity than general pooling methods due to the SVD computations required. However, it has lower forward time than MPN (matrix power normalized covariance pooling), which is a lightweight second-order pooling method. The burdensome SVD computations may make it difficult to apply directly to embedded systems or mobile environments.",
    "1 MHz A-scan rate",
    "SLD-371-HP3 superluminescent diode with center wavelength of 838 nm, full-width at half-maximum of 81 nm, and optical power of 27.2 mW",
    "8 vol/s for an image range of 250 \u00d7 250 \u00d7 2048 pixels (9 \u00d7 4.5 \u00d7 5 mm)",
    "Protective film: 100 \u03bcm, transparent film: 250 \u03bcm, deco film: 150 \u03bcm, base film: 100 \u03bcm",
    "139 dB for camera #1 and 137 dB for camera #2",
    "C++, CUDA, and Qt",
    "By using identical but reversed trigger sequences of 250 kHz with 50% duty cycle to each camera, capturing two continuous A-lines in one period, guaranteeing 500 kHz effective imaging rate which is twice the maximum sampling rate",
    "8 seconds for the whole-range scanning",
    "40 \u03bcm, 20 \u03bcm, and 40 \u03bcm gaps between the layers",
    "2048 pixels resolution (e2v OCTOPLUS, TELEDYNE e2v, U.K.)"
  ],
  "predicted_answers": [
    "The Gander project had two main components: an empirical component focused on problem conceptualization and a development component focused on creating a code review platform with eye-tracking.",
    "The three typical categories of participants are (1) employees or other stakeholders of software development organizations, (2) students or other beneficiaries of the university involved, and (3) independent participants. They are distinguished based on legal and ethical concerns regarding voluntary participation and data openness.",
    "The conceptual data pipeline consists of four steps: 1) Data cleaning, 2) Data exploration and visualization, 3) Model building and analysis, and 4) Findings presentation.\n\nIn a quantitative study, data cleaning involves transforming data for statistical tools, while in a qualitative study, it includes transcribing data and anonymization. Data exploration and visualization are steps for identifying outliers in quantitative studies and for initial understanding and coding in qualitative studies. Model building and analysis involves statistical modeling, coding, and theory building in qualitative studies. Finally, findings are presented in publications, reports, or supplementary material.",
    "The framework defines three steps of artifact generalization toward reuse: publication for reproduction, generalization for continued development, and generalization for general use.\n*   **Publication for reproduction** requires making artifacts available in their original, non-editable state.\n*   **Generalization for continued development** requires releasing artifacts with guidance on how to adapt them, such as through a community.\n*   **Generalization for general use** requires releasing artifacts in an editable state.",
    "The risks identified in the paper regarding sharing qualitative interview data from the Gander project include potential disclosure of sensitive company information, such as critical events or internal criticisms, and the difficulty of anonymizing individuals given the project's established collaboration with the companies involved.",
    "R1: Open research artifacts should be given persistent DOIs to enable traceability, independently of storage solution.\nR2: Research software should be made open source with an appropriate license, and research institutes and funding agencies should cover costs related to governing OSS communities for such software.\nR4: Quantitative data should be shared openly, if and only if, the data is anonymized sufficiently to protect the identity of the individual or company (if requested).",
    "The Gander project encountered a licensing issue when preparing its platform for open-source release, specifically regarding a gaze data analysis project that was initially shared without a license. After identifying this issue, a license file (the MIT license) was added to the project, resolving the licensing conflict.",
    "The Gander project developed an experimental code review platform incorporating eye-tracking to explore gaze-based assistance. A key aspect was to use gaze data to trigger visualizations of use-declaration relationships in code. The platform was used to develop a simple gaze assistant tested with eight participants.\n\nA proof-of-concept assistant was developed that triggered visualizations of use-declaration relationships in code based on gaze fixation point on variable names. The platform was connected to GitHub and incorporated open-source projects, leading to licensing considerations and the addition of a license to a dependency.",
    "Quantitative survey data are primarily quantitative or semiquantitative, making them easier to anonymize compared to qualitative data. The authors recommend that quantitative data be shared openly only if it is sufficiently anonymized to protect individual or company identities.",
    "The authors recommend a balance between openness and closeness, advocating for open access to artifacts like codebooks and study protocols while cautioning against open publication of qualitative data due to participant integrity concerns.",
    "To replace the heavy cost of training candidate networks with inexpensive indicators.",
    "The three types of features considered by the proposed feature fusion-based indicator (FFI) are CJ, NLR, CNNTK, and OS. Each captures a specific factor of the network.",
    "The feature fusion mechanism combines different indicators by sampling weights based on an analysis of their performance. This is necessary to improve performance, as combining indicators leads to enhanced results compared to using them individually.",
    "The benchmark datasets used were NAS-Bench-101, NAS-Bench-201, CIFAR-10, CIFAR-100, and ImageNet-16-120.",
    "The proposed FI outperforms standalone indicators in all performance assessment methods for all datasets on NAS-Bench-201.",
    "Kendall\u2019s Tau correlation and Spearman Rank-Order Correlation Coefficient (SROCC) were used to evaluate the indicator. Pearson Linear Correlation Coefficient (PLCC) and Root Mean Square Error (RMSE) were also used.",
    "What ablation studies were conducted to validate the effectiveness of the feature fusion strategy?\n\nThe document outlines several strategies to mitigate potential threats to research validity, including repeated experimentation with different random seeds and the use of benchmark datasets like NAS-Bench-101/201. However, it does not detail specific ablation studies conducted to validate the feature fusion strategy itself.",
    "The answer is not available in the provided context.",
    "The authors provided insights regarding the robustness of FFI across different search spaces by demonstrating its superior performance compared to other indicators on NAS-Bench-101. Specifically, the proposed FI achieved 5% higher KROCC and SROCC than the CJ-based single indicator.",
    "The authors suggest multiple indicators, each expressing different characteristics of a network, and a training approach to learn appropriate weights for each indicator within a fusion method.",
    "The main motivation for developing a hybrid gaze distance estimation method is to overcome the limitations of existing approaches, which are prone to errors due to human factors and environmental conditions.",
    "The two primary cues integrated in the proposed hybrid method are binocular vergence and gaze-mapped depth. They complement each other by providing independent estimates of gaze distance, which are then cross-referenced to derive a more precise final estimate.",
    "The experiments were conducted with three gaze targets at different distances (0.5m, 1.0m, and 1.3m) \u2013 near, middle, and far targets, respectively. Participants gazed at each target for several seconds to encourage natural eye blinking and relaxation. The experimental setup included a chin holder to prevent unintended gaze distance variations, a laptop with a reflective screen, and three challenging test scenarios (S1, S2, and S3) designed to induce human and environmental errors. Twelve participants were used under uniform conditions, with temporal consistency weights for pupil and depth set to 0.95.",
    "The proposed cross-referencing mechanism decides which cue (vergence or depth) to rely on by concurrently utilizing an eye camera and a depth camera, assessing the confidence of each method, and then computing an adaptive weighted average to derive a more precise gaze distance estimation.",
    "The proposed method significantly outperformed current methods with a visual angle error of 0.132 degrees under ideal conditions and consistently maintained robustness against human and environmental errors, achieving an error range of 0.14 to 0.21 degrees even in demanding environments.",
    "The answer is not available in the provided context.",
    "The paper suggests potential applications in:\n*   Developing user gaze distance estimation modules for varifocal AR/VR devices\n*   Digital healthcare\n*   Human-computer interaction\n*   Analyzing user behavior and preferences",
    "The authors acknowledge that qualitative data are tightly linked with the context, so changing context make them meaningless, and that issues related to confidentiality and anonymity prevent open data.",
    "The hybrid method addresses the 'mid-range gap' problem by concurrently estimating gaze distance using vergence and gaze-mapped depth, assessing the confidence of each method, and calculating an adaptive weighted average.",
    "The authors suggest applying the hybrid method to developing user gaze distance estimation modules for varifocal AR/VR devices and exploring potential applications in digital healthcare, human-computer interaction, and analyzing user behavior and preferences.",
    "The four quality attributes used in the proposed trust evaluation model are availability, reliability, data integrity, and efficiency.",
    "The proposed model jointly optimizes trust and communication delay by maximizing trust and minimizing delay within a cloud service provider (CSP) environment.",
    "The Genetic Algorithm (GA) is proposed as a heuristic for near-optimal resource allocation solutions due to the problem\u2019s complexity.\nThe performance of the proposed approach is compared with the exact optimized solution and shows that the GA achieves almost the same objective values as the optimizer.",
    "Short-term resilience is significantly reduced when a generator is taken off-line, while the long-term resilience is reduced at a slower rate due to continued online generation.",
    "The main causes of reliability degradation in the proposed model\u2019s experiments include:\n\n*   Rapid elasticity, leading to service failure if not adaptable.\n*   Measured service issues due to resource loss or unavailability.\n*   Resource contention, fragmentation, over-provisioning, and under-provisioning.\n*   The complexity of managing multiple users and their resource demands.",
    "The paper proposes a model to consider trust and delay in cloud resource allocation, incorporating CSP credentials and present resource constraints.",
    "HELICS, an open-source co-simulation platform, was used. It coordinates off-the-shelf simulators and applications, including GridLAB-D and Python federates.",
    "How does the weighting of trust versus delay impact the model\u2019s outcomes?\n\nThe model aims to maximize trust while minimizing communication delay through a joint optimization, considering CSP credentials and present resource constraints. The model integrates trust, estimated through availability, reliability, data integrity, and efficiency, into a resource allocation framework.",
    "The proposed models were executed in four distinct scenarios, varying the number of servers and CSPs.",
    "The authors propose the need for artifact evaluation and open research practices within computer science.",
    "Quantitative EPR measurements on 28 Si single crystal AVO28 were conducted to increase the reliability of the mass deficit correction in the XRCD method. The measurements determined the concentration of phosphorus impurity under illumination and identified anisotropic vacancy defects, leading to an estimated mass deficit correction of 0.0(2) \u03b8g for 1-kg AVO28 spheres.",
    "EPR measurements were conducted at four different angles (\u03b8 = 0\u00b0, 30\u00b0, 60\u00b0, and 90\u00b0) to collect spectra under halogen lamp illumination.",
    "The nine types of vacancy defects with unpaired electrons were found to be less than 1 \u00d7 10 12 cm -3 .",
    "EPR measurements on 28Si revealed concentrations of phosphorus impurity and defects on mechanically damaged surfaces below 1 \u00d7 10^12 cm^-3 at 25 K.",
    "EPR spectra were recorded at four different directions \u03b8 = 0\u00b0, 30\u00b0, 60\u00b0, and 90\u00b0 by rotating the sample about the [011] direction.",
    "Quantitative EPR measurements on a silicon crystal are being performed to increase the reliability of mass deficit correction in the XRCD method. The concentration of phosphorus impurity in the crystal is 3.2(5) \u00d7 10^12 cm^-3, and the concentrations of nine types of vacancy defects with unpaired electrons are less than 1 \u00d7 10^12 cm^-3 at 25 K.",
    "The concentration of phosphorus impurity obtained in this paper, 3.2(5) \u00d7 10^12 cm^-3, agrees with that estimated using Fourier transform infrared spectroscopy for 28 Si crystal Si28-23Pr11.",
    "The sample cut out from 28 Si single crystal AVO28 was measured at 25 K using EPR spectroscopy. Under illumination, the number and concentration of EPR-active phosphorus impurity (Si4 \u2261 _ _ P\u00a5) in the sample were determined to be 0.21(3) \u00d7 10 12 and 3.2(5) \u00d7 10 12 cm -3 , respectively.",
    "The advantages of 28Si single crystals offered compared to natural silicon for EPR studies include the detection of phosphorus impurity and the observation of nine types of vacancy defects with low concentrations (less than 1 \u00d7 10 12 cm -3 ).",
    "The authors propose a method to enhance the reliability of the Avogadro constant determination by measuring the mass deficit in silicon crystals caused by defects.",
    "The primary causes of process-induced random variation are line edge roughness (LER), random dopant fluctuation (RDF), and work function variation (WFV). Line edge roughness is considered the most severe because it can induce deformation of the device structure, thereby degrading performance more severely than the other variations.",
    "The three parameters used to characterize line-edge roughness (LER) are amplitude (1), correlation length (3x and 3y), and roughness exponent (3).",
    "The datasets for training and validation consist of 130 datasets, each containing performance metrics of 50 FinFETs, and 10 datasets each containing performance metrics of 250 FinFETs.",
    "The evaluation method used earth-mover's distance (EMD) score to compare ANN predictions against TCAD results. The EMD score measures the difference between two probability distributions, calculated by comparing the cumulative distribution functions (CDFs) of the datasets.",
    "The optimized ANN with the mixture of MVNs has 3 neurons for the input layer, 81 neurons for the first hidden layer, 162 neurons for the second hidden layer, 324 for the third hidden layer, and 324 neurons for the output layer.",
    "Negative log likelihood (Negloglik) was used as the loss function.",
    "The mixture-MVN ANN successfully predicts skewness, kurtosis, and non-linear correlation, which is distinct from plain MVN.",
    "The time spent to train the ANN model was reduced from 1412 sec to 185 sec.",
    "The proposed ANN models shortened the simulation time by 6 times compared to the previous ML-based model.",
    "The ANN-based approach shortens simulation time by six times compared to the previous ML-based model, enabling the simulation of electrical behavior and DC characteristics of digital circuit blocks like SRAM bit cells. It improves prediction accuracy by successfully capturing non-Gaussian features of device performance metrics, which the previous model could not.",
    "Line-edge roughness (LER) becomes a more significant source of variation as FinFET dimensions scale down because the amplitude of LER does not shrink as much as the feature size shrinkage.",
    "The three main parameters used to generate 3D LER profiles in the simulations are:\n1.  RMS amplitude (GLYPH<27>)\n2.  x(y)-axis correlation length (GLYPH<24> X and GLYPH<24> Y)\n3.  Roughness exponent (GLYPH<11>)",
    "The dataset for training and testing the HS-BNN comprised 169 different datasets, each containing 50 FinFETs with identical LER and device parameters. Eighteen types of FinFET device structure were chosen, and the LER parameters were randomly selected within specified ranges.",
    "The HS-BNN predictions were evaluated using K-fold crossvalidation, and metrics like MAPE, RMSE, and PLL were used. The target variables predicted were GLYPH<22> IDS and GLYPH<27> IDS.",
    "The HS-BNN uses horseshoe priors to enable automatic model selection, particularly for compact layer sizes when data are limited.",
    "The HS-BNN predictions were evaluated using K-fold crossvalidation, demonstrating improved prediction accuracy compared to the previous Bayesian linear regression (BLR) model. Specifically, the prediction for GLYPH<27> IDS was improved by 6.66% versus 19.59% for the previous work.",
    "The HS-BNN's prediction for GLYPH<27> IDS showed almost the same results (MAPEs 7%) as the Gaussian-BNN when the number of nodes exceeded 200, while the Gaussian-BNN showed different results for varying numbers of nodes.",
    "K-fold crossvalidation was used for evaluating HS-BNN performance, specifically a 5-fold crossvalidation procedure repeated 10 times to obtain an average performance metric.",
    "The HS-BNN predictions for GLYPH<27> IDS and GLYPH<22> IDS were improved compared to the previous Bayesian linear regression (BLR) model, with GLYPH<27> IDS showing a 6.66% improvement and GLYPH<22> IDS showing a 0.55% improvement.",
    "The HS-BNN predictions were evaluated using K-fold crossvalidation and metrics like MAPE, RMSE, and PLL, demonstrating improved prediction accuracy compared to previous Bayesian linear regression (BLR) models, particularly for GLYPH<27> IDS.",
    "The innovations in the physics package design included using a large slotted tube microwave cavity, a rubidium absorption cell with a diameter of 40 mm, optical and isotope double-filtering technique, and a low phase noise microwave synthesizer.",
    "A sealed box was designed to isolate the PP from the barometric environment, reducing the barometric effect by nearly one order of magnitude.",
    "The slotted tube cavity\u2019s size could be flexibly designed, allowing for a larger absorption cell to enhance the atomic signal SNR.",
    "The experimentally optimized operating temperatures for the absorption cell, filter cell, and lamp bulb are 68 \u00b0C, 93 \u00b0C, and 109 \u00b0C, respectively.",
    "The stability of the RAFS was predicted to be 7.6 \u00d7 10 -14 \u03c4 -1 / 2 based on the SNR limited stability of 4.7 \u00d7 10 -14 \u03c4 -1 / 2 and the phase noise limited stability of 6.0 \u00d7 10 -14 \u03c4 -1 / 2.",
    "The phase noise of the 6.835-GHz microwave was measured at 2 fM, with a value of -110 dBc/Hz.",
    "The environmental factors besides barometric pressure quantified were absorption cell temperature, light intensity, microwave power, and magnetic field. The largest impact on stability was from the barometric effect.",
    "The H-maser and OMG references showed different stability results. The RAFS stability with the H-maser reference was 9.0 \u00d7 10\u207b\u00b9\u2074 \u03c4\u207b\u00b9/\u2082 with \u03c4 up to 100 s, decreasing to 9.0 \u00d7 10\u207b\u00b9\u2074 \u03c4\u207b\u00b9/\u2082 with \u03c4 up to 100 s. The RAFS stability with the OMG reference was 9.7 \u00d7 10\u207b\u00b9\u2074 \u03c4\u207b\u00b9/\u2082 for \u03c4 \u2264 100 s, and 9.1 \u00d7 10\u207b\u00b9\u2074 \u03c4\u207b\u00b9/\u2082 up to 100 s.",
    "Achieving 10\u207b\u00b9\u2074\u03c4\u207b\u00b9\u00b2/\u00b2 stability represents a significant advancement compared to the previous best result of 1.2 \u00d7 10\u207b\u00b9\u00b3\u03c4\u207b\u00b9\u00b2/\u00b2, which was realized on a pulsed laser-pumped RAFS.",
    "The predicted contributions to RAFS stability are:\n*   SNR limited stability (\u03c3 SNR)\n*   Phase noise limited stability (\u03c3 PN)\n*   Environmental effect limited stability (\u03c3 EE)",
    "The LTM indicator relies on three statistical conditions for a group of stocks to be classified as the Leading Temporal Module (LTM). These conditions are:\n1.  A group of stocks displays an average within PCC that drastically increases in absolute value.\n2.  The average between PCC of stocks in this group and other stocks in the rest of the system will greatly decrease in absolute value.\n3.  The average AC of stocks belonging to this group increases in absolute value.",
    "The synthetic indicator I LTM t is a dynamic indicator whose members may vary in time. Changes in the LTM composition are important to identify the drivers of the upcoming period of instability.",
    "What is the DFA method, and what does it measure?\nThe DFA method involves shifting and integrating stock returns, then segmenting the data into windows. A log-log graph of the window size against the average squared residual is created to assess the presence of self-similarity and long-term memory. The slope of this graph indicates the scaling exponent (\u03b1), which describes the signal's memory characteristics.",
    "What is the methodology used to test the LTM indicator's predictive performance, and what percentile threshold is employed?",
    "What happens to the variance, covariance, and auto-covariance of the original system variables as the dominant eigenvalue \u03bb\u2081 approaches 1?\n\nThe variance, covariance, and auto-covariance of the original variables increase as the dominant eigenvalue \u03bb\u2081 approaches 1.",
    "What are the true positive, false positive, false negative, and true negative rates achieved by the LTM-based investment strategy, and how do they compare to the VaR-based strategy?",
    "The LTM indicator successfully anticipated market events and crises during the 2006-2017 study period, including the 2008 global financial crisis, the Lehman Brothers failure, the American Recovery and Reinvestment Act of 2009, the European Debt crisis of 2011, and the Chinese stock market crisis of 2015-2016.\n\nThe indicator\u2019s dynamics mimicked market behavioral attitudes, such as positive feedbacks and herding behaviors, and it increased around periods of market instability.\n\nChanges in the LTM composition, particularly the stability of its membership, were identified as important drivers of market instability and potential triggers for investment strategy shifts.",
    "The study found no evidence of a relationship between the daily number of mentions of a company\u2019s name and the daily return of the company\u2019s stocks when direction of movement is considered.",
    "How does the study's approach relate to thermodynamic systems, and what role does the I^LTM_t indicator play in this analogy?",
    "The LTM strategy achieved an annual profit and loss performance of 7.07 in 2008 and 30.87 in 2009, compared to a Buy&Hold strategy. The LTM strategy\u2019s performance was significantly less reactive during market downturns, such as the 2008 financial crisis.",
    "The four complementary dimensions identified by Tinbergen for analyzing animal behavior are:\n1.  Function\n2.  Mechanism\n3.  Development\n4.  Evolutionary history\n\nThese dimensions can be adapted for studying machine behavior, focusing on how machines fulfill functions, their underlying mechanisms, how they develop, and their evolutionary histories.",
    "Flash crashes exemplify unintended collective machine behavior in financial markets by illustrating how algorithmic trading systems, responding to each other and market events, can create larger market crises than any single trader might initiate.",
    "The three primary motivations are algorithm prevalence, analytical limitations, and the difficulty of predicting the effects of intelligent algorithms.",
    "The complexities of algorithms and their environments make formalizing their attributes and behaviors difficult.",
    "Researchers face legal and ethical barriers when studying machine behavior, particularly concerning data collection methods. Interview and focus group data present challenges due to the rich, contextual nature of qualitative data, making anonymization difficult. Survey data, primarily quantitative, offer better opportunities for anonymization. Observational data, especially when technology-based, require careful consideration of privacy and consent. Archival data, derived for other purposes, necessitate evaluation in relation to original contributors.",
    "Machine behavior evolves differently from animal behavior. Algorithms are more flexible and have a designer with an objective in the background, unlike animals which have a simple inheritance system. The human environment strongly influences how algorithms evolve, and machine behavior may spread due to replication and institutional factors like software patents and regulatory constraints.",
    "The three scales of inquiry for studying machine behavior are: individual machines, collectives of machines, and hybrid human-machine systems.",
    "The paper provides examples of how algorithms can exhibit bias across different domains, including:\n\n*   Financial trading environments, where algorithms can produce inefficiencies and potentially contribute to market crises.\n*   Human-machine interactions in areas like social interaction and decision-making, such as how human biases combine with AI to alter human emotions or beliefs.\n*   Robotic and software-driven automation of human labor, examining scenarios where machines enhance or replace human work.",
    "The paper describes the 'black box' problem as the difficulty in interpreting the exact functional processes that generate outputs of AI systems, even for the scientists who create them.",
    "The context does not explicitly state a key finding about coordination from the hybrid human-machine study. However, it does indicate that interactions with simple bots can increase human coordination.",
    "The standard gravity model is inadequate for accurately describing airline networks due to the inclusion of transfer flights and intermediate stops.",
    "The authors define the two components of passenger flow, f_ij, as direct passengers (f_ij_g) traveling directly from origin i to destination j, and transit passengers (f_ij_transit) utilizing the connection as part of a longer journey.",
    "What percentage of total passenger flow in the world does the authors\u2019 model correctly predict?\n\nThe authors\u2019 model correctly predicts more than 98% of the total passenger flow in the world.",
    "The formula for the probability function p(i \u2192 j \u2192 k) is given by:\n\np(i \u2192 j \u2192 k) = C * f(r<sub>ij</sub>, r<sub>jk</sub>) / (xi * xj)\n\nwhere C is a normalization constant, f(r<sub>ij</sub>, r<sub>jk</sub>) is a function reflecting the tendency of passengers to choose the shortest connections, and xi and xj are the real GDPs of countries i and j, respectively.",
    "The authors determined \u03b1 = 1.5 for 1996 and \u03b1 = 1.6 for 2004.",
    "The paper identifies three historical events that influenced the distance coefficient in air transportation:\n\n1.  Attacks in New York and Washington D.C. in September 2001.\n2.  The SARS epidemic.\n3.  Additional terrorist attempts, wars, and rising oil prices.",
    "The answer is not available in the provided context.",
    "What percentage of possible connections between countries were direct flights versus shortest paths with length 2 in 2004?\nThe context does not provide the answer.",
    "The standard gravity model is inadequate for describing airline passenger flows due to the inclusion of indirect flights and transfer passengers.",
    "The authors use earth-mover's distance (EMD) to quantitatively assess the difference between datasets, specifically comparing the cumulative distribution functions of the data obtained from the new ANN model and the actual passenger flow data. The EMD score measures the minimal amount of work required to transform one distribution into another, with a score of zero indicating identical distributions.",
    "Chinese stock market analysis is more complex than English-based analysis due to several linguistic challenges, including the large number of characters (Hanzi), the lack of explicit word boundaries, and a high number of homophones.",
    "The HS-BNN model was evaluated using K-fold crossvalidation and metrics including MAPE, RMSE, and PLL.",
    "The study proposes newly developed ANN models with shortened simulation times and improved accuracy in predicting transistor behavior.",
    "What keywords were identified as having the highest betweenness centrality in the textual network?\nThe keywords \"Hotspot( \u70ed\u70b9 )\" and \"Short Term( \u77ed\u671f )\" are central points of information flow, followed by \"Pessimistic ( \u60b2\u89c2 )\" and \"Input Market ( \u6295\u5165\u5e02\u573a )\".\n\nWhat does betweenness centrality measure in this context?\nIt measures the number of times a node (keyword) acts as a link along the shortest path between two other nodes, indicating its influence on information flow within the network.",
    "The study constructs a textual network based on co-occurrence of keywords within text windows. A link between nodes indicates statistically stronger co-occurrence, capturing meaningful information. The network density is 0.1695, indicating a sparse network with 522 links in a 56x56 adjacency matrix.",
    "The research collected data from 65 security companies, resulting in 2082 research reports. The typical word count range for these reports was between 500 and 2000 words.",
    "The study investigates stock market prediction using a textual network, focusing on analysts' research reports. It identifies a delay of at least one month for investor reactions to online information, including stock market reactions.\n\nPrevious research found that investor sentiment, explored through online social media, can affect stock prices with a delay of one month. Early research indicated a positive relationship between downgrade reports and stock market reactions, and that reports could be a stable and trustworthy source of information.\n\nThe study reveals a long-term memory effect in stock prices, where the strength of this effect decreases as the time scale increases. The Hurst exponent, indicating the degree of long-range dependence, is consistently less than 0.5, suggesting an antipersistent behavior and a correlation between price increments.",
    "The proportion of identified keywords that were effective predictors was 56%, and effective keywords were defined as those that demonstrated a statistically stronger co-occurrence within a two-keyword window.",
    "The striking finding is that only five out of 25 terms have a positive impact on financial markets, and \u201cImagine\u201d is the strongest positive indicator, while \u201cConcern\u201d is the strongest negative indicator.",
    "What specific steps were used in the keyword vector extraction process?\nThe process involves building a dictionary using the Sogou Pinyin input method, text segmentation using Jieba, and cleaning the text by removing stop words based on a frequency threshold (80%).\n\nWhat significance level was used for the final chi-square filtering?\nThe chi-square filtering used a 5% significance level.",
    "The computational complexity of the second step (pruning indirect routes) is O(L^3).",
    "To validate route pruning, the algorithm relies on the parameter \u03b2.\nThe accuracy of the pruning algorithm is measured using Precision, Recall, and F1-score.\nA TP is a route that is part of the ground truth and also detected by the pruning algorithm.\nThe fully connected graph was pruned with Algorithm 1, using several values of the pruning parameter \u03b2.",
    "In three of the eight direct routes that were not detected by the algorithm, the location Mbile in the southwestern part of the region is involved, which is only 11 km away from the location Lolo. For instance, the route [Baboua - Mbile] is direct with a distance of 299 km. Adding up the distances of the routes [Baboua - Lolo] with 295 km and [Lolo - Mbile] with 11 km results in a total distance of 306 km, which is less than 1509 km, so the connection is pruned by the algorithm.",
    "The paper uses state-of-the-art batched shortest path algorithms, such as MLD or CH, with a time complexity of O (( | EG | + LG log LG ) L ) to compute pairwise distances between locations.",
    "The algorithm uses the triangle inequality to identify indirect routes. Specifically, it compares the distance between two locations to the sum of the distances between those locations and a common third location. If the route distance is similar to the sum of the distances, the algorithm considers the route to be indirect and prunes it.",
    "What specific problem occurred with the route between Kolbermoor and Prien am Chiemsee in the German-Austrian border region?\n\nThe route between Kolbermoor and Prien am Chiemsee was pruned from the fully connected graph by the route pruning algorithm, even though it is a direct route. Specifically, the route [Frohnleiten - Knittelfeld], which totals 72 km, and the route [Frohnleiten - Bruck an der Mur - Knittelfeld] which totals 71 km, were removed.",
    "The study constructed location graphs for four regions, including two in Europe and two in Africa, and compared them to manually created ground truths. In three of the four regions, the algorithm achieved an F1-score exceeding 0.9.",
    "The Static-Triangle strategy has a time complexity of O(L\u00b7R), while the Iterative-Global strategy has a time complexity of O(R(R+L)logL).",
    "The ground truth was created by inspecting if the fastest route (shortest time) between each location pair is direct, based on OSM data. A connection is labelled direct if there is no other location on or nearby the fastest route.",
    "The paper mentions several applications that require location graphs, including:\n*   route optimisation\n*   load optimisation in electrical and transportation networks\n*   agent-based modelling applications such as transportation of goods, evacuation models, traffic simulations, disease transmission, movement of people, and migration simulation.",
    "The DIRE curve has five time scales of resilience: recon, resist, respond, recover, and restore.",
    "The adaptive capacity of a generation asset is bounded by its operational generation limits, given by the real and reactive power components and temporal limits defined by latency and ramp rates.",
    "The primary resilience challenge for the St. Mary\u2019s microgrid is fuel availability for the diesel generators.",
    "The mathematical relationship defines how system frequency responds to power disturbances, and inertia plays a role by slowing the rate of frequency response to disturbances. Inertia is quantified by kinetic energy, which is dependent on the generator\u2019s mass moment of inertia, frequency, and the difference between generation and load.",
    "The 900 kW, 52-meter rotor diameter type IV pitch-controlled wind turbine was installed by the Alaska Village Electric Cooperative (AVEC) on January 5, 2019, and it started producing power at that time.",
    "The study observes a trade-off between short-term and long-term resilience. Specifically, when inertia-based generation assets are taken offline, short-term resilience drops, but long-term resilience is retained due to reduced fuel consumption.",
    "HELICS coordinates GridLAB-D and Python federates, performing time management and synchronization, and facilitating data exchanges between them.",
    "The diesel generators are assumed to have an inertia constant of 2, a ramping capability of reaching full output in 10 seconds, and their rate of burning fuel is constant.",
    "The frequency limit used to define the short-term resilience metric is not explicitly stated in the provided context.",
    "When a generator is taken off-line, it has a large negative impact on short-term resilience due to reduced inertia and ramping capability, but a positive effect on long-term resilience by conserving fuel.",
    "The researchers implemented a strategy in which they took long positions following a decrease in search volume and never took short positions, and another strategy in which they took short positions following an increase in search volume, never taking long positions. Buy/sell decisions were based on changes in Google Trends search volume data.",
    "The best-performing Google Trends strategy yielded a cumulative return of 0.60 standard deviations from the mean return of uncorrelated random investment strategies, based on U.S. search volume data. The search term used was \u201cdebt\u201d.",
    "Researchers quantified financial relevance by calculating the frequency of each search term in the online edition of the Financial Times, normalized by the number of Google hits for each term. They found a positive correlation between search term frequency and trading performance.",
    "Strategies based on U.S. search volume data performed better than those using global search volume data for the U.S. market. This is likely due to the higher proportion of traders on the U.S. market among U.S. Internet users.",
    "The 'Google Trends strategy' yielded significantly higher returns than the 'random investment strategy' (60% vs. 0.60 standard deviations). The 'Dow Jones strategy' achieved only 33% profit, equivalent to 0.45 standard deviations.",
    "The paper investigates whether technical trading strategies produce detectable signals in stock price dynamics and explores the presence of memory effects. It focuses on identifying potential support and resistance levels and measures the conditional probability of bounces on these levels, finding that the probability of a bounce increases with the number of previous bounces, suggesting a self-reinforcing feedback loop among investors' beliefs.",
    "Herbert Simon\u2019s model of decision making.",
    "How many search terms were analyzed?\nNinety-eight search terms were analyzed.\n\nWhat criteria were used for selecting the search terms?\nThe search terms were selected intentionally, with some financial bias, and included terms related to stock markets, incorporating terms suggested by the Google Sets service, and the best-performing terms in the analyses.",
    "The researchers used two shades of blue for positive returns and two shades of red for negative returns to improve the readability of the search terms.",
    "Returns from the Google Trends strategies tested were significantly higher overall than returns from the random strategies (R.US = 0.60; t = 8.65, df = 97, p < 0.001, one sample t-test).",
    "The study analyzed a corpus of daily issues of the Financial Times from 2nd January 2007 to 31st December 2012.",
    "How many different words occurred throughout the Financial Times corpus analyzed in the study?\nThe Financial Times corpus analyzed contained 891,171 unique words.",
    "The median correlation coefficient between daily mentions of company names and transaction volumes across all 31 companies is 0.000.",
    "A greater transaction volume for a company\u2019s stocks on a given day is therefore related to a greater number of mentions of that company in the Financial Times on the following day.",
    "The Financial Times is released each day from Monday to Saturday at 5 am London time.",
    "Data retrieval and preprocessing: The analysis utilizes a corpus of daily issues of the Financial Times from January 2nd, 2007, to December 31st, 2012. The data were retrieved from PDF form from the website http://www.ft.com/. Five dates were excluded due to technical problems. The data were preprocessed by converting the PDFs to text, removing special characters, and converting all words to lowercase. Digits without letters or symbols were also removed.",
    "LTM stability coefficient shows a stable dynamics or low turnover of stocks in the LTM during periods of market stability.",
    "At any one time, the Dow Jones Industrial Average (DJIA) is derived from the stock prices of 30 companies.",
    "The median correlation coefficient between daily mentions of company names and absolute returns is 0.040.",
    "The analysis investigated whether there is a relationship between daily mentions of a company's name and the daily return of the corresponding company's stocks. The results showed that the correlation coefficients were not significantly different from zero (median correlation coefficient 5 0.000; mean correlation coefficient 5 0.002).",
    "The researchers defined a quantitative definition of support and resistance based on bounce analysis.",
    "The study analyzed 9 stocks from the London Stock Exchange in 2002.\nThe time period of the analysis was 2002 (251 trading days).",
    "The average Hurst exponent was 0.44. This value indicates an anticorrelation effect of the price increments, suggesting a potential increase in bounces and mimicking a memory of the price on a support or resistance.",
    "What is the primary focus of the research?\nThe research investigates whether technical trading strategies, specifically focusing on supports and resistances, produce detectable signals in price time series and if they introduce memory effects in price dynamics.",
    "The time between consecutive bounces is described by a power law distribution. The maximum distance from support/resistance levels is described by an exponentially truncated power law.",
    "The paper investigates whether technical trading strategies produce detectable signals in stock price dynamics and explores the presence of memory effects. It focuses on identifying potential support and resistance levels and measures the conditional probability of bounces on these levels.\n\nThe researchers used Bayesian inference to estimate the conditional probability of bounces (p(b<sub>j</sub> | b<sub>prev</sub>)) given the number of previous bounces (b<sub>prev</sub>). They inferred p(b<sub>j</sub> | b<sub>prev</sub>) from the number of bounces (nb<sub>prev</sub>) and the total number of trials (N), assuming nb<sub>prev</sub> is a Bernoulli process.\n\nThe expected value and variance of p(b<sub>j</sub> | b<sub>prev</sub>) were evaluated using Bayes' theorem, with mathematical details provided in the supporting information. The slopes of the best fit lines of p(b | b<sub>prev</sub>) were analyzed at different time scales, showing a decrease in the memory effect as the time scale increases.",
    "What is the core idea behind technical trading, according to the provided text?",
    "The conditional probabilities of bounce on resistance/supports were found to be nearly 1.5 to 2, while the shuffled time series probabilities were nearly constant around 1.5 to 2. The small bias towards a value larger than 1.5 to 2 for the shuffled series is due to the finiteness of the stripe.",
    "The context does not specify the statistical tests employed or the significance levels used. It describes a data pipeline framework and the decreasing accessibility of data stages, but it does not detail the specific statistical methods used to validate findings.",
    "The paper investigates if technical trading produces detectable signals and memory effects in price dynamics. It focuses on supports and resistances, finding that the probability of a bounce on these levels increases with the number of previous bounces.",
    "The three components that make up the total cost C_i for a one-level dependent activity according to Zachary\u2019s framework are economic (CF), social (CS), and environmental (CE).",
    "The framework defines sustainability based on whether an activity can be sustained according to its duration, cost, and availability of resources, or if substitution is possible.\nThe environmental cost (C_E) is proportional to the impact (I), pollutant (p), and capacity (c).\nCapacity (c) is defined as the amount required or needed to meet a certain demand.",
    "The two conditions for sustainability are a strong condition requiring no substitution and a weak condition allowing for substitution.",
    "The set of all sustainable activities forms a subset of an N-level union of sustainable activities, which is a topological cover of sustainable activities.",
    "The three conditions for an activity to be considered sustainable, according to Proposition 1, are:\n1.  The duration, cost, and chain of dependent activities satisfy the demand.\n2.  The demand is met with no substitution.\n3.  The demand is met via substitution.",
    "The paper identifies five requirements for sustainable agriculture addressable by Zachary\u2019s framework:\n\n1.  Satisfying human food and fiber needs.\n2.  Enhancing environmental quality and the natural resource based upon the agricultural economy.\n3.  Making the most efficient use of non-renewable resources and on-farm resources.\n4.  Integrating, where appropriate, natural biological cycles and controls.\n5.  Sustaining the economic viability of farm operations.",
    "The context outlines the historical development of ecological economics and sustainability science, highlighting key figures and approaches.",
    "The paper addresses higher-order dependencies by illustrating them with the example of tuna harvesting. It explains that unsustainable activity, such as overfishing, can lead to cascading effects throughout the food chain, impacting even smaller fish populations.",
    "The context acknowledges that quantifying the social pillar of sustainability is particularly challenging, especially in connecting local issues with global ones.",
    "The eight possible classifications of activities are:\n1.  Physico-Economic\n2.  Biophysical-Energy\n3.  Systems-Ecological\n4.  Ecological Engineering\n5.  Human Ecology\n6.  Socio-Biological\n7.  Historical-Institutional\n8.  Ethical-Utopian",
    "The proposed system achieves 5 times faster speed than existing fastest systems, and it can be raised further by using higher-end elements. The system\u2019s speed is largely determined by the operating frequency of galvanic mirrors and the size of the DMD micro-mirrors.\n\nThe maximum modulation frequency achieved is 97 kHz, which is approximately 5 times faster than that of the fastest DMD. This speed is limited by the galvanic mirror\u2019s working frequency.",
    "The final modulation frequency of the sweeping-based ghost imaging system is determined by the scanning frequency of the two galvanic mirrors GM1 and GM2, and the size of the DMD micro-mirrors. Specifically, it is expressed as Fg multiplied by \u2206 x, where Fg is the scanning frequency of the galvanic mirrors, \u2206 x is the scanning range of the beam on the DMD, and \u03b4 is the size of each micro-mirror of the DMD.",
    "The authors achieved 42 Hz at 80 \u00d7 80-pixel resolution and 5 times faster than state-of-the-arts, with potential for further multiplication.",
    "What is the theoretical maximum modulation speed that could be achieved using high-end galvanic mirrors like the CTI 6200H?\n\nThe theoretical maximum modulation speed achievable with high-end galvanic mirrors like the CTI 6200H is 485 kHz, approximately 5 times faster than the fastest DMD, and can be raised further by hardware upgrades.",
    "The software algorithm used for STDM-OCT is presented in Fig. 2(a).",
    "The mathematical relationship derived for edge detection using consecutive patterns in the sweeping system involves subtracting correlated measurements (yi and yi-1) to reconstruct the edges of the image. This allows for reconstruction from measurements calculated following a specific equation (Eq. 6).",
    "The system utilizes two galvanic mirrors rotating around their axes, with a distance of 12 cm between them. The distance between the DMD and the second galvanic mirror is approximately 8 cm. The rotating angle of the first galvanic mirror (GM1) is related to the beam's hitting position on the DMD by the equation x = d sin(2\u03b8), where d is the distance between the mirrors and \u03b8 is the rotating angle. Calibration establishes a linear relationship between the GM input voltage and the corresponding DMD beam position.",
    "The geometric relationship between the rotating angles of the galvanic mirrors and the hitting position of the beam on the DMD is described by the equation x = d sin(2 \u03b8), where x is the distance from the beam's hitting position at GM1 to that at the DMD, d is the distance between the galvanic mirrors, and \u03b8 is the rotating angle of GM1.",
    "What potential improvement in modulation speed could be achieved using acoustic optical deflectors (AOD) instead of galvanic mirrors?\n\nAODs can produce 20 kHz scanning frequency, resulting in a 20 times faster illumination patterning speed (9.5 MHz) compared to galvanic mirrors.",
    "The main limitations of the proposed sweeping-based ghost imaging approach are:\n\n*   It trades spatial resolution for modulation frequency.\n*   The system requires careful mechanical mounting for calibration.\n*   It currently works best with random patterns and is not suitable for specific patterns like Hadamard or sinusoidal patterns.\n*   The speed is limited by the operating frequency of the galvanic mirrors and the size of the DMD micro-mirrors.",
    "The novel event extraction and representation method combines syntactic analysis using HanLP, RBM training, and sentence2vec.",
    "The M-MI model addresses the lack of true instance-level labels by predicting a label for each instance based on the predicted label of its multi-source super group.",
    "The M-MI model\u2019s objective function incorporates three levels of loss functions.",
    "The study\u2019s primary goal was to combine textual network analysis with stock market prediction.",
    "M-MI improves F1-score by 6.9% in 2015 and 9.2% in 2016 compared to nMIL.",
    "The paper justifies the use of a shared estimated true label across different data sources in the hinge loss functions because different data sources commonly indicate the same sign (index rise or fall) due to the Efficient Market Hypothesis.",
    "The three-level syntactic tree captures structured event extraction by representing the sentence with a root node (the core verb), second-level nodes as subjects and objects, and child nodes as modifiers. Core words are selected as the root node, subject, and object.",
    "As the number of history days increases, the F-1 scores generally rise initially but then decline. The possible reason is that the impact of news, sentiments, and quantitative indices decays over time.",
    "The LDA-S method is used for extracting sentiments from social media posts. It was chosen because extracting sentiments discarding topics may not be sufficient, as sentiment polarities often depend on the underlying topics or domains.",
    "The framework uses a Multiple Instance Learning (MIL) model, where a group of instances are given group labels assumed to be an association function of the instance-level labels.\nSource-specific weights reveal how related a specific source is to the index movement.",
    "Agricultural land consolidation, rural construction land consolidation, land reclamation, and land development.",
    "The priority remediation area includes 27 administrative villages, spanning a total area of 28090 hm2, which represents 32.75% of the overall region.",
    "The calculation of the relative risk value (RS\u1d62) involves density of risk sources and habitat abundance.",
    "The SOFM neural network parameters are set with 10 input nodes, 30 output nodes, and 1000 iterations.",
    "The ecological risks of landscape pattern and soil in A County of Shaanxi Province are markedly higher than those of the other three types of ecological receptors.",
    "The calculation of mixing distance is written as Dij.",
    "The three main weight categories are news events, quantitative data, and sentiments. Their respective values are 0.05, 0.05, and 0.5.",
    "IoT sensors were deployed within the land consolidation project areas to monitor soil quality, meteorological conditions, water quality, and farmland conditions.",
    "Area C demonstrates the lowest ecological risk levels across all ecological risk aspects.",
    "The proximity function hij can use the Gaussian function to measure the proximity among neurons.",
    "Leonhard Euler in the 18th century.",
    "The primary driving factor behind the development of relational algebra and RDBMS in the 1960s was the need to efficiently compress data due to storage limitations and high costs.",
    "Graphs are expressed in node-arc-node (subject-predicate-object) triples.",
    "GPU (graphics processing unit) hardware alternatives have emerged to accelerate large-scale graph processing.",
    "Key performance limitations of RDBMS when dealing with complex relationship structures include difficulty with many-to-many relationships and inflexibility due to schema constraints.",
    "The hierarchical model and the network model of the late 1960s were early attempts to model objects and their relationships, respectively.",
    "There is little semantic commonality among the various graph languages in use and their rules of syntax.",
    "Many graph databases support the Atomic, Consistent, Isolated, and Durable (ACID) consistency model, which is a spin-off from RDBMS technology.",
    "Scale introduces yet another qualitative concern: as more instances are brought to bear, query complexity grows. Subgraphs can be intelligently reduced to more salient subgraphs for better management and querying.",
    "Built-in mathematical functions residing in graph databases could deepen understanding and enable quantitative design of diverse networks.",
    "The experiments used Earth-standard pressure (1000 mbar) and Mars-like pressure (8 mbar).",
    "A relative error of 6.79%, 7.68%, and 7.76% was observed for the sandy, intermediate, and bedrock soils, respectively, when comparing the thermal inertia estimations based on Perseverance\u2019s data with the MEC-based thermal inertia estimations.",
    "The increase in the difference between surface and subsurface temperatures for soil A increased by 26.72% when pressure decreased from Earth to Martian conditions.",
    "The thermal inertia of a given soil is estimated as Ts = T max - T min, where Ts is the thermal inertia, T max is the maximum surface temperature, and T min is the minimum surface temperature.",
    "The IR viewport window was chosen to allow 8 to 14 \u00b5m radiation to pass through with minimal losses. It was designed to withstand pressure and temperature extremes.",
    "The radiometric images were saved as plain text 640 \u00d7 480 matrices with each cell containing the temperature in degree Celsius.",
    "Thermal conductivity is influenced by three mechanisms: transfer across pore spaces (kr), conduction between grain contacts (kc), and conduction of the gas within the pores (kg). Pressure significantly affects which of these mechanisms dominates, with gas conduction being most relevant at pressures between 0.1 and 1000 mbar.",
    "At Earth\u2019s pressure, the bedrock is distinguishable, while granular soils exhibit similar values.\nUnder Martian pressure, three prominent groups of soils are observed: soil C, soils A and B, and bedrock.\nThe relative difference in thermal inertia increases significantly under Martian conditions compared to Earth\u2019s pressure.\nBedrock exhibits the highest thermal inertia, and sandy soils the lowest.",
    "The thermal camera is a PI-640i by Optris based on uncooled microbolometer technology. It is a 320-g LWIR camera that works in the spectral range of 8-14 \u00b5m, has a resolution of 640 \u00d7 480 pixels, and a germanium optic with an FOV of 60 \u25e6 \u00d7 45 \u25e6 . It can measure temperatures from -20 \u25e6 C to 900 \u25e6 C with a thermal sensitivity of 0 . 04 \u25e6 C.",
    "The simplified surface energy budget equation is: G = (\u03c3B * Ts^4) - (R sw + R lw + F CO2) + (\u2202 T /\u2202 Z \u2032 ) | Z \u2032 = 0, where G is the net heat flux, A is the albedo, \u03c3B is the Stefan-Boltzmann's constant, R sw is the downwelling shortwave radiation, R lw is the downwelling longwave radiation, F CO2 is the seasonal CO2 condensation, Ts is the surface temperature, and (\u2202 T /\u2202 Z \u2032 ) | Z \u2032 = 0 is the temperature gradient evaluated at the surface.",
    "Singular vectors with similar information are randomly distributed in two areas due to sign ambiguity.\nSingular vectors are inefficient to learn on manifold space through a general neural network scheme.",
    "SVP shows 36.07%, 28.18%, and 34.21% higher accuracies than GAP, GMP, and MPN with FGSM adversarial attacks on the CIFAR10 dataset.",
    "Aligning e V based on e U as a reference removes sign ambiguity.",
    "The arccosine function is approximated using a first-order Taylor expansion to avoid divergence.",
    "The loss function used to learn the center vector uc is KL-divergence, based on the assumption that each component of the aligned N u has a Gaussian distribution.",
    "The silhouette score of SVP on the training dataset is better by 0.444, 0.510, and 0.551 than that of GAP, GMP, and MPN, respectively.",
    "The rotation process centers singular vectors to mitigate a discontinuity problem by transforming them to align with the coordinate center.",
    "The experiment shows that KD-SVP improves network performance by learning singular vectors more effectively than KD-SVD, leading to performance gains when using more singular vectors.",
    "If the sign ambiguity removal step is omitted, the overall performance deteriorates by about 5.01% because singular vectors are hard to learn by Euclidean geometry.",
    "SVP has a somewhat higher computational complexity than general pooling methods like GAP and GMP.",
    "The STDM-OCT system achieved a 1-MHz A-scan rate.",
    "The broadband light source used in the STDM-OCT system is the SLD-371-HP3 from Superlum, Ireland, with a center wavelength of 838 nm, a full-width at half-maximum of 81 nm, and an optical power of 27.2 mW.",
    "The volumetric imaging rate achieved was 8 vol/s for an image range of 250 \u00d7 250 \u00d7 2048 pixels (9 \u00d7 4.5 \u00d7 5 mm).",
    "The measured thicknesses of the four layers are 100, 250, 150, and 100 \u00b5m, respectively.",
    "What are the peak sensitivities measured for camera #1 and camera #2 at the 100th pixel?\nThe measured peak sensitivities (at the 100th pixel) were 139 dB and 137 dB, respectively.",
    "C++, CUDA, and Qt were used to develop the control software platform.",
    "The STDM-OCT provides a 1-MHz A-scan rate, reducing inspection time and cost compared to SS-OCT.",
    "The total inspection time required for scanning the entire OTF sample was 8 seconds.",
    "The measured thicknesses of each layer in the OTF sample were 100, 250, 150, and 100 \u00b5m, respectively, and the total thickness including vacuum gaps was 700 \u00b5m with gaps of 40, 20, and 40 \u00b5m.",
    "The STDM-OCT system utilizes a broadband light source (838 nm) and a 50:50 fiber coupler to distribute light to each interferometer. Each interferometer consists of a reference arm with a collimator and mirror, and a sample arm. The system employs a galvanometer scanner for scanning and a linear-motor stage for 3D inspection."
  ]
}