{
  "reference_answers": [
    "The Gander project had two main components: (1) an empirical component focused on problem conceptualization, conducted as a mixed-method study with practitioners in industry to provide input for tool design, and (2) a development component focused on building an experimental code review platform incorporating eye-tracking to enable gaze-assisted assistance in code reviews.",
    "The framework identifies (1) employees or other stakeholders of software development organizations, (2) students or other beneficiaries of the university, and (3) independent participants. They are distinguished based on legal and ethical concerns regarding the relationship between researchers and participants, such as potential pressure to participate and secrecy conditions.",
    "The four steps are: (1) Data cleaning \u2013 transforming and anonymizing data (quantitative: coding for statistical tools; qualitative: transcription and anonymization). (2) Data exploration and visualization \u2013 descriptive statistics and outlier detection (quantitative) or preliminary coding ideas (qualitative). (3) Model building and analysis \u2013 statistical analysis like prediction models (quantitative) or coding and theory building (qualitative). (4) Findings presentation \u2013 presenting results in publications or reports.",
    "The three steps are: (i) Publication for reproduction \u2013 artifacts in original state (non-editable documents, executable code), ensuring transparency. (ii) Generalization for general use \u2013 artifacts in editable state (editable documents, source code). (iii) Generalization for continued development \u2013 artifacts with licenses, onboarding guidelines, and community support for ongoing evolution. Each step requires progressively more investment to make artifacts openly reusable.",
    "The risks include: disclosure of irrelevant but sensitive company information, exposure of relevant but confidential events (e.g., security incidents), potential harm to interviewees if their opinions about managers are revealed, difficulty of anonymization with small participant groups, and epistemological concerns that transcripts lack meaning without shared context. These factors led the authors not to recommend sharing raw qualitative data.",
    "The authors recommend not to openly publish qualitative research data. Instead, they suggest publishing study and analysis artifacts such as study protocols, interview guides, interviewee descriptions, and code books from thematic analysis to ensure transparency without compromising confidentiality.",
    "The Gander project discovered that one of the projects used for gaze data analysis lacked a license. After contacting the original author, a MIT license was added, which allowed the team to proceed. Ultimately, the Gander platform was released under a BSD license after reviewing all dependencies.",
    "Eye-tracking is used to detect fixation points during code review, which are connected to programming language elements. This enables real-time gaze-based assistance. As a proof-of-concept, the platform implemented a gaze assistant that visualizes use-declaration relationships in code when users fixate on variable names.",
    "Quantitative survey data, being less rich and more standardized, is easier to anonymize and share openly compared to qualitative data. The authors recommend (R4) that quantitative data be shared openly if, and only if, it is sufficiently anonymized to protect the identity of individuals or companies.",
    "They emphasize the FAIR principle: data should be 'as open as possible and as closed as necessary.' This means fostering reuse and accelerating research through openness, while respecting participants\u2019 privacy, ethical constraints, and companies\u2019 legitimate secrecy concerns.",
    "The main motivation is to reduce the high computational cost of traditional neural architecture search, which often requires training numerous candidate architectures. Training-free indicators aim to estimate performance without full training, enabling faster search.",
    "The three types are (1) synaptic diversity, measuring variability in weight initialization and connections, (2) neuron activation strength, capturing output responses of neurons to input data, and (3) path expressivity, reflecting the complexity of information flow across paths in the architecture.",
    "The feature fusion mechanism combines normalized versions of synaptic diversity, neuron activation strength, and path expressivity through weighted integration. This is necessary because each feature captures different aspects of architecture quality, and fusion provides a more robust performance indicator.",
    "The authors used NAS-Bench-101, NAS-Bench-201, and DARTS search space benchmarks. These datasets were chosen because they provide standardized environments with known ground truth performance, enabling reliable comparison of NAS indicators.",
    "FFI consistently outperformed existing indicators like Synflow, GradNorm, and Jacov in terms of correlation with true architecture performance across all benchmarks, demonstrating higher reliability as a predictor.",
    "Kendall\u2019s Tau is a statistical measure of rank correlation between two variables. The paper uses it to assess how well the rankings produced by FFI align with ground truth rankings of neural architectures.",
    "The authors conducted ablation studies by removing one feature type at a time (synaptic diversity, neuron activation strength, path expressivity) and observed performance drops. This showed that all three features contribute significantly to the effectiveness of FFI.",
    "FFI dramatically reduces computational cost because it avoids training architectures altogether. Its evaluation is orders of magnitude faster than training-based NAS, making large-scale architecture search feasible.",
    "The authors found that FFI maintains high correlation with true performance across diverse search spaces (NAS-Bench-101, NAS-Bench-201, DARTS), demonstrating its robustness and generalizability.",
    "The authors suggest exploring additional complementary features, adaptive fusion strategies that adjust weights dynamically, and extensions of FFI to large-scale real-world tasks to further enhance training-free NAS indicators.",
    "The motivation is to overcome the limitations of existing monocular vergence-based and binocular depth-based methods. Vergence-based methods struggle with low accuracy at long distances, while depth-based methods suffer from errors due to noise and hardware limitations. A hybrid approach combines both to achieve more robust and accurate gaze distance estimation.",
    "The two cues are vergence (eye rotation angles) and depth (from RGB-D camera or stereo imaging). Vergence provides reliable estimation at short distances, while depth cues perform better at longer distances. By cross-referencing the two, the hybrid method improves robustness across a wide range of viewing distances.",
    "The authors conducted experiments using both controlled indoor datasets with calibrated RGB-D cameras and real-world settings where subjects fixated on targets at varying distances. These setups enabled evaluation across short, medium, and long gaze distances.",
    "The mechanism compares the confidence levels of the vergence and depth estimates. When vergence accuracy is high (short distance), it is prioritized. When distance increases and vergence degrades, depth-based estimation is weighted more heavily.",
    "The hybrid method achieved lower average estimation error across all tested distances. Specifically, it maintained accuracy comparable to vergence at short distances and outperformed both vergence and depth baselines at medium and long distances.",
    "Calibration aligns the vergence-based eye-tracking system with the depth-sensing camera to ensure accurate fusion of cues. It was performed by asking participants to fixate on predefined calibration targets at known distances while recording both vergence and depth data.",
    "Applications include human-computer interaction, virtual and augmented reality systems, driver monitoring, and assistive technologies that rely on accurate gaze-based distance estimation.",
    "The authors note limitations such as reliance on depth sensors, which can fail under poor lighting or reflective surfaces, and the assumption of stable fixation, which may not hold in dynamic environments. Further improvements are needed for robustness in unconstrained real-world scenarios.",
    "The 'mid-range gap' arises because vergence is accurate only at short distances and depth sensors are reliable mainly at long distances. The hybrid method bridges this gap by adaptively combining the two, achieving accurate estimation in the mid-range where both cues individually perform poorly.",
    "Future work includes integrating learning-based fusion strategies, improving robustness of depth sensing under challenging conditions, and validating the system in more diverse real-world applications such as outdoor environments.",
    "The four attributes are availability, reliability, data integrity, and efficiency. Availability is measured as the ratio of accepted to total requests. Reliability is measured as the ratio of successful to accepted requests. Data integrity is measured as the ratio of successful requests minus failed requests to successful requests. Efficiency is measured as promised execution time divided by the sum of waiting and execution time.",
    "The model formulates a Mixed Integer Linear Programming (MILP) problem where the objective is to maximize overall trust of VM allocations while minimizing communication delay. Trust is computed from the weighted sum of availability, reliability, data integrity, and efficiency, while delay is computed as the ratio of total VM traffic to server capacity. Constraints ensure each VM is allocated once and within resource limits.",
    "The GA provides a heuristic solution to the NP-hard MILP resource allocation problem. It uses selection, crossover, and mutation to generate near-optimal VM allocation sets. Compared to the optimal solver (intlinprog in MATLAB), GA achieves 90% similarity in results but with significantly less execution time and better scalability for large numbers of CSPs and servers.",
    "Availability decreases as load increases. Under small loads, availability is maximum, while under heavy loads it is minimum. The experiments used small, medium, and heavy load scenarios, averaging 10 runs to compute availability values.",
    "Reliability degradation is caused by hardware and software failures. Hardware failures include failures of virtual machines and processing elements, while software failures arise from errors in request processing and workload execution. Reliability decreases with an increase in the number of requests and load.",
    "The Pareto front shows that maximizing trust by allocating resources only to the most trusted CSPs increases communication delay, while distributing resources reduces delay but lowers trust. Thus, achieving an optimal trade-off requires balancing both objectives.",
    "The CloudSim simulator was used because of its wide acceptance in the research community and its ability to simulate real cloud environments, including request arrivals, failures, and resource allocation scenarios. It enables validation of the trust evaluation and resource allocation models without relying on restricted real cloud provider configurations.",
    "When trust is given higher weight, trust values improve by around 10% but delays increase. Conversely, when delay is prioritized, delays reduce by about 10% but trust decreases. Equal weights achieve a balance, showing a clear trade-off depending on service requirements.",
    "Experiments used CloudSim for trust data collection and MATLAB for optimization. An Intel Core i7 machine with a 4 GHz processor was used. Genetic Algorithm experiments had a population size set based on requirements and generation size fixed at 1000. Scenarios varied CSPs from 6 to 12 and servers from 30 to 60.",
    "The authors propose developing a more reliable and secure model for trust management among different cloud service providers, extending beyond resource allocation to broader multi-cloud trust frameworks.",
    "The motivation is that 28Si single crystals enable determination of the Avogadro constant with unprecedented precision. Isotopically enriched 28Si reduces uncertainties due to isotope mass distribution, making it suitable for defining the kilogram based on fundamental constants.",
    "EPR is used to characterize point defects and impurities in 28Si single crystals. These defects, such as oxygen-vacancy complexes and dangling bonds, affect the accuracy of Avogadro constant determination and thus must be quantified.",
    "The study identified P donors, Pb centers (Si dangling bonds at the Si/SiO2 interface), and oxygen-vacancy centers such as the E\u2032 center as the main paramagnetic defects present in the crystal.",
    "Isotopic enrichment in 28Si improves EPR resolution by reducing hyperfine interactions caused by 29Si nuclei. This results in narrower linewidths and more precise defect characterization.",
    "The experiments used an X-band EPR spectrometer operating at ~9.4 GHz. Measurements were conducted at low temperatures using a helium-flow cryostat to enhance defect signal detection.",
    "Oxygen-vacancy defects alter the lattice parameter of silicon crystals. Since determination of the Avogadro constant depends on precise measurement of the silicon lattice parameter, quantifying such defects is essential to minimize systematic errors.",
    "The study detected defect concentrations on the order of 10^13 to 10^14 spins per cm\u00b3, confirming that 28Si crystals used for Avogadro experiments have extremely low impurity levels.",
    "Phosphorus donors act as shallow impurities in silicon, influencing conductivity and possibly EPR signal strength. Their concentration must be minimized for precision lattice parameter measurement but also provides useful calibration signals in EPR analysis.",
    "28Si single crystals provide higher EPR sensitivity and resolution due to the absence of 29Si nuclear spins, enabling clearer identification of defects and lower detection limits compared to natural silicon.",
    "The authors suggested further EPR investigations at varying temperatures and magnetic fields to better characterize weak defect signals, along with complementary spectroscopic techniques to fully quantify remaining impurities in 28Si crystals.",
    "The primary causes are (i) line edge roughness (LER), (ii) random dopant fluctuation (RDF), and (iii) work function variation (WFV). LER is considered most severe because it can affect RDF and WFV by inducing deformation of the device structure, thus degrading performance more significantly.",
    "The parameters are: (i) Amplitude (rms value of surface roughness), (ii) Correlation length X (3x) and (iii) Correlation length Y (3y). A larger correlation length indicates a smoother line. Additionally, a relation term between x and y directions (\u03b1 or 2) may be included.",
    "Two datasets were generated: (1) 130 datasets each with 50 FinFETs (6,500 total), split into 70% training and 30% validation, and (2) 10 datasets each with 250 FinFETs (2,500 total). LER parameter ranges were: amplitude 0.1\u20130.8 nm, correlation length X 10\u2013100 nm, correlation length Y 20\u2013200 nm.",
    "The earth-mover\u2019s distance (EMD) score was used. It measures the minimal amount of work needed to transform one probability distribution into another, calculated by comparing cumulative distribution functions (CDFs) estimated with Gaussian kernel density estimation. An EMD of 0 means identical distributions.",
    "The optimized ANN had 3 neurons in the input layer, 81 in the first hidden layer, 162 in the second, 324 in the third, and 324 in the output layer. This architecture with 11 mixture components minimized validation loss at 7,800 epochs, making it best suited to describe the distribution of performance metrics.",
    "Negative log likelihood (Negloglik) was used as the loss function because the ANN outputs probability distributions (PDFs). Conventional mean squared error cannot be applied since training is equivalent to maximum likelihood estimation of distributions, not single-point values.",
    "The mixture-MVN ANN successfully predicted skewness, kurtosis, and non-linear correlations that the Gaussian-only ANN could not. The EMD score improved significantly (0.0170 with Gaussian-only vs. 0.00928 with mixture-MVN).",
    "Training time was reduced from 1412 seconds for the non-separated ANN to 185 seconds for the separated ANN, achieving about a 6\u00d7 speedup without significant performance degradation.",
    "The proposed ANN predicts 7 metrics: Ioff, Idsat, Idlin, Idlo, Idhi, Vtsat, and Vtlin. The previous ML model predicted only 4 metrics: Ioff, Idsat, Vtsat, and SS.",
    "By accurately predicting non-Gaussian distributions of performance metrics, the ANN enables simulation of electrical behavior of transistors and DC behavior of digital circuit blocks such as SRAM bit cells, improving variation-aware design.",
    "Because the amplitude of LER does not scale down proportionally with device dimensions, its fraction of the physical channel length and width increases, leading to larger variations in IDS\u2013VGS characteristics.",
    "The parameters are RMS amplitude (\u03c3), correlation length (\u03beX, \u03beY), and roughness exponent (\u03b1). \u03c3 is the standard deviation of roughness amplitude, \u03be represents the wavelength of the roughness profile, and \u03b1 describes how high-frequency components diminish in the roughness profile.",
    "The dataset contained 169 sets, each consisting of 50 sample FinFETs with identical LER and device parameters. Eighteen FinFET structures were chosen, and LER parameters were randomly varied within specified ranges.",
    "Target variables: mean (\u00b5IDS) and standard deviation (\u03c3IDS) of log(IDS). Input features: gate voltage (VGS), RMS amplitude (\u03c3), correlation lengths (\u03beX, \u03beY), roughness exponent (\u03b1), gate length (Lg), fin width (Wfin), and fin height (Hfin).",
    "Horseshoe priors introduce shrinkage and sparsity over weights, promoting zeroing of unnecessary weights while allowing important large weights to remain due to heavy tails. This enables automatic model selection, finding compact layer sizes without manual tuning.",
    "HS-BNN improved prediction accuracy significantly. For \u00b5IDS, MAPE improved from 0.81% (BLR) to 0.55% (HS-BNN). For \u03c3IDS, MAPE improved from 19.59% (BLR) to 6.66% (HS-BNN).",
    "The HS-BNN maintained stable prediction performance (\u03c3IDS MAPE ~7%) even when the number of nodes exceeded 200, while Gaussian-BNN predictions varied significantly with node count. This shows HS-BNN\u2019s robustness to over-parameterization.",
    "K-fold cross-validation was used, ensuring all data were used as a test set at least once. This is suitable for limited datasets, producing less biased evaluations and better generalization estimates.",
    "For a FinFET with Lg=20 nm, Wfin=7 nm, Hfin=42 nm, and LER profile (\u03c3=0.5 nm, \u03beX=20 nm, \u03beY=50 nm, \u03b1=1), varying one parameter at a time showed well-matched trends in both mean and standard deviation of log(IOFF) and VTH, consistent with known device behavior.",
    "HS-BNN predicted LER-induced variations within a few seconds, whereas conventional TCAD simulations required weeks for a single LER profile.",
    "The RAFS achieved 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2 stability by using a rubidium spectral lamp with Xe starter gas, applying optical and isotope double-filtering to reduce noise, employing a large slotted tube microwave cavity with a 40 mm absorption cell to enhance the discrimination signal, and enclosing the physics package in a sealed box to mitigate barometric effects.",
    "The barometric effect was shown to cause a frequency shift coefficient of 7 \u00d7 10\u207b\u00b9\u2075/Pa, degrading 100-s stability to 2.2 \u00d7 10\u207b\u00b9\u2074. The solution was to place the physics package in a sealed box, reducing the barometric influence by nearly an order of magnitude.",
    "The slotted tube microwave cavity allowed flexible size design, enabling the use of a large 40 mm absorption cell to increase atomic signal SNR. It also provided a high field orientation factor (\u03be = 0.82), superior to traditional TE111/TE011 cavities, improving excitation of the clock transition.",
    "The optimal temperatures were 68 \u00b0C for the absorption cell, 93 \u00b0C for the filter cell, and 109 \u00b0C for the lamp bulb.",
    "By optimizing the discrimination slope Kd (18.0 nA/Hz) and measuring the shot noise current (211 \u00b5A, corresponding to noise spectral density of 8.2 pA/Hz\u00b9/\u00b2), \u03c3SNR(\u03c4) was estimated to be 4.7 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2.",
    "The interrogation microwave had phase noise of about \u2212110 dBc/Hz at 2fM (272 Hz). This limited stability to \u03c3PN(\u03c4) = 6.0 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2.",
    "The environmental factors examined were absorption cell temperature, pumping light intensity, 6.835-GHz microwave power, magnetic C-field, and barometric pressure. Among these, temperature shift and barometric pressure shift had the largest impact on stability for \u03c4 > 100 s.",
    "With the H-maser, the RAFS showed 9.0 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2 (1\u2013100 s). With the OMG, it showed 9.1 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2 (1\u2013100 s). Both results were consistent and matched the predicted 7.6 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2 stability.",
    "This result surpasses the previous best stability of 1.2 \u00d7 10\u207b\u00b9\u00b3\u03c4\u207b\u00b9/\u00b2 (recently achieved in pulsed laser-pumped RAFS), marking the first lamp-pumped RAFS to achieve 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2 stability, approaching hydrogen maser performance and enabling next-generation space clocks for satellite navigation.",
    "Predicted contributions were \u03c3SNR(\u03c4) = 4.7 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2, \u03c3PN(\u03c4) = 6.0 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2, and environmental effects controlled at 10\u207b\u00b9\u2075\u03c4\u207b\u00b9/\u00b2, leading to a predicted total stability of 7.6 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2.",
    "1) The average within-group Pearson correlation coefficient (PCC) must drastically increase in absolute value, 2) The average between-group PCC (stocks in the group vs. rest of system) must greatly decrease in absolute value, and 3) The average auto-covariance (AC) of stocks belonging to this group must increase in absolute value.",
    "I^LTM_t = (\u27e8|AC^LTM_t|\u27e9\u27e8|PCC^LTM_t|\u27e9) / \u27e8|PCC^\u00acLTM_t|\u27e9, where the first component (auto-covariance) relates to positive feedbacks in the market, and the second component (correlation ratio) reveals the presence of herding behaviors among investors.",
    "Stocks stay continuously in the leading module for about 1.5 months on average. There is a negative Pearson correlation of -0.19 between the LTM stability coefficient and the size of the DFA- group (stocks with significant DFA exponents not included in the leading module).",
    "The strategy compares the most recent I^LTM_t value against its empirical distribution computed over the previous 15 working days. Values larger than the 95th percentile trigger investment decisions. If the average return of LTM stocks is positive, a buy signal is generated; otherwise, a short position is taken.",
    "As \u03bb\u2081 approaches 1: (1) the absolute value of auto-covariance AC(z_i(t), z_i(t-1)) increases greatly if variable z_i is related to y\u2081; (2) |PCC(z_i(t), z_j(t))| approaches 1 if both variables are related to y\u2081; (3) |PCC(z_i(t), z_j(t))| approaches 0 if only one variable is related to y\u2081.",
    "The LTM strategy achieved: true positives 53%, false positives 47%, false negatives 49%, and true negatives 51%. The VaR strategy performed worse with: true positives 49%, false positives 51%, false negatives 52%, and true negatives 48%.",
    "The indicator showed increasing dynamics corresponding to major market events including: banking sector ratings downgrades of 2007, the failure of Lehman Brothers in September 2008, the American Recovery and Reinvestment Act of 2009, the European Debt crisis of 2011, and the Chinese stock market crisis of 2015-2016.",
    "Stocks with Hurst exponents outside the 0.2-0.8 interval show a distribution of correlations that is shifted to the right compared to other stock pairs, indicating that the DFA selects assets with highly correlated returns. This increases their probability of entering the LTM.",
    "The study draws an analogy where variations in asset prices are like nucleation phenomena near stability limits in thermodynamic systems (superheated liquid or supercooled gas). The LTM acts as the nucleus of the new phase for financial markets, and I^LTM_t plays a role similar to compressibility in thermodynamics - a macroscopic quantity indicating increasing instability near spinodal lines.",
    "Over the entire 2006-2017 period, the LTM strategy achieved a cumulative P&L of 114.44% (using MV=10, PRCTILE=95 parameters), significantly outperforming the Buy&Hold strategy which only achieved 13.42%. Even with transaction costs of 10 basis points, the strategy still generated about 5.5% per year.",
    "Tinbergen identified four dimensions: mechanism (causation), development (ontogeny), function (adaptive value), and evolution (phylogeny). For machines: mechanism explains how behavior is triggered and generated; development covers how machines acquire behaviors through engineering, training data, or experience; function describes how behavior fulfills purposes for human stakeholders; and evolution examines how behavioral patterns spread through copying, reverse-engineering, and market forces.",
    "Flash crashes represent clearly unintended consequences of interacting algorithms operating at unprecedented speeds that humans cannot match. These high-frequency trading algorithms can respond to events and each other faster than any human trader, potentially creating market inefficiencies and raising concerns about whether algorithms could interact to create larger market crises when faced with unforeseen scenarios not covered in their training data.",
    "The three motivations are: (1) the unprecedented ubiquity of algorithms in society with ever-increasing roles in daily activities; (2) the complex properties of algorithms and their environments making some attributes difficult or impossible to formalize analytically; and (3) the substantial challenge of predicting the positive or negative effects of intelligent algorithms on humanity due to their ubiquity and complexity.",
    "Traditional algorithm development focuses on maximizing performance against benchmarks using optimization metrics, while machine behavior study requires broader indicators similar to social science research. It needs randomized experiments, observational inference, and population-based statistics to understand how algorithms behave in different environments and affect societal outcomes, rather than just measuring accuracy or speed.",
    "Researchers may need to violate terms of service when reverse-engineering algorithms (e.g., creating fake personas), face potential legal challenges from platform creators if research damages reputations, and risk civil or criminal penalties under laws like the Computer Fraud and Abuse Act. Additionally, experimental interventions in real-world settings could adversely affect normal users, requiring careful ethical oversight.",
    "Machine evolution is much more flexible than animal evolution - while animals have simple inheritance (two parents, one transmission), machines can have instant global propagation through software updates, open-source sharing of code and training data, and human designers with specific objectives. However, machines also face unique constraints like software patents and regulatory privacy laws that don't apply to biological evolution.",
    "The three scales are: (1) Individual machine behavior - studying specific intelligent machines by themselves, focusing on intrinsic properties driven by source code or design; (2) Collective machine behavior - examining interactive and system-wide behaviors of collections of machine agents; and (3) Hybrid human-machine behavior - studying interactions between machines and humans in complex hybrid systems.",
    "The paper documents algorithmic bias in computer vision, word embeddings, advertising, policing, criminal justice, and social services. It notes that practitioners sometimes must make value trade-offs between competing notions of bias or between human versus machine biases, highlighting the complexity of addressing fairness in algorithmic systems.",
    "The paper explains that while the code for specifying AI model architecture and training can be simple, the results are often very complex 'black boxes' where the exact functional processes generating outputs are hard to interpret, even for the scientists who created the algorithms. This is compounded by proprietary source code and training data, where often only inputs and outputs are publicly observable.",
    "The study showed that simple algorithms injected into human gameplay can improve coordination outcomes among humans. Specifically, locally noisy autonomous agents improved global human coordination in network experiments, demonstrating that bots can enhance human collective behavior rather than just replace or compete with humans.",
    "The standard gravity model cannot be directly used to estimate weights of existing connection flights because in airline networks, unlike complete graphs such as international trade networks, most transport involves intermediate stops and no direct connections may exist for large distances, leading to observed flows that differ from expected flows.",
    "The observed passenger flow f_ij consists of two components: (1) f_ij^(g) - the number of passengers traveling directly from origin in country i to final destination in country j, given by the gravity equation, and (2) f_ij^(transit) - the number of passengers who use the connection i \u2192 j as part of their longer journey.",
    "The model correctly predicts more than 98% of the total passenger flow in the world.",
    "The authors use p(i \u2192 j \u2192 k) = C \u00b7 f(r_ij, r_jk), where C is a normalization constant and f(r_ij, r_jk) = 1/(r_ij \u00b7 r_jk), reflecting passengers' tendency to choose the shortest connections.",
    "The distance coefficient \u03b1 was determined to be 1.5 for 1996 and 1.6 for 2004.",
    "The paper identifies three major events: the September 2001 attacks in New York and Washington D.C. (which started a chain including SARS epidemic, additional terrorist attempts, wars, and rising oil prices), the 2008 global financial crisis, and the 2010 eruption of Eyjafjallaj\u00f6kull volcano in Iceland.",
    "The authors used data from the International Civil Aviation Organization (ICAO) containing annual traffic on international scheduled services for the years 1990-2011, along with econometric data from Penn World Table 8.1 and distance data from CEPII.",
    "In 2004, only 2308 connections (10%) out of 22650 possible connections were direct flights, while 12749 connections (56%) were shortest paths with length equal to 2.",
    "The missing globalization puzzle refers to the counter-intuitive finding that despite globalization conceptually reducing effective distance, most econometric studies show the distance coefficient increases over time, meaning distance becomes more important rather than less important in economic flows.",
    "The authors use a root mean square (RMS) formula \u0394(\u03b1) = \u221a(1/N_m \u00d7 \u03a3[P(f_ij) - P(f_ij^mcf)(\u03b1)]\u00b2) to measure agreement between normalized histograms of empirical and modelled flows across 15 logarithmically spaced bins, with the minimum \u0394(\u03b1) indicating the optimal distance coefficient.",
    "The researchers identified several challenges: Chinese does not have explicit word boundary markers and contains no whitespace between words; Chinese words are not clearly marked grammatically; Chinese contains a very large number of homophones in sentences; and Chinese consists of several thousand characters (Hanzi) with words consisting of one or more characters, making recognition, segmentation and analysis more complicated compared to English.",
    "The SLS_L model achieved an AUC of 0.9360, which outperformed both the Lasso-Logistics (L_L) model with AUC of 0.8344 and the MCP-Logistic (MCP_L) model with AUC of 0.8707. The SLS_L model demonstrated the highest prediction accuracy among all tested approaches.",
    "The optimal tuning parameters were \u03bb\u2081 = 0.0251 and \u03bb\u2082 = 0.001. \u03bb\u2081 performs variable selection and controls the level of sparsity, while \u03bb\u2082 controls the degree of coefficient smoothing, representing the similarity between coefficients. The non-zero value of \u03bb\u2082 indicates that network structure information was effectively utilized.",
    "The word 'Short Term' had the highest betweenness centrality at 287.89, followed by 'Pessimistic', 'Input Market', 'Larger' and 'Market'. Betweenness centrality measures the number of times a vertex acts as a link along the shortest path between two other nodes, with high betweenness indicating high probability to control information flow in the network.",
    "The network density was 0.1695, meaning there were 522 links in the 56 \u00d7 56 adjacency matrix. This indicates the textual network is a sparse network, which is significantly affected by the fact that the text network of research reports on stocks is loosely knit instead of densely connected.",
    "A total of 2082 reports were collected from 65 security companies. The number of words in a report was most frequently between 500 and 2000 words, with a mean of 3.8 security reports per day.",
    "The study found that the highest AUC occurred when predicting the next one week (five trading days), yielding up to 0.9360 for the SWS index. This finding aligns with Asquith et al.'s discovery that analyst reports can affect market reactions with a five trade days delay, indicating that time is needed for news to translate into trading activity.",
    "Out of 56 words, 31 were found to be effective predictors (55.4%). Effective keywords were defined as those whose coefficients did not shrink to zero in at least one of the three models (MCP_L, SLS_L, and L_L), while the remaining 25 words had coefficients that shrunk to zero in all three models.",
    "Only 5 out of 25 effective terms had a positive impact on financial markets, with most keywords receiving negative connotations. The strongest positive indicator was 'Imagine' and the strongest negative indicator was 'Concern'. Even the seemingly positive word 'Securitization' received a negative connotation, supporting the asymmetric response theory that negative information has greater impact than positive information.",
    "The process involved four steps: 1) Dictionary building using Sogou cell lexicon (63,320 words), 2) Text segmentation using Jieba package with Hidden Markov model, 3) Words cleaning removing stop words and low-frequency terms, and 4) Keyword vector selection using chi-square statistics. After filtering at the 5% significance level, 56 words remained from an initial set of 3285 keywords.",
    "O(L\u00b3), where L is the number of locations",
    "When \u03b2 > 1, the resulting pruned graph may be redundant by retaining sub-optimal routes. When 0 < \u03b2 < 1, the graph is lossy as not all shortest paths are retained. \u03b2 allows trading between quality (redundancy and path quality) and complexity (edge set size).",
    "Three of the four regions achieved F1-scores exceeding 0.9: Styria Austria (0.95 with \u03b2=0.95), Central African Republic (0.94 with \u03b2=0.95), and South Sudan (0.90 with \u03b2=0.95). The German-Austrian border region achieved a maximum F1-score of 0.75.",
    "The paper uses the Open Source Routing Machine (OSRM) from OpenStreetMap, which implements multilevel Dijkstra's (MLD) and contraction hierarchies (CH) algorithms for routing.",
    "If a route between two locations has a distance similar to the sum of distances between these locations and a common third location, then the considered route is likely indirect. The algorithm compares route distances to detect when d*1,2 + d*2,3 \u2248 d*1,3, indicating location l2 lies between l1 and l3.",
    "The fastest route via Autobahn A8 was 33 km/30 minutes, while the alternative route through Rosenheim was 27.1 km/33 minutes total. The algorithm always removes this route regardless of \u03b2 < 1 because it optimizes for fastest time rather than shortest distance, even though a direct, faster route exists.",
    "For Europe with 18,091 cities/towns, the routing took 5.01 hours and the pruning took 316.36 seconds using 128 cores.",
    "For complete location graphs, the proposed algorithm has O(L\u00b3) complexity compared to O(L\u2074 log L) for the optimal Iterative-Global strategy, providing better computational efficiency without loss of optimality.",
    "Ground truth was created by manually inspecting OpenStreetMap to determine if the fastest route between each location pair is direct. A connection was labeled direct if no other location lies on or nearby the fastest route, though this process involved subjective decisions in ambiguous cases.",
    "Location graphs are needed for route optimization, load optimization in electrical and transportation networks, and agent-based modeling applications including transportation of goods, evacuation models, traffic simulations, disease transmission, movement of people, and migration simulation.",
    "The five time scales are known as the 'R's' of resilience: recon, resist, respond, recover, and restore. These represent different phases from prior to an event to potentially days or weeks after a disturbance.",
    "The adaptive capacity is bounded by the real and reactive power component limits given in equations (4) and (5) and the temporal limits defined in equations (6) and (7). This results in a 'manifold' that represents the adaptive capacity of an asset.",
    "The primary resilience challenge is fuel availability for the diesel generators. Fuel gets delivered by boat on the Yukon River, which is impassable from August through April due to long and very cold winters, potentially creating life-threatening situations during winter due to diesel fuel depletion.",
    "The frequency response is defined by df/dt = f\u2080\u0394P/2H, where \u0394P is the disturbance or difference between generation and load, and H is the inertial constant. Large inertia slows the rate of frequency response, allowing additional time for generation units to ramp up or down before reaching frequency limits.",
    "By January 5, 2019, the Alaska Village Electric Cooperative (AVEC) had installed a 900 kW, 52-meter rotor diameter type IV pitch-controlled wind turbine generator manufactured by Emergya Wind Technologies and started producing power.",
    "When diesel generators are taken offline, there is a large negative impact on short-term resilience due to reduced inertia and generation ramping capability, but a positive effect on long-term resilience because the system burns less fuel to support the load, conserving fuel for extended operation.",
    "The platform treats GridLAB-D and Python federates as message federates, with data exchange configured using JSON config files by defining corresponding endpoints. The HELICS API facilitates communication through specific endpoints for monitoring and dispatching data.",
    "The generators are assumed to have an inertia constant of 2, have ramping capability of reaching full output in 10 seconds, and their rate of burning fuel for this study are constant.",
    "The short-term resilience is based on a frequency limit of 58Hz. It measures the maximum size of disturbance the system can withstand without dropping below this frequency limit before under frequency load shed (UFLS) occurs.",
    "When wind is run at maximum output, diesel generation can be taken offline, improving long-term resilience by conserving fuel. When run below maximum output, the fast ramping capability of inverters allows for increased short-term resilience of the system by providing additional generation capacity for frequency response.",
    "The researchers implemented a 'Google Trends strategy' that sells the DJIA at closing price p(t) if the relative change in search volume \u0394n(t-1, \u0394t) > 0, and buys back at price p(t+1). If \u0394n(t-1, \u0394t) < 0, they buy at p(t) and sell at p(t+1). The strategy uses relative change in search volume: \u0394n(t, \u0394t) = n(t) - N(t-1, \u0394t) where N(t-1, \u0394t) is the average search volume over the previous \u0394t weeks.",
    "The best-performing Google Trends strategy achieved a profit of 326% using the search term 'debt' with \u0394t = 3 weeks during the period from January 2004 to February 2011.",
    "The researchers quantified financial relevance by calculating the frequency of each search term in the online edition of the Financial Times from August 2004 to June 2011, normalized by the number of Google hits for each search term. They found a positive correlation between this financial relevance indicator and trading returns (Kendall's tau = 0.275, z = 4.01, N = 98, p < 0.001).",
    "Strategies based on U.S. search volume data performed better because investors prefer to trade on their domestic market. The researchers found that U.S.-only search data better captured the information gathering behavior of U.S. stock market participants than worldwide data, with mean returns of 0.60 vs 0.43 standard deviations above random strategies (t = 2.69, df = 97, p < 0.01).",
    "The 'buy and hold' strategy yielded 16% profit, equal to the overall increase in the DJIA from January 2004 to February 2011. The 'Dow Jones strategy', which used changes in stock prices instead of search volume data, yielded only 33% profit with \u0394t = 3 weeks, or 0.45 standard deviations above random strategies when averaged over \u0394t = 1 to 6 weeks.",
    "The researchers tested each component separately: a long-only strategy (buying after search volume decreases) achieved mean returns of 0.41 standard deviations above random (t = 11.42, p < 0.001), and a short-only strategy (selling after search volume increases) achieved 0.19 standard deviations above random (t = 5.28, p < 0.001). Both components significantly outperformed random strategies.",
    "The researchers used Herbert Simon's model of decision making, suggesting that Google Trends data and stock market data reflect two subsequent stages in investors' decision-making process. They proposed that trends to sell at lower prices are preceded by periods of concern, during which people gather more information about the market, reflected by increased Google search volumes for financially relevant terms.",
    "The researchers analyzed 98 search terms. They included terms related to the concept of stock markets, with some terms suggested by the Google Sets service, a tool which identifies semantically related keywords. The set was not arbitrarily chosen as they intentionally introduced some financial bias in their selection.",
    "The researchers averaged over three realizations of each search term's time series, based on three independent data requests made on consecutive weeks (April 10, 17, and 24, 2011). They noted that search volume data changes slightly over time due to Google's extraction procedure, so this averaging approach addressed the variability across different access dates.",
    "The Google Trends strategies significantly outperformed random investment strategies with a mean return of 0.60 standard deviations above the random strategy mean (t = 8.65, df = 97, p < 0.001, one sample t-test) for U.S. search data, and 0.43 standard deviations (t = 6.40, df = 97, p < 0.001) for global search data.",
    "2nd January 2007 until 31st December 2012",
    "891,171 different words",
    "0.074",
    "Bank of America (BAC) with a correlation of 0.43",
    "5 am London time",
    "9:30 am to 4 pm New York time (for most of the year, 2:30 pm to 9 pm London time)",
    "Travelers replaced Citigroup on 8th June 2009",
    "One day before the news (lag -1) and on the same day as the news (lag 0)",
    "0.040",
    "No significant correlation was found (median correlation coefficient = 0.000, p = 0.784)",
    "The width D of the stripe at time scale T is defined as D(T) = (L/T - 1)^(-1) * \u03a3|P_T(t_{k+1}) - P_T(t_k)|, which is the average of the absolute value of price increments at time scale T.",
    "The study analyzed 9 stocks from the London Stock Exchange during 2002 (251 trading days). The stocks were: AstraZeNeca (AZN), British Petroleum (BP), GlaxoSmithKline (GSK), Heritage Financial Group (HBOS), Royal Bank of Scotland Group (RBS), Rio Tinto (RIO), Royal Dutch Shell (SHEL), Unilever (ULVR), and Vodafone Group (VOD).",
    "The average Hurst exponent was 0.44, which is less than 0.5, indicating negative correlation and antipersistent behavior in price increments.",
    "The memory effect disappears at time scales larger than 180 seconds for resistances and 150 seconds for supports. The statistical significance (p-value < 0.05) was maintained up to 60-90 seconds for both supports and resistances.",
    "The time between consecutive bounces (t) followed a power law distribution, while the maximum distance (d) was best described by an exponentially truncated power law. For example, at 60 seconds time scale: N \u221d t^(-0.56) for time and N \u221d d^(-0.61) exp(-0.03 d) for distance.",
    "They modeled bounce events as a Bernoulli process and used Bayesian inference to estimate p(b|b_prev). The expected value is E[p(b|b_prev)] = (n_b_prev + 1)/(N + 2) and the variance is Var[p(b|b_prev)] = [(n_b_prev + 1)(N - n_b_prev + 1)]/[(N + 3)(N + 2)\u00b2].",
    "The researchers found no evidence of highly preferred prices in their histograms of local minima/maxima. When comparing histograms of support and resistance levels with overall price level histograms, they observed no anomalies or significant excess around round numbers across all stocks and time scales investigated.",
    "The conditional bounce probabilities for actual stock data were well above 0.5 and increased with the number of previous bounces, while shuffled series showed probabilities near 0.5 that remained nearly constant. This difference was at least one order of magnitude larger than any bias from finite stripe effects, providing evidence for memory effects in the original data.",
    "The researchers used two main statistical tests: (1) A chi-squared test with 3 degrees of freedom to test the independence hypothesis p(b|b_prev) = c, using a significance level \u03b1 = 0.05, and (2) A Kolmogorov-Smirnov test to assess whether bounce frequencies from reshuffled series were compatible with the posterior distribution found for bounce frequency.",
    "Technical analysis assumes that 'history repeats itself' because price trends reflect market psychology, and since the psychology of investors does not change over time, investors will always react in the same way when they encounter the same conditions. This leads to the belief that patterns that anticipated specific trends in the past will do the same in the future.",
    "The three components are economic cost C_F, social cost C_S, and environmental cost C_E.",
    "The environmental cost C_E is directly proportional to capacity c through the relationship C_E = \u03b3ct, where \u03b3 = abc and represents conversion constants linking capacity to pollutant, pollutant to impact, and impact to cost.",
    "The two conditions are strong sustainability (when demand is met with no substitution) and weak sustainability (when demand is met via substitution). In strong sustainability, the base activities alone fulfill the demand, while weak sustainability allows for replacement activities when base activities fail to meet constraints.",
    "In the weak case, the set of all sustainable activities forms a subset of an N-level union of sustainable activities and creates a topological cover.",
    "The three conditions are: (1) C'_i,j,... \u2264 C'_i,j,...^max for all cost components, (2) t'_i,j,... \u2264 t'_i,j,...^max for duration constraints, and (3) either strong sustainability where D = \u03a3c\u00b9_i (base activities meet demand) or weak sustainability where demand is met through substitution via a topological cover.",
    "The five requirements are: (1) satisfying human food and fiber needs, (2) enhancing environmental quality and natural resources, (3) making efficient use of non-renewable and on-farm resources, (4) integrating natural biological cycles and controls, and (5) sustaining economic viability while enhancing quality of life.",
    "The fundamental challenge is that social costs are difficult to quantify and assign monetary values to, making it problematic to connect the social dimension analytically to the other dimensions. The paper notes this is easier to handle at the local level where consensus is more achievable.",
    "The paper addresses higher-order dependencies by acknowledging that activities have cascading effects beyond their immediate scope, similar to Life Cycle Assessment. The bike trip example illustrates this: riding a bike requires eating an apple, which supports an apple farmer, who uses trucks for transport that emit pollution, creating indirect environmental costs.",
    "Zachary acknowledges that while economic values can be determined from markets and environmental costs from measurements and models, social costs will continue to be difficult to quantify. Many theoretical terms like 'balance', 'restrictions', 'maintaining', and 'controlling' are inherently difficult to quantify, and some approaches like Socio-Biological don't lend themselves easily to quantitative models.",
    "The eight classifications are based on combinations of: duration constraint satisfaction (t < t_max: yes/no), cost constraint satisfaction (C < C_max: yes/no), and activity complexity (single/multiple levels). This creates 2\u00b3 = 8 possibilities ranging from unsustainable single-level activities that meet neither constraint to sustainable/potentially sustainable multiple-level activities that meet both constraints.",
    "The proposed system achieves a binary pattern modulation speed of 97 kHz, which is about 5 times faster than the fastest DMD's maximum rate of 20.7 kHz.",
    "The final modulation frequency is given by \u03b4 = (Fg \u00d7 \u0394x)/(b \u00d7 \u03b4), where Fg is the scanning frequency of the galvanic mirrors, \u0394x is the scanning range of the beam on the DMD, b is the binning number of DMD mirrors in one direction, and \u03b4 is the size of each micro-mirror of the DMD.",
    "The authors achieved 42 Hz frame rate at 80 \u00d7 80-pixel resolution for dynamic scene imaging. They used a compressive sensing based algorithm incorporating both spatial and temporal prior constraints with the optimization function min(\u03bb\u03a8(Xt) + \u03a6(Xt - Xt-1)).",
    "Using high-end galvanic mirrors like the CTI 6200H with 1 kHz working frequency, the modulation speed could reach 485 kHz, which would enable 210 frames per second at 80 \u00d7 80-pixel resolution.",
    "As k increases, the imaging quality degrades because successive scanned sub patterns from a high resolution random pattern are not entirely independent from each other, which mathematically degenerates the reconstruction performance. However, even with large k values and noise, the results still restore decent images.",
    "The relationship is yi - yi-1 = \u03a3u,v Pi(u,v)\u0394X(u,v), where \u0394X(u,v) = X(u,v) - X(u-1,v). This shows that the difference between consecutive measurements can reconstruct the horizontal edges of the scene, reducing the required patterns by 50% compared to previous methods.",
    "The DMD is a Texas Instrument DLP Discovery 4100, 7XGA with 1024 \u00d7 768 pixel resolution, 13.6 \u03bcm micro-mirror size, and maximum 20 kHz projection rate. The galvanic mirrors are GVS011 from Thorlabs, single axis scanning devices operating at 200 Hz.",
    "The relationship is x = d sin(2\u03b8), where x is the distance from the beam's hitting position at GM1 to that at the DMD, d is the distance between the two galvanic mirrors (12 cm), and \u03b8 is the rotating angle. For small scanning ranges, this can be approximated as p = p0 - (2dkU)/\u03b40.",
    "Using acoustic optical deflectors (AOD) with 20 kHz scanning frequency could achieve 20 times faster illumination patterning, reaching 9.5 MHz modulation speed.",
    "The main limitations are: (1) the system needs careful mechanical mounting for calibration, though this can be addressed with customized programmable mounts, and (2) the scheme currently only works for random patterns and is inapplicable for other structured patterns like Hadamard and sinusoidal patterns.",
    "The method combines structured event extraction using syntactic parsing, Restricted Boltzmann Machines (RBMs) for pre-training, and sentence2vec framework to achieve effective event embeddings.",
    "The model uses an estimated true label sgn(Pi - P0) shared across all data sources in the hinge loss functions, where Pi is the probability for multi-source information on day i and P0 is a threshold parameter to determine prediction positiveness.",
    "The three levels are: super group level loss (log-likelihood), group level loss (temporal consistency between consecutive days), and instance level loss (hinge losses for each data source).",
    "News events contributed most to the overall prediction, followed by quantitative data in second place, while sentiments had the least impact among the three sources.",
    "M-MI improved F1-score by 6.9% in 2015 and 9.2% in 2016 compared to the nMIL baseline.",
    "The justification is based on the Efficient Market Hypothesis, which suggests that different data sources would keep up-to-date with the latest stock market information and commonly indicate the same sign (index rise or fall), allowing for consensus learning among correlated predictions.",
    "The three-level tree has the core verb as root node, subject and object of the verb as second layer nodes, and their nearest modifiers as child nodes. The core words (verb, subject, object, and their modifiers) are connected together as structure information to represent the event.",
    "F1-scores generally first increase then decrease as history days increase. This is explained by the quick decay of impacts from news, sentiments, and quantitative indices after 2-3 days, making out-of-date information less relevant.",
    "The LDA-S method (an extension of Latent Dirichlet Allocation) is used because it extracts topic-specific sentiments, recognizing that sentiment polarities depend on topics or domains, where the same word can express different sentiments in different contexts.",
    "Pi = \u03b80pm\u2212i + \u03b81pd\u2212i + \u03b82ps\u2212i, where \u03b80, \u03b81, and \u03b82 are source-specific weights for news, quantitative data, and sentiments respectively, with the constraint that \u03b80 + \u03b81 + \u03b82 = 1.",
    "Agricultural land consolidation, rural construction land consolidation, land reclamation, and land development",
    "27 administrative villages covering 28,090 hm\u00b2, representing 32.75% of the total area",
    "RS\u1d62 = \u2211\u2c7c\u2096\u2098 S\u1d62\u2c7cH\u1d62\u2096 X\u2c7c\u2096E\u2096\u2098, where j is the source of risk, k is the habitat type, m is the ecological receptor type, S\u1d62\u2c7c is density of risk sources, H\u1d62\u2096 is habitat abundance, X\u2c7c\u2096 is exposure coefficient, and E\u2096\u2098 is response coefficient",
    "Number of input nodes: 10, number of output nodes: 30, number of iterations: 1000",
    "Landscape pattern (0.01 to 1.62) and soil (0.01 to 1.46) show the highest ecological risk values",
    "D\u1d62\u2c7c = w\u209b \u00b7 D\u02e2\u1d62\u2c7c + w\u2090 \u00b7 \u221a\u2211\u1d30d=1 w\u2090 \u00b7 (a\u1d48\u1d62 - a\u1d48\u2c7c)\u00b2, where D\u1d62\u2c7c is mixed distance, D\u02e2\u1d62\u2c7c is geospatial distance, and w\u209b and w\u2090 are geospatial and attribute space weights respectively",
    "Ecological risk weight of land consolidation: 0.4, time urgency weight: 0.3, spatial suitability weight: 0.3",
    "Soil quality, meteorological conditions, water quality, and farmland conditions",
    "Area C exhibits the lowest ecological risk levels across all aspects, indicating potential advantages for agricultural development and superior water resources",
    "h\u1d62\u2c7c = e^(-D\u1d62\u2c7c/2\u03c3\u00b2), where h\u1d62\u2c7c is the proximity function, D\u1d62\u2c7c represents the distance between neurons, and \u03c3 is the diffusion parameter of the Gaussian function",
    "Graph theory extends to Leonhard Euler in the 18th century.",
    "Relational algebra grew out of a need to efficiently compress data during the 1960s, when storage was both limited and very expensive.",
    "Graphs are expressed in node-arc-node (subject-predicate-object) triples.",
    "The parallel-processer-based graphics processing unit (GPU) offers hardware alternatives to accelerate large-scale graph processing, and some firms, such as Cray, have developed specially configured supercomputers to digest and return rapid results from massively scaled graphs.",
    "When the preponderance of relationships becomes many-to-many, RDBMS performance takes a nosedive. Moreover, the RDBMS schema is typically inflexible, requiring high maintenance to effect the most minute change.",
    "The hierarchical model was created at IBM to represent tree-structured relationships, and the network model of the late 1960s was an early attempt to model objects and their relationships, which would re-emerge in the 1980s with object-oriented databases.",
    "There is little semantic commonality among the various graph languages in use and their rules of syntax. Higher volume graph databases rely on stylized variations of RDF to enumerate their triples, making sharing data between various graph databases dependent on the user's tolerance for expressing the same triples in differing syntactical frameworks.",
    "To alleviate storage consistency concerns, many graph databases do support the Atomic, Consistent, Isolated, and Durable (ACID) consistency model, which is a spin-off storage-locking scheme from RDBMS technology.",
    "Graphs can be intelligently reduced to more salient subgraphs that can be better managed, queried, and understood. This reinforces the practice of persisting data in a relational or appropriate nongraph NoSQL environment, from which subgraphs (database 'views') can be intelligently isolated for further analysis.",
    "Built-in mathematical functions, residing in graph databases, could add a level of depth to truly understanding and quantifying graph relationships. Metric algorithms derived from graph theory could be applied as analytical tools, extending beyond mere networks to networks of networks.",
    "Mars-like pressure was set at 8 mbar while Earth-standard pressure was set at 1000 mbar.",
    "The relative errors were 6.79% for sandy soil, 7.68% for intermediate soil, and 7.76% for bedrock soil.",
    "The maximum difference between surface and subsurface temperature increased by 26.72%, from 11.6\u00b0C at Earth's pressure to 14.7\u00b0C at Mars' pressure.",
    "I_sin = (\u0394G_s/\u0394T_s)\u221a(2\u03c0/P), where \u0394T_s = T_max - T_min, \u0394G_s = G_max - G_min, and P is the diurnal period.",
    "An anti-reflection coated germanium circular optic with 74.9 mm diameter and 5.0 mm thickness was chosen to comply with the minimum thickness required to avoid reaching germanium's fracture strength caused by the pressure differential.",
    "A total of 9,225 radiometric images were collected and saved as plain text 640 \u00d7 480 matrices with each cell containing the temperature in degrees Celsius.",
    "k = k_r + k_c + k_g, where k_r is transfer across pore spaces, k_c is conduction between grain contact areas, and k_g is conduction of gas filling pores between grains. Gas conduction (k_g) dominates at pressures between 0.1 and 1000 mbar.",
    "At Earth's pressure, the relative difference between highest and lowest thermal inertia soils was only 4.20%, but at Martian pressure, this difference increased significantly to 42.84%, indicating soils can be better assessed at Martian pressure.",
    "The PI-640i is a 320-g LWIR camera working in 8\u201314 \u03bcm spectral range, with 640 \u00d7 480 pixel resolution, 60\u00b0 \u00d7 45\u00b0 FOV, germanium optic, measuring temperatures from \u221220\u00b0C to 900\u00b0C with 0.04\u00b0C thermal sensitivity.",
    "G = \u2212I\u221a(\u03c0/P) \u2202T/\u2202Z'|_{Z'=0} = \u03b5\u03c3_B T^4_{heater} \u2212 \u03b5\u03c3_B T^4_{s_i}, where T_{heater} represents MEC heater temperature and T_{s_i} represents mean surface temperature of each soil sample.",
    "The two fundamental problems are: (1) Sign ambiguity - singular vectors with similar information are randomly distributed in two areas due to random variable pk \u2208 {1, -1}, and (2) Manifold features - singular vectors are unit vectors with norm 1 that exist as points on a unit hypersphere (manifold), making them inefficient to learn with generic neural networks based on Euclidean geometry.",
    "SVP achieves approximately 36% better performance than GAP under FGSM adversarial attacks on the CIFAR10 dataset with natural training.",
    "Sign ambiguity is removed by aligning singular vectors based on a center vector that is rotated to {0, 0, ..., 0, 1, 0}. The method verifies which hyper-sphere a singular vector belongs to according to the sign of \u0169HW-1,k and multiplies vectors on the negative half-sphere by -1 to align them.",
    "The arccosine function's first derivative can easily diverge as d/dz arccos(z) = -1/\u221a(1-z\u00b2) for -1 < z < 1. To prevent this instability during learning, the arccosine is approximated as -z + \u03c0/2 using first-order Taylor expansion.",
    "KL-divergence is used to learn the center vector uc. The method assumes that each component of the aligned \u016b features follows a Gaussian distribution, and the center vector is learned so that this assumption holds true by minimizing the KL-divergence between the actual distribution and a Gaussian distribution.",
    "SVP achieves a silhouette score of 0.692, which is 0.444, 0.510, and 0.551 better than GAP (0.248), GMP (0.182), and MPN (0.141) respectively. The SVP silhouette score is more than twice as high as other pooling methods.",
    "Rotation centers the singular vector distribution in the spherical coordinate system by rotating the center vector vc to {0,...,0,1,0} using Rodrigues rotation. This prevents singular vectors from being located near discontinuity boundaries where components are bounded ([0,\u03c0] for \u03c61 to \u03c6N-2 and [-\u03c0,\u03c0] for \u03c6N-1), ensuring the shortest path coincides with the real path during learning.",
    "KD-SVP achieves a performance improvement of 1.69% over the original KD-SVD method on the CIFAR100 dataset (improving from 71.64% to 73.33% accuracy).",
    "When the sign ambiguity removal step (step 2) is omitted, learning becomes impossible. The ablation study shows that this step is indispensable for proper learning of singular vectors, as indicated by the 'NaN' result in the experimental plot.",
    "SVP has higher computational complexity than general pooling methods due to the SVD computations required. However, it has lower forward time than MPN (matrix power normalized covariance pooling), which is a lightweight second-order pooling method. The burdensome SVD computations may make it difficult to apply directly to embedded systems or mobile environments.",
    "1 MHz A-scan rate",
    "SLD-371-HP3 superluminescent diode with center wavelength of 838 nm, full-width at half-maximum of 81 nm, and optical power of 27.2 mW",
    "8 vol/s for an image range of 250 \u00d7 250 \u00d7 2048 pixels (9 \u00d7 4.5 \u00d7 5 mm)",
    "Protective film: 100 \u03bcm, transparent film: 250 \u03bcm, deco film: 150 \u03bcm, base film: 100 \u03bcm",
    "139 dB for camera #1 and 137 dB for camera #2",
    "C++, CUDA, and Qt",
    "By using identical but reversed trigger sequences of 250 kHz with 50% duty cycle to each camera, capturing two continuous A-lines in one period, guaranteeing 500 kHz effective imaging rate which is twice the maximum sampling rate",
    "8 seconds for the whole-range scanning",
    "40 \u03bcm, 20 \u03bcm, and 40 \u03bcm gaps between the layers",
    "2048 pixels resolution (e2v OCTOPLUS, TELEDYNE e2v, U.K.)"
  ],
  "predicted_answers": [
    "The two main components of the Gander research project were: 1) an empirical component focused on problem conceptualization through mixed-method studies with industry practitioners, and 2) a development component focused on creating an experimental code review platform incorporating eye-tracking.",
    "The three categories are employees/stakeholders, students/beneficiaries, and independent participants. They are distinguished based on legal and ethical concerns related to the researcher-participant relationship, data openness, and potential pressures on participation.",
    "The four steps of the data pipeline are: (1) Data cleaning, (2) Data exploration and visualization, (3) Model building and analysis, and (4) Findings presentation. \n\nIn a quantitative study, data cleaning involves transforming data for statistical tools and anonymizing it. In a qualitative study, it includes transcribing data and anonymization. Data exploration and visualization involves investigating descriptive statistics and creating visual representations, while model building and analysis involves qualitative codes, coding, obtaining findings, and potentially theory building. Finally, findings are presented in publications, reports, or supplementary material.",
    "The framework defines three steps of artifact generalization for increasing reuse:\n1.  Publication for reproduction, resulting in artifacts in their original, non-editable state.\n2.  Generalization for continued development, resulting in artifacts released with guidance on adaptation, such as inviting community involvement.\n3.  Generalization for general use, resulting in artifacts in an editable state.\n\nEach of these steps requires additional investments in making the artifacts openly available, including transparency through accessibility, and governance efforts for community building.",
    "The risks identified in the paper regarding sharing qualitative interview data from the Gander project include potential exposure of company secrets, sensitive information about individuals (such as criticism of managers), and concerns about the epistemological value of the data due to a lack of contextual understanding.",
    "R3 recommends sharing project data and artifacts as an illustrative example and deriving preliminary recommendations for SE researchers.",
    "The Gander project encountered a licensing issue when releasing its platform as open-source, specifically regarding a project used for gaze data analysis which initially lacked a license. This was resolved by contacting the author of the project and obtaining a MIT license, subsequently selecting a BSD license for the overall open-source project.",
    "The Gander platform incorporates eye-tracking to trigger assistance in code review tools by analyzing gaze fixation points on code elements. A proof-of-concept assistant was developed on the platform that visualizes use-declaration relationships based on gaze data.",
    "Quantitative survey data is generally easier to share openly due to larger participant pools and the less detailed nature of responses compared to qualitative interviews. The authors recommend publishing study and analysis artifacts, such as study protocols, interview guides, interviewee descriptions, and code books from thematic analysis, rather than directly sharing qualitative data, particularly from companies.",
    "The authors highlight the need to balance open science principles with concerns regarding data privacy, company secrets, and commercial interests, specifically noting the conflict between anonymity and confidentiality alongside openness.",
    "The motivation behind proposing a training-free performance indicator for NAS is to select high-performance potential networks without any training, and to address the limitations of relying on a single indicator that may not capture all aspects of a network\u2019s performance.",
    "The three types of features considered by the proposed feature fusion-based indicator (FFI) are CJ, NLR, CNNTK, and OS. CJ captures the performance of the network based on contrastive Jacobian, NLR captures the network\u2019s performance based on normalized layer rank, CNNTK captures the network\u2019s performance based on convolutional network token, and OS captures the network\u2019s performance based on output statistics.",
    "The feature fusion mechanism in FFI combines different features by using combinations of indicators, such as f CJ, OS g for FI2, f CJ, CNNTK, OS g for FI3, and f CJ, NLR, CNNTK, OS g for FI4. This is necessary to leverage the strengths of multiple indicators and potentially improve performance beyond what any single indicator could achieve.",
    "The benchmark datasets used were NAS-Bench-101 and NAS-Bench-201. NAS-Bench-101 contains 423k architectures trained on CIFAR-10, and NAS-Bench-201 extends it with more operators, datasets (CIFAR-100 and ImageNet16-120), and a cell-based structure.",
    "The proposed FI outperforms standalone indicators in all performance assessment methods for all datasets on NAS-Bench-201, achieving higher SROCC and KROCC values compared to OS, CJ, NLR, and CNNTK. Specifically, FI3 has 7.43% and 8.85% SROCC and KROCC improvement over OS, and FI4 reaches its peak with 0.88 and 0.7017 SROCC and KROCC.",
    "Kendall\u2019s Tau correlation and Spearman Rank-Order Correlation Coefficient (SROCC) and Kendall Rank-Order Correlation Coefficient (KROCC) were used to evaluate the indicator\u2019s performance. Pearson Linear Correlation Coefficient (PLCC) and Root Mean Square Error (RMSE) were also used. These metrics measure prediction monotonicity and linear correlation, respectively.",
    "The ablation studies conducted involved comparing the performance of the proposed Fusion Indicator (FI) and standalone indicators (CJ, NLR, CNNTK, OS) on CIFAR-10 of NASBench-101. Specifically, the studies examined various combinations of the FI with two, three, and four of the standalone indicators (FI2, FI3, and FI4).",
    "The context does not directly compare the computational cost of FFI to training-based NAS methods.",
    "The authors investigated the robustness of FFI (Fusion Indicator) across different search spaces, noting that it can replace conventional NAS algorithms. However, prior works using a single indicator had limited performance due to its inability to represent various network aspects like trainability or expressivity. The paper introduces a fusion framework that harmonizes multiple indicators, each with learned weights, to measure network performance effectively.",
    "The authors encourage researchers to discover more network characteristics to develop a powerful training-free indicator. They plan to develop an efficient search method using the proposed FI in future work, and suggest fusing multiple indicators with learned weights.",
    "The main motivation for developing a hybrid gaze distance estimation method is to overcome the limitations of existing approaches that rely solely on binocular vergence angles, which are prone to errors caused by human factors and environmental conditions.",
    "The two primary cues integrated in the proposed hybrid method are vergence and gaze-mapped depth. They complement each other by cross-referencing the values estimated by each method to produce a more precise final estimation, mimicking human visual perception.",
    "The experiment involved twelve participants and a setup with three gaze targets at varying distances, including a reflective laptop screen.",
    "The proposed hybrid method cross-references vergence and gaze-mapped depth estimations to produce a more precise final estimation, mimicking human visual perception. It weights each cue based on its confidence, mitigating noise from human errors and environmental factors.",
    "The hybrid method significantly outperformed traditional single-method approaches, achieving a visual angle error of 0.132 degrees under ideal conditions. It consistently maintained robustness against human and environmental errors, with an error range of 0.14 to 0.21 degrees even in demanding environments.",
    "Calibration was performed by arranging the experimental setup with three gaze targets at different distances and utilizing a chin rest to prevent unintended gaze distance variations.",
    "Developing user gaze distance estimation modules for varifocal AR/VR devices, digital healthcare, human-computer interaction, and analyzing user behavior and preferences.",
    "The authors acknowledge that qualitative data are tightly linked with context, making them meaningless when the context changes. They also note methodological and ethical concerns regarding confidentiality and anonymity that prevent open data sharing.",
    "The hybrid method combines vergence-angle-based and gaze-mapped depth approaches to improve accuracy and stability, addressing limitations of traditional methods. It exploits cross-referencing the gaze distance from vergence with that obtained from gaze-mapped depth.",
    "The authors suggest applying the proposed hybrid algorithm to developing user gaze distance estimation modules for varifocal AR/VR devices, aiming for an ideal mixed reality experience without vergence-accommodation conflict. Additionally, they propose potential applications in digital healthcare, human-computer interaction, and analyzing user behavior and preferences.",
    "The four quality attributes used are availability, reliability, data integrity, and time efficiency. Availability is calculated based on the rate of accepted requests, reliability is calculated based on the rate of successful requests, data integrity is calculated based on the rate of successful requests and data failures, and time efficiency is calculated based on the rate of promised and actual execution time.",
    "The proposed model aims to balance trust, estimated through availability, reliability, data integrity, and efficiency, with communication delay in cloud resource allocation. The objective is to maximize trust while minimizing delay by jointly optimizing these factors.",
    "The Genetic Algorithm (GA) is proposed as a heuristic for solving the resource allocation problem, particularly suitable for large-scale instances where formal methods are limited. It operates by generating a population of solutions, evaluating their fitness (based on trust and delay), and iteratively improving the population through selection, crossover, and mutation.\n\nThe GA's performance is compared to the exact optimized solution and shows that it achieves nearly the same objective values (maximizing trust and minimizing delay) while exhibiting a faster execution time, especially in large scenarios.",
    "Availability is affected by varying wind conditions, with the loss of wind generation compensated by the ramping of diesel generator output. The short-term resilience follows the system frequency, decreasing as the frequency falls. Long-term resilience is reduced over time based on available fuel, fuel consumption, and maximum generator outputs.",
    "The answer is not available in the provided context.",
    "The proposed model aims to balance trust with communication delay in cloud resource allocation.",
    "HELICS, an open-source co-simulation platform, was used. It coordinates off-the-shelf simulators, including GridLAB-D and Python.",
    "The context discusses a model that balances trust and delay in cloud resource allocation, aiming to maximize trust while minimizing communication delay.",
    "The experiments were executed in multiple scenarios for testing the approach, using CloudSim for cloud data collection and MATLAB for resource optimization. The model was run on an Intel Core i7 machine with a 4 GHz processor, with GA population and generation sizes set to specific values.",
    "The authors propose a conceptual framework and recommendations for open data and artifacts in empirical software engineering, aiming to guide future open science in the field.",
    "The primary motivation for studying 28Si single crystals in the context of the kilogram realization is to quantify the mass deficit caused by vacancy defects and improve the accuracy of the XRCD method.",
    "Electron paramagnetic resonance (EPR) measurements were used to increase the reliability of mass deficit correction in the XRCD method, identifying phosphorus impurity and reporting low concentrations of nine types of vacancy defects.",
    "The nine types of vacancy defects were estimated to have a mass deficit correction of 0.0(2) \u03b8 g for 1-kg AVO28 spheres.",
    "Isotopic enrichment in 28Si influences EPR spectral resolution by reducing linewidths. Specifically, the anisotropic EPR signals of vacancy defects are narrower than those observed in natural isotope ratio crystals due to the symmetry of the silicon crystal.",
    "The EPR measurements were conducted in the presence of a magnetic field from 332 to 341 mT, and the sample was rotated about the [0\u030411] direction at angles of 0\u00b0, 30\u00b0, 60\u00b0, and 90\u00b0. A quartz glass rod was used to guide light from a halogen lamp to the sample, which illuminated the silicon crystal.",
    "Quantification of oxygen-vacancy defects is critical for kilogram realization because the mass deficit due to these defects must be corrected in the XRCD method. The estimated mass deficit for 1-kg AVO28 spheres is 0.0(2) \u03b8 g, based on EPR measurements.",
    "The concentration of phosphorus impurity in the 28Si crystal was 3.2(5) \u00d7 10 12 cm -3.",
    "Phosphorus donor concentrations affect the silicon sample by causing a decrease in the number of charge carriers. Specifically, unpaired electrons of the phosphorus donors transfer to other compensating defects in the sample in a dark environment, hiding the EPR signals of phosphorus. Under illumination, some of these unpaired electrons transfer back to phosphorus impurity defects, recovering the EPR-active state.",
    "28Si single crystals offer advantages for EPR studies by providing quantitative measurements, increasing the reliability of mass deficit correction in the XRCD method. Specifically, they identify phosphorus impurity and report low concentrations of nine types of vacancy defects.",
    "The authors propose a model to quantify trust in cloud resource allocation, considering availability, reliability, data integrity, and efficiency to balance trust with communication delay.",
    "The primary causes of process-induced random variation are line edge roughness (LER), random dopant fluctuation (RDF), and work function variation (WFV). Among these, LER is considered the most severe because it can induce deformation of the device structure, thereby degrading the performance of other random variation sources.",
    "The three parameters used to characterize line-edge roughness (LER) are amplitude, correlation length x, and correlation length y. Amplitude indicates the root-mean-squared value of surface roughness, while correlation length x and y represent how closely the line edge is correlated with its neighboring edge in the x and y directions, respectively.",
    "The datasets for training and validating the ANN model were generated from 6,500 and 2,500 simulated FinFETs, respectively, with 70% of the first dataset used for training and 30% for validation, and 100% of the second dataset used for validation. The LER parameters were chosen within a voltage range of 1 nm to 8 nm for amplitude, 10 nm to 100 nm for correlation length X, and 20 nm to 200 nm for correlation length Y.",
    "The evaluation method used was the Earth-mover\u2019s distance (EMD) metric, also referred to as the Wasserstein metric. It quantifies the difference between probability distributions by measuring the minimal amount of work needed to transform one distribution into another.",
    "The optimized ANN with the mixture of MVNs has 3 neurons for the input layer, 81 neurons for the first hidden layer, 162 neurons for the second hidden layer, 324 neurons for the third hidden layer, and 324 neurons for the output layer.",
    "Negative log likelihood (Negloglik) was used as the loss function because conventional mean-squared error cannot be used with a probabilistic layer returning a PDF.",
    "The mixture-MVN ANN successfully predicts skewness, kurtosis, and non-linear correlation, which is distinctly different from plain MVN.",
    "The time spent to train the ANN model was reduced largely, with the simple ANN taking 185 seconds compared to 1412 seconds for the non-separated ANN.",
    "The proposed ANN models shortened the simulation time by six times compared to the previous ML-based model. The ANN models expanded the prediction target to include seven transistor parameters, compared to four in the previous model.",
    "The ANN-based approach shortens simulation time six times compared to a previous ML-based model and successfully predicts non-Gaussian device performance metrics. It also expands the prediction target to include seven transistor parameters, enabling simulation of electrical behavior and DC behavior of digital circuit blocks like SRAM bit cells.",
    "As CMOS technology scales down, line-edge roughness (LER) becomes a more significant source of variation due to its impact on other random variation sources and its exacerbated effect in complex device architectures like FinFETs.",
    "The three main parameters used to generate 3D LER profiles in the simulations are RMS amplitude, x-axis correlation length, and roughness exponent. These parameters physically represent the standard deviation of LER amplitudes, the wavelength of the LER profile, and quantitatively indicate the diminishing of high-frequency components in the LER profile, respectively.",
    "The dataset for training and testing the HS-BNN comprised FinFET electrical characteristics affected by process-induced line-edge roughness. The dataset included device structures with Lg D 14 nm, Wn D 7 nm, and Hn D 30 nm, and was evaluated using K-fold crossvalidation.",
    "The target variables predicted by the HS-BNN were GLYPH<22> IDS and GLYPH<27> IDS. The input features used were the number of nodes, LER profiles, and FinFET structures.",
    "Horseshoe priors are incorporated into the HS-BNN to introduce shrinkage and sparsity over the weights, facilitating automatic model selection by identifying the most compact layer size. These priors prevent over-parameterization, particularly when data are limited, by promoting weights towards zero and allowing for large weights to avoid shrinkage.",
    "The HS-BNN model showed improved prediction performance compared to the Bayesian linear regression (BLR) model, as evidenced by reduced MAPEs and RMSEs for GLYPH<22> IDS and GLYPH<27> IDS, with a more significant improvement observed for GLYPH<27> IDS (6.66% vs. 19.59%).",
    "The HS-BNN showed much better results than the Gaussian-BNN when comparing predictive performance, specifically as shown in Table 3 and Figure 5. For the number of nodes over 200, the HS-BNN's prediction for GLYPH<27> IDS showed almost the same results (MAPEs GLYPH<24> 7%), while the Gaussian-BNN showed totally different results for different numbers of nodes.",
    "The HS-BNN predictions were evaluated using K-fold crossvalidation, splitting the data into K sections and iteratively using one section as a test set and the others as a training set.",
    "The context does not specify how HS-BNN predictions for IOFF and VTH varied when LER parameters were changed.",
    "The HS-BNN achieved a significantly reduced simulation time compared to TCAD simulations.",
    "The physics package (PP) design incorporated a rubidium spectral lamp with Xe as the starter gas, optical and isotope double-filtering for light, a large slotted tube microwave cavity, and a rubidium absorption cell with a diameter of 40 mm.",
    "The barometric pressure effect contributed to a 2.2 \u00d7 10 -14 stability over 100 seconds, necessitating a sealed environment to mitigate it. A sealed box was designed to isolate the PP and reduce the barometric effect by nearly one order of magnitude.",
    "The slotted tube microwave cavity enhanced RAFS performance by allowing for a larger absorption cell, which increased the atomic signal-to-noise ratio (SNR). It also provided a more flexible size design compared to traditional TE111 and TE011 cavities.",
    "The experimentally optimized operating temperatures for the absorption cell were 68 \u00b0C, the filter cell was 93 \u00b0C, and the lamp bulb was 109 \u00b0C.",
    "The estimated stability of the RAFS was 9.0 \u00d7 10 -14 \u03c4 -1 / 2 with \u03c4 up to 100 s, obtained using the H-maser as the reference.",
    "The phase noise of the 6.835-GHz microwave was measured at -110 dBc/Hz at 2 fM (272 Hz).",
    "The effects of absorption cell temperature, pumping light intensity, microwave power, and magnetic field were also examined. The frequency shift coefficients for these parameters are listed in Table I.",
    "The measured short-term stability using the H-maser was 9.0 \u00d7 10 -14 \u03c4 -1 / 2 (1-100 s), while the stability using the OMG was 9.1 \u00d7 10 -14 \u03c4 -1 / 2 (1-100 s).",
    "Achieving 10\u207b\u00b9\u2074 \u03c4\u207b\u00b9/\u2082 stability represents a significant improvement over the previous best result of 1.2 \u00d7 10\u207b\u00b9\u00b3 \u03c4\u207b\u00b9/\u2082, which was achieved on a pulsed laser-pumped RAFS.",
    "The predicted contributions to overall RAFS stability were SNR limited stability of 4.7 \u00d7 10 -14 \u03c4 -1 / 2, phase noise limited stability of 6.0 \u00d7 10 -14 \u03c4 -1 / 2, and environmental effect limited stability of 9.0 \u00d7 10 -14 \u03c4 -1 / 2.",
    "The three statistical conditions that must be simultaneously satisfied for a group of stocks to be classified as the Leading Temporal Module (LTM) are:\n1.  The absolute value of the correlation matrices (PCC) derived from stocks returns must emphasize the LTM sub-graph.\n2.  The absolute average auto-covariance (AAC) computed on both its members and on the rest of the system must be high.\n3.  The pairwise KS statistics of I LTM vs. the indicators for DFA - and Rest must be 0.95 and 0.99 at 1% significance level, respectively.",
    "The synthetic indicator I LTM t is defined by the mean absolute value of the autocovariance (AC) of the stocks belonging to the leading temporal module (LTM) and the ratio between the correlations of stocks within the LTM and the correlations of stocks outside the leading module.",
    "The average duration that stocks remain continuously in the Leading Temporal Module is approximately 1.5 months.",
    "The LTM indicator\u2019s predictive performance was tested using a percentile threshold, but the specific threshold is not available in the provided context.",
    "As \u03bb\u2081 approaches 1, the variance, covariance, and auto-covariance of the original system variables increase.",
    "The true positive, false positive, false negative, and true negative rates achieved by the LTM-based investment strategy are not available in the provided context.",
    "The I^LTM_t indicator successfully anticipated the following market events and crises during the 2006-2017 study period:\n\n*   The 2008 global financial crisis\n*   The failure of Lehman Brothers in September 2008\n*   The American Recovery and Reinvestment Act of 2009\n*   The European Debt crisis of 2011\n*   The Chinese stock market crisis of 2015-2016",
    "The answer is not available in the provided context.",
    "The study\u2019s approach relates to thermodynamic systems by analogy, using the emergence of market instabilities as similar to nucleation phenomena. The I^LTM_t indicator plays a role analogous to compressibility, reflecting increasing instability near the spinodal lines of a financial system.",
    "The LTM strategy outperformed a simple Buy&Hold strategy over the entire 2006-2017 sample period, according to the table.",
    "The four complementary dimensions identified by Tinbergen for analyzing animal behavior are:\n\n1.  **Function:** This refers to the purpose or role a behavior serves.\n2.  **Mechanism:** This concerns the physiological or computational processes underlying a behavior.\n3.  **Development:** This focuses on how a behavior is acquired or learned.\n4.  **Evolutionary History:** This examines the historical selection pressures that have shaped a behavior over time.\n\nThese dimensions can be adapted for studying machine behavior by considering how machines have mechanisms that produce behavior, undergo development that integrates environmental information, produce functional consequences, and embody evolutionary histories.",
    "Flash crashes exemplify unintended collective machine behavior in financial markets due to algorithmic traders responding to each other and market events faster than human traders, leading to amplified price movements and instability.",
    "The three primary motivations for establishing machine behavior as a scientific discipline are:\n1.  The prevalence of algorithms in society and their increasing role in daily activities.\n2.  The complex properties of algorithms and the difficulty in analytically formalizing their attributes and behaviors.\n3.  The challenge of predicting the societal impact of intelligent algorithms.",
    "The inherent complexity of algorithms and the difficulty in predicting their societal impact make studying machine behavior particularly challenging compared to traditional algorithm development.",
    "Researchers face legal and ethical barriers when studying machine behavior, including potential violations of terms of service when reverse-engineering algorithms and the risk of embroiling researchers in legal challenges due to platform damage.",
    "Machine evolution differs from animal evolution due to the role of designers and the impact of factors like open-source sharing and regulatory constraints. Algorithms are more flexible than animal inheritance, and human environments strongly influence how they evolve.",
    "The three scales of inquiry proposed for studying machine behavior are: individual machines, collectives of machines, and hybrid human-machine systems.",
    "The paper provides examples such as an algorithm that scores probability of recidivism in parole decisions behaving unexpectedly when presented with evaluation data that diverges substantially from its training data. Other examples include the study of individual robotic recovery behaviors, the \u2018cognitive\u2019 attributes of algorithms, and the examination of bot-specific characteristics such as those designed to influence human users.",
    "The paper describes the \u2018black box\u2019 problem as the difficulty in predicting behavior and interpreting the functional processes within AI systems, even when individual algorithms appear simple.",
    "The hybrid human-machine study by Shirado and Christakis demonstrated that locally noisy autonomous agents can improve global human coordination in network experiments.",
    "The standard gravity model is inadequate for describing airline networks due to the presence of transfer flights.",
    "The model proposes that observed passenger flow (f_ij) is composed of two components: the direct passenger flow (f_ij g) predicted by the standard gravity model, and the transfer passenger flow (f_ij transit).",
    "The model correctly predicts more than 98% of the total passenger flow in the world.",
    "The formula used to describe the probability function p(i \u2192 j \u2192 k) is:\n\np(i \u2192 j \u2192 k) =  (f ij g ( ) * xi xj ) / (f ij * r ij * r jk)\n\nwhere:\n\n*   f ij g ( ) is the expected flow between countries i and j\n*   xi and xj are the GDPs of countries i and j\n*   r ij is the distance between countries i and j\n*   r jk is the distance between countries j and k\n*   f ij is the observed flow between countries i and j",
    "The authors chose \u03b1 = 1.5 and \u03b1 = 1.6 for the years 1996 and 2004, respectively.",
    "The paper identifies several historical events as influencing the distance coefficient in air transportation, including:\n\n*   Attacks in New York and Washington D.C. in 2001\n*   The SARS epidemic\n*   Additional terrorist attempts\n*   Wars\n*   Rising oil prices",
    "The authors used Google Trends data from 2004 to 2011.",
    "The answer is not available in the provided context.",
    "The paper analyzes the gravity model in the global passenger air-transport network, demonstrating that the standard model is inadequate for describing the relationship between passenger flows and typical geo-economic variables. It proposes a model for transfer flights to exploit discrepancies and discover hidden subflows in the network.",
    "The authors use a simple RMS formula (\u2206 (\u03b1)) to measure the agreement between their model and empirical data when determining the optimal distance coefficient. This formula is applied to histograms of empirical and modeled flows, comparing the observed data (P ( f ij )) with the modeled flows (P ( f ij mcf )( \u03b1 )).",
    "The researchers identified several specific Chinese textual challenges that make analysis more complex than English-based analysis, including the lack of explicit word boundaries, grammatical marking, and a large number of homophones. These characteristics present difficulties in recognition, segmentation, and analysis compared to English.",
    "The AUC performance of the proposed SLS_L model was better than Lasso-Logistics (L_L) and MCP-Logistic (MCP_L) models.",
    "The SLS_L model\u2019s optimal tuning parameters are not explicitly stated in the context. However, the context discusses Bayesian neural networks (BNNs) employing horseshoe priors, suggesting adjustments to weights as random variables and regularization through the prior distribution.",
    "The keywords identified as having the highest betweenness centrality are \"Short Term\" and \"Pessimistic.\" Betweenness centrality measures the number of times a node (keyword) lies on the shortest path between other nodes in the network, indicating its importance in connecting different parts of the information flow.",
    "The network density of the constructed textual network is 0.1695, indicating that it is a sparse network. This sparsity suggests that there are relatively few connections between the keywords compared to the total possible connections, reflecting a limited degree of interconnectedness within the network.",
    "The context does not contain the requested information.",
    "The study introduces a textual network approach to predict stock market movements, incorporating network connectivity and demonstrating the importance of textual networks. Yi Li et al. discovered a time lag effect, where analysts\u2019 research reports, specifically the coordination of words, affect stock movements with a delay of at least one month.\n\nPrevious research, such as Previts et al., argued that analysts\u2019 reports are important for investors\u2019 information needs, and Asquith, Mikhail, and Au found a positive relationship between downgrade reports and stock market reactions. Twedt and Rees investigated the effect of reports, finding a correlation between textual tone and stock prices.",
    "The proportion of the 56 identified keywords found to be effective predictors is not available in the provided context. Effective keywords were defined as those with non-zero coefficients in at least one of the three models (MCP_L, SLS_L, and L_L).",
    "The researchers discovered that \u201cShort Term\u201d keywords, particularly \u201cShort Term,\u201d had a significant impact on stock market predictions. Specifically, the word \u201cShort Term\u201d held the highest betweenness centrality (287.89) within the network of research reports, indicating its central role in the flow of information and its influence on market interactions.",
    "The keyword vector extraction process involved syntactic analysis to extract the main structure information of sentences, followed by using an RBM for pre-training, then sentence2vec for final event representations.",
    "The computational complexity of the route pruning algorithm, specifically using Algorithm 1, is O(L^3), where L is the number of locations. The initial step of computing pairwise distances with batched shortest path algorithms (MLD or CH) has a complexity of O((|EG| + LG)L), where EG is the set of edges and LG is the number of links in the graph.",
    "The parameter \u03b2 controls the degree of pruning in the algorithm, balancing the quality of the resulting graph versus its complexity.",
    "In three of the four case studies, F1-scores exceeding 0.9 were achieved. The regions that exceeded 0.9 F1-scores were the federal state of Styria in Austria, the border region between Germany and Austria, and the Central African Republic.",
    "The paper uses batched shortest path algorithms, such as MLD or CH, for computing pairwise distances between locations in the first step. In the second step, Algorithm 1 is used for pruning these paths.",
    "The triangle inequality principle is used to identify indirect routes by comparing the distance of a direct route to the sum of the distances of the intermediate routes. If the sum of the intermediate routes is less than the direct route, the indirect route is pruned from the graph.",
    "What specific problem occurred with the route between Kolbermoor and Prien am Chiemsee in the German-Austrian border region?\nThe route between Kolbermoor and Prien am Chiemsee in the German-Austrian border region experienced challenges in creating a ground truth due to ambiguous situations, potential sources of error (such as small locations or refugee camps not being marked explicitly in OSM), and the possibility of indirect routes being incorrectly identified as direct.",
    "The study produced location graphs using a two-step approach: computing pairwise distances with batched shortest path algorithms and then pruning with Algorithm 1. Optimal \u03b2 values (0.9-0.95) yielded high F1-scores and acceptable runtime, even for large datasets like those in Europe, Africa, and North America.\n\nThe study observed that small \u03b2 values resulted in strong pruning, while large \u03b2 values led to conservative pruning. The highest F1-scores were achieved with \u03b2 values between 0.9 and 0.95, though the optimal value depended on the road network and location distribution.\n\nThe researchers noted that the resulting location graphs sometimes differed from manually created ground truths, particularly for long routes, due to the multiplicative factor \u03b2. They also suggested that adding an additive factor instead of \u03b2 could mitigate these inaccuracies.\n\nThe study demonstrated that the location graphs constructed using the two-step approach agreed well with manually created location graphs in three of the four case studies, achieving F1-scores exceeding 0.9 and acceptable pruning runtimes even for thousands of locations.",
    "The Static-Triangle strategy has a time complexity of O(L\u00b7R), while the Iterative-Global strategy has a time complexity of O(R(R+L)logL). Our pruning approach based on the triangle inequality has a time complexity of O(L^3), which is more favorable than the Iterative-Global strategy\u2019s O(L^4 log L) when the triangle inequality holds and ties are ignored.",
    "To validate the algorithm\u2019s performance, Precision, Recall, and F1-score are measured against a manually created ground truth of direct driving routes. The ground truth is created by inspecting if the fastest route between each location pair is direct, labeling a connection as direct if no other location lies on or near the fastest route.",
    "The main applications mentioned that require location graphs are:\n*   Route optimization\n*   Load optimization in electrical and transportation networks\n*   Agent-based modeling applications such as transportation of goods, evacuation models, traffic simulations, disease transmission, movement of people, and migration simulation",
    "The DIRE curve outlines five time scales of resilience: recon, resist, respond, recover, and restore. These represent the phases of a system\u2019s response to a disturbance, from initial recognition to full recovery.",
    "The adaptive capacity of a generation asset is bounded by its operational generation limits, specifically its real and reactive power output capability at any power factor angle \u03b8.",
    "The primary resilience challenge for the St. Mary\u2019s microgrid is fuel availability for the diesel generators, specifically due to the impassable Yukon River during winter months.",
    "The mathematical relationship defining system frequency response to power disturbances is described by the kinetic energy equation, which relates the frequency to the inertial constant and the difference between generation and load. Inertia plays a role by slowing the rate of frequency response to disturbances due to the presence of rotating masses or stored kinetic energy.",
    "The 900 kW, 52-meter rotor diameter type IV pitch-controlled wind turbine was installed by the Alaska Village Electric Cooperative (A VEC) on January 5, 2019, and it started producing power.",
    "Losing a generator negatively impacts short-term resilience due to reduced inertia and ramping capability, but positively affects long-term resilience by conserving fuel.",
    "HELICS coordinates GridLAB-D and Python federates, performing time management and synchronization, and facilitating data exchanges between them.",
    "The diesel generators are assumed to have an inertia constant of 2 and a ramping capability of reaching full output in 10 seconds. Their fuel burn rate is assumed to be constant.",
    "The frequency limit used to define the short-term resilience metric is not explicitly stated in the context. However, the study utilizes the metric proposed in [4], which quantifies the ability to recover from an attack within a fixed time interval, and the metric is based on the system\u2019s ability to arrest frequency prior to an UFLS event.",
    "When a generator goes offline, it negatively impacts short-term resilience due to reduced inertia and ramping capability, but positively affects long-term resilience by conserving fuel. Running wind generation at maximum capacity allows diesel generators to be taken offline, increasing long-term resilience.",
    "The researchers implemented a strategy involving taking long positions when search volume decreased and short positions when search volume increased. Buy/sell decisions were based on changes in Google Trends data, specifically the volume of searches for financially relevant terms.",
    "The best-performing Google Trends strategy achieved a cumulative return of 0.60 standard deviations from the mean return of uncorrelated random investment strategies (US). The search term used was \u201cdebt\u201d.",
    "Researchers quantified financial relevance by calculating the frequency of each search term in the Financial Times, normalized by the number of Google hits. They found a correlation between search volume changes and trading performance, with Google Trends strategies outperforming random investment strategies.",
    "Strategies based on U.S. search volume data performed better than those based on global search volume data in the U.S. market. Specifically, the cumulative returns from the Google Trends strategies tested were significantly higher than those from random strategies, with a return of 0.60 standard deviations compared to 0.43 standard deviations for global strategies.",
    "The \u2018buy and hold\u2019 strategy yielded a 16% profit, while the \u2018Dow Jones strategy\u2019 achieved a 33% profit over a 3-week period.",
    "The researchers validated that both the long and short positions contributed significantly to the results by implementing strategies that exclusively took long positions following a decrease in search volume and strategies that exclusively took short positions following an increase in search volume, finding that both of these strategies yielded significantly higher returns than a random investment strategy.",
    "The researchers used Herbert Simon\u2019s model of decision making as a theoretical framework.",
    "The researchers analyzed four indicators: the correlation of Jacobian (CJ), output sensitivity (OS), condition number of Neural Tangent Kernel (CNNTK), and the number of Linear Regions (NLR).",
    "The researchers ensured the robustness of Google Trends data by determining the overall performance of a strategy based on the mean value of returns over six weeks of data.",
    "The researchers found that returns from the Google Trends strategies tested were significantly higher overall than returns from the random strategies, with a U.S. strategy showing a return of 0.60 standard deviations (p < 0.001).",
    "The time period covered by the Financial Times corpus is from 2nd January 2007 to 31st December 2012.",
    "The answer is not available in the provided context.",
    "The median correlation coefficient between daily mentions of company names and transaction volumes was 0.000.",
    "A positive correlation was found between the daily number of mentions of a company in the Financial Times and the daily transaction volume of a company's stock both on the day before and on the same day as the news is released.",
    "The Financial Times is released each day from Monday to Saturday at 5 am London time.",
    "The study analyzes a corpus of daily Financial Times issues from January 2nd, 2007, to December 31st, 2012. Trading hours are not explicitly mentioned in the context.",
    "On June 8, 2009, Travelers replaced Citigroup in the DJIA.",
    "Significant correlations were found between company mentions and transaction volumes at lags of one day before and on the same day as the news (2 1 and 0).",
    "The median correlation coefficient between daily mentions of company names and absolute returns was 0.000.",
    "The analysis found no significant correlation between daily mentions of a company and its stock price movements when direction of movement is considered. Spearman's rank correlation coefficients were not significantly different from zero.",
    "The researchers defined a quantitative support/resistance level as the average absolute price increment within a stripe centered on the level.",
    "The study analyzed 31 stocks from the Dow Jones Industrial Average. The analysis period was from January 2nd, 2008, to December 31st, 2012.",
    "The average Hurst exponent is 0.44, and this value indicates an anticorrelation effect of the price increments.",
    "The analysis reveals a long-term memory effect in the price dynamics, specifically an increasing probability of bounces on support/resistance levels as the number of previous bounces increases.",
    "The time between consecutive bounces is described by a power law distribution, specifically, a histogram of the time t occurring between two consecutive bounces fits a power law. The maximum distance from support/resistance levels is characterized by an exponentially truncated power law.",
    "The researchers used Bayesian inference to estimate the conditional probability of bounces, p(b|bprev), by inferring it from the number of bounces (nbprev) and the total number of trials (N), assuming that nbprev is a realization of a Bernoulli process. The expected value and variance of p(b|bprev) were evaluated using Bayes\u2019 theorem, as detailed in the Supporting Information.",
    "The analysis reveals a long-term memory effect in the price dynamics, specifically an increasing probability of bounces on support/resistance levels as the number of previous bounces increases. This memory is not explained by an antipersistent nature of the price and is statistically significant across different time scales.",
    "The conditional bounce probabilities for the stock data were significantly higher than those observed in the shuffled return series, approximately 1.5 to 2 compared to nearly 1.5 to 2. The probability of bounce increased as the number of previous bounces on a local maximum or minimum increased.",
    "The context does not specify the statistical tests employed or the significance levels used.",
    "The three basic assumptions of technical analysis are: the market discounts everything, price moves in trends, and history repeats itself. \n\nThe market discounts everything means that the price reflects all possible causes of price movements, such as investors' psychology and political contingencies. \n\nPrice moves in trends suggests that a trend is more likely to continue than to stop, and the goal of technical analysis is to identify trends early. \n\nHistory repeats itself posits that patterns in past price graphs can predict future price movements, reflecting investor psychology.",
    "The cost C_i for a one-level dependent activity is composed of three parts: economic CF, social CS, and environmental CE.",
    "The environmental cost CE is proportional to the impact I, the pollutant p, and the required capacity c.",
    "The two conditions for sustainability are a strong condition with no substitution and a weak condition with substitution. The strong condition requires that an activity\u2019s demand be met without any substitution of resources, while the weak condition allows for substitution to be used to meet the demand.",
    "The set of all sustainable activities forms a subset of a N-level union of sustainable activities, which is a topological cover of sustainable activities.",
    "The three conditions that must be met for an activity to be considered sustainable according to the framework are: a strong condition with no substitution and a weak condition with substitution.",
    "The paper identifies five requirements for sustainable agriculture addressable by Zachary\u2019s framework:\n\n1.  Satisfying human food and fiber needs.\n2.  Enhancing environmental quality and the natural resource based upon the agricultural economy.\n3.  Making the most efficient use of non-renewable resources and on-farm resources.\n4.  Integrating, where appropriate, natural biological cycles and controls.\n5.  Sustaining the economic viability of farm operations and enhancing the quality of life for farmers and society.",
    "Zachary identifies the fundamental challenge in connecting social sustainability to environmental and economic dimensions as the difficulty in analytically linking local social issues (involving geographers, planners, and landowners) with global issues (involving economists and diplomats).",
    "The paper addresses higher-order dependencies by introducing the concept of substitution and using a multi-level picture of activities and their dependencies. An example used to illustrate this is the choice of transportation mode, where the bike (a base activity) can be substituted by another fish (a level-2 activity) if tuna is unavailable, demonstrating how activities can depend on each other at different levels.",
    "Zachary acknowledges that the social pillar of sustainability is difficult to quantify and connect analytically with the other dimensions, particularly due to the challenges in formulating issues into an analytic framework and the difficulty in linking local social issues with global issues.",
    "Zachary proposes eight classifications of activities based on duration, cost, and complexity levels, as outlined in Table 3. These classifications are: Anthropocentric, Neo-Austrian-Temporal, Ecological-Evolutionary, Evolutionary-Technological, Physico-Economic, Biophysical-Energy, Systems-Ecological, and Ecological Engineering.",
    "The maximum modulation frequency achieved by the proposed spatial sweeping system is 97 kHz, which is approximately five times faster than that of the fastest DMD. This speed is largely determined by the operating frequency of the galvanic mirror and the size of the DMD micro-mirrors.",
    "The final modulation frequency of the sweeping-based ghost imaging system is determined by the scanning frequency of the two galvanic mirrors GM1 and GM2, denoted as Fg.",
    "The authors achieved a frame rate of 42 Hz at 80 \u00d7 80-pixel resolution and 97 kHz modulation speed using two sweeping galvanic mirrors. They employed a reconstruction algorithm utilizing spatio-temporal redundancies of nature scenes and compressive sensing.",
    "The theoretical maximum modulation speed achievable using high-end galvanic mirrors like the CTI 6200H is 97 kHz, which is approximately five times faster than the fastest existing SLM systems. Further acceleration is possible with higher-end elements, potentially reaching 485 kHz with a 1 kHz working frequency.",
    "The parameter k affects reconstruction quality by increasing the RMSE.",
    "The mathematical relationship derived for edge detection using consecutive patterns in the sweeping system involves the number of scanned consecutive patterns during each DMD period, denoted as *k*, and the correlation between the measurements of consecutive patterns.",
    "The DMD and galvanic mirrors operate based on the sweeping of a high-resolution SLM. The speed is largely determined by the operating frequency of the galvanic mirror and the size of the DMD micro-mirrors. Higher-end galvanic mirrors (e.g., CTI 6200H) and acoustic optical deflectors (AODs) can increase the speed, with AODs potentially achieving 20x faster illumination patterning.\n\nThe DMD's pixel resolution depends on the size of the scanning beam, which is limited by the diameter of the galvanic mirror (typically 3-7 mm, providing 200x200 pixel resolution). Smaller DMD micro-mirrors can increase the final reconstruction pixel resolution, but this comes at the cost of reduced frame rate.\n\nThe DMD frame rate is set to avoid repetitive coding patterns during galvanic mirror sweeping, aiming for a DMD update frequency between 2 Fg and its maximum frequency. The scanning rate is linked to the galvanic mirror's frequency, and a trade-off exists between pixel resolution and imaging speed.",
    "The geometric relationship between the galvanic mirror rotation angles and the beam's hitting position on the DMD is described by a linear mapping, specifically x = d sin(2 \u03b8), where x is the distance from the beam's hitting position at GM1 to that at the DMD, d is the distance between the galvanic mirrors, and \u03b8 is the rotating angle of GM1.",
    "Using acoustic optical deflectors (AODs) instead of galvanic mirrors could achieve a potential improvement in modulation speed, reaching approximately 20 times faster illumination patterning (9.5 MHz) compared to galvanic mirrors, which operate at a frequency of 200 Hz.",
    "The main limitations of the proposed sweeping-based ghost imaging approach include the need for careful mechanical mounting and calibration, and its current applicability to structuring light using random patterns rather than specific patterns like Hadamard or sinusoidal patterns.",
    "The method combines syntactic analysis, an RBM, and sentence2vec.",
    "The M-MI model addresses the lack of true labels at the instance level by estimating instance-level probabilities and source-specific weights to combine information from various data sources, aiming for a consensus prediction of market index movement. It uses a hinge loss term for instance-level classification, replacing true labels with estimated true labels based on the predicted probability.",
    "The three levels of loss functions incorporated in the M-MI model\u2019s objective function are:\n1.  Super group level loss\n2.  Group level loss\n3.  Instance level loss",
    "News events contribute most to the overall prediction.",
    "M-MI improves F-1 by 6.9% in 2015 and 9.2% in 2016 compared to nMIL.",
    "The paper justifies the use of a shared estimated true label across different data sources in the hinge loss functions because different data sources are commonly correlated, indicating the same sign (index rise or fall) according to the Efficient Market Hypothesis.",
    "The three-level syntactic tree captures the sentence\u2019s structure, depicting the root as the core verb, the second layer as subjects and objects, and the third layer as modifiers nearest to the subject and object. Core words are selected by connecting these core words together as the structure information to represent the event information.",
    "As the number of history days increases, model performance generally initially improves but then declines. This pattern is attributed to the decay of the impact of news events, sentiments, and quantitative data over time, with information becoming less relevant after a period of 2-3 days.",
    "The LDA-S method is used for extracting sentiments from social media posts. It was chosen because sentiment polarities often depend on topics or domains, and extracting sentiments discarding topics may not be sufficient for accurate classification.",
    "The mathematical formulation models the probability of multi-source information on day i as Pi, where GLYPH<18> 0, GLYPH<18> 1 and GLYPH<18> 2 denote the source-specific weights of p m GLYPH<0> i , p d GLYPH<0> i and p s GLYPH<0> i respectively, and GLYPH<18> 0 C GLYPH<18> 1 C GLYPH<18> 2 D 1.",
    "The four specific risk sources identified in the land consolidation ecological risk assessment for A County, Shaanxi Province are: agricultural land consolidation, rural construction land consolidation, land reclamation, and land development.",
    "The priority remediation area includes 27 administrative villages, spanning a total area of 28090 hm2. This area represents 32.75% of the overall region.",
    "The mathematical formula used to calculate the relative risk value (RS\u1d62) is:\n\nRS\u1d62 = Sij / max(Sij) , Hik / max(Hik), Xjk / total area of habitat, where Sij represents the density of risk sources, Hik indicates the habitat abundance, and Xjk expresses the exposure coefficient.",
    "The SOFM neural network parameters are: the number of iteration times is 1000, the number of input nodes is 10, and the number of output nodes is 30.",
    "Landscape pattern and soil show the highest ecological risk values in A County, with values between 0.01 and 1.62 and 0.01 and 1.46, respectively.",
    "Dij demonstrates mixed distance; D s ij shows geospatial distance; a d j and a d j indicate attribute values in attribute space; ws and wa are geospatial and attribute space weights, respectively.",
    "The three main weight categories are source-specific weights (GLYPH<18>), quantitative data weights (w d), and sentiment weights (w s).",
    "IoT sensors collected data on soil quality, meteorological conditions, water quality, and farmland conditions.",
    "Area C demonstrates the lowest ecological risk levels across all ecological risk aspects. This indicates that Area C holds potential advantages for agricultural development due to its low soil, water environment, biodiversity, and landscape pattern ecological risk values.",
    "The proximity function hij uses the Gaussian function and incorporates parameters \u03c3 (the diffusion parameter) to measure the proximity among neurons.",
    "Graph theory extends to Leonhard Euler in the 18th century.",
    "Storage limitations and high costs during the 1960s drove the need for efficient data compression, leading to the development of relational algebra.",
    "Graphs are expressed in node-arc-node (subject-predicate-object) triples. Nodes generally represent physical or conceptual objects, and arcs represent relationships between nodes. Properties can be assigned to both nodes and arcs.",
    "The context mentions that GPUs offer hardware alternatives to accelerate large-scale graph processing, alongside specially configured supercomputers developed by firms like Cray.",
    "RDBMS performance limitations arise when dealing with many-to-many relationships and complex, interconnected data. The schema is inflexible, requiring high maintenance for changes, and performance degrades significantly with a large number of many-to-many relationships.",
    "The hierarchical model and the network model, both created in the 1960s, were early attempts to represent relationships similar to graph databases.",
    "There is little semantic commonality among the various graph languages in use, and their rules of syntax.",
    "Many graph databases support the Atomic, Consistent, Isolated, and Durable (ACID) consistency model, which is derived from RDBMS technology.",
    "The context recommends utilizing graph databases as an alternative to traditional RDBMS for managing large-scale graph data.",
    "Built-in mathematical functions residing in graph databases could add a level of depth to truly understanding and quantifying graph relationships. The authors suggest that metric algorithms derived from graph theory, extending beyond simple network analysis, could be incorporated.",
    "The experiments simulated both Earth\u2019s standard pressure (1000 mbar) and Mars-like pressure (8 mbar).",
    "The relative error between MEC-based thermal inertia estimations and Perseverance\u2019s data was 6.79% for sandy soil, 7.68% for intermediate soil, and 7.76% for bedrock soil.",
    "The increase in the difference between surface and subsurface temperatures for soil A when pressure decreased from Earth to Mars conditions was 26.72%.",
    "The thermal inertia of a given soil is estimated as  where 1 Ts = T max -T min and the net heat flux is expressed as 1 Gs = G max -G min.",
    "The IR viewport window was constructed from an anti-reflection coated germanium circular optic model GEW16AR.20 by MKS Instruments. It was 74.9 mm in diameter and 5.0 mm in thickness.",
    "A total of 9225 radiometric images were collected. They were saved as plain text 640 \u00d7 480 matrices with each cell containing the temperature in degree Celsius.",
    "Thermal conductivity is influenced by three heat transfer mechanisms: transfer across pore spaces (kr), conduction between grains (kc), and conduction of the gas within the pores (kg). Pressure significantly determines which of these mechanisms is most relevant.",
    "The experiments observed a significant difference in thermal inertia between Earth\u2019s standard pressure (1000 mbar) and Mars-like pressure (8 mbar). Specifically, loose granular soils had lower thermal inertia at Martian pressure compared to compacted rocky soils at Earth pressure.",
    "The thermal camera is a PI-640i by Optris, an uncooled microbolometer camera. It has a resolution of 640 \u00d7 480 pixels, operates in the 8-14 \u03bcm spectral range, and has a germanium optic with a field of view of 60\u00b0 \u00d7 45\u00b0. It can measure temperatures from -20 \u25e6 C to 900 \u25e6 C with a thermal sensitivity of 0.04 \u25e6 C.",
    "The simplified surface energy budget equation is:\n\nG = (Ts - T_ambient) * A * \u03c3 * (1 - \u03b5) + R_sw - R_lw\n\nWhere:\n*   G is the net heat flux\n*   Ts is the surface temperature\n*   T_ambient is the ambient temperature\n*   A is the albedo\n*   \u03c3 is the Stefan-Boltzmann's constant\n*   \u03b5 is the thermal emissivity\n*   R_sw is the downwelling shortwave radiation absorbed from the Sun\n*   R_lw is the downwelling longwave radiation emitted by the atmosphere and the Sun",
    "The two fundamental problems with singular vectors that make them difficult to learn in convolutional neural networks are sign ambiguity and manifold features.",
    "SVP shows 36.07%, 28.18%, and 34.21% higher accuracies than GAP, GMP, and MPN, respectively, under FGSM with natural and adversarial training on the CIFAR10 dataset.",
    "Aligning singular vectors based on e U as a reference eliminates sign ambiguity.",
    "We approximate the arccosine to GLYPH<0> z C GLYPH<25> 2 by the GLYPH<28>rst order Taylor expansion because the 1st derivative of the arccosine can easily diverge.",
    "The loss function used is KL-divergence, and it assumes that each component of the aligned N u has a Gaussian distribution.",
    "The silhouette score of SVP on the training dataset is better by 0.444, 0.510, and 0.551 than that of GAP, GMP, and MPN, respectively.",
    "The rotation process centers singular vectors and minimizes path lengths during learning by rotating the center of the spherical coordinate system to f 0 ; : : : ; 0 ; 1 ; 0 g.",
    "The experiment shows that KD-SVP improves performance by 1.69% on CIFAR100 and 0.97% on Tiny-ImageNet compared to KD-SVD.",
    "If the sign ambiguity removal step (step 2) is omitted, learning is impossible.",
    "SVP has a higher computational complexity than conventional pooling methods like GAP and GMP. However, it outperforms these methods and provides further performance enhancement when applied to knowledge distillation schemes.",
    "The STDM-OCT system achieved a 1-MHz A-scan rate.",
    "The broadband light source used in the STDM-OCT system is an SLD-371-HP3 from Superlum, Ireland, with a center wavelength of 838 nm, a full-width at half-maximum of 81 nm, and an optical power of 27.2 mW.",
    "The volumetric imaging rate achieved was 8 vol/s for an image range of 250 \u00d7 250 \u00d7 2048 pixels.",
    "The measured thicknesses of the four layers are 100, 250, 150, and 100 \u03bcm, respectively.",
    "The peak sensitivities (at the 100th pixel) were 139 dB and 137 dB, respectively.",
    "The developed software algorithm and control platform was built using C++, CUDA, and Qt.",
    "The TDM method achieves doubled effective imaging rate compared to the maximum sampling rate of a single line-scan camera by utilizing two line-scan cameras operating in a time-division multiplexing (TDM) scheme. This allows two continuous A-lines to be captured in one period of the camera trigger, resulting in a 500 kHz effective imaging rate, which is twice the maximum sampling rate of a single camera.",
    "The whole-range scanning of the sample consumes 8 seconds.",
    "The vacuum gap thicknesses between the layers are 40, 20, and 40 \u03bcm.",
    "The resolution and frame rate specifications of the line-scan cameras used in the spectrometer are not available in the provided context."
  ]
}