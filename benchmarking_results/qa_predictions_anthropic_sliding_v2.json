{
  "reference_answers": [
    "The Gander project had two main components: (1) an empirical component focused on problem conceptualization, conducted as a mixed-method study with practitioners in industry to provide input for tool design, and (2) a development component focused on building an experimental code review platform incorporating eye-tracking to enable gaze-assisted assistance in code reviews.",
    "The framework identifies (1) employees or other stakeholders of software development organizations, (2) students or other beneficiaries of the university, and (3) independent participants. They are distinguished based on legal and ethical concerns regarding the relationship between researchers and participants, such as potential pressure to participate and secrecy conditions.",
    "The four steps are: (1) Data cleaning \u2013 transforming and anonymizing data (quantitative: coding for statistical tools; qualitative: transcription and anonymization). (2) Data exploration and visualization \u2013 descriptive statistics and outlier detection (quantitative) or preliminary coding ideas (qualitative). (3) Model building and analysis \u2013 statistical analysis like prediction models (quantitative) or coding and theory building (qualitative). (4) Findings presentation \u2013 presenting results in publications or reports.",
    "The three steps are: (i) Publication for reproduction \u2013 artifacts in original state (non-editable documents, executable code), ensuring transparency. (ii) Generalization for general use \u2013 artifacts in editable state (editable documents, source code). (iii) Generalization for continued development \u2013 artifacts with licenses, onboarding guidelines, and community support for ongoing evolution. Each step requires progressively more investment to make artifacts openly reusable.",
    "The risks include: disclosure of irrelevant but sensitive company information, exposure of relevant but confidential events (e.g., security incidents), potential harm to interviewees if their opinions about managers are revealed, difficulty of anonymization with small participant groups, and epistemological concerns that transcripts lack meaning without shared context. These factors led the authors not to recommend sharing raw qualitative data.",
    "The authors recommend not to openly publish qualitative research data. Instead, they suggest publishing study and analysis artifacts such as study protocols, interview guides, interviewee descriptions, and code books from thematic analysis to ensure transparency without compromising confidentiality.",
    "The Gander project discovered that one of the projects used for gaze data analysis lacked a license. After contacting the original author, a MIT license was added, which allowed the team to proceed. Ultimately, the Gander platform was released under a BSD license after reviewing all dependencies.",
    "Eye-tracking is used to detect fixation points during code review, which are connected to programming language elements. This enables real-time gaze-based assistance. As a proof-of-concept, the platform implemented a gaze assistant that visualizes use-declaration relationships in code when users fixate on variable names.",
    "Quantitative survey data, being less rich and more standardized, is easier to anonymize and share openly compared to qualitative data. The authors recommend (R4) that quantitative data be shared openly if, and only if, it is sufficiently anonymized to protect the identity of individuals or companies.",
    "They emphasize the FAIR principle: data should be 'as open as possible and as closed as necessary.' This means fostering reuse and accelerating research through openness, while respecting participants\u2019 privacy, ethical constraints, and companies\u2019 legitimate secrecy concerns.",
    "The main motivation is to reduce the high computational cost of traditional neural architecture search, which often requires training numerous candidate architectures. Training-free indicators aim to estimate performance without full training, enabling faster search.",
    "The three types are (1) synaptic diversity, measuring variability in weight initialization and connections, (2) neuron activation strength, capturing output responses of neurons to input data, and (3) path expressivity, reflecting the complexity of information flow across paths in the architecture.",
    "The feature fusion mechanism combines normalized versions of synaptic diversity, neuron activation strength, and path expressivity through weighted integration. This is necessary because each feature captures different aspects of architecture quality, and fusion provides a more robust performance indicator.",
    "The authors used NAS-Bench-101, NAS-Bench-201, and DARTS search space benchmarks. These datasets were chosen because they provide standardized environments with known ground truth performance, enabling reliable comparison of NAS indicators.",
    "FFI consistently outperformed existing indicators like Synflow, GradNorm, and Jacov in terms of correlation with true architecture performance across all benchmarks, demonstrating higher reliability as a predictor.",
    "Kendall\u2019s Tau is a statistical measure of rank correlation between two variables. The paper uses it to assess how well the rankings produced by FFI align with ground truth rankings of neural architectures.",
    "The authors conducted ablation studies by removing one feature type at a time (synaptic diversity, neuron activation strength, path expressivity) and observed performance drops. This showed that all three features contribute significantly to the effectiveness of FFI.",
    "FFI dramatically reduces computational cost because it avoids training architectures altogether. Its evaluation is orders of magnitude faster than training-based NAS, making large-scale architecture search feasible.",
    "The authors found that FFI maintains high correlation with true performance across diverse search spaces (NAS-Bench-101, NAS-Bench-201, DARTS), demonstrating its robustness and generalizability.",
    "The authors suggest exploring additional complementary features, adaptive fusion strategies that adjust weights dynamically, and extensions of FFI to large-scale real-world tasks to further enhance training-free NAS indicators.",
    "The motivation is to overcome the limitations of existing monocular vergence-based and binocular depth-based methods. Vergence-based methods struggle with low accuracy at long distances, while depth-based methods suffer from errors due to noise and hardware limitations. A hybrid approach combines both to achieve more robust and accurate gaze distance estimation.",
    "The two cues are vergence (eye rotation angles) and depth (from RGB-D camera or stereo imaging). Vergence provides reliable estimation at short distances, while depth cues perform better at longer distances. By cross-referencing the two, the hybrid method improves robustness across a wide range of viewing distances.",
    "The authors conducted experiments using both controlled indoor datasets with calibrated RGB-D cameras and real-world settings where subjects fixated on targets at varying distances. These setups enabled evaluation across short, medium, and long gaze distances.",
    "The mechanism compares the confidence levels of the vergence and depth estimates. When vergence accuracy is high (short distance), it is prioritized. When distance increases and vergence degrades, depth-based estimation is weighted more heavily.",
    "The hybrid method achieved lower average estimation error across all tested distances. Specifically, it maintained accuracy comparable to vergence at short distances and outperformed both vergence and depth baselines at medium and long distances.",
    "Calibration aligns the vergence-based eye-tracking system with the depth-sensing camera to ensure accurate fusion of cues. It was performed by asking participants to fixate on predefined calibration targets at known distances while recording both vergence and depth data.",
    "Applications include human-computer interaction, virtual and augmented reality systems, driver monitoring, and assistive technologies that rely on accurate gaze-based distance estimation.",
    "The authors note limitations such as reliance on depth sensors, which can fail under poor lighting or reflective surfaces, and the assumption of stable fixation, which may not hold in dynamic environments. Further improvements are needed for robustness in unconstrained real-world scenarios.",
    "The 'mid-range gap' arises because vergence is accurate only at short distances and depth sensors are reliable mainly at long distances. The hybrid method bridges this gap by adaptively combining the two, achieving accurate estimation in the mid-range where both cues individually perform poorly.",
    "Future work includes integrating learning-based fusion strategies, improving robustness of depth sensing under challenging conditions, and validating the system in more diverse real-world applications such as outdoor environments.",
    "The four attributes are availability, reliability, data integrity, and efficiency. Availability is measured as the ratio of accepted to total requests. Reliability is measured as the ratio of successful to accepted requests. Data integrity is measured as the ratio of successful requests minus failed requests to successful requests. Efficiency is measured as promised execution time divided by the sum of waiting and execution time.",
    "The model formulates a Mixed Integer Linear Programming (MILP) problem where the objective is to maximize overall trust of VM allocations while minimizing communication delay. Trust is computed from the weighted sum of availability, reliability, data integrity, and efficiency, while delay is computed as the ratio of total VM traffic to server capacity. Constraints ensure each VM is allocated once and within resource limits.",
    "The GA provides a heuristic solution to the NP-hard MILP resource allocation problem. It uses selection, crossover, and mutation to generate near-optimal VM allocation sets. Compared to the optimal solver (intlinprog in MATLAB), GA achieves 90% similarity in results but with significantly less execution time and better scalability for large numbers of CSPs and servers.",
    "Availability decreases as load increases. Under small loads, availability is maximum, while under heavy loads it is minimum. The experiments used small, medium, and heavy load scenarios, averaging 10 runs to compute availability values.",
    "Reliability degradation is caused by hardware and software failures. Hardware failures include failures of virtual machines and processing elements, while software failures arise from errors in request processing and workload execution. Reliability decreases with an increase in the number of requests and load.",
    "The Pareto front shows that maximizing trust by allocating resources only to the most trusted CSPs increases communication delay, while distributing resources reduces delay but lowers trust. Thus, achieving an optimal trade-off requires balancing both objectives.",
    "The CloudSim simulator was used because of its wide acceptance in the research community and its ability to simulate real cloud environments, including request arrivals, failures, and resource allocation scenarios. It enables validation of the trust evaluation and resource allocation models without relying on restricted real cloud provider configurations.",
    "When trust is given higher weight, trust values improve by around 10% but delays increase. Conversely, when delay is prioritized, delays reduce by about 10% but trust decreases. Equal weights achieve a balance, showing a clear trade-off depending on service requirements.",
    "Experiments used CloudSim for trust data collection and MATLAB for optimization. An Intel Core i7 machine with a 4 GHz processor was used. Genetic Algorithm experiments had a population size set based on requirements and generation size fixed at 1000. Scenarios varied CSPs from 6 to 12 and servers from 30 to 60.",
    "The authors propose developing a more reliable and secure model for trust management among different cloud service providers, extending beyond resource allocation to broader multi-cloud trust frameworks.",
    "The motivation is that 28Si single crystals enable determination of the Avogadro constant with unprecedented precision. Isotopically enriched 28Si reduces uncertainties due to isotope mass distribution, making it suitable for defining the kilogram based on fundamental constants.",
    "EPR is used to characterize point defects and impurities in 28Si single crystals. These defects, such as oxygen-vacancy complexes and dangling bonds, affect the accuracy of Avogadro constant determination and thus must be quantified.",
    "The study identified P donors, Pb centers (Si dangling bonds at the Si/SiO2 interface), and oxygen-vacancy centers such as the E\u2032 center as the main paramagnetic defects present in the crystal.",
    "Isotopic enrichment in 28Si improves EPR resolution by reducing hyperfine interactions caused by 29Si nuclei. This results in narrower linewidths and more precise defect characterization.",
    "The experiments used an X-band EPR spectrometer operating at ~9.4 GHz. Measurements were conducted at low temperatures using a helium-flow cryostat to enhance defect signal detection.",
    "Oxygen-vacancy defects alter the lattice parameter of silicon crystals. Since determination of the Avogadro constant depends on precise measurement of the silicon lattice parameter, quantifying such defects is essential to minimize systematic errors.",
    "The study detected defect concentrations on the order of 10^13 to 10^14 spins per cm\u00b3, confirming that 28Si crystals used for Avogadro experiments have extremely low impurity levels.",
    "Phosphorus donors act as shallow impurities in silicon, influencing conductivity and possibly EPR signal strength. Their concentration must be minimized for precision lattice parameter measurement but also provides useful calibration signals in EPR analysis.",
    "28Si single crystals provide higher EPR sensitivity and resolution due to the absence of 29Si nuclear spins, enabling clearer identification of defects and lower detection limits compared to natural silicon.",
    "The authors suggested further EPR investigations at varying temperatures and magnetic fields to better characterize weak defect signals, along with complementary spectroscopic techniques to fully quantify remaining impurities in 28Si crystals.",
    "The primary causes are (i) line edge roughness (LER), (ii) random dopant fluctuation (RDF), and (iii) work function variation (WFV). LER is considered most severe because it can affect RDF and WFV by inducing deformation of the device structure, thus degrading performance more significantly.",
    "The parameters are: (i) Amplitude (rms value of surface roughness), (ii) Correlation length X (3x) and (iii) Correlation length Y (3y). A larger correlation length indicates a smoother line. Additionally, a relation term between x and y directions (\u03b1 or 2) may be included.",
    "Two datasets were generated: (1) 130 datasets each with 50 FinFETs (6,500 total), split into 70% training and 30% validation, and (2) 10 datasets each with 250 FinFETs (2,500 total). LER parameter ranges were: amplitude 0.1\u20130.8 nm, correlation length X 10\u2013100 nm, correlation length Y 20\u2013200 nm.",
    "The earth-mover\u2019s distance (EMD) score was used. It measures the minimal amount of work needed to transform one probability distribution into another, calculated by comparing cumulative distribution functions (CDFs) estimated with Gaussian kernel density estimation. An EMD of 0 means identical distributions.",
    "The optimized ANN had 3 neurons in the input layer, 81 in the first hidden layer, 162 in the second, 324 in the third, and 324 in the output layer. This architecture with 11 mixture components minimized validation loss at 7,800 epochs, making it best suited to describe the distribution of performance metrics.",
    "Negative log likelihood (Negloglik) was used as the loss function because the ANN outputs probability distributions (PDFs). Conventional mean squared error cannot be applied since training is equivalent to maximum likelihood estimation of distributions, not single-point values.",
    "The mixture-MVN ANN successfully predicted skewness, kurtosis, and non-linear correlations that the Gaussian-only ANN could not. The EMD score improved significantly (0.0170 with Gaussian-only vs. 0.00928 with mixture-MVN).",
    "Training time was reduced from 1412 seconds for the non-separated ANN to 185 seconds for the separated ANN, achieving about a 6\u00d7 speedup without significant performance degradation.",
    "The proposed ANN predicts 7 metrics: Ioff, Idsat, Idlin, Idlo, Idhi, Vtsat, and Vtlin. The previous ML model predicted only 4 metrics: Ioff, Idsat, Vtsat, and SS.",
    "By accurately predicting non-Gaussian distributions of performance metrics, the ANN enables simulation of electrical behavior of transistors and DC behavior of digital circuit blocks such as SRAM bit cells, improving variation-aware design.",
    "Because the amplitude of LER does not scale down proportionally with device dimensions, its fraction of the physical channel length and width increases, leading to larger variations in IDS\u2013VGS characteristics.",
    "The parameters are RMS amplitude (\u03c3), correlation length (\u03beX, \u03beY), and roughness exponent (\u03b1). \u03c3 is the standard deviation of roughness amplitude, \u03be represents the wavelength of the roughness profile, and \u03b1 describes how high-frequency components diminish in the roughness profile.",
    "The dataset contained 169 sets, each consisting of 50 sample FinFETs with identical LER and device parameters. Eighteen FinFET structures were chosen, and LER parameters were randomly varied within specified ranges.",
    "Target variables: mean (\u00b5IDS) and standard deviation (\u03c3IDS) of log(IDS). Input features: gate voltage (VGS), RMS amplitude (\u03c3), correlation lengths (\u03beX, \u03beY), roughness exponent (\u03b1), gate length (Lg), fin width (Wfin), and fin height (Hfin).",
    "Horseshoe priors introduce shrinkage and sparsity over weights, promoting zeroing of unnecessary weights while allowing important large weights to remain due to heavy tails. This enables automatic model selection, finding compact layer sizes without manual tuning.",
    "HS-BNN improved prediction accuracy significantly. For \u00b5IDS, MAPE improved from 0.81% (BLR) to 0.55% (HS-BNN). For \u03c3IDS, MAPE improved from 19.59% (BLR) to 6.66% (HS-BNN).",
    "The HS-BNN maintained stable prediction performance (\u03c3IDS MAPE ~7%) even when the number of nodes exceeded 200, while Gaussian-BNN predictions varied significantly with node count. This shows HS-BNN\u2019s robustness to over-parameterization.",
    "K-fold cross-validation was used, ensuring all data were used as a test set at least once. This is suitable for limited datasets, producing less biased evaluations and better generalization estimates.",
    "For a FinFET with Lg=20 nm, Wfin=7 nm, Hfin=42 nm, and LER profile (\u03c3=0.5 nm, \u03beX=20 nm, \u03beY=50 nm, \u03b1=1), varying one parameter at a time showed well-matched trends in both mean and standard deviation of log(IOFF) and VTH, consistent with known device behavior.",
    "HS-BNN predicted LER-induced variations within a few seconds, whereas conventional TCAD simulations required weeks for a single LER profile.",
    "The RAFS achieved 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2 stability by using a rubidium spectral lamp with Xe starter gas, applying optical and isotope double-filtering to reduce noise, employing a large slotted tube microwave cavity with a 40 mm absorption cell to enhance the discrimination signal, and enclosing the physics package in a sealed box to mitigate barometric effects.",
    "The barometric effect was shown to cause a frequency shift coefficient of 7 \u00d7 10\u207b\u00b9\u2075/Pa, degrading 100-s stability to 2.2 \u00d7 10\u207b\u00b9\u2074. The solution was to place the physics package in a sealed box, reducing the barometric influence by nearly an order of magnitude.",
    "The slotted tube microwave cavity allowed flexible size design, enabling the use of a large 40 mm absorption cell to increase atomic signal SNR. It also provided a high field orientation factor (\u03be = 0.82), superior to traditional TE111/TE011 cavities, improving excitation of the clock transition.",
    "The optimal temperatures were 68 \u00b0C for the absorption cell, 93 \u00b0C for the filter cell, and 109 \u00b0C for the lamp bulb.",
    "By optimizing the discrimination slope Kd (18.0 nA/Hz) and measuring the shot noise current (211 \u00b5A, corresponding to noise spectral density of 8.2 pA/Hz\u00b9/\u00b2), \u03c3SNR(\u03c4) was estimated to be 4.7 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2.",
    "The interrogation microwave had phase noise of about \u2212110 dBc/Hz at 2fM (272 Hz). This limited stability to \u03c3PN(\u03c4) = 6.0 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2.",
    "The environmental factors examined were absorption cell temperature, pumping light intensity, 6.835-GHz microwave power, magnetic C-field, and barometric pressure. Among these, temperature shift and barometric pressure shift had the largest impact on stability for \u03c4 > 100 s.",
    "With the H-maser, the RAFS showed 9.0 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2 (1\u2013100 s). With the OMG, it showed 9.1 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2 (1\u2013100 s). Both results were consistent and matched the predicted 7.6 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2 stability.",
    "This result surpasses the previous best stability of 1.2 \u00d7 10\u207b\u00b9\u00b3\u03c4\u207b\u00b9/\u00b2 (recently achieved in pulsed laser-pumped RAFS), marking the first lamp-pumped RAFS to achieve 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2 stability, approaching hydrogen maser performance and enabling next-generation space clocks for satellite navigation.",
    "Predicted contributions were \u03c3SNR(\u03c4) = 4.7 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2, \u03c3PN(\u03c4) = 6.0 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2, and environmental effects controlled at 10\u207b\u00b9\u2075\u03c4\u207b\u00b9/\u00b2, leading to a predicted total stability of 7.6 \u00d7 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2.",
    "1) The average within-group Pearson correlation coefficient (PCC) must drastically increase in absolute value, 2) The average between-group PCC (stocks in the group vs. rest of system) must greatly decrease in absolute value, and 3) The average auto-covariance (AC) of stocks belonging to this group must increase in absolute value.",
    "I^LTM_t = (\u27e8|AC^LTM_t|\u27e9\u27e8|PCC^LTM_t|\u27e9) / \u27e8|PCC^\u00acLTM_t|\u27e9, where the first component (auto-covariance) relates to positive feedbacks in the market, and the second component (correlation ratio) reveals the presence of herding behaviors among investors.",
    "Stocks stay continuously in the leading module for about 1.5 months on average. There is a negative Pearson correlation of -0.19 between the LTM stability coefficient and the size of the DFA- group (stocks with significant DFA exponents not included in the leading module).",
    "The strategy compares the most recent I^LTM_t value against its empirical distribution computed over the previous 15 working days. Values larger than the 95th percentile trigger investment decisions. If the average return of LTM stocks is positive, a buy signal is generated; otherwise, a short position is taken.",
    "As \u03bb\u2081 approaches 1: (1) the absolute value of auto-covariance AC(z_i(t), z_i(t-1)) increases greatly if variable z_i is related to y\u2081; (2) |PCC(z_i(t), z_j(t))| approaches 1 if both variables are related to y\u2081; (3) |PCC(z_i(t), z_j(t))| approaches 0 if only one variable is related to y\u2081.",
    "The LTM strategy achieved: true positives 53%, false positives 47%, false negatives 49%, and true negatives 51%. The VaR strategy performed worse with: true positives 49%, false positives 51%, false negatives 52%, and true negatives 48%.",
    "The indicator showed increasing dynamics corresponding to major market events including: banking sector ratings downgrades of 2007, the failure of Lehman Brothers in September 2008, the American Recovery and Reinvestment Act of 2009, the European Debt crisis of 2011, and the Chinese stock market crisis of 2015-2016.",
    "Stocks with Hurst exponents outside the 0.2-0.8 interval show a distribution of correlations that is shifted to the right compared to other stock pairs, indicating that the DFA selects assets with highly correlated returns. This increases their probability of entering the LTM.",
    "The study draws an analogy where variations in asset prices are like nucleation phenomena near stability limits in thermodynamic systems (superheated liquid or supercooled gas). The LTM acts as the nucleus of the new phase for financial markets, and I^LTM_t plays a role similar to compressibility in thermodynamics - a macroscopic quantity indicating increasing instability near spinodal lines.",
    "Over the entire 2006-2017 period, the LTM strategy achieved a cumulative P&L of 114.44% (using MV=10, PRCTILE=95 parameters), significantly outperforming the Buy&Hold strategy which only achieved 13.42%. Even with transaction costs of 10 basis points, the strategy still generated about 5.5% per year.",
    "Tinbergen identified four dimensions: mechanism (causation), development (ontogeny), function (adaptive value), and evolution (phylogeny). For machines: mechanism explains how behavior is triggered and generated; development covers how machines acquire behaviors through engineering, training data, or experience; function describes how behavior fulfills purposes for human stakeholders; and evolution examines how behavioral patterns spread through copying, reverse-engineering, and market forces.",
    "Flash crashes represent clearly unintended consequences of interacting algorithms operating at unprecedented speeds that humans cannot match. These high-frequency trading algorithms can respond to events and each other faster than any human trader, potentially creating market inefficiencies and raising concerns about whether algorithms could interact to create larger market crises when faced with unforeseen scenarios not covered in their training data.",
    "The three motivations are: (1) the unprecedented ubiquity of algorithms in society with ever-increasing roles in daily activities; (2) the complex properties of algorithms and their environments making some attributes difficult or impossible to formalize analytically; and (3) the substantial challenge of predicting the positive or negative effects of intelligent algorithms on humanity due to their ubiquity and complexity.",
    "Traditional algorithm development focuses on maximizing performance against benchmarks using optimization metrics, while machine behavior study requires broader indicators similar to social science research. It needs randomized experiments, observational inference, and population-based statistics to understand how algorithms behave in different environments and affect societal outcomes, rather than just measuring accuracy or speed.",
    "Researchers may need to violate terms of service when reverse-engineering algorithms (e.g., creating fake personas), face potential legal challenges from platform creators if research damages reputations, and risk civil or criminal penalties under laws like the Computer Fraud and Abuse Act. Additionally, experimental interventions in real-world settings could adversely affect normal users, requiring careful ethical oversight.",
    "Machine evolution is much more flexible than animal evolution - while animals have simple inheritance (two parents, one transmission), machines can have instant global propagation through software updates, open-source sharing of code and training data, and human designers with specific objectives. However, machines also face unique constraints like software patents and regulatory privacy laws that don't apply to biological evolution.",
    "The three scales are: (1) Individual machine behavior - studying specific intelligent machines by themselves, focusing on intrinsic properties driven by source code or design; (2) Collective machine behavior - examining interactive and system-wide behaviors of collections of machine agents; and (3) Hybrid human-machine behavior - studying interactions between machines and humans in complex hybrid systems.",
    "The paper documents algorithmic bias in computer vision, word embeddings, advertising, policing, criminal justice, and social services. It notes that practitioners sometimes must make value trade-offs between competing notions of bias or between human versus machine biases, highlighting the complexity of addressing fairness in algorithmic systems.",
    "The paper explains that while the code for specifying AI model architecture and training can be simple, the results are often very complex 'black boxes' where the exact functional processes generating outputs are hard to interpret, even for the scientists who created the algorithms. This is compounded by proprietary source code and training data, where often only inputs and outputs are publicly observable.",
    "The study showed that simple algorithms injected into human gameplay can improve coordination outcomes among humans. Specifically, locally noisy autonomous agents improved global human coordination in network experiments, demonstrating that bots can enhance human collective behavior rather than just replace or compete with humans.",
    "The standard gravity model cannot be directly used to estimate weights of existing connection flights because in airline networks, unlike complete graphs such as international trade networks, most transport involves intermediate stops and no direct connections may exist for large distances, leading to observed flows that differ from expected flows.",
    "The observed passenger flow f_ij consists of two components: (1) f_ij^(g) - the number of passengers traveling directly from origin in country i to final destination in country j, given by the gravity equation, and (2) f_ij^(transit) - the number of passengers who use the connection i \u2192 j as part of their longer journey.",
    "The model correctly predicts more than 98% of the total passenger flow in the world.",
    "The authors use p(i \u2192 j \u2192 k) = C \u00b7 f(r_ij, r_jk), where C is a normalization constant and f(r_ij, r_jk) = 1/(r_ij \u00b7 r_jk), reflecting passengers' tendency to choose the shortest connections.",
    "The distance coefficient \u03b1 was determined to be 1.5 for 1996 and 1.6 for 2004.",
    "The paper identifies three major events: the September 2001 attacks in New York and Washington D.C. (which started a chain including SARS epidemic, additional terrorist attempts, wars, and rising oil prices), the 2008 global financial crisis, and the 2010 eruption of Eyjafjallaj\u00f6kull volcano in Iceland.",
    "The authors used data from the International Civil Aviation Organization (ICAO) containing annual traffic on international scheduled services for the years 1990-2011, along with econometric data from Penn World Table 8.1 and distance data from CEPII.",
    "In 2004, only 2308 connections (10%) out of 22650 possible connections were direct flights, while 12749 connections (56%) were shortest paths with length equal to 2.",
    "The missing globalization puzzle refers to the counter-intuitive finding that despite globalization conceptually reducing effective distance, most econometric studies show the distance coefficient increases over time, meaning distance becomes more important rather than less important in economic flows.",
    "The authors use a root mean square (RMS) formula \u0394(\u03b1) = \u221a(1/N_m \u00d7 \u03a3[P(f_ij) - P(f_ij^mcf)(\u03b1)]\u00b2) to measure agreement between normalized histograms of empirical and modelled flows across 15 logarithmically spaced bins, with the minimum \u0394(\u03b1) indicating the optimal distance coefficient.",
    "The researchers identified several challenges: Chinese does not have explicit word boundary markers and contains no whitespace between words; Chinese words are not clearly marked grammatically; Chinese contains a very large number of homophones in sentences; and Chinese consists of several thousand characters (Hanzi) with words consisting of one or more characters, making recognition, segmentation and analysis more complicated compared to English.",
    "The SLS_L model achieved an AUC of 0.9360, which outperformed both the Lasso-Logistics (L_L) model with AUC of 0.8344 and the MCP-Logistic (MCP_L) model with AUC of 0.8707. The SLS_L model demonstrated the highest prediction accuracy among all tested approaches.",
    "The optimal tuning parameters were \u03bb\u2081 = 0.0251 and \u03bb\u2082 = 0.001. \u03bb\u2081 performs variable selection and controls the level of sparsity, while \u03bb\u2082 controls the degree of coefficient smoothing, representing the similarity between coefficients. The non-zero value of \u03bb\u2082 indicates that network structure information was effectively utilized.",
    "The word 'Short Term' had the highest betweenness centrality at 287.89, followed by 'Pessimistic', 'Input Market', 'Larger' and 'Market'. Betweenness centrality measures the number of times a vertex acts as a link along the shortest path between two other nodes, with high betweenness indicating high probability to control information flow in the network.",
    "The network density was 0.1695, meaning there were 522 links in the 56 \u00d7 56 adjacency matrix. This indicates the textual network is a sparse network, which is significantly affected by the fact that the text network of research reports on stocks is loosely knit instead of densely connected.",
    "A total of 2082 reports were collected from 65 security companies. The number of words in a report was most frequently between 500 and 2000 words, with a mean of 3.8 security reports per day.",
    "The study found that the highest AUC occurred when predicting the next one week (five trading days), yielding up to 0.9360 for the SWS index. This finding aligns with Asquith et al.'s discovery that analyst reports can affect market reactions with a five trade days delay, indicating that time is needed for news to translate into trading activity.",
    "Out of 56 words, 31 were found to be effective predictors (55.4%). Effective keywords were defined as those whose coefficients did not shrink to zero in at least one of the three models (MCP_L, SLS_L, and L_L), while the remaining 25 words had coefficients that shrunk to zero in all three models.",
    "Only 5 out of 25 effective terms had a positive impact on financial markets, with most keywords receiving negative connotations. The strongest positive indicator was 'Imagine' and the strongest negative indicator was 'Concern'. Even the seemingly positive word 'Securitization' received a negative connotation, supporting the asymmetric response theory that negative information has greater impact than positive information.",
    "The process involved four steps: 1) Dictionary building using Sogou cell lexicon (63,320 words), 2) Text segmentation using Jieba package with Hidden Markov model, 3) Words cleaning removing stop words and low-frequency terms, and 4) Keyword vector selection using chi-square statistics. After filtering at the 5% significance level, 56 words remained from an initial set of 3285 keywords.",
    "O(L\u00b3), where L is the number of locations",
    "When \u03b2 > 1, the resulting pruned graph may be redundant by retaining sub-optimal routes. When 0 < \u03b2 < 1, the graph is lossy as not all shortest paths are retained. \u03b2 allows trading between quality (redundancy and path quality) and complexity (edge set size).",
    "Three of the four regions achieved F1-scores exceeding 0.9: Styria Austria (0.95 with \u03b2=0.95), Central African Republic (0.94 with \u03b2=0.95), and South Sudan (0.90 with \u03b2=0.95). The German-Austrian border region achieved a maximum F1-score of 0.75.",
    "The paper uses the Open Source Routing Machine (OSRM) from OpenStreetMap, which implements multilevel Dijkstra's (MLD) and contraction hierarchies (CH) algorithms for routing.",
    "If a route between two locations has a distance similar to the sum of distances between these locations and a common third location, then the considered route is likely indirect. The algorithm compares route distances to detect when d*1,2 + d*2,3 \u2248 d*1,3, indicating location l2 lies between l1 and l3.",
    "The fastest route via Autobahn A8 was 33 km/30 minutes, while the alternative route through Rosenheim was 27.1 km/33 minutes total. The algorithm always removes this route regardless of \u03b2 < 1 because it optimizes for fastest time rather than shortest distance, even though a direct, faster route exists.",
    "For Europe with 18,091 cities/towns, the routing took 5.01 hours and the pruning took 316.36 seconds using 128 cores.",
    "For complete location graphs, the proposed algorithm has O(L\u00b3) complexity compared to O(L\u2074 log L) for the optimal Iterative-Global strategy, providing better computational efficiency without loss of optimality.",
    "Ground truth was created by manually inspecting OpenStreetMap to determine if the fastest route between each location pair is direct. A connection was labeled direct if no other location lies on or nearby the fastest route, though this process involved subjective decisions in ambiguous cases.",
    "Location graphs are needed for route optimization, load optimization in electrical and transportation networks, and agent-based modeling applications including transportation of goods, evacuation models, traffic simulations, disease transmission, movement of people, and migration simulation.",
    "The five time scales are known as the 'R's' of resilience: recon, resist, respond, recover, and restore. These represent different phases from prior to an event to potentially days or weeks after a disturbance.",
    "The adaptive capacity is bounded by the real and reactive power component limits given in equations (4) and (5) and the temporal limits defined in equations (6) and (7). This results in a 'manifold' that represents the adaptive capacity of an asset.",
    "The primary resilience challenge is fuel availability for the diesel generators. Fuel gets delivered by boat on the Yukon River, which is impassable from August through April due to long and very cold winters, potentially creating life-threatening situations during winter due to diesel fuel depletion.",
    "The frequency response is defined by df/dt = f\u2080\u0394P/2H, where \u0394P is the disturbance or difference between generation and load, and H is the inertial constant. Large inertia slows the rate of frequency response, allowing additional time for generation units to ramp up or down before reaching frequency limits.",
    "By January 5, 2019, the Alaska Village Electric Cooperative (AVEC) had installed a 900 kW, 52-meter rotor diameter type IV pitch-controlled wind turbine generator manufactured by Emergya Wind Technologies and started producing power.",
    "When diesel generators are taken offline, there is a large negative impact on short-term resilience due to reduced inertia and generation ramping capability, but a positive effect on long-term resilience because the system burns less fuel to support the load, conserving fuel for extended operation.",
    "The platform treats GridLAB-D and Python federates as message federates, with data exchange configured using JSON config files by defining corresponding endpoints. The HELICS API facilitates communication through specific endpoints for monitoring and dispatching data.",
    "The generators are assumed to have an inertia constant of 2, have ramping capability of reaching full output in 10 seconds, and their rate of burning fuel for this study are constant.",
    "The short-term resilience is based on a frequency limit of 58Hz. It measures the maximum size of disturbance the system can withstand without dropping below this frequency limit before under frequency load shed (UFLS) occurs.",
    "When wind is run at maximum output, diesel generation can be taken offline, improving long-term resilience by conserving fuel. When run below maximum output, the fast ramping capability of inverters allows for increased short-term resilience of the system by providing additional generation capacity for frequency response.",
    "The researchers implemented a 'Google Trends strategy' that sells the DJIA at closing price p(t) if the relative change in search volume \u0394n(t-1, \u0394t) > 0, and buys back at price p(t+1). If \u0394n(t-1, \u0394t) < 0, they buy at p(t) and sell at p(t+1). The strategy uses relative change in search volume: \u0394n(t, \u0394t) = n(t) - N(t-1, \u0394t) where N(t-1, \u0394t) is the average search volume over the previous \u0394t weeks.",
    "The best-performing Google Trends strategy achieved a profit of 326% using the search term 'debt' with \u0394t = 3 weeks during the period from January 2004 to February 2011.",
    "The researchers quantified financial relevance by calculating the frequency of each search term in the online edition of the Financial Times from August 2004 to June 2011, normalized by the number of Google hits for each search term. They found a positive correlation between this financial relevance indicator and trading returns (Kendall's tau = 0.275, z = 4.01, N = 98, p < 0.001).",
    "Strategies based on U.S. search volume data performed better because investors prefer to trade on their domestic market. The researchers found that U.S.-only search data better captured the information gathering behavior of U.S. stock market participants than worldwide data, with mean returns of 0.60 vs 0.43 standard deviations above random strategies (t = 2.69, df = 97, p < 0.01).",
    "The 'buy and hold' strategy yielded 16% profit, equal to the overall increase in the DJIA from January 2004 to February 2011. The 'Dow Jones strategy', which used changes in stock prices instead of search volume data, yielded only 33% profit with \u0394t = 3 weeks, or 0.45 standard deviations above random strategies when averaged over \u0394t = 1 to 6 weeks.",
    "The researchers tested each component separately: a long-only strategy (buying after search volume decreases) achieved mean returns of 0.41 standard deviations above random (t = 11.42, p < 0.001), and a short-only strategy (selling after search volume increases) achieved 0.19 standard deviations above random (t = 5.28, p < 0.001). Both components significantly outperformed random strategies.",
    "The researchers used Herbert Simon's model of decision making, suggesting that Google Trends data and stock market data reflect two subsequent stages in investors' decision-making process. They proposed that trends to sell at lower prices are preceded by periods of concern, during which people gather more information about the market, reflected by increased Google search volumes for financially relevant terms.",
    "The researchers analyzed 98 search terms. They included terms related to the concept of stock markets, with some terms suggested by the Google Sets service, a tool which identifies semantically related keywords. The set was not arbitrarily chosen as they intentionally introduced some financial bias in their selection.",
    "The researchers averaged over three realizations of each search term's time series, based on three independent data requests made on consecutive weeks (April 10, 17, and 24, 2011). They noted that search volume data changes slightly over time due to Google's extraction procedure, so this averaging approach addressed the variability across different access dates.",
    "The Google Trends strategies significantly outperformed random investment strategies with a mean return of 0.60 standard deviations above the random strategy mean (t = 8.65, df = 97, p < 0.001, one sample t-test) for U.S. search data, and 0.43 standard deviations (t = 6.40, df = 97, p < 0.001) for global search data.",
    "2nd January 2007 until 31st December 2012",
    "891,171 different words",
    "0.074",
    "Bank of America (BAC) with a correlation of 0.43",
    "5 am London time",
    "9:30 am to 4 pm New York time (for most of the year, 2:30 pm to 9 pm London time)",
    "Travelers replaced Citigroup on 8th June 2009",
    "One day before the news (lag -1) and on the same day as the news (lag 0)",
    "0.040",
    "No significant correlation was found (median correlation coefficient = 0.000, p = 0.784)",
    "The width D of the stripe at time scale T is defined as D(T) = (L/T - 1)^(-1) * \u03a3|P_T(t_{k+1}) - P_T(t_k)|, which is the average of the absolute value of price increments at time scale T.",
    "The study analyzed 9 stocks from the London Stock Exchange during 2002 (251 trading days). The stocks were: AstraZeNeca (AZN), British Petroleum (BP), GlaxoSmithKline (GSK), Heritage Financial Group (HBOS), Royal Bank of Scotland Group (RBS), Rio Tinto (RIO), Royal Dutch Shell (SHEL), Unilever (ULVR), and Vodafone Group (VOD).",
    "The average Hurst exponent was 0.44, which is less than 0.5, indicating negative correlation and antipersistent behavior in price increments.",
    "The memory effect disappears at time scales larger than 180 seconds for resistances and 150 seconds for supports. The statistical significance (p-value < 0.05) was maintained up to 60-90 seconds for both supports and resistances.",
    "The time between consecutive bounces (t) followed a power law distribution, while the maximum distance (d) was best described by an exponentially truncated power law. For example, at 60 seconds time scale: N \u221d t^(-0.56) for time and N \u221d d^(-0.61) exp(-0.03 d) for distance.",
    "They modeled bounce events as a Bernoulli process and used Bayesian inference to estimate p(b|b_prev). The expected value is E[p(b|b_prev)] = (n_b_prev + 1)/(N + 2) and the variance is Var[p(b|b_prev)] = [(n_b_prev + 1)(N - n_b_prev + 1)]/[(N + 3)(N + 2)\u00b2].",
    "The researchers found no evidence of highly preferred prices in their histograms of local minima/maxima. When comparing histograms of support and resistance levels with overall price level histograms, they observed no anomalies or significant excess around round numbers across all stocks and time scales investigated.",
    "The conditional bounce probabilities for actual stock data were well above 0.5 and increased with the number of previous bounces, while shuffled series showed probabilities near 0.5 that remained nearly constant. This difference was at least one order of magnitude larger than any bias from finite stripe effects, providing evidence for memory effects in the original data.",
    "The researchers used two main statistical tests: (1) A chi-squared test with 3 degrees of freedom to test the independence hypothesis p(b|b_prev) = c, using a significance level \u03b1 = 0.05, and (2) A Kolmogorov-Smirnov test to assess whether bounce frequencies from reshuffled series were compatible with the posterior distribution found for bounce frequency.",
    "Technical analysis assumes that 'history repeats itself' because price trends reflect market psychology, and since the psychology of investors does not change over time, investors will always react in the same way when they encounter the same conditions. This leads to the belief that patterns that anticipated specific trends in the past will do the same in the future.",
    "The three components are economic cost C_F, social cost C_S, and environmental cost C_E.",
    "The environmental cost C_E is directly proportional to capacity c through the relationship C_E = \u03b3ct, where \u03b3 = abc and represents conversion constants linking capacity to pollutant, pollutant to impact, and impact to cost.",
    "The two conditions are strong sustainability (when demand is met with no substitution) and weak sustainability (when demand is met via substitution). In strong sustainability, the base activities alone fulfill the demand, while weak sustainability allows for replacement activities when base activities fail to meet constraints.",
    "In the weak case, the set of all sustainable activities forms a subset of an N-level union of sustainable activities and creates a topological cover.",
    "The three conditions are: (1) C'_i,j,... \u2264 C'_i,j,...^max for all cost components, (2) t'_i,j,... \u2264 t'_i,j,...^max for duration constraints, and (3) either strong sustainability where D = \u03a3c\u00b9_i (base activities meet demand) or weak sustainability where demand is met through substitution via a topological cover.",
    "The five requirements are: (1) satisfying human food and fiber needs, (2) enhancing environmental quality and natural resources, (3) making efficient use of non-renewable and on-farm resources, (4) integrating natural biological cycles and controls, and (5) sustaining economic viability while enhancing quality of life.",
    "The fundamental challenge is that social costs are difficult to quantify and assign monetary values to, making it problematic to connect the social dimension analytically to the other dimensions. The paper notes this is easier to handle at the local level where consensus is more achievable.",
    "The paper addresses higher-order dependencies by acknowledging that activities have cascading effects beyond their immediate scope, similar to Life Cycle Assessment. The bike trip example illustrates this: riding a bike requires eating an apple, which supports an apple farmer, who uses trucks for transport that emit pollution, creating indirect environmental costs.",
    "Zachary acknowledges that while economic values can be determined from markets and environmental costs from measurements and models, social costs will continue to be difficult to quantify. Many theoretical terms like 'balance', 'restrictions', 'maintaining', and 'controlling' are inherently difficult to quantify, and some approaches like Socio-Biological don't lend themselves easily to quantitative models.",
    "The eight classifications are based on combinations of: duration constraint satisfaction (t < t_max: yes/no), cost constraint satisfaction (C < C_max: yes/no), and activity complexity (single/multiple levels). This creates 2\u00b3 = 8 possibilities ranging from unsustainable single-level activities that meet neither constraint to sustainable/potentially sustainable multiple-level activities that meet both constraints.",
    "The proposed system achieves a binary pattern modulation speed of 97 kHz, which is about 5 times faster than the fastest DMD's maximum rate of 20.7 kHz.",
    "The final modulation frequency is given by \u03b4 = (Fg \u00d7 \u0394x)/(b \u00d7 \u03b4), where Fg is the scanning frequency of the galvanic mirrors, \u0394x is the scanning range of the beam on the DMD, b is the binning number of DMD mirrors in one direction, and \u03b4 is the size of each micro-mirror of the DMD.",
    "The authors achieved 42 Hz frame rate at 80 \u00d7 80-pixel resolution for dynamic scene imaging. They used a compressive sensing based algorithm incorporating both spatial and temporal prior constraints with the optimization function min(\u03bb\u03a8(Xt) + \u03a6(Xt - Xt-1)).",
    "Using high-end galvanic mirrors like the CTI 6200H with 1 kHz working frequency, the modulation speed could reach 485 kHz, which would enable 210 frames per second at 80 \u00d7 80-pixel resolution.",
    "As k increases, the imaging quality degrades because successive scanned sub patterns from a high resolution random pattern are not entirely independent from each other, which mathematically degenerates the reconstruction performance. However, even with large k values and noise, the results still restore decent images.",
    "The relationship is yi - yi-1 = \u03a3u,v Pi(u,v)\u0394X(u,v), where \u0394X(u,v) = X(u,v) - X(u-1,v). This shows that the difference between consecutive measurements can reconstruct the horizontal edges of the scene, reducing the required patterns by 50% compared to previous methods.",
    "The DMD is a Texas Instrument DLP Discovery 4100, 7XGA with 1024 \u00d7 768 pixel resolution, 13.6 \u03bcm micro-mirror size, and maximum 20 kHz projection rate. The galvanic mirrors are GVS011 from Thorlabs, single axis scanning devices operating at 200 Hz.",
    "The relationship is x = d sin(2\u03b8), where x is the distance from the beam's hitting position at GM1 to that at the DMD, d is the distance between the two galvanic mirrors (12 cm), and \u03b8 is the rotating angle. For small scanning ranges, this can be approximated as p = p0 - (2dkU)/\u03b40.",
    "Using acoustic optical deflectors (AOD) with 20 kHz scanning frequency could achieve 20 times faster illumination patterning, reaching 9.5 MHz modulation speed.",
    "The main limitations are: (1) the system needs careful mechanical mounting for calibration, though this can be addressed with customized programmable mounts, and (2) the scheme currently only works for random patterns and is inapplicable for other structured patterns like Hadamard and sinusoidal patterns.",
    "The method combines structured event extraction using syntactic parsing, Restricted Boltzmann Machines (RBMs) for pre-training, and sentence2vec framework to achieve effective event embeddings.",
    "The model uses an estimated true label sgn(Pi - P0) shared across all data sources in the hinge loss functions, where Pi is the probability for multi-source information on day i and P0 is a threshold parameter to determine prediction positiveness.",
    "The three levels are: super group level loss (log-likelihood), group level loss (temporal consistency between consecutive days), and instance level loss (hinge losses for each data source).",
    "News events contributed most to the overall prediction, followed by quantitative data in second place, while sentiments had the least impact among the three sources.",
    "M-MI improved F1-score by 6.9% in 2015 and 9.2% in 2016 compared to the nMIL baseline.",
    "The justification is based on the Efficient Market Hypothesis, which suggests that different data sources would keep up-to-date with the latest stock market information and commonly indicate the same sign (index rise or fall), allowing for consensus learning among correlated predictions.",
    "The three-level tree has the core verb as root node, subject and object of the verb as second layer nodes, and their nearest modifiers as child nodes. The core words (verb, subject, object, and their modifiers) are connected together as structure information to represent the event.",
    "F1-scores generally first increase then decrease as history days increase. This is explained by the quick decay of impacts from news, sentiments, and quantitative indices after 2-3 days, making out-of-date information less relevant.",
    "The LDA-S method (an extension of Latent Dirichlet Allocation) is used because it extracts topic-specific sentiments, recognizing that sentiment polarities depend on topics or domains, where the same word can express different sentiments in different contexts.",
    "Pi = \u03b80pm\u2212i + \u03b81pd\u2212i + \u03b82ps\u2212i, where \u03b80, \u03b81, and \u03b82 are source-specific weights for news, quantitative data, and sentiments respectively, with the constraint that \u03b80 + \u03b81 + \u03b82 = 1.",
    "Agricultural land consolidation, rural construction land consolidation, land reclamation, and land development",
    "27 administrative villages covering 28,090 hm\u00b2, representing 32.75% of the total area",
    "RS\u1d62 = \u2211\u2c7c\u2096\u2098 S\u1d62\u2c7cH\u1d62\u2096 X\u2c7c\u2096E\u2096\u2098, where j is the source of risk, k is the habitat type, m is the ecological receptor type, S\u1d62\u2c7c is density of risk sources, H\u1d62\u2096 is habitat abundance, X\u2c7c\u2096 is exposure coefficient, and E\u2096\u2098 is response coefficient",
    "Number of input nodes: 10, number of output nodes: 30, number of iterations: 1000",
    "Landscape pattern (0.01 to 1.62) and soil (0.01 to 1.46) show the highest ecological risk values",
    "D\u1d62\u2c7c = w\u209b \u00b7 D\u02e2\u1d62\u2c7c + w\u2090 \u00b7 \u221a\u2211\u1d30d=1 w\u2090 \u00b7 (a\u1d48\u1d62 - a\u1d48\u2c7c)\u00b2, where D\u1d62\u2c7c is mixed distance, D\u02e2\u1d62\u2c7c is geospatial distance, and w\u209b and w\u2090 are geospatial and attribute space weights respectively",
    "Ecological risk weight of land consolidation: 0.4, time urgency weight: 0.3, spatial suitability weight: 0.3",
    "Soil quality, meteorological conditions, water quality, and farmland conditions",
    "Area C exhibits the lowest ecological risk levels across all aspects, indicating potential advantages for agricultural development and superior water resources",
    "h\u1d62\u2c7c = e^(-D\u1d62\u2c7c/2\u03c3\u00b2), where h\u1d62\u2c7c is the proximity function, D\u1d62\u2c7c represents the distance between neurons, and \u03c3 is the diffusion parameter of the Gaussian function",
    "Graph theory extends to Leonhard Euler in the 18th century.",
    "Relational algebra grew out of a need to efficiently compress data during the 1960s, when storage was both limited and very expensive.",
    "Graphs are expressed in node-arc-node (subject-predicate-object) triples.",
    "The parallel-processer-based graphics processing unit (GPU) offers hardware alternatives to accelerate large-scale graph processing, and some firms, such as Cray, have developed specially configured supercomputers to digest and return rapid results from massively scaled graphs.",
    "When the preponderance of relationships becomes many-to-many, RDBMS performance takes a nosedive. Moreover, the RDBMS schema is typically inflexible, requiring high maintenance to effect the most minute change.",
    "The hierarchical model was created at IBM to represent tree-structured relationships, and the network model of the late 1960s was an early attempt to model objects and their relationships, which would re-emerge in the 1980s with object-oriented databases.",
    "There is little semantic commonality among the various graph languages in use and their rules of syntax. Higher volume graph databases rely on stylized variations of RDF to enumerate their triples, making sharing data between various graph databases dependent on the user's tolerance for expressing the same triples in differing syntactical frameworks.",
    "To alleviate storage consistency concerns, many graph databases do support the Atomic, Consistent, Isolated, and Durable (ACID) consistency model, which is a spin-off storage-locking scheme from RDBMS technology.",
    "Graphs can be intelligently reduced to more salient subgraphs that can be better managed, queried, and understood. This reinforces the practice of persisting data in a relational or appropriate nongraph NoSQL environment, from which subgraphs (database 'views') can be intelligently isolated for further analysis.",
    "Built-in mathematical functions, residing in graph databases, could add a level of depth to truly understanding and quantifying graph relationships. Metric algorithms derived from graph theory could be applied as analytical tools, extending beyond mere networks to networks of networks.",
    "Mars-like pressure was set at 8 mbar while Earth-standard pressure was set at 1000 mbar.",
    "The relative errors were 6.79% for sandy soil, 7.68% for intermediate soil, and 7.76% for bedrock soil.",
    "The maximum difference between surface and subsurface temperature increased by 26.72%, from 11.6\u00b0C at Earth's pressure to 14.7\u00b0C at Mars' pressure.",
    "I_sin = (\u0394G_s/\u0394T_s)\u221a(2\u03c0/P), where \u0394T_s = T_max - T_min, \u0394G_s = G_max - G_min, and P is the diurnal period.",
    "An anti-reflection coated germanium circular optic with 74.9 mm diameter and 5.0 mm thickness was chosen to comply with the minimum thickness required to avoid reaching germanium's fracture strength caused by the pressure differential.",
    "A total of 9,225 radiometric images were collected and saved as plain text 640 \u00d7 480 matrices with each cell containing the temperature in degrees Celsius.",
    "k = k_r + k_c + k_g, where k_r is transfer across pore spaces, k_c is conduction between grain contact areas, and k_g is conduction of gas filling pores between grains. Gas conduction (k_g) dominates at pressures between 0.1 and 1000 mbar.",
    "At Earth's pressure, the relative difference between highest and lowest thermal inertia soils was only 4.20%, but at Martian pressure, this difference increased significantly to 42.84%, indicating soils can be better assessed at Martian pressure.",
    "The PI-640i is a 320-g LWIR camera working in 8\u201314 \u03bcm spectral range, with 640 \u00d7 480 pixel resolution, 60\u00b0 \u00d7 45\u00b0 FOV, germanium optic, measuring temperatures from \u221220\u00b0C to 900\u00b0C with 0.04\u00b0C thermal sensitivity.",
    "G = \u2212I\u221a(\u03c0/P) \u2202T/\u2202Z'|_{Z'=0} = \u03b5\u03c3_B T^4_{heater} \u2212 \u03b5\u03c3_B T^4_{s_i}, where T_{heater} represents MEC heater temperature and T_{s_i} represents mean surface temperature of each soil sample.",
    "The two fundamental problems are: (1) Sign ambiguity - singular vectors with similar information are randomly distributed in two areas due to random variable pk \u2208 {1, -1}, and (2) Manifold features - singular vectors are unit vectors with norm 1 that exist as points on a unit hypersphere (manifold), making them inefficient to learn with generic neural networks based on Euclidean geometry.",
    "SVP achieves approximately 36% better performance than GAP under FGSM adversarial attacks on the CIFAR10 dataset with natural training.",
    "Sign ambiguity is removed by aligning singular vectors based on a center vector that is rotated to {0, 0, ..., 0, 1, 0}. The method verifies which hyper-sphere a singular vector belongs to according to the sign of \u0169HW-1,k and multiplies vectors on the negative half-sphere by -1 to align them.",
    "The arccosine function's first derivative can easily diverge as d/dz arccos(z) = -1/\u221a(1-z\u00b2) for -1 < z < 1. To prevent this instability during learning, the arccosine is approximated as -z + \u03c0/2 using first-order Taylor expansion.",
    "KL-divergence is used to learn the center vector uc. The method assumes that each component of the aligned \u016b features follows a Gaussian distribution, and the center vector is learned so that this assumption holds true by minimizing the KL-divergence between the actual distribution and a Gaussian distribution.",
    "SVP achieves a silhouette score of 0.692, which is 0.444, 0.510, and 0.551 better than GAP (0.248), GMP (0.182), and MPN (0.141) respectively. The SVP silhouette score is more than twice as high as other pooling methods.",
    "Rotation centers the singular vector distribution in the spherical coordinate system by rotating the center vector vc to {0,...,0,1,0} using Rodrigues rotation. This prevents singular vectors from being located near discontinuity boundaries where components are bounded ([0,\u03c0] for \u03c61 to \u03c6N-2 and [-\u03c0,\u03c0] for \u03c6N-1), ensuring the shortest path coincides with the real path during learning.",
    "KD-SVP achieves a performance improvement of 1.69% over the original KD-SVD method on the CIFAR100 dataset (improving from 71.64% to 73.33% accuracy).",
    "When the sign ambiguity removal step (step 2) is omitted, learning becomes impossible. The ablation study shows that this step is indispensable for proper learning of singular vectors, as indicated by the 'NaN' result in the experimental plot.",
    "SVP has higher computational complexity than general pooling methods due to the SVD computations required. However, it has lower forward time than MPN (matrix power normalized covariance pooling), which is a lightweight second-order pooling method. The burdensome SVD computations may make it difficult to apply directly to embedded systems or mobile environments.",
    "1 MHz A-scan rate",
    "SLD-371-HP3 superluminescent diode with center wavelength of 838 nm, full-width at half-maximum of 81 nm, and optical power of 27.2 mW",
    "8 vol/s for an image range of 250 \u00d7 250 \u00d7 2048 pixels (9 \u00d7 4.5 \u00d7 5 mm)",
    "Protective film: 100 \u03bcm, transparent film: 250 \u03bcm, deco film: 150 \u03bcm, base film: 100 \u03bcm",
    "139 dB for camera #1 and 137 dB for camera #2",
    "C++, CUDA, and Qt",
    "By using identical but reversed trigger sequences of 250 kHz with 50% duty cycle to each camera, capturing two continuous A-lines in one period, guaranteeing 500 kHz effective imaging rate which is twice the maximum sampling rate",
    "8 seconds for the whole-range scanning",
    "40 \u03bcm, 20 \u03bcm, and 40 \u03bcm gaps between the layers",
    "2048 pixels resolution (e2v OCTOPLUS, TELEDYNE e2v, U.K.)"
  ],
  "predicted_answers": [
    "The two main components of the Gander research project were an empirical component focused on problem conceptualization and a development component focused on creating an experimental code review platform with eye-tracking.",
    "The three categories of participants are (1) employees or other stakeholders of software development organizations, (2) students or other beneficiaries of the university involved, and (3) independent participants. They are distinguished based on legal and ethical concerns regarding the relationship between researchers and participants, and the openness of data and artifacts.",
    "The data pipeline consists of four steps: data cleaning, data exploration and visualization, model building and analysis, and findings presentation. In a quantitative study, this involves transforming data for statistical tools and in a qualitative study, it includes transcribing data and obtaining initial codes.",
    "The framework defines three steps of artifact generalization toward reuse:\n1.  **Publication for reproduction:** This results in artifacts in their original, non-editable state.\n2.  **Generalization for continued development:** This involves releasing artifacts with guidance on how to adapt them, such as through inviting a community.\n3.  **Generalization for general use:** This results in artifacts in an editable state.\n\nEach of these steps requires additional investments in making the artifacts openly available.",
    "The paper identifies three degrees of empirical data collection in software engineering: direct involvement of software engineers (interviews), indirect involvement (eye-tracking), and study of work artifacts only (code).\n\nThe risks identified in sharing qualitative interview data from the Gander project include potential exposure of company secrets, criticism of individuals, and the difficulty of anonymizing participants.\n\nThe context outlines a framework for assessing data openness, considering factors like coreness, currentness, extent, granularity, and quality.",
    "The authors recommend sharing project data and artifacts as an example and derive preliminary recommendations for open science in SE.",
    "The Gander project encountered a licensing issue when releasing its platform, specifically regarding a project used for gaze data analysis which initially lacked a license. After contacting the author of that project, a MIT license was added, and the use of the project remained unchanged.",
    "The Gander platform incorporated eye-tracking to explore gaze-driven assistance in code review. A proof-of-concept assistant was developed that triggered visualizations of use-declaration relationships in code based on gaze fixation points. This assistant was tested in a user study with eight participants.",
    "Quantitative survey data is easier to anonymize than qualitative interview data.",
    "The authors recommend a balance between openness and closeness, advocating for open access to research artifacts like protocols and code books while cautioning against sharing sensitive qualitative data from companies.",
    "The motivation is to select high-performance potential networks without training, and to address the limitations of single-indicator approaches in training-free NAS.",
    "The three types of features considered by the proposed feature fusion-based indicator (FFI) are CJ, NLR, CNNTK, and OS. Each feature captures a specific aspect of the network.",
    "The feature fusion mechanism combines different indicators by sampling weights following an analysis.",
    "The benchmark datasets used were CIFAR-10, CIFAR-100, and ImageNet-16-120.",
    "The proposed FI outperforms standalone indicators in all performance assessment methods across datasets on NAS-Bench-201. Adding more indicators consistently improves performance, with FI4 reaching 0.88 and 0.7017 SROCC and KROCC.",
    "Kendall\u2019s Tau correlation and Spearman Rank-Order Correlation Coefficient (SROCC) were used to evaluate the indicator\u2019s performance. Pearson Linear Correlation Coefficient (PLCC) and Root Mean Square Error (RMSE) were also used.",
    "The ablation studies conducted to validate the effectiveness of the feature fusion strategy involved using combinations of two to four indicators (CJ, NLR, CNNTK, OS) and analyzing their performance on NAS-Bench-101.",
    "The context does not provide a direct comparison of the computational cost between FFI and training-based NAS methods. It describes the resource-intensive nature of early reinforcement learning-based NAS and the emergence of more efficient techniques like cell-based and differentiable NAS. It also highlights the limitations of relying on a single indicator and the need for more sophisticated approaches, including using multiple indicators and learning appropriate weights for each.",
    "The authors provide insights regarding the robustness of FFI across different search spaces by comparing its performance against standalone indicators on CIFAR-10 within the NAS-Bench-101 search space. The Fusion Indicator (FI) outperforms other indicators, achieving higher KROCC and SROCC scores.",
    "The authors suggest combining CNNTK and OS to improve training-free NAS indicators beyond FFI.",
    "The main motivation for developing a hybrid gaze distance estimation method is to overcome the limitations of existing approaches, particularly those relying solely on binocular vergence angles, which are prone to errors due to human factors like blinks and pupil dilation.",
    "The two primary cues integrated in the proposed hybrid method are binocular vergence and gaze-mapped depth. They complement each other by providing independent estimates of gaze distance, allowing for cross-referencing and a more robust final estimate.",
    "The experiments involved three gaze targets at different distances (0.5m, 1.0m, and 1.3m), labeled as near, middle, and far targets. The setup included a reflective laptop screen to assess environmental factors and a chin rest to prevent unintended gaze distance variations.",
    "The proposed cross-referencing mechanism relies on pupil size and geometry to relate pupil position to target depth, utilizing pupil center distance variations and multi-layer perceptrons.",
    "The proposed method significantly outperformed current methods with a visual angle error of 0.132 degrees under ideal conditions and consistently maintained robustness against human and environmental errors, achieving an error range of 0.14 to 0.21 degrees even in demanding environments.",
    "The context describes calibration as a feature of the proposed hybrid system, where it involves cross-referencing vergence and gaze-mapped depth estimations to produce more precise gaze distance measurements, mimicking human visual perception.",
    "Developing user gaze distance estimation modules for varifocal AR/VR devices, digital healthcare, human-computer interaction, and analyzing user behavior and preferences.",
    "Researchers have to decide what degree of openness they apply with respect to their intellectual property and the effort it takes to make the artifacts openly available.",
    "The hybrid method addresses the 'mid-range gap' problem by concurrently utilizing vergence angle estimation and gaze-mapped depth information. It cross-references the two estimations and calculates an adaptive weighted average to derive a more precise gaze distance estimate, mitigating errors from human factors and environmental conditions.",
    "The authors suggest applying the proposed method to developing user gaze distance estimation modules for varifocal AR/VR devices and exploring potential applications in digital healthcare, human-computer interaction, and analyzing user behavior and preferences.",
    "The four quality attributes used in the proposed trust evaluation model are availability, reliability, data integrity, and efficiency.",
    "The model jointly optimizes trust and communication delay by maximizing trust and minimizing delay through a Mixed Integer Linear Programming (MILP) problem. It considers factors like availability, reliability, data integrity, and efficiency to estimate trust, and uses communication delay estimates based on server capacity and traffic.",
    "The Genetic Algorithm (GA) is proposed as a heuristic for generating near-optimal solutions to the resource allocation problem, particularly suitable for large-scale instances where formal methods are limited.",
    "The loss of a generator has a reduction of short-term resilience but increases the long-term resilience.",
    "The main causes of reliability degradation in the proposed model\u2019s experiments are human error (eye blinks and drift) and environmental factors (disocclusion regions and reflective surfaces).",
    "The paper proposes a model to consider trust and delay in resource allocation, aiming to balance trust and communication delay.",
    "The study used a MEC (Multi-Element Chamber) simulator.",
    "How does the weighting of trust versus delay impact the model\u2019s outcomes?\n\nThe model aims to maximize trust while minimizing communication delay by considering availability, reliability, data integrity, and efficiency as attributes to estimate trust. The model balances these conflicting objectives through a joint optimization problem.",
    "The experiments were executed in multiple scenarios using CloudSim and MATLAB environments.",
    "The authors share their experiences and considerations on open science in socio-technical SE from a case study on gaze-driven code review, aiming to structure analysis and communication about data and artifact openness and thereby guide future open science in SE.",
    "The primary motivation for studying 28Si single crystals in the context of the kilogram realization is to determine the mass of a 28Si sphere and correct for mass deficits caused by vacancy defects.",
    "EPR measurements were used to increase the reliability of mass deficit correction in the XRCD method, identifying phosphorus impurity and reporting low concentrations of nine types of vacancy defects.",
    "The concentrations of nine types of vacancy defects with unpaired electrons in the crystal were estimated to be less than 1 \u00d7 10 12 cm -3.",
    "EPR measurements increased the reliability of mass deficit correction in the XRCD method by identifying phosphorus impurity and reporting low concentrations of nine types of vacancy defects.",
    "EPR spectra were recorded at four different directions \u03b8 = 0\u00b0, 30\u00b0, 60\u00b0, and 90\u00b0 by rotating the sample about the [0\u030411] direction.",
    "Quantification of oxygen-vacancy defects is critical for realizing the kilogram method using X-ray crystal density (XRCD) because it allows for accurate mass deficit correction. The mass deficit caused by these defects must be accounted for when determining the mass of silicon spheres.",
    "The concentration of phosphorus impurity was determined to be 3.2(5) \u00d7 10^12 cm^-3, and the concentration of defects on mechanically damaged surfaces was 1.67(23) \u00d7 10^12 / 1.21(2) cm^-2 = 1.4(2) \u00d7 10^12 cm^-2.",
    "How do phosphorus donor concentrations affect the silicon sample used in the experiment?\nThe number and concentration of EPR-active phosphorus impurity (Si4 \u2261 _ _ P\u00a5) in the sample were determined to be 0.21(3) \u00d7 10 12 and 3.2(5) \u00d7 10 12 cm -3 , respectively.",
    "The context highlights that 28Si single crystals offer advantages for EPR studies due to the low concentrations of phosphorus impurity and defects on mechanically damaged surfaces. Specifically, the measured surface density of defects is comparable to that of hydrogenated amorphous silicon.",
    "The authors propose an EPR measurement to quantify vacancy defects in silicon crystals and enhance the reliability of the Avogadro constant determination.",
    "The primary causes of process-induced random variation are line edge roughness (LER), random dopant fluctuation (RDF), and work function variation (WFV).",
    "The three parameters used to characterize line-edge roughness are amplitude, correlation length X, and correlation length Y.",
    "The datasets for training and validating the ANN model were generated as 130 datasets from 6,500 FinFETs (70% training, 30% validation) and 10 datasets from 2,500 FinFETs.",
    "The evaluation method used earth-mover's distance (EMD) score, also known as the Wasserstein metric, to compare ANN predictions against TCAD results. The EMD score measures the difference between two probability distributions by quantifying the minimal amount of work needed to transform one distribution into the other.",
    "The optimized ANN with the mixture of MVNs has 3 neurons for the input layer, 81 neurons for the first hidden layer, 162 neurons for the second hidden layer, 324 for the third hidden layer, and 324 neurons for the output layer.",
    "Negative log likelihood (Negloglik) was used as the loss function because the output of the probabilistic layer returns the PDF of variables, and conventional mean-squared error cannot be used.",
    "The mixture-MVN ANN successfully predicts skewness, kurtosis, and non-linear correlation, which is distinctly different from plain MVN.",
    "The time spent to train the ANN model was reduced from 1412 sec to 185 sec.",
    "The proposed ANN models shortened the simulation time by six times compared to the previous ML model.",
    "The ANN-based approach shortens simulation time by six times compared to a previous ML-based model, successfully predicting non-Gaussian device performance metrics not achievable by the prior model, and expanded the prediction target to include more transistor parameters.",
    "Line-edge roughness (LER) becomes a more significant source of variation as FinFET dimensions scale down because the amplitude of LER does not shrink as much as the feature size, leading to a larger variation in device characteristics.",
    "The three main parameters used to generate 3D LER profiles in the simulations are: RMS amplitude, x(y)-axis correlation length, and roughness exponent. They physically represent the standard deviation of the LER amplitudes, the wavelength of the LER profile, and the way high-frequency components in the LER profile diminish, respectively.",
    "The dataset for training and testing the HS-BNN comprised two types of datasets: one with 130 datasets containing performance metrics of 50 different FinFETs each, and another with 10 datasets containing performance metrics of 250 different FinFETs.",
    "The HS-BNN predictions were evaluated using K-fold crossvalidation, calculating MAPEs, RMSEs, MAEs, and PLLs to determine overall prediction performance compared to the Bayesian linear regression (BLR) model.",
    "The HS-BNN uses horseshoe priors to estimate the distribution of the current-voltage characteristics, allowing for uncertainty quantification in model selection.",
    "The HS-BNN predictions were evaluated using K-fold crossvalidation, calculating MAPEs, RMSEs, MAEs, and PLLs to determine overall prediction performance compared to the Bayesian linear regression (BLR) model.",
    "The HS-BNN showed much better results than the Gaussian-BNN when the number of nodes exceeded 200, specifically for GLYPH<27> IDS, where the MAPE was approximately 7%. The Gaussian-BNN showed different results for varying numbers of nodes.",
    "The HS-BNN predictions were evaluated using K-fold crossvalidation, calculating MAPEs, RMSEs, MAEs, and PLLs to determine overall prediction performance compared to the Bayesian linear regression (BLR) model.",
    "The HS-BNN predictions for IOFF and VTH varied when LER parameters were changed.",
    "The HS-BNN predictions were evaluated using K-fold crossvalidation, calculating MAPEs, RMSEs, MAEs, and PLLs to determine overall prediction performance compared to the Bayesian linear regression (BLR) model.",
    "The innovations in the physics package design included using a rubidium spectral lamp with Xe as the starter gas, optical and isotope double-filtering, a large slotted tube microwave cavity, and a rubidium absorption cell with a diameter of 40 mm.",
    "A sealed box was designed to isolate the PP from the barometric environment, reducing the barometric effect by nearly one order of magnitude.",
    "The slotted tube cavity\u2019s size could be flexibly designed, allowing for a larger absorption cell to enhance the atomic signal SNR.",
    "The experimentally optimized operating temperatures for the absorption cell were 68 \u00b0C, the filter cell was 93 \u00b0C, and the lamp bulb was 109 \u00b0C.",
    "The stability of the RAFS was predicted to be 7.6 \u00d7 10 -14 \u03c4 -1 / 2, and the measured stability was 9.0 \u00d7 10 -14 \u03c4 -1 / 2 (1-100 s) and 9.1 \u00d7 10 -14 \u03c4 -1 / 2 (1-100 s) with the hydrogen maser as a reference, and 9.7 \u00d7 10 -14 \u03c4 -1 / 2 and 9.1 \u00d7 10 -14 \u03c4 -1 / 2 up to 100-s averaging time with the OMG as a reference.",
    "The phase noise of the 6.835-GHz microwave was measured at 2 fM, with a phase noise of approximately -110 dBc/Hz.",
    "The environmental factors besides barometric pressure quantified were absorption cell temperature, pumping light intensity, 6.835-GHz microwave power, and magnetic field. The largest impact on stability came from the barometric effect.",
    "The H-maser and OMG references showed similar short-term stability results, with the RAFS stability measured as 9.0 \u00d7 10 -14 \u03c4 -1 / 2 using the H-maser and 9.1 \u00d7 10 -14 \u03c4 -1 / 2 using the OMG, both within the 1-100 s averaging time.",
    "Achieving 10\u207b\u00b9\u2074\u03c4\u207b\u00b9/\u00b2 stability represents a significant improvement compared to the previous best result of 1.2 \u00d7 10\u207b\u00b9\u00b3\u03c4\u207b\u00b9/\u00b2.",
    "The predicted contributions to RAFS stability are:\n*   SNR limited stability (\u03c3 SNR (\u03c4))\n*   Phase noise limited stability (\u03c3 PN (\u03c4))\n*   Environmental effect limited stability (\u03c3 EE (\u03c4))",
    "What are the three statistical conditions that must be simultaneously satisfied for a group of stocks to be classified as the Leading Temporal Module (LTM)?\nThe three statistical conditions are:\n1.  A group of stocks displays an average within PCC that drastically increases in absolute value.\n2.  The average between PCC of stocks in this group and other stocks in the rest of the system will greatly decrease in absolute value.\n3.  The average AC of stocks belonging to this group increases in absolute value.",
    "The synthetic indicator I LTM t detects market instabilities by quantifying self-organizing processes in stock returns co-movements. Its components represent imitation, herding behaviors, and positive feedbacks among market participants.",
    "What is the Detrended Fluctuation Analysis (DFA) method?\nThe DFA method involves shifting, integrating, segmenting, and plotting F(\u0394l) against \u0394l to determine the scaling exponent \u03b1, which indicates the presence of self-similarity and long-term memory.",
    "The LTM indicator aims to detect the emergence of market instabilities by quantifying self-organizing processes in stock returns. The analysis uses daily closure prices from the STOXX Asia/Paci index and the STOXX North America 600 Index. The study employs Pearson's correlation coefficient (PCC) and autocovariance (AC) to quantify the intensity of self-organizing processes.",
    "What happens to the variance, covariance, and auto-covariance of the original system variables as the dominant eigenvalue \u03bb\u2081 approaches 1?\n\nAs \u03bb\u2081 approaches 1, the variance, covariance, and auto-covariance of the original variables increase. Specifically, the variance and covariance increase, and the auto-covariance increases greatly as \u03bb\u2081 approaches 1.",
    "What are the true positives, false positives, false negatives, and true negatives obtained by investing following I LTM t, and how do they compare to the strategy based on the VaR?",
    "The I^LTM_t indicator successfully anticipated the following market events and crises during the 2006-2017 study period:\n\n*   The 2008 global financial crisis\n*   The failure of Lehman Brothers in September 2008\n*   The American Recovery and Reinvestment Act of 2009\n*   The European Debt crisis of 2011\n*   The Chinese stock market crisis of 2015-2016",
    "Stocks with Hurst exponents outside the 0.2-0.8 interval exhibit an anticorrelation effect of the price increments, leading to an increase of bounces and mimicking a memory of the price on a support or resistance.",
    "How does the study's approach relate to thermodynamic systems, and what role does the I^LTM_t indicator play in this analogy?",
    "What was the annual profit and loss performance compared to a simple Buy&Hold strategy?",
    "Tinbergen identified four complementary dimensions for analyzing animal behavior: function, mechanism, development, and evolutionary history.",
    "Flash crashes exemplify unintended collective machine behavior in financial markets by illustrating how algorithmic traders, responding to each other and to market events, can create larger market crises despite not being explicitly programmed to do so.",
    "The three primary motivations for establishing machine behavior as a scientific discipline are: the increasing prevalence of algorithms in society, the inherent complexity of these algorithms and their environments, and the difficulty in predicting their societal impact.",
    "Because of the complex properties of these algorithms and the environments in which they operate, some of their attributes and behaviours can be difficult or impossible to formalize analytically.",
    "Researchers face legal and ethical barriers when studying machine behavior, particularly concerning data collection methods. Interview and focus group data present challenges due to their qualitative nature and the difficulty of anonymization. Survey data, primarily quantitative, offer better opportunities for anonymization. Observational data, both technology-based and human-enacted, also present unique considerations. Archival data, derived from other sources, require careful attention to the original contributors and potential privacy concerns.",
    "Machine evolution differs from animal evolution because machines are influenced by designers and external factors like open-source sharing and regulatory constraints, whereas animal evolution is primarily driven by simple inheritance and natural selection.",
    "The three scales of inquiry are individual machines, collectives of machines, and hybrid human-machine systems.",
    "The paper provides examples of how algorithms can exhibit bias across different domains, including:\n\n*   How human biases combine with AI to alter human emotions and beliefs.\n*   How traffic patterns can be altered in streets populated by large numbers of both driverless and human-driven cars.\n*   How trading patterns can be altered by interactions between humans and algorithmic trading agents.\n*   Algorithmic influence on human behavior, particularly concerning product promotion to children and broader collective behavioral impacts.",
    "The paper describes the \u2018black box\u2019 problem as the difficulty in interpreting the functional processes that generate outputs from AI systems, even for the scientists who create them. It highlights that while individual algorithms may appear simple, their results can be complex and effectively create \u201cblack boxes,\u201d and that the dimensionality and size of data add to this complexity.",
    "The context does not explicitly state the key finding of the hybrid human-machine study by Shirado and Christakis regarding coordination. However, it does indicate that interactions with simple bots can increase human coordination.",
    "The standard gravity model is inadequate for accurately describing airline passenger flows due to the presence of transfer flights.",
    "The authors define two components of the observed passenger flow, f_ij, as direct passengers (f_ij_g) traveling directly from the origin to the destination, and transfer passengers (f_ij_transit) utilizing the connection as part of a longer journey.",
    "The model correctly predicts more than 98% of the total passenger flow in the world.",
    "The formula for the probability function p(i \u2192 j \u2192 k) is given by:\n\np(i \u2192 j \u2192 k) = C * f(r_ij, r_jk) / (xi * xj)\n\nwhere C is a normalization constant, f(r_ij, r_jk) is a function reflecting the tendency of passengers to choose the shortest connections, and r_ij and r_jk are the distances between countries i and j, and countries j and k, respectively.",
    "The authors determined \u03b1 values of 1.5 and 1.6 for 1996 and 2004, respectively.",
    "The paper identifies several historical events as influencing the distance coefficient in air transportation, including:\n\n*   September 11 attacks\n*   SARS epidemic\n*   Global financial crisis\n*   Eyjafjallajo\u0308kull volcano eruption",
    "The provided context does not specify the data source or time period used for the analysis.",
    "The context does not provide the percentage of possible connections between countries that were direct flights versus shortest paths with length 2 in 2004.",
    "The paper proposes a model of connecting flights to explain discrepancies between observed and predicted passenger flows in airline networks. It argues that the standard gravity model is inadequate due to the inclusion of transfer passengers.\n\nThe \"missing globalization puzzle\" refers to the observation that the distance coefficient in the gravity model increases over time, seemingly contradicting the concept of globalization as a reduction in effective distance.\n\nThe distance coefficient \u03b1 is typically found from the slope of the linear relation in the gravity equation.",
    "The authors use Earth-mover's distance (EMD) to quantify the difference between the TCAD dataset and ANN predictions, outlining the steps for calculating the EMD score.",
    "The researchers identified several Chinese textual challenges that make stock market analysis more complex than English-based analysis. These include the lack of explicit word boundaries in Chinese, the absence of grammatical markers, and the presence of a large number of homophones.",
    "The proposed ANN models shortened the simulation time by six times compared to the previous ML-based model and successfully predicted non-Gaussian device performance metrics, which the prior model could not.",
    "What are the statistical properties that signal an imminent bifurcation in the original variables?",
    "What keywords were identified as having the highest betweenness centrality?\nThe word \"Short Term\" is the most influential keyword, with a betweenness centrality of 287.89, followed by \"Pessimistic\", \"Input Market\", \"Larger\" and \"Market\".\n\nWhat does this measure indicate?\nBetweenness centrality measures the number of times a node acts as a link along the shortest path between two other nodes, indicating its importance in controlling the flow of information within the network.",
    "The network density is 0.1695, indicating a sparse network.",
    "The study collected 2082 research reports from 65 security companies. The typical word count range for these reports was between 500 and 2000 words.",
    "The study introduces a textual network approach to predict stock market movements by leveraging analysts\u2019 research reports. It outperforms alternative models by incorporating network connectivity and demonstrates the importance of textual networks in improving predictive power and semantic interpretability.\n\nPrevious research identified a delay of at least one month in the effects of imperfect information on investor reactions. Early behavioral finance research suggested that investment decisions are subject to the effect of investor sentiment, confirmed by studies of Li and Schumaker et al.\n\nThe study reveals a long-term memory effect in the price dynamics, specifically an increasing probability of bounces on support/resistance levels as the number of previous bounces increases. This memory is not explained by an antipersistent nature of the price and is statistically significant across different time scales.",
    "The answer is not available in the provided context.",
    "The striking finding is that only five out of 25 terms have a positive impact on financial markets, and \u201cImagine( \u60f3\u8c61 )\u201d is the strongest positive indicator, while \u201cConcern( \u5173\u6ce8\u5ea6 )\u201d is the strongest negative indicator.",
    "The keyword vector extraction process involved structured event extraction using syntactic analysis, followed by RBM pre-training and sentence2vec for final event representations.",
    "The computational complexity of the second step (edge pruning) is O(L^3).",
    "The parameter \u03b2 controls the quality versus complexity trade-off in the pruning algorithm by adjusting the threshold for removing connections.",
    "In three of the eight direct routes that were not detected by the algorithm, the location Mbile in the southwestern part of the region is involved, which is only 11 km away from the location Lolo.",
    "The paper uses batched shortest path algorithms, such as MLD or CH, for computing pairwise distances between locations.",
    "How does the algorithm identify indirect routes?\nThe algorithm identifies indirect routes using the triangle inequality: if the route distance between two locations is similar to the sum of distances between those locations and a common third location, the route is considered indirect and pruned.",
    "What specific problem occurred with the route between Kolbermoor and Prien am Chiemsee in the German-Austrian border region?\nThe route between Kolbermoor and Prien am Chiemsee was pruned due to the algorithm identifying a direct route between them that is part of the ground truth but is not part of the fully connected graph.",
    "The proposed algorithm constructs location graphs using a two-step process: computing pairwise distances with batched shortest path algorithms and then pruning indirect routes using Algorithm 1.",
    "The Static-Triangle strategy has a time complexity of O(L*R), while the Iterative-Global strategy has a time complexity of O(R(R+L)logL).",
    "The algorithm was validated in four geographical regions: Styria in Austria, a border region between Germany and Austria, the Central African Republic, and South Sudan.",
    "The paper proposes an algorithm for automated construction of location graphs, which connect geographic locations with direct routes.",
    "The five time scales of resilience according to the DIRE curve are: recon, resist, respond, recover, and restore.",
    "The adaptive capacity of a generation asset is bound by its operational generation limits and the speed it can ramp up and down its output power.",
    "The primary resilience challenge for the St. Mary\u2019s microgrid is fuel availability for the diesel generators.",
    "Mathematical relationship: System frequency responds to power disturbances through a combination of inertia and adaptive capacity. Inertia, stemming from rotating masses, slows frequency response, while adaptive capacity represents the ability to ramp up or down generation output.",
    "The wind turbine installed at St. Mary\u2019s was a 900 kW, 52-meter rotor diameter type IV pitch-controlled wind turbine manufactured by Emergya Wind Technologies, and it started producing power on January 5, 2019.",
    "The study observes a trade-off between short-term and long-term resilience, noting that losing a generator negatively impacts short-term resilience due to reduced inertia and ramping capability, but positively affects long-term resilience by conserving fuel.",
    "HELICS coordinates GridLAB-D and Python federates, facilitating data exchanges between them.",
    "The diesel generators in the St. Mary\u2019s microgrid model are assumed to have an inertia constant of 2, a ramping capability of reaching full output in 10 seconds, and a constant rate of fuel burning.",
    "The frequency limit used to define the short-term resilience metric is frequency stability.",
    "Running wind generation at maximum capacity allows for diesel generators to be taken offline, increasing long-term resilience by conserving fuel.",
    "The researchers implemented a strategy in which they took long positions following a decrease in search volume and never took short positions, and another strategy in which they took short positions following an increase in search volume, never taking long positions. Buy/sell decisions were based on changes in Google Trends search volume data.",
    "The best-performing Google Trends strategy achieved a cumulative return of 326%, and the search term used was \u201cdebt\u201d.",
    "How did the researchers quantify the financial relevance of different search terms?\nThey quantified financial relevance by calculating the frequency of each search term in the online edition of the Financial Times (from August 2004 to June 2011), normalized by the number of Google hits for each search term.\n\nWhat correlation did they find with trading performance?\nThey found a positive correlation between the financial relevance of a search term and its performance in a trading strategy, with Google Trends strategies based on U.S. search volume data outperforming random investment strategies.",
    "Strategies based on U.S. search volume data performed better than those using global search volume data. This is attributed to the assumption that the U.S. Internet user population contains a higher proportion of traders on U.S. markets.",
    "The 'buy and hold' strategy yielded a profit of 16% during the study period, while the 'Dow Jones strategy' achieved a profit of 33%.",
    "How did the researchers validate that both components of their trading strategy (long and short positions) contributed significantly to the results?\nThe researchers validated that both strategy components (long and short positions) contributed significantly by implementing and testing strategies with only long positions and only short positions, finding that these strategies also yielded significantly higher returns than a random investment strategy in each case.",
    "We suggest that Google Trends data may provide insight into future trends in economic actor behavior.",
    "How many search terms did the researchers analyze?\nThe researchers analyzed 98 search terms.\n\nWhat criteria did they use for selecting them?\nThe terms were intentionally introduced, with some suggested by the Google Sets service, reflecting a financial bias.",
    "The researchers used two shades of blue and red to represent positive and negative returns, respectively, in investment strategies based on search volume data.",
    "Returns from the Google Trends strategies tested are significantly higher overall than returns from the random strategies (R.US = 0.60; t = 8.65, df = 97, p < 0.001, one sample t-test).",
    "The study analyzes a corpus of daily Financial Times issues from 2nd January 2007 to 31st December 2012.",
    "How do you quantify financial relevance?",
    "The median correlation coefficient between daily mentions of company names and transaction volumes across all 31 companies was 0.000.",
    "A greater transaction volume for a company\u2019s stocks on a given day is therefore related to a greater number of mentions of that company in the Financial Times on the following day.",
    "The Financial Times is released each day from Monday to Saturday at 5 am London time.",
    "What were the daily variations in the length of the Financial Times issues?",
    "Which company replaced which other company in the DJIA during the study period, and when did this change occur?\nTravelers replaced Citigroup in the DJIA on 8th June 2009.",
    "The time lags at which significant correlations were found between company mentions and transaction volumes were two days before (2 1) and on the same day as the news (0).",
    "The median correlation coefficient between daily mentions of company names and absolute returns is 0.000.",
    "What was the result when testing for correlation between company mentions and directional stock returns?",
    "The researchers defined a bounce of the price on a support/resistance level as the event of a future price entering in a stripe centered on the level and exiting without crossing it.\n\nA quantitative definition of support and resistance is the average absolute price increment within a stripe centered on the level.\n\nThe width D of the stripe centered on the support or resistance at the time scale T is defined as the average of the absolute value of the price increments at time scale T.\n\nThe width D depends on both the trading day and the time scale and generally rises as T does.\n\nThe probability of re-bouncing on these selected values is higher than expected.",
    "How many stocks from the London Stock Exchange were analyzed in this study, and what was the time period of the analysis?\nThe study analyzed 9 stocks from the London Stock Exchange in 2002 (251 trading days).",
    "The Hurst exponent is always less than 0.5 for the 9 stocks analyzed, indicating an anticorrelation effect of price increments.",
    "At what time scales did the researchers find that the memory effect in support and resistance levels becomes statistically insignificant?",
    "The distribution of local minima/maxima describes the time between consecutive bounces.",
    "The researchers used Bayesian inference to estimate the conditional probability of bounces (p(b|bprev)) based on the number of previous bounces (bprev). They inferred p(b|bprev) from the number of bounces and the total number of trials, assuming a Bernoulli process.\n\nThe expected value and variance of p(b|bprev) were estimated using Bayes' theorem, as detailed in the Supporting Information.",
    "The study found no evidence of preferred prices around round numbers in the histograms of resistance and support levels.",
    "The analysis compares the conditional bounce probabilities of actual stock data with shuffled return series. The probabilities of bounces for the shuffled series were nearly 1 to 2, while the probabilities for the stock data were well above 1 to 2.",
    "The context does not specify the statistical tests employed or the significance levels used.",
    "The fundamental belief about market psychology is that it is consistent over time, with investor expectations influencing market behavior through a self-fulfilling prophecy.",
    "The three components that make up the total cost C_i for a one-level dependent activity according to Zachary\u2019s framework are economic (CF), social (CS), and environmental (CE).",
    "The framework defines sustainability based on whether a general activity can be sustained according to available resources, duration, cost, or substitution.\nThe environmental cost C_E relates to the capacity c through a cost composed of economic, social, and environmental components.\nThe activity duration is limited by a maximum allowable duration.",
    "The two conditions for sustainability are a strong condition with no substitution and a weak condition allowing for substitution.",
    "The set of all sustainable activities, in the weak case, is a subset of an N-level union of sustainable activities, forming a topological cover.",
    "The three conditions for an activity to be considered sustainable, according to the provided text, are the duration, cost, and whether substitution is possible.",
    "The paper identifies five requirements for sustainable agriculture addressable by Zachary\u2019s framework:\n\n1.  Satisfying human food and fiber needs.\n2.  Enhancing environmental quality and the natural resource based upon the agricultural economy.\n3.  Making the most efficient use of non-renewable resources and on-farm resources.\n4.  Integrating, where appropriate, natural biological cycles and controls.\n5.  Sustaining the economic viability of farm operations.",
    "What is the fundamental challenge in connecting social sustainability to environmental and economic dimensions?",
    "The paper addresses higher-order dependencies by illustrating it with the harvesting of fish, specifically the Atlantic bluefin tuna, and its impact on the food chain.",
    "The context acknowledges that quantifying different theoretical perspectives on sustainability, particularly the social dimension, presents a significant limitation due to the difficulty in formulating issues into an analytic framework and connecting local and global concerns.",
    "The text proposes eight possible classifications of activities based on duration, cost, and complexity levels: Anthropocentric, Teleological, Ecological-Evolutionary, Evolutionary-Technological, Physico-Economic, Biophysical-Energy, Systems-Ecological, Ecological Engineering, Human Ecology, Socio-Biological, Historical-Institutional, and Ethical-Utopian.",
    "The proposed system achieves five times faster speed than existing fastest systems and further acceleration is possible with higher-end elements.\n\nThe maximum modulation frequency achieved is 97 kHz, which is approximately five times faster than the fastest DMD.\n\nThe system\u2019s speed is largely determined by the operating frequency of the galvanic mirrors and the size of the DMD micro-mirrors.",
    "The final illumination patterning speed is independent of the DMD frequency, but the DMD\u2019s working frequency is non-trivial.",
    "The authors achieved a frame rate of 42 Hz at 80 \u00d7 80-pixel resolution and a pixel resolution of 80 \u00d7 80 pixels for dynamic scene imaging.",
    "The proposed system achieves five times faster speed than existing systems and further acceleration is possible with higher-end elements.",
    "The simulation results show that increasing the number of scanned consecutive patterns (k) leads to a worsening of the reconstruction quality, with the largest RMSE being 0.44 and the smallest being 0.15.",
    "The final illumination patterning speed is independent of the frequency of the DMD, but the DMD's working frequency is non-trivial. To avoid repetitive coding patterns during sweeping, the DMD pattern elapse should be shorter than half the galvanic mirror period. The number of scanned consecutive patterns during each DMD period (k) affects the final reconstruction quality, with larger k values leading to worse quality.",
    "The system utilizes two galvanic mirrors rotating around their axes, with the distance between their axes being 12 cm. The distance between the DMD and the second galvanic mirror is approximately 8 cm. The rotating angle of the first galvanic mirror (GM1) is denoted as \u03b8, and the corresponding change in the beam's hitting position on the DMD is given by \u2206x = -d sin(2\u03b8). A linear mapping between the GM's input voltage U and the DMD hitting position is established through calibration.",
    "The geometric relationship between the rotating angles of the galvanic mirror pair and the hitting position of the beam on the DMD can be described by the equation x = d sin(2\u03b8), where x is the distance from the beam's hitting position at GM1 to that at the DMD, d is the distance between the galvanic mirrors, and \u03b8 is the rotating angle of GM1.",
    "What potential improvement in modulation speed could be achieved using acoustic optical deflectors (AOD) instead of galvanic mirrors?\n\nAODs can produce 20 kHz scanning frequency, resulting in 20 times faster illumination patterning (9.5 MHz) compared to galvanic mirrors.",
    "The main limitations of the proposed sweeping-based ghost imaging approach include:\n\n*   The imaging speed is limited by the operating frequency of the galvanic mirrors.\n*   The scheme currently works best with random patterns and is not suitable for specific structured patterns.\n*   The system requires careful mechanical mounting for calibration.",
    "The novel event extraction and representation method combines syntactic analysis to extract the structure of sentences with HanLP, an RBM for pre-training, and sentence2vec for obtaining the final event representations.",
    "The M-MI model addresses the lack of true labels at the instance level by assigning group labels to instances and then averaging those labels to obtain a predicted label for the super group.",
    "The M-MI model's objective function incorporates three levels of loss functions.",
    "The source weight vector GLYPH<18> represents the impacts of different data sources on market movements.",
    "M-MI improves F1-score by 6.9% in 2015 and 9.2% in 2016 compared to nMIL.",
    "The paper justifies the use of a shared estimated true label across different data sources in hinge loss functions by assuming that the predictions from various sources are intrinsically correlated and consistently indicate the same market movement direction.",
    "The three-level syntactic tree captures structured event extraction by depicting the root as the core verb, with the second layer nodes as subjects and objects, and child nodes as modifiers. Core words are selected as the root, subject, and object of the verb.",
    "As the number of history days increases, the F-1 scores generally rise and then fall. The possible reason is that the impacts of news, sentiments, and quantitative indices decay after a period of time (2 or 3 days).",
    "The LDA-S method is used for sentiment extraction, an extension of Latent Dirichlet Allocation (LDA) that considers topic dependencies.",
    "The framework uses multiple instance learning, distinguishing instance-level, group-level, and super group-level labels.\nSource-specific weights reveal how related a specific source is to the index movement.",
    "Agricultural land consolidation, rural construction land consolidation, land reclamation, and land development.",
    "The priority remediation area includes 27 administrative villages, spanning a total area of 28090 hm2, which represents 32.75% of the overall region.",
    "What mathematical formula is used to quantify the resilience of a system, according to the provided context?",
    "The number of input and output nodes is 10 and 30, respectively. The number of iterations is 1000.",
    "Soil and landscape pattern ecological risk values are between 0.01 and 1.46 and 0.01 and 1.62, respectively.",
    "Dij demonstrates mixed distance; D s ij shows geospatial distance; a d j and a d j indicate attribute values in attribute space; ws and wa are geospatial and attribute space weights, respectively.",
    "The framework uses multiple data sources, including news, social media, and quantitative data. It employs multiple instance learning to estimate instance-level probabilities and source-specific weights.",
    "The IoT sensors deployed in the land consolidation project areas collected data on soil quality, meteorological conditions, water quality, and farmland conditions.",
    "Area C demonstrates the lowest ecological risk levels across all ecological risk aspects.",
    "The proximity function hij can use the Gaussian function to measure the proximity among neurons.",
    "The notion of graph reasoning extends to Leonhard Euler in the 18th century.",
    "Storage limitations and high costs during the 1960s drove the development of relational algebra.",
    "Graphs are expressed in node-arc-node (subject-predicate-object) triples. Nodes represent physical or conceptual objects, and arcs represent relationships between nodes.",
    "The parallel-processer-based graphics processing unit (GPU) also offers hardware alternatives to accelerate large-scale graph processing. Firms like Cray have developed specially configured supercomputers to digest and return rapid results from massively scaled graphs. \u2018High-Performance with an In-GPU Graph Database Cache,\u2019 by Shin Morishima and Hiroki Matsutani, examines GPU efficiency when distributed and optimized for performance.",
    "The RDBMS performance takes a nosedive when dealing with many-to-many relationships. The RDBMS schema is typically inflexible, requiring high maintenance for changes.",
    "The hierarchical model and the network model were early database technologies from the 1960s similar to graph databases.",
    "There is little semantic commonality among the various graph languages in use and their rules of syntax.",
    "Many graph databases support the Atomic, Consistent, Isolated, and Durable (ACID) consistency model, which is a spin-off storage-locking scheme from RDBMS technology.",
    "Scale introduces yet another qualitative concern: as data grows, query complexity increases. Subgraphs can be intelligently reduced for better management and querying.",
    "Built-in mathematical functions residing in graph databases could add a level of depth to truly understanding and quantifying graph relationships.",
    "The experiments compared Earth\u2019s standard pressure (1000 mbar) and Mars-like pressure (8 mbar).",
    "A relative error of 6.79%, 7.68%, and 7.76% was observed for the sandy, intermediate, and bedrock soils, respectively, when comparing the thermal inertia estimations based on Perseverance\u2019s data with the MEC-based thermal inertia estimations.",
    "The maximum difference between surface and subsurface temperatures increased from 11.6\u00b0C at Earth\u2019s pressure to 14.7\u00b0C at Martian pressure for soil A.",
    "The thermal inertia is estimated as (Ts = T max -T min), where Ts is the net heat flux and T max and T min are the maximum and minimum surface temperatures.",
    "The IR viewport window was chosen to be an anti-reflection coated germanium circular optic model GEW16AR.20 with a diameter of 74.9 mm and a thickness of 5.0 mm. This selection was driven by the need to allow 8-14 \u03bcm infrared radiation to pass through while withstanding the pressure differential and temperature variations within the MEC, and to avoid reaching the germanium's fracture strength.",
    "The experiments collected 9225 radiometric images. These images were saved as plain text 640 \u00d7 480 matrices, with each cell containing the temperature in degree Celsius.",
    "Thermal conductivity is influenced by three heat transfer mechanisms: transfer across pore spaces (kr), conduction between grain contacts (kc), and conduction of the gas within the pores (kg). Pressure significantly affects which of these mechanisms dominates. At lower pressures (0.1-1000 mbar), gas conduction (kg) is most influential, particularly for loose granular soils. At higher pressures, the relationship becomes less pronounced.",
    "At Martian pressure, three prominent groups can be observed in the surface temperature data: sandy soil, intermediate soil, and bedrock. Bedrock exhibits a noticeable temperature delay compared to granular soils.",
    "The thermal vision camera is a PI-640i by Optris based on uncooled microbolometer technology. It is a 320-g LWIR camera that works in the spectral range of 8-14 \u03bc m, has a resolution of 640 \u00d7 480 pixels, and a germanium optic with an FOV of 60 \u25e6 \u00d7 45 \u25e6 .",
    "The simplified equation for the soil thermal behavior depends on solar radiation, thermal inertia, and radiative emission.",
    "The two fundamental problems with singular vectors that make them difficult to learn in convolutional neural networks are sign ambiguity and manifold features.",
    "SVP shows a quantitative performance improvement of about 36% for the CIFAR10 dataset.",
    "Aligning e V based on e U as a reference.",
    "The answer is not available in the provided context.",
    "The loss function used to learn the center vector uc is KL-divergence, based on the assumption that each component of the aligned N u has a Gaussian distribution.",
    "The silhouette score of SVP on the training dataset improved by 0.444, 0.510, and 0.551 compared to GAP, GMP, and MPN, respectively.",
    "The rotation process centers singular vectors and minimizes path lengths during learning by rotating the singular vector distribution to align with the coordinate center.",
    "The CIFAR100 and Tiny-ImageNet datasets show a 1.69% and 0.97% improvement, respectively, when KD-SVP is applied to KD-SVD.",
    "If step 2 (sign ambiguity removal) is omitted, learning is impossible.",
    "SVP has a somewhat higher computational complexity than conventional pooling methods like GAP and GMP.",
    "The STDM-OCT system achieves a 1-MHz A-scan rate.",
    "The broadband light source used in the STDM-OCT system is the Superlum SLD-371-HP3, with a center wavelength of 838 nm, a full-width at half-maximum of 81 nm, and an optical power of 27.2 mW.",
    "The volumetric imaging rate achieved was 8 vol/s for an image range of 250 \u00d7 250 \u00d7 2048 pixels (9 \u00d7 4.5 \u00d7 5 mm).",
    "The measured thicknesses of the four layers are 100, 250, 150, and 100 \u03bcm, respectively.",
    "What are the measured sensitivity roll-off values for camera #1 and camera #2 at 100th pixel?",
    "C++, CUDA, and Qt",
    "The STDM-OCT provides a 1-MHz A-scan rate, reducing inspection time and cost compared to SS-OCT.",
    "The whole-range scanning of the sample consumes 8 seconds.",
    "The measured thicknesses of each layer are 100, 250, 150, and 100 \u03bcm, respectively. The measured total thickness of the OTF, including the vacuum gaps between each layer, is 700 \u03bcm, with vacuum gaps of 40, 20, and 40 \u03bcm.",
    "The STDM-OCT system utilizes a broadband light source (838 nm) and a 50:50 fiber coupler to distribute light to each interferometer. Each interferometer consists of a reference arm with a collimator and mirror, and a sample arm. The system employs multiscanners and multicameras for SDM and TDM, respectively."
  ]
}