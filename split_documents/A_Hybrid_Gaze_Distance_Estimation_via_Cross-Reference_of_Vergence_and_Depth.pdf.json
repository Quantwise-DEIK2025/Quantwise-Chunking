[
    {
        "text": "Introduces a hybrid algorithm combining eye-tracking and depth-mapping to improve the accuracy and stability of gaze distance estimation, addressing limitations of traditional vergence-angle-based methods.\n\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000",
        "original_text": "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000",
        "context": "Introduces a hybrid algorithm combining eye-tracking and depth-mapping to improve the accuracy and stability of gaze distance estimation, addressing limitations of traditional vergence-angle-based methods.",
        "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
        "pages": [
            1
        ],
        "id": "95c9c5cc4a08bf7cbfb06ae644547a5ee4d953c0729eb4b4bcfc79a2e96622c0"
    },
    {
        "text": "Introduces the paper's focus on improving gaze distance estimation for Mixed Reality (MR) experiences, highlighting the challenges of current eye-tracking methods and presenting a novel hybrid algorithm combining vergence angle and depth camera data for more accurate and stable estimations.\n\n1 Korea Institute of Science and Technology, Seoul, South Korea\n2 KHU-KIST Department of Converging Science and Technology, Kyung Hee University, Seoul, South Korea\nCorresponding author: Min-Koo Kang (e-mail: minkoo@kist.re.kr).\nThis work was financially supported by the Institute of Civil-Military Technology Cooperation Program funded by the Defense Acquisition Program Administration and Ministry of Trade, Industry and Energy of Korean government (grant No. 17-CM-DP-29).\nABSTRACT To deliver an optimal Mixed Reality (MR) experience, wherein virtual elements and real-world objects are seamlessly merged, it is vital to ensure a consistent vergence-accommodation distance. This necessitates the advancement of technology to precisely estimate the user's gaze distance. Presently, various MR devices employ small eye-tracking cameras to capture both eyes and infer the gaze distance based on vergence angle data. However, this technique faces significant challenges, as it is highly sensitive to several human errors, such as strabismus, blinking, and fatigue of the eyes due to prolonged use. To address these issues, this paper introduces an innovative hybrid algorithm for estimating gaze distances. The proposed approach concurrently utilizes an eye camera and a depth camera to conduct parallel estimations: one based on the conventional vergence angle and the other on gaze-mapped depth information. The confidence of each method is then assessed and cross-referenced, and an adaptive weighted average is computed to derive a more precise and stable gaze distance estimation. In the experiment, three challenging test scenarios designed to induce human and environmental errors were administered to 12 subjects under uniform conditions to evaluate the accuracy and stability of the proposed method. The experimental results were validated through both qualitative and quantitative analysis. The findings showed that the proposed method significantly outperformed current methods with a visual angle error of 0.132 degrees under ideal conditions. Furthermore, it consistently maintained robustness against human and environmental errors, achieving an error range of 0.14 to 0.21 degrees even in demanding environments.\nINDEX TERMS Augmented reality (AR), extended reality (XR), eye tracking, gaze distance estimation, mixed reality (MR), varifocal, vergence-accommodation conflict, virtual reality (VR)",
        "original_text": "1 Korea Institute of Science and Technology, Seoul, South Korea\n2 KHU-KIST Department of Converging Science and Technology, Kyung Hee University, Seoul, South Korea\nCorresponding author: Min-Koo Kang (e-mail: minkoo@kist.re.kr).\nThis work was financially supported by the Institute of Civil-Military Technology Cooperation Program funded by the Defense Acquisition Program Administration and Ministry of Trade, Industry and Energy of Korean government (grant No. 17-CM-DP-29).\nABSTRACT To deliver an optimal Mixed Reality (MR) experience, wherein virtual elements and real-world objects are seamlessly merged, it is vital to ensure a consistent vergence-accommodation distance. This necessitates the advancement of technology to precisely estimate the user's gaze distance. Presently, various MR devices employ small eye-tracking cameras to capture both eyes and infer the gaze distance based on vergence angle data. However, this technique faces significant challenges, as it is highly sensitive to several human errors, such as strabismus, blinking, and fatigue of the eyes due to prolonged use. To address these issues, this paper introduces an innovative hybrid algorithm for estimating gaze distances. The proposed approach concurrently utilizes an eye camera and a depth camera to conduct parallel estimations: one based on the conventional vergence angle and the other on gaze-mapped depth information. The confidence of each method is then assessed and cross-referenced, and an adaptive weighted average is computed to derive a more precise and stable gaze distance estimation. In the experiment, three challenging test scenarios designed to induce human and environmental errors were administered to 12 subjects under uniform conditions to evaluate the accuracy and stability of the proposed method. The experimental results were validated through both qualitative and quantitative analysis. The findings showed that the proposed method significantly outperformed current methods with a visual angle error of 0.132 degrees under ideal conditions. Furthermore, it consistently maintained robustness against human and environmental errors, achieving an error range of 0.14 to 0.21 degrees even in demanding environments.\nINDEX TERMS Augmented reality (AR), extended reality (XR), eye tracking, gaze distance estimation, mixed reality (MR), varifocal, vergence-accommodation conflict, virtual reality (VR)",
        "context": "Introduces the paper's focus on improving gaze distance estimation for Mixed Reality (MR) experiences, highlighting the challenges of current eye-tracking methods and presenting a novel hybrid algorithm combining vergence angle and depth camera data for more accurate and stable estimations.",
        "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
        "pages": [
            1
        ],
        "id": "53d9884230df76f031443d1e69929c1ae3a7108e786e985d56c3a63304fd19ad"
    },
    {
        "text": "This section introduces the context of XR technologies and highlights the critical challenge of the vergence-accommodation conflict (VAC) as a key obstacle to their widespread adoption. It then details existing gaze distance estimation methods and their limitations, setting the stage for the proposed hybrid approach.\n\nExtended Reality (XR) technologies, including Augmented Reality (AR), Virtual Reality (VR), and Mixed Reality (MR), are currently utilized across various industries such as the integrated visual augmentation system for individual soldiers in the defense field and image-guided surgery in the medical field. Additionally, in the everyday lives of general consumers, XR enables convenient, time and space-transcending experiences such as virtual reality gaming, augmented realitybased shopping, virtual classrooms, remote health care and exercise programs, and virtual travel. Despite the rapid market growth driven by strong industry demand and active investments from global companies, XR technologies still faces many technical obstacles that must be swiftly addressed to ensure sustainable growth and commercialization.\nTo achieve an ideal XR where virtual images seamlessly blend with real environments, both AR and VR require common features: lightweight and comfortable wearability, wide field of view with high resolution, high-brightness displays ensuring visibility even outdoors, and long operational hours without battery or heat issues. Above all, overcoming the vergence-accommodation conflict (VAC) is crucial [1]. VAC can cause severe visual fatigue when viewing virtual images for extended periods and prevents users from seeing real objects at various distances and virtual images at a fixed focal distance simultaneously. Therefore, many global companies and research institutions are currently focusing on developing new technologies to resolve VAC issues, such as as varifocal, multi-focal, and all-in-focus technologies [2]-[5]. However, to control the focal distance of virtual images so that vergence and accommodation distances match, the development of gaze distance estimation technology must be preceded.\nIEEE Access\nCurrently, most commercial AR and VR devices incorporate small near-infrared eye cameras to perform eye-tracking, but this pertains to 2D gaze point tracking for human-friendly interaction with XR applications. Although research on accurately performing gaze distance estimation is till lacking, typically, in wearable XR devices, gaze distance is estimated by capturing both eyes with an eye camera, as depicted in Fig.1(a) and Fig.1(b), and using the convergence angle ( θ ) and inter pupillary distance ( β ) parameters to derive the gaze distance [6], [7]. However, this method is highly susceptible to human errors such as blinking, squinting, and eye relaxation [8], limiting the accuracy of estimating the gaze distance that matches the optical system's focal distance.\nAs an alternative to the above method, depth estimation commonly used in the computer vision field can be considered. Depth estimation generally involves analyzing the disparity using infrared (IR) pattern light and stereo vision, and depth cameras that provide distance information between the camera and surrounding environmental elements in the form of 2D images (i.e., depth maps) are already widely used. Thus, as depicted in Fig. 1(c), by estimating the user's 2D gaze point coordinates with an eye-camera and mapping it to the depth map acquired by a depth-camera, the corresponding depth value can be retrieved to estimate the user's gaze distance [9][11]. This method can solidly eliminate human errors arising from the convergence angle-based method. However, it can easily malfunction due to external environmental factors such as disocclusion regions or reflective media.\nThis paper proposes a novel hybrid method that mimics the human visual perception mechanism [12] to complement the limitations of the two conventional gaze distance estimation approaches. Human vergence-accommodation response is generally modeled as two dual-parallel feedback control systems, where these responses interact to enhance gaze distance perception and ensure better visual stability. Similarly, the proposed method combines the vergence angle-based approach with the gaze-mapped depth approach, allowing these two methods to interact and achieve synergy. This enhances the accuracy of each method and applies adaptive weighted averaging to assess the confidence of each method, thereby ensuring the stability of the final derived gaze distance.",
        "original_text": "Extended Reality (XR) technologies, including Augmented Reality (AR), Virtual Reality (VR), and Mixed Reality (MR), are currently utilized across various industries such as the integrated visual augmentation system for individual soldiers in the defense field and image-guided surgery in the medical field. Additionally, in the everyday lives of general consumers, XR enables convenient, time and space-transcending experiences such as virtual reality gaming, augmented realitybased shopping, virtual classrooms, remote health care and exercise programs, and virtual travel. Despite the rapid market growth driven by strong industry demand and active investments from global companies, XR technologies still faces many technical obstacles that must be swiftly addressed to ensure sustainable growth and commercialization.\nTo achieve an ideal XR where virtual images seamlessly blend with real environments, both AR and VR require common features: lightweight and comfortable wearability, wide field of view with high resolution, high-brightness displays ensuring visibility even outdoors, and long operational hours without battery or heat issues. Above all, overcoming the vergence-accommodation conflict (VAC) is crucial [1]. VAC can cause severe visual fatigue when viewing virtual images for extended periods and prevents users from seeing real objects at various distances and virtual images at a fixed focal distance simultaneously. Therefore, many global companies and research institutions are currently focusing on developing new technologies to resolve VAC issues, such as as varifocal, multi-focal, and all-in-focus technologies [2]-[5]. However, to control the focal distance of virtual images so that vergence and accommodation distances match, the development of gaze distance estimation technology must be preceded.\nIEEE Access\nCurrently, most commercial AR and VR devices incorporate small near-infrared eye cameras to perform eye-tracking, but this pertains to 2D gaze point tracking for human-friendly interaction with XR applications. Although research on accurately performing gaze distance estimation is till lacking, typically, in wearable XR devices, gaze distance is estimated by capturing both eyes with an eye camera, as depicted in Fig.1(a) and Fig.1(b), and using the convergence angle ( θ ) and inter pupillary distance ( β ) parameters to derive the gaze distance [6], [7]. However, this method is highly susceptible to human errors such as blinking, squinting, and eye relaxation [8], limiting the accuracy of estimating the gaze distance that matches the optical system's focal distance.\nAs an alternative to the above method, depth estimation commonly used in the computer vision field can be considered. Depth estimation generally involves analyzing the disparity using infrared (IR) pattern light and stereo vision, and depth cameras that provide distance information between the camera and surrounding environmental elements in the form of 2D images (i.e., depth maps) are already widely used. Thus, as depicted in Fig. 1(c), by estimating the user's 2D gaze point coordinates with an eye-camera and mapping it to the depth map acquired by a depth-camera, the corresponding depth value can be retrieved to estimate the user's gaze distance [9][11]. This method can solidly eliminate human errors arising from the convergence angle-based method. However, it can easily malfunction due to external environmental factors such as disocclusion regions or reflective media.\nThis paper proposes a novel hybrid method that mimics the human visual perception mechanism [12] to complement the limitations of the two conventional gaze distance estimation approaches. Human vergence-accommodation response is generally modeled as two dual-parallel feedback control systems, where these responses interact to enhance gaze distance perception and ensure better visual stability. Similarly, the proposed method combines the vergence angle-based approach with the gaze-mapped depth approach, allowing these two methods to interact and achieve synergy. This enhances the accuracy of each method and applies adaptive weighted averaging to assess the confidence of each method, thereby ensuring the stability of the final derived gaze distance.",
        "context": "This section introduces the context of XR technologies and highlights the critical challenge of the vergence-accommodation conflict (VAC) as a key obstacle to their widespread adoption. It then details existing gaze distance estimation methods and their limitations, setting the stage for the proposed hybrid approach.",
        "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
        "pages": [
            1,
            2
        ],
        "id": "c41d64ca686313077df261af3b8668dd771aa3b0392fbe521708d9a6c36e3c78"
    },
    {
        "text": "This section details established methods for estimating 3D gaze depth, primarily relying on techniques like Prukinje images, pupil center distance variations, and multi-layer perceptrons trained on gaze normal vectors. These methods, including those by Kwon et al., Lee et al., and Mlot et al., demonstrate a range of approaches to capturing and estimating gaze depth, often utilizing pupil size and geometry to relate pupil position to target depth.\n\nThe vergence has been regarded as one of the most important distance cues of the human visual system for several reasons: simple principles of geometry, natural solution in computer vision field, and effectiveness, etc [13].\nKwon et al. [6] proposed geometry-based gaze depth estimation method. They utilized Prukinje images from two IR light sources with pupil center, then triangulated them to estimate gaze direction. With the idea that the pupil center distance (PCD) changes according to the target depth, of which PCD increases and decreases when the target object appears far and near from the eyes, respectively, they estimated gaze depth using PCD variations. Lee et al. [14] also\nFIGURE 1. (a) Hardware configuration for data acquisition; RGB scene image and corresponding depth image with two eye images. Conventional methods for gaze distance estimation using (b) vergence angle and (c) 2D gaze-mapped depth images.\nproposed 3D gaze estimation method with Prukinje images. Different with previous studies, they applied a multi-layer perceptron (MLP) to estimate gaze distance and designed three features used to train of which are the relative position of first and fourth Prukinje images to the pupil center, interdistance between these two Prukinje images, and pupil size. Note that they utilized the pupil size based on the fact that pupil accommodation happens according to the gaze distance changes. They finally estimated 2D gaze position with the pupil center based on a geometric transform considering the gaze distance obtained by MLP.\nMlot et al. [7] proposed fast and robust method to estimate the 3D gaze position based on the eye vergence. They utilized the ExCuSe algorithm [15] to find a pupil position, then applied a 2 nd order polynomial mapping function that describes the relationship between the pupil position and the target depth to estimate 3D gaze.\nLee et al. [16] also utilized MLP model to estimate the gaze depth. They trained the network using the gaze normal vectors obtained by a commercial eye tracker, Pupil Labs [8], instead Prukinje images, which differs from previous studies.\nThe gaze normal vector represented the three-dimensional direction of the eye's line of sight. They collected the training data by recording gaze normal vectors along with corresponding gaze depths, instructing participants to focus on a target positioned at distances ranging from 1 meter to 5 meters.",
        "original_text": "The vergence has been regarded as one of the most important distance cues of the human visual system for several reasons: simple principles of geometry, natural solution in computer vision field, and effectiveness, etc [13].\nKwon et al. [6] proposed geometry-based gaze depth estimation method. They utilized Prukinje images from two IR light sources with pupil center, then triangulated them to estimate gaze direction. With the idea that the pupil center distance (PCD) changes according to the target depth, of which PCD increases and decreases when the target object appears far and near from the eyes, respectively, they estimated gaze depth using PCD variations. Lee et al. [14] also\nFIGURE 1. (a) Hardware configuration for data acquisition; RGB scene image and corresponding depth image with two eye images. Conventional methods for gaze distance estimation using (b) vergence angle and (c) 2D gaze-mapped depth images.\nproposed 3D gaze estimation method with Prukinje images. Different with previous studies, they applied a multi-layer perceptron (MLP) to estimate gaze distance and designed three features used to train of which are the relative position of first and fourth Prukinje images to the pupil center, interdistance between these two Prukinje images, and pupil size. Note that they utilized the pupil size based on the fact that pupil accommodation happens according to the gaze distance changes. They finally estimated 2D gaze position with the pupil center based on a geometric transform considering the gaze distance obtained by MLP.\nMlot et al. [7] proposed fast and robust method to estimate the 3D gaze position based on the eye vergence. They utilized the ExCuSe algorithm [15] to find a pupil position, then applied a 2 nd order polynomial mapping function that describes the relationship between the pupil position and the target depth to estimate 3D gaze.\nLee et al. [16] also utilized MLP model to estimate the gaze depth. They trained the network using the gaze normal vectors obtained by a commercial eye tracker, Pupil Labs [8], instead Prukinje images, which differs from previous studies.\nThe gaze normal vector represented the three-dimensional direction of the eye's line of sight. They collected the training data by recording gaze normal vectors along with corresponding gaze depths, instructing participants to focus on a target positioned at distances ranging from 1 meter to 5 meters.",
        "context": "This section details established methods for estimating 3D gaze depth, primarily relying on techniques like Prukinje images, pupil center distance variations, and multi-layer perceptrons trained on gaze normal vectors. These methods, including those by Kwon et al., Lee et al., and Mlot et al., demonstrate a range of approaches to capturing and estimating gaze depth, often utilizing pupil size and geometry to relate pupil position to target depth.",
        "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
        "pages": [
            2
        ],
        "id": "70cd40d43461b6c3628100b26257f9807642a5134874c9779abae1eac409dad3"
    },
    {
        "text": "Existing research utilizing RGB-D cameras for gaze distance estimation primarily focuses on 3D gaze estimation, particularly for depth, while traditional methods prioritize minimizing angular errors in vergence. Previous studies haven’t effectively combined RGB-D data with vergence-based methods to improve gaze distance accuracy.\n\nThough the depth cue from an active sensor has an advantage that is invariant to human factors, there are a few studies exploiting the RGB-D camera for gaze distance estimation.\nElmadjian et al. [9] proposed a calibration procedure to estimate 3D gaze information using an uncalibrated headmounted binocular eye tracker coupled with a RGB-D camera. They compared the accuracy of 3D points of regard (PoR) using both geometric and regressor approaches. Based on\nIEEE Access\nFIGURE 2. (a) Human visual perception mechanism introduced in [12] and (b) overall framework of the proposed method by mimicking the visual perception mechanism.\nthe geometric model, they calculated intersection points of each eye gaze ray to determine the PoRs. For the regressor approach, they employed a Gaussian regressor to estimate both gaze direction and gaze depth. Because the hardware was uncalibrated, they tackled the issue that the inter pupillary distance (IPD) used to estimate a vergence angle did not align with the coordinate system of the scene camera. To address this issue, they designed a regressor pipeline consisting of two separate Gaussian regressors, one for estimating gaze direction and the other for computing the corresponding gaze depth. Despite demonstrating performance improvements, they also noted that there is still room for improvement in the accuracy of gaze distance estimation.\nLiu et al. [10] proposed an automatic calibration method for 3D gaze estimation by integrating gaze vectors with saliency maps. This approach addresses the limitations of traditional calibration methods, which typically require predefined targets and can be time-consuming. To mitigate these issues, they used saliency maps [17] to generate gaze targets based on the premise that human are more likely to fixate on salient features in scene images. To avoid redundant scene images in the calibration data, they applied bag-of-word algorithm [18] to measure image similarity. They then computed the 2D gaze points corresponding to the scene images by calibrating the relationship between the scene images and the gaze vectors from both eyes. Finally, they reconstructed a dense 3D environmental point cloud using an RGB-D camera to accuratelyt determine the 3D points of regards (PoRs).\nThe aforementioned studies utilized RGB-D cameras to estimate 3D gaze, particularly for gaze depths, whereas conventional 3D gaze estimation methods focused on minimizing angular errors in vergence using learning-based approaches. Although the depth information from RGB-D cameras was used to estimate gaze depth, it was not employed to enhance gaze distance accuracy through guided information.",
        "original_text": "Though the depth cue from an active sensor has an advantage that is invariant to human factors, there are a few studies exploiting the RGB-D camera for gaze distance estimation.\nElmadjian et al. [9] proposed a calibration procedure to estimate 3D gaze information using an uncalibrated headmounted binocular eye tracker coupled with a RGB-D camera. They compared the accuracy of 3D points of regard (PoR) using both geometric and regressor approaches. Based on\nIEEE Access\nFIGURE 2. (a) Human visual perception mechanism introduced in [12] and (b) overall framework of the proposed method by mimicking the visual perception mechanism.\nthe geometric model, they calculated intersection points of each eye gaze ray to determine the PoRs. For the regressor approach, they employed a Gaussian regressor to estimate both gaze direction and gaze depth. Because the hardware was uncalibrated, they tackled the issue that the inter pupillary distance (IPD) used to estimate a vergence angle did not align with the coordinate system of the scene camera. To address this issue, they designed a regressor pipeline consisting of two separate Gaussian regressors, one for estimating gaze direction and the other for computing the corresponding gaze depth. Despite demonstrating performance improvements, they also noted that there is still room for improvement in the accuracy of gaze distance estimation.\nLiu et al. [10] proposed an automatic calibration method for 3D gaze estimation by integrating gaze vectors with saliency maps. This approach addresses the limitations of traditional calibration methods, which typically require predefined targets and can be time-consuming. To mitigate these issues, they used saliency maps [17] to generate gaze targets based on the premise that human are more likely to fixate on salient features in scene images. To avoid redundant scene images in the calibration data, they applied bag-of-word algorithm [18] to measure image similarity. They then computed the 2D gaze points corresponding to the scene images by calibrating the relationship between the scene images and the gaze vectors from both eyes. Finally, they reconstructed a dense 3D environmental point cloud using an RGB-D camera to accuratelyt determine the 3D points of regards (PoRs).\nThe aforementioned studies utilized RGB-D cameras to estimate 3D gaze, particularly for gaze depths, whereas conventional 3D gaze estimation methods focused on minimizing angular errors in vergence using learning-based approaches. Although the depth information from RGB-D cameras was used to estimate gaze depth, it was not employed to enhance gaze distance accuracy through guided information.",
        "context": "Existing research utilizing RGB-D cameras for gaze distance estimation primarily focuses on 3D gaze estimation, particularly for depth, while traditional methods prioritize minimizing angular errors in vergence. Previous studies haven’t effectively combined RGB-D data with vergence-based methods to improve gaze distance accuracy.",
        "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
        "pages": [
            2,
            3
        ],
        "id": "5eae6f653ef942e6d8350fa7684d1ab047f0ecb16a16ad6f59bf55e097a304cc"
    },
    {
        "text": "Introduces a hybrid method for eye-gaze distance estimation, combining vergence-based and gaze-mapped depth approaches to improve accuracy and stability.\n\nIn this section, we explain a hybrid method for eye-gaze distance estimation. Note that we do not explain specifics of 2D gaze and a gaze distance from vergence because we only applied one of the commercial eye trackers, Pupil-Labs [8] of which provides 2D gaze with an accuracy of 0 . 60 ◦ and a precision of 0 . 02 ◦ . It is compatible with Intel RealSense RGB-D camera [19] (Fig. 1(a)) as the scene camera, and consists of two near-infrared (NIR) eye cameras, where the 2D gaze is calculated by the intersection of the gaze vectors from each eye in the scene camera. We exploit the human visual perception mechanism [12] (Fig. 2(a)) within the eyegaze distance estimation framework by cross-referencing the gaze distance derived from vergence with that obtained from\nIEEE Access gaze-mapped depth as shown in Fig. 2(b). We utilize the gaze distance from vergence, obtained with Pupil-Labs [8], as the initial estimate for vergence and the depth from the depth image pointed by 2D gaze as the initial estimate for gazemapped depth.\nFIGURE 3. Visualization of gaze distances from vergence captured with Pupil Eye Tracker [8] by gazing at targets placed at different distances, 0.5m, 1.0m, and 1.3m. The gaze distance from vergence has a linear correlation with target depths.",
        "original_text": "In this section, we explain a hybrid method for eye-gaze distance estimation. Note that we do not explain specifics of 2D gaze and a gaze distance from vergence because we only applied one of the commercial eye trackers, Pupil-Labs [8] of which provides 2D gaze with an accuracy of 0 . 60 ◦ and a precision of 0 . 02 ◦ . It is compatible with Intel RealSense RGB-D camera [19] (Fig. 1(a)) as the scene camera, and consists of two near-infrared (NIR) eye cameras, where the 2D gaze is calculated by the intersection of the gaze vectors from each eye in the scene camera. We exploit the human visual perception mechanism [12] (Fig. 2(a)) within the eyegaze distance estimation framework by cross-referencing the gaze distance derived from vergence with that obtained from\nIEEE Access gaze-mapped depth as shown in Fig. 2(b). We utilize the gaze distance from vergence, obtained with Pupil-Labs [8], as the initial estimate for vergence and the depth from the depth image pointed by 2D gaze as the initial estimate for gazemapped depth.\nFIGURE 3. Visualization of gaze distances from vergence captured with Pupil Eye Tracker [8] by gazing at targets placed at different distances, 0.5m, 1.0m, and 1.3m. The gaze distance from vergence has a linear correlation with target depths.",
        "context": "Introduces a hybrid method for eye-gaze distance estimation, combining vergence-based and gaze-mapped depth approaches to improve accuracy and stability.",
        "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
        "pages": [
            3,
            4
        ],
        "id": "f5bc3efc084edd5e7f70cc2f2e5481cb7190ca7d42cacd156c2bc269c75c520b"
    },
    {
        "text": "Addresses individual user variations and potential scale errors in vergence-based gaze distance estimation, utilizing gaze-mapped depth as a corrective measure due to its robustness to human factors and linear relationship with vergence distance.\n\nDifferent face shapes of users (i.e. eye position, eye ball size, inter pupillary distance, etc.) can cause scale errors of gaze distance from vergence even though the 2D gaze point is well estimated. We exploit the gaze distance from gazemapped depth as a guided information to refine the scale of the initial gaze distance from vergence because the IR active sensor is relatively robust to human factors. Based on the characteristics of which the gaze distance from vergence changes roughly linearly to that of gaze-mapped depth as shown in Fig. 3, we exploit 1 st order polynomial function to refine the initial gaze distance from vergence.\n<!-- formula-not-decoded -->\nwhere ˆ V is the refined gaze distance from vergence for given initial gaze distance from vergence V and α 0 , α 1 are the polynomial coefficients. The optimal coefficients can be estimated by minimizing an objective function defined as\n<!-- formula-not-decoded -->\nwhere N is the number of samples, V n , and D n are n th initial gaze distance from vergence and corresponding that of gazemapped depth, respectively.\nAfter gaze fixation, the relaxation of ocular muscles can lead to drifts in the gaze distance derived from vergence, causing inaccuracies in estimation. Since pupil diameter tends to increase when such drift occurs (Fig. 4), we define a basis gaze distance from vergence to minimize the drift error. This defined basis is continuously updated only when the pupil\nFIGURE 4. Comparison of gaze distance from vergence and normalized pupil diameter. The pupil diameter increases when the gaze distance from vergence drifts (red arrows).\ndiameter constricts, effectively helping to suppress the drift error, maintain estimation accuracy, and stabilize the overall gaze distance measurements.\n<!-- formula-not-decoded -->\nwhere V t is a refined gaze distance from vergence at time t , V basis is the basis gaze distance from vergence, and ▽ P size is the normalized differentiation of the pupil diameter.\nBecause the pupil diameter changes smoothly over time, we apply temporal consistency to it.\n<!-- formula-not-decoded -->\nwhere t pupil is a temporal weight for pupil diameter and P T size is the pupil diameter at time T .",
        "original_text": "Different face shapes of users (i.e. eye position, eye ball size, inter pupillary distance, etc.) can cause scale errors of gaze distance from vergence even though the 2D gaze point is well estimated. We exploit the gaze distance from gazemapped depth as a guided information to refine the scale of the initial gaze distance from vergence because the IR active sensor is relatively robust to human factors. Based on the characteristics of which the gaze distance from vergence changes roughly linearly to that of gaze-mapped depth as shown in Fig. 3, we exploit 1 st order polynomial function to refine the initial gaze distance from vergence.\n<!-- formula-not-decoded -->\nwhere ˆ V is the refined gaze distance from vergence for given initial gaze distance from vergence V and α 0 , α 1 are the polynomial coefficients. The optimal coefficients can be estimated by minimizing an objective function defined as\n<!-- formula-not-decoded -->\nwhere N is the number of samples, V n , and D n are n th initial gaze distance from vergence and corresponding that of gazemapped depth, respectively.\nAfter gaze fixation, the relaxation of ocular muscles can lead to drifts in the gaze distance derived from vergence, causing inaccuracies in estimation. Since pupil diameter tends to increase when such drift occurs (Fig. 4), we define a basis gaze distance from vergence to minimize the drift error. This defined basis is continuously updated only when the pupil\nFIGURE 4. Comparison of gaze distance from vergence and normalized pupil diameter. The pupil diameter increases when the gaze distance from vergence drifts (red arrows).\ndiameter constricts, effectively helping to suppress the drift error, maintain estimation accuracy, and stabilize the overall gaze distance measurements.\n<!-- formula-not-decoded -->\nwhere V t is a refined gaze distance from vergence at time t , V basis is the basis gaze distance from vergence, and ▽ P size is the normalized differentiation of the pupil diameter.\nBecause the pupil diameter changes smoothly over time, we apply temporal consistency to it.\n<!-- formula-not-decoded -->\nwhere t pupil is a temporal weight for pupil diameter and P T size is the pupil diameter at time T .",
        "context": "Addresses individual user variations and potential scale errors in vergence-based gaze distance estimation, utilizing gaze-mapped depth as a corrective measure due to its robustness to human factors and linear relationship with vergence distance.",
        "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
        "pages": [
            4
        ],
        "id": "2342b78717dbd5003ea60dd2263f9650d17a6a95a6d628f468d6839ea0d2a516"
    },
    {
        "text": "This section details strategies for mitigating errors in gaze distance estimation caused by unreliable depth data from a gaze-mapped depth sensor, specifically addressing issues with disocclusion and reflective surfaces.\n\nThe gaze distances from gaze-mapped depth in disocclusion regions and reflective material are unreliable for eye-gazedistance estimation. To avoid using unreliable gaze distances from gaze-mapped depth in eye-gaze distance estimation, we define activation functions for each environment as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere ▽ D is the normalized differentiation of the gaze distance from gaze-mapped depth, threshdis and errref are the minimum depth value considered as reliable and the maximum error of depth sensor in the reflection, respectively. Because the active depth sensor [19] has a sensing range of 0.3m to 10.0m and returns a near-zero depth value in disoccluded areas, we set the threshold, threshdis , to 0.25. In addition, because the depth sensor typically returns temporally stable depth values but becomes unstable in reflective areas, we empirically set the error threshold, errref , to 0.9925.",
        "original_text": "The gaze distances from gaze-mapped depth in disocclusion regions and reflective material are unreliable for eye-gazedistance estimation. To avoid using unreliable gaze distances from gaze-mapped depth in eye-gaze distance estimation, we define activation functions for each environment as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere ▽ D is the normalized differentiation of the gaze distance from gaze-mapped depth, threshdis and errref are the minimum depth value considered as reliable and the maximum error of depth sensor in the reflection, respectively. Because the active depth sensor [19] has a sensing range of 0.3m to 10.0m and returns a near-zero depth value in disoccluded areas, we set the threshold, threshdis , to 0.25. In addition, because the depth sensor typically returns temporally stable depth values but becomes unstable in reflective areas, we empirically set the error threshold, errref , to 0.9925.",
        "context": "This section details strategies for mitigating errors in gaze distance estimation caused by unreliable depth data from a gaze-mapped depth sensor, specifically addressing issues with disocclusion and reflective surfaces.",
        "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
        "pages": [
            4
        ],
        "id": "a6317ec53c6bcb169e68ed93a4b4b1336beb398a0a242b3a3b371a93dc564cdb"
    },
    {
        "text": "Evaluates the stability and accuracy of a hybrid gaze distance estimation method, incorporating both vergence-based and gaze-mapped depth cues, while mitigating noise from human errors like blinking and environmental factors like reflections and occlusion. Specifically, it designs confidence measures for both vergence and gaze-mapped depth, leveraging temporal consistency and weighting these cues to produce a temporally consistent and robust eye-gaze distance estimate.\n\nHuman errors such as blinking can be transferred to the gaze distance from vergence in the form of peak noises. To avoid utilizing such noisy gaze distances from vergence for eyegaze distance estimation, we design a confidence measure of gaze distance from vergence as\n<!-- formula-not-decoded -->\nwhere ▽ ˆ V is a normalized differentiation of the gaze distance from vergence. In addition, the gaze distance from gazemapped depth may have oscillation according to the environments. To avoid relying on noisy gaze distance from gazemapped depth, we design a confidence measure for the gaze distance from gaze-mapped depth as\n<!-- formula-not-decoded -->\nwhere ▽ D is a normalized differentiation of the gaze distance from gaze-mapped depth. Because the gaze distance from gaze-mapped depth remains temporally consistent when fixating on the same object, we apply temporal consistency to the confidence of gaze distance from gaze-mapped depth as\n<!-- formula-not-decoded -->\nwhere d T conf and t depth denote the gaze-mapped depth confidence at time T and its temporal weight, respectively.\nBased on the gaze distance from vergence confidence and that from the gaze-mapped depth confidence, we estimate the eye-gaze distance by weighting each cue as follows:\n<!-- formula-not-decoded -->\nNote that because temporal consistency is applied to both the pupil diameter used for refining the gaze distance from vergence and the gaze-mapped depth confidence, the estimated eye-gaze distance also remains temporally consistent.",
        "original_text": "Human errors such as blinking can be transferred to the gaze distance from vergence in the form of peak noises. To avoid utilizing such noisy gaze distances from vergence for eyegaze distance estimation, we design a confidence measure of gaze distance from vergence as\n<!-- formula-not-decoded -->\nwhere ▽ ˆ V is a normalized differentiation of the gaze distance from vergence. In addition, the gaze distance from gazemapped depth may have oscillation according to the environments. To avoid relying on noisy gaze distance from gazemapped depth, we design a confidence measure for the gaze distance from gaze-mapped depth as\n<!-- formula-not-decoded -->\nwhere ▽ D is a normalized differentiation of the gaze distance from gaze-mapped depth. Because the gaze distance from gaze-mapped depth remains temporally consistent when fixating on the same object, we apply temporal consistency to the confidence of gaze distance from gaze-mapped depth as\n<!-- formula-not-decoded -->\nwhere d T conf and t depth denote the gaze-mapped depth confidence at time T and its temporal weight, respectively.\nBased on the gaze distance from vergence confidence and that from the gaze-mapped depth confidence, we estimate the eye-gaze distance by weighting each cue as follows:\n<!-- formula-not-decoded -->\nNote that because temporal consistency is applied to both the pupil diameter used for refining the gaze distance from vergence and the gaze-mapped depth confidence, the estimated eye-gaze distance also remains temporally consistent.",
        "context": "Evaluates the stability and accuracy of a hybrid gaze distance estimation method, incorporating both vergence-based and gaze-mapped depth cues, while mitigating noise from human errors like blinking and environmental factors like reflections and occlusion. Specifically, it designs confidence measures for both vergence and gaze-mapped depth, leveraging temporal consistency and weighting these cues to produce a temporally consistent and robust eye-gaze distance estimate.",
        "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
        "pages": [
            5
        ],
        "id": "0b5ad2fedd5a2228d299860b6f31034ea3d09ab985b6c157f8b88af79611ae72"
    },
    {
        "text": "Evaluates the hybrid method’s accuracy and stability in realistic settings, considering potential errors and comparing it to single-method approaches. The experiment involved twelve participants and a setup with three gaze targets at varying distances, including a reflective laptop screen to assess environmental factors.\n\nIn this research, we did not solely focus on comparing the optimal gaze distance estimation accuracy achievable under perfect experimental conditions. Rather, we performed experiments to quantitatively and qualitatively evaluate the accuracy and stability of the proposed hybrid method in more realistic settings, taking into account various potential errors, in comparison to traditional single-method approaches. For this purpose, we arranged the experimental setup as shown in Figure 5. We set up three gaze targets at different distances (0.5m, 1.0m, 1.3m), labeled as near, middle, and far targets, respectively. Specifically, to examine the human error aspect of the existing vergence angle-based gaze distance estimation technique, participants were asked to gaze at each target for several seconds to encourage natural eye blinking and relaxation. To assess errors from external environmental factors\nIEEE Access\nFIGURE 5. Experimental environments setup for the evaluation. Chin holder is used to fixate head. Three targets with different markers are placed in front of the chin holder at 0.5m, 1.0m, and 1.3m. Additional marker is displayed on the laptop with reflective screen.\nin the gaze-mapped depth-based gaze distance estimation approach, a laptop with a reflective screen was placed next to the middle target, and the near and far targets were arranged to overlap, creating non-occluded areas. Furthermore, all experiments were carried out with twelve participants under identical conditions, setting the temporal consistency weights for pupil and depth, t pupil and t depth as 0.95. A chin rest was also utilized to prevent unintended gaze distance variations due to user posture.\nFIGURE 6. Three experimental scenarios; S1: gazing at the near target only, S2: gazing at the middle target and the reflective screen located at the same distance alternately, and S3: gazing at the far target and disocclusion alternately.",
        "original_text": "In this research, we did not solely focus on comparing the optimal gaze distance estimation accuracy achievable under perfect experimental conditions. Rather, we performed experiments to quantitatively and qualitatively evaluate the accuracy and stability of the proposed hybrid method in more realistic settings, taking into account various potential errors, in comparison to traditional single-method approaches. For this purpose, we arranged the experimental setup as shown in Figure 5. We set up three gaze targets at different distances (0.5m, 1.0m, 1.3m), labeled as near, middle, and far targets, respectively. Specifically, to examine the human error aspect of the existing vergence angle-based gaze distance estimation technique, participants were asked to gaze at each target for several seconds to encourage natural eye blinking and relaxation. To assess errors from external environmental factors\nIEEE Access\nFIGURE 5. Experimental environments setup for the evaluation. Chin holder is used to fixate head. Three targets with different markers are placed in front of the chin holder at 0.5m, 1.0m, and 1.3m. Additional marker is displayed on the laptop with reflective screen.\nin the gaze-mapped depth-based gaze distance estimation approach, a laptop with a reflective screen was placed next to the middle target, and the near and far targets were arranged to overlap, creating non-occluded areas. Furthermore, all experiments were carried out with twelve participants under identical conditions, setting the temporal consistency weights for pupil and depth, t pupil and t depth as 0.95. A chin rest was also utilized to prevent unintended gaze distance variations due to user posture.\nFIGURE 6. Three experimental scenarios; S1: gazing at the near target only, S2: gazing at the middle target and the reflective screen located at the same distance alternately, and S3: gazing at the far target and disocclusion alternately.",
        "context": "Evaluates the hybrid method’s accuracy and stability in realistic settings, considering potential errors and comparing it to single-method approaches. The experiment involved twelve participants and a setup with three gaze targets at varying distances, including a reflective laptop screen to assess environmental factors.",
        "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
        "pages": [
            5
        ],
        "id": "7d87c624c373aac3ae7ec885243685b8017fc8ca21faa6f84e5d22498cf404f4"
    },
    {
        "text": "Evaluates the impact of human factors and environmental conditions on gaze distance estimation accuracy through three distinct test scenarios (S1-S3), designed to assess performance under varying conditions like blinking, eye relaxation, and reflective surfaces.\n\nTo verify the stability and accuracy of the proposed method against the aforementioned human and environmental factors, we designed three scenarios of gaze fixation change (S1, S2,\nIEEE Access\nTABLE 1. Average accuracy of the proposed method for each scenario with Euclidean distance and angular difference metrics.\n\nA, Section/Scenario. = S1. A, Vergence [8].mm = 33.678. A, Vergence [8].deg = 0.248. A, Gazed-Depth [19].mm = 26.658. A, Gazed-Depth [19].deg = 0.185. A, Proposed.mm = 23.861. A, Proposed.deg = 0.1777. B, Section/Scenario. = S2. B, Vergence [8].mm = 65.410. B, Vergence [8].deg = 0.205. B, Gazed-Depth [19].mm = 417.046. B, Gazed-Depth [19].deg = 0.987. B, Proposed.mm = 64.807. B, Proposed.deg = 0.199. C, Section/Scenario. = S3. C, Vergence [8].mm = 77.587. C, Vergence [8].deg = 0.108. C, Gazed-Depth [19].mm = 484.946. C, Gazed-Depth [19].deg = 0.280. C, Proposed.mm = 111.133. C, Proposed.deg = 0.141. D, Section/Scenario. = S1. D, Vergence [8].mm = 109.184. D, Vergence [8].deg = 0.628. D, Gazed-Depth [19].mm = 16.583. D, Gazed-Depth [19].deg = 0.114. D, Proposed.mm = 56.801. D, Proposed.deg = 0.210. E, Section/Scenario. = S2. E, Vergence [8].mm = 152.996. E, Vergence [8].deg = 0.531. E, Gazed-Depth [19].mm = 481.143. E, Gazed-Depth [19].deg = 1.267. E, Proposed.mm = 37.421. E, Proposed.deg = 0.132. Overall, Section/Scenario. = Overall. Overall, Vergence [8].mm = 90.867. Overall, Vergence [8].deg = 0.312. Overall, Gazed-Depth [19].mm = 352.98. Overall, Gazed-Depth [19].deg = 0.635. Overall, Proposed.mm = 66.868. Overall, Proposed.deg = 0.167\nS3) as shown in Fig. 6. S1 is a test scenario designed to evaluate the impact of gaze distance estimation errors that may occur due to human factors, such as peak noise from blinking or drift caused by eye relaxation. In this scenario, the user is instructed to continuously gaze at the near target for 8 seconds. S2 and S3 are test scenarios intended to assess the impact of environmental factors on gaze distance estimation errors. In S2, the user is instructed to gaze at the middle target, then shift their gaze to the laptop located at the same distance for 7 seconds, and finally return to the middle target. This scenario is designed to examine the influence of the laptop's reflective surface on gaze estimation. In S3, the user is instructed to gaze at the far target, initially focusing on the center of the target and then shifting their gaze to the lower right corner of the same target for 8 seconds. Most commercial depth cameras use infrared (IR) pattern light, and when multiple objects overlap in the captured scene, the object closer to the camera may block the pattern light, creating disoccluded areas on the more distant object, making depth estimation challenging. The S3 scenario is designed to evaluate the impact of these disoccluded regions. The three test scenarios mentioned were applied in a continuous experimental sequence in the order of S1-S2-S3-S1-S2.",
        "original_text": "To verify the stability and accuracy of the proposed method against the aforementioned human and environmental factors, we designed three scenarios of gaze fixation change (S1, S2,\nIEEE Access\nTABLE 1. Average accuracy of the proposed method for each scenario with Euclidean distance and angular difference metrics.\n\nA, Section/Scenario. = S1. A, Vergence [8].mm = 33.678. A, Vergence [8].deg = 0.248. A, Gazed-Depth [19].mm = 26.658. A, Gazed-Depth [19].deg = 0.185. A, Proposed.mm = 23.861. A, Proposed.deg = 0.1777. B, Section/Scenario. = S2. B, Vergence [8].mm = 65.410. B, Vergence [8].deg = 0.205. B, Gazed-Depth [19].mm = 417.046. B, Gazed-Depth [19].deg = 0.987. B, Proposed.mm = 64.807. B, Proposed.deg = 0.199. C, Section/Scenario. = S3. C, Vergence [8].mm = 77.587. C, Vergence [8].deg = 0.108. C, Gazed-Depth [19].mm = 484.946. C, Gazed-Depth [19].deg = 0.280. C, Proposed.mm = 111.133. C, Proposed.deg = 0.141. D, Section/Scenario. = S1. D, Vergence [8].mm = 109.184. D, Vergence [8].deg = 0.628. D, Gazed-Depth [19].mm = 16.583. D, Gazed-Depth [19].deg = 0.114. D, Proposed.mm = 56.801. D, Proposed.deg = 0.210. E, Section/Scenario. = S2. E, Vergence [8].mm = 152.996. E, Vergence [8].deg = 0.531. E, Gazed-Depth [19].mm = 481.143. E, Gazed-Depth [19].deg = 1.267. E, Proposed.mm = 37.421. E, Proposed.deg = 0.132. Overall, Section/Scenario. = Overall. Overall, Vergence [8].mm = 90.867. Overall, Vergence [8].deg = 0.312. Overall, Gazed-Depth [19].mm = 352.98. Overall, Gazed-Depth [19].deg = 0.635. Overall, Proposed.mm = 66.868. Overall, Proposed.deg = 0.167\nS3) as shown in Fig. 6. S1 is a test scenario designed to evaluate the impact of gaze distance estimation errors that may occur due to human factors, such as peak noise from blinking or drift caused by eye relaxation. In this scenario, the user is instructed to continuously gaze at the near target for 8 seconds. S2 and S3 are test scenarios intended to assess the impact of environmental factors on gaze distance estimation errors. In S2, the user is instructed to gaze at the middle target, then shift their gaze to the laptop located at the same distance for 7 seconds, and finally return to the middle target. This scenario is designed to examine the influence of the laptop's reflective surface on gaze estimation. In S3, the user is instructed to gaze at the far target, initially focusing on the center of the target and then shifting their gaze to the lower right corner of the same target for 8 seconds. Most commercial depth cameras use infrared (IR) pattern light, and when multiple objects overlap in the captured scene, the object closer to the camera may block the pattern light, creating disoccluded areas on the more distant object, making depth estimation challenging. The S3 scenario is designed to evaluate the impact of these disoccluded regions. The three test scenarios mentioned were applied in a continuous experimental sequence in the order of S1-S2-S3-S1-S2.",
        "context": "Evaluates the impact of human factors and environmental conditions on gaze distance estimation accuracy through three distinct test scenarios (S1-S3), designed to assess performance under varying conditions like blinking, eye relaxation, and reflective surfaces.",
        "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
        "pages": [
            5,
            6
        ],
        "id": "5ec191def5ee5f64478896bc1a91b93aa2c0b5156889553208df4c174155d80c"
    },
    {
        "text": "Evaluates the hybrid gaze distance estimation method’s performance under challenging conditions, specifically highlighting its robustness against human error (blinks, drift) and environmental factors (disocclusion, reflections). The method cross-references vergence and gaze-mapped depth estimations to produce more precise results, mimicking human visual perception.\n\nInitially, the experimental results comparing the impact of imposing a pupil diameter constraint on refining gaze distance estimations due to human error using the vergence are depicted in Fig. 7. In the plot, the method without the pupil diameter constraint, detailed in Eq. 1, is shown by a blue line; the method including the pupil diameter constraint, detailed in Eq. 3, by a red line; and the ground truth by a green line. The findings indicate that enforcing the pupil diameter constraint effectively mitigates peak noise caused by eye blinks across all segments and substantially reduces drift noise due to eye relaxation in segments A and D, where a gaze fixation is sustained. However, some deviations between the refined and the true values are noticeable in segments B, C, and E. These discrepancies could be due to variations in initial gaze distance estimates following gaze shifts, indicating inherent limitations in the precision of this method.\nSimilarly, the experimental findings comparing the ef-\ngaze distance from vergence gaze distance from vergence W pupil constraint)\nground truth\nFIGURE 7. Comparison of the gaze distance from vergence with/without the pupil diameter constraint. Gaze distance from vergence with pupil diameter constraint successively suppresses both drift noise caused by eye relaxation and peak noise caused by eye blinks.\nfectiveness of the proposed noise filter in mitigating gaze distance estimation errors due to environmental factors using gaze-mapped depth are depicted in Fig. 8. Particularly, Fig. 8(a) illustrates the efficiency of the proposed noise filter in detecting noise when gazing at disocclusion regions, while Fig. 8 shows its performance in identifying noise when gazing at reflective surfaces. Across the graphs, the gaze distance estimates based on gaze-mapped depth are denoted by red lines, and the estimation errors due to disoccluded regions and reflective surfaces are overlaid with green dotted lines and blue dotted lines, respectively. The findings indicate that the proposed gaze-mapped depth noise filter effectively identifies depth estimation errors caused by environmental factors. This implies that selectively employing gaze-mapped depth-based gaze distance estimation methods based on their confidence can result in highly accurate gaze distance measurements.\nFigure 9 presents a qualitative comparison of the perfor-\nIEEE Access\nFIGURE 8. Results of the proposed gaze-mapped depth noise filters described in Sec. III-B. Shading describes the frames of which the filters detected noise; (a) disocclusion and (b) reflection.\nmance of the proposed hybrid method. To aid understanding, representative data are overlaid in Fig. 9(a) for each of the tests S1, S2, and S3, including the user's gaze coordinates, estimated gaze distance, and the positions of both pupils and the depth map acquired by the depth camera. Figure 9(b) indicates the confidence of vergence-based and gaze-mapped depth-based gaze distance estimates at each timestamp, along with the weights assigned to these estimates in computing the final gaze distance estimation from Eq. 10. Figure 9(c) shows the final ground truth gaze distance values (green solid line), vergence-based gaze distance estimates (blue solid line), gaze-mapped depth-based gaze distance estimates (orange solid line), and the proposed hybrid method's estimates (red solid line). The graphs reveal that the proposed hybrid method remains robust against drift and peak noise in segments corresponding to S1, which commonly suffer from human error, and it also performs reliably in segments corresponding to S2 and S3, which are susceptible to depth estimation errors due to environmental factors like reflective surfaces and occlusion areas. Ultimately, the proposed hybrid method effectively mitigates each type of noise in challenging conditions, yielding estimates close to the ground truth values.\nTo quantitatively evaluate the proposed hybrid approach, we utilized the Euclidean distance and angular difference metrics [7], which are defined as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere Gest , Ggt , and O are the estimated gaze point, ground truth, and origin, represented as ( xest , yest , zest ) , ( xgt , ygt , zgt ) , and (0 , 0 , 0) , respectively.\nCommercial eye trackers typically perform with a visual angle error ranging from 0.5 to 1 degree. Despite the difficulty of fair evaluation due to differing experimental setups, considering the challenging test conditions discussed in this paper, Table 1 illustrates that the proposed method demonstrates superior quantitative performance comparable to existing methods and commercial products. Nevertheless, the primary aim of this study is to validate the adequate performance and stability of the proposed method under practically challenging circumstances. Within this context, the results of the quantitative performance assessment reveal several important insights. First, as observed in Segments A, B, and E, the proposed hybrid method does not solely choose the more reliable value from the two existing methods running in parallel. Instead, it cross-references the values estimated by each method to produce a more precise final estimation. This feature closely mimics the human visual perception mechanism previously discussed. Secondly, as seen in Segments C and D, unless one of the existing methods experiences a severe error, the proposed method remains resilient against errors even in various demanding environments, achieving an accuracy of up to 0.132 degrees. Lastly, even when severe errors occur in one of the two existing methods, as seen in Segments C and D, the proposed method effectively manages these errors, maintaining a moderate accuracy of approximately 0.14 to 0.21 degrees.",
        "original_text": "Initially, the experimental results comparing the impact of imposing a pupil diameter constraint on refining gaze distance estimations due to human error using the vergence are depicted in Fig. 7. In the plot, the method without the pupil diameter constraint, detailed in Eq. 1, is shown by a blue line; the method including the pupil diameter constraint, detailed in Eq. 3, by a red line; and the ground truth by a green line. The findings indicate that enforcing the pupil diameter constraint effectively mitigates peak noise caused by eye blinks across all segments and substantially reduces drift noise due to eye relaxation in segments A and D, where a gaze fixation is sustained. However, some deviations between the refined and the true values are noticeable in segments B, C, and E. These discrepancies could be due to variations in initial gaze distance estimates following gaze shifts, indicating inherent limitations in the precision of this method.\nSimilarly, the experimental findings comparing the ef-\ngaze distance from vergence gaze distance from vergence W pupil constraint)\nground truth\nFIGURE 7. Comparison of the gaze distance from vergence with/without the pupil diameter constraint. Gaze distance from vergence with pupil diameter constraint successively suppresses both drift noise caused by eye relaxation and peak noise caused by eye blinks.\nfectiveness of the proposed noise filter in mitigating gaze distance estimation errors due to environmental factors using gaze-mapped depth are depicted in Fig. 8. Particularly, Fig. 8(a) illustrates the efficiency of the proposed noise filter in detecting noise when gazing at disocclusion regions, while Fig. 8 shows its performance in identifying noise when gazing at reflective surfaces. Across the graphs, the gaze distance estimates based on gaze-mapped depth are denoted by red lines, and the estimation errors due to disoccluded regions and reflective surfaces are overlaid with green dotted lines and blue dotted lines, respectively. The findings indicate that the proposed gaze-mapped depth noise filter effectively identifies depth estimation errors caused by environmental factors. This implies that selectively employing gaze-mapped depth-based gaze distance estimation methods based on their confidence can result in highly accurate gaze distance measurements.\nFigure 9 presents a qualitative comparison of the perfor-\nIEEE Access\nFIGURE 8. Results of the proposed gaze-mapped depth noise filters described in Sec. III-B. Shading describes the frames of which the filters detected noise; (a) disocclusion and (b) reflection.\nmance of the proposed hybrid method. To aid understanding, representative data are overlaid in Fig. 9(a) for each of the tests S1, S2, and S3, including the user's gaze coordinates, estimated gaze distance, and the positions of both pupils and the depth map acquired by the depth camera. Figure 9(b) indicates the confidence of vergence-based and gaze-mapped depth-based gaze distance estimates at each timestamp, along with the weights assigned to these estimates in computing the final gaze distance estimation from Eq. 10. Figure 9(c) shows the final ground truth gaze distance values (green solid line), vergence-based gaze distance estimates (blue solid line), gaze-mapped depth-based gaze distance estimates (orange solid line), and the proposed hybrid method's estimates (red solid line). The graphs reveal that the proposed hybrid method remains robust against drift and peak noise in segments corresponding to S1, which commonly suffer from human error, and it also performs reliably in segments corresponding to S2 and S3, which are susceptible to depth estimation errors due to environmental factors like reflective surfaces and occlusion areas. Ultimately, the proposed hybrid method effectively mitigates each type of noise in challenging conditions, yielding estimates close to the ground truth values.\nTo quantitatively evaluate the proposed hybrid approach, we utilized the Euclidean distance and angular difference metrics [7], which are defined as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere Gest , Ggt , and O are the estimated gaze point, ground truth, and origin, represented as ( xest , yest , zest ) , ( xgt , ygt , zgt ) , and (0 , 0 , 0) , respectively.\nCommercial eye trackers typically perform with a visual angle error ranging from 0.5 to 1 degree. Despite the difficulty of fair evaluation due to differing experimental setups, considering the challenging test conditions discussed in this paper, Table 1 illustrates that the proposed method demonstrates superior quantitative performance comparable to existing methods and commercial products. Nevertheless, the primary aim of this study is to validate the adequate performance and stability of the proposed method under practically challenging circumstances. Within this context, the results of the quantitative performance assessment reveal several important insights. First, as observed in Segments A, B, and E, the proposed hybrid method does not solely choose the more reliable value from the two existing methods running in parallel. Instead, it cross-references the values estimated by each method to produce a more precise final estimation. This feature closely mimics the human visual perception mechanism previously discussed. Secondly, as seen in Segments C and D, unless one of the existing methods experiences a severe error, the proposed method remains resilient against errors even in various demanding environments, achieving an accuracy of up to 0.132 degrees. Lastly, even when severe errors occur in one of the two existing methods, as seen in Segments C and D, the proposed method effectively manages these errors, maintaining a moderate accuracy of approximately 0.14 to 0.21 degrees.",
        "context": "Evaluates the hybrid gaze distance estimation method’s performance under challenging conditions, specifically highlighting its robustness against human error (blinks, drift) and environmental factors (disocclusion, reflections). The method cross-references vergence and gaze-mapped depth estimations to produce more precise results, mimicking human visual perception.",
        "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
        "pages": [
            6,
            7
        ],
        "id": "d5948d204d107a412b6ec54bab23b45770f82d12d2d8473c92c3510d3582fd18"
    },
    {
        "text": "Qualitative and quantitative evaluations indicate that the proposed method is robust against errors arising from human factors and environmental conditions, which are the inherent drawbacks of existing methods.\n\nThis paper proposed a novel hybrid gaze distance estimation method to overcome the limitations of existing approaches. Typically, gaze distance estimation in wearable XR devices is mainly based on binocular vergence angles. However, this approach is highly prone to errors induced by human factors (e.g., eye blinks, pupil dilation). As an alternative, a\nIEEE Access\nFIGURE 9. Qualitative comparison of the gaze distances estimated by vergence, gaze-mapped depth, and the proposed hybrid method with ground truth. (a) describes gaze transitions for each scenario with pupil detection results. (b) and (c) depict the results of the proposed confidence measures and the estimated gaze distance values, respectively.\nmethod integrating depth estimation techniques from computer vision with 2D gaze point tracking to utilize gazemapped depth information can be considered. However, this method often encounters difficulties in accurately measuring depth due to external environmental factors (e.g. disocclusion regions, reflective surfaces). To tackle these challenges, the introduced method conducts parallel gaze distance estimations: one based on binocular vergence and the other on gaze-mapped depth. It subsequently assesses the confidence of each approach and calculates an adaptive weighted sum through cross-referencing to derive an optimal gaze distance estimate. Qualitative and quantitative evaluations indicate that the proposed method is robust against errors arising from human factors and environmental conditions, which are the inherent drawbacks of existing methods. Additionally, it is evidenced that the proposed technique does not simply select the most reliable value from the two existing methods but crossreferences the estimated values to yield a more precise final estimate. The introduced method can be directly applied to developing user gaze distance estimation modules for varifocal AR/VR devices, aiming for an ideal mixed reality experience without VAC. In addition, it has potential applications in diverse fields, including digital healthcare, human-computer interaction, and analyzing user behavior and preferences.",
        "original_text": "This paper proposed a novel hybrid gaze distance estimation method to overcome the limitations of existing approaches. Typically, gaze distance estimation in wearable XR devices is mainly based on binocular vergence angles. However, this approach is highly prone to errors induced by human factors (e.g., eye blinks, pupil dilation). As an alternative, a\nIEEE Access\nFIGURE 9. Qualitative comparison of the gaze distances estimated by vergence, gaze-mapped depth, and the proposed hybrid method with ground truth. (a) describes gaze transitions for each scenario with pupil detection results. (b) and (c) depict the results of the proposed confidence measures and the estimated gaze distance values, respectively.\nmethod integrating depth estimation techniques from computer vision with 2D gaze point tracking to utilize gazemapped depth information can be considered. However, this method often encounters difficulties in accurately measuring depth due to external environmental factors (e.g. disocclusion regions, reflective surfaces). To tackle these challenges, the introduced method conducts parallel gaze distance estimations: one based on binocular vergence and the other on gaze-mapped depth. It subsequently assesses the confidence of each approach and calculates an adaptive weighted sum through cross-referencing to derive an optimal gaze distance estimate. Qualitative and quantitative evaluations indicate that the proposed method is robust against errors arising from human factors and environmental conditions, which are the inherent drawbacks of existing methods. Additionally, it is evidenced that the proposed technique does not simply select the most reliable value from the two existing methods but crossreferences the estimated values to yield a more precise final estimate. The introduced method can be directly applied to developing user gaze distance estimation modules for varifocal AR/VR devices, aiming for an ideal mixed reality experience without VAC. In addition, it has potential applications in diverse fields, including digital healthcare, human-computer interaction, and analyzing user behavior and preferences.",
        "context": "Qualitative and quantitative evaluations indicate that the proposed method is robust against errors arising from human factors and environmental conditions, which are the inherent drawbacks of existing methods.",
        "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
        "pages": [
            8,
            7
        ],
        "id": "861372246dc73e55716b93740dff088eed9b6d9349f16e2d7e692ff988f24ef5"
    },
    {
        "text": "Provides supporting evidence for the argument on economic inequality.\n\n- [1] D. M Hoffman, A. R Girshick, K. Akeley, and M. S Banks. ''Vergenceaccommdation conflicts hinder visual performance and cause visual fatigue,'' Journal of vision , vol.8, no.3, pp. 33-33, 2008.\n- [2] A. K. Bhowmick, A. Jamali, D. Bryant, S. Pintz, and P. J. Bos. ''Liquid Crystal Based 5 cm Adaptive Focus Lens to Solve AccommodationConvergence (AC) Mismatch Issue of AR/VR/3D Displays.'' Society of Information Display (SID) , June 28, 2021.\n- [3] A. K. Bhowmick, A. Jamali, D. Bryant, S. Pintz, S. and P. J. Bos. ''Optical performance characterization of 5 cm aperture size continuous focus tunable liquid crystal lens for resolving Accommodation-Convergence mismatch conflict of AR/VR/3D HMDs.'' In Dig. Tech. Pap.-Soc. Inf. Disp. Int. Symp . Vol. 53, no. 1, pp. 166-169, 2022.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\nIEEE Access\n- [4] D. Dunn, C. Tippets, K. Torell, H. Fuchs, P. Kellnhofer, K. Myszkowski, ... and D. Luebke. ''Membrane AR: varifocal, wide field of view augmented reality display from deformable membranes.'' In ACM SIGGRAPH 2017 Emerging Technologies , pp. 1-2, 2017.\n- [5] S. G. Park, K. Kim, and J. Ha, ''T-Glasses: Light-Weight Hight Efficient Augmented Reality Smart Glasses.'' Proceedings of the International Display Workshops , 2022.\n- [6] Y. M. Kwon, K. W. Jeon, J. Ki, Q. M. Shahab, S. Jo, and S. K. Kim. ''3d gaze estimation and interaction to stereo display,'' International Journal of Virtual Reality , vol.5, no.3, pp. 41-45, 2006.\n- [7] E. G. Mlot, H. Bahmani, S. Wahl, and E. Kasneci. ''3d gaze estimation using eye vergence,'' International Conference on Health Informatics , vol.6, pp. 125-131, Scitepress, 2016.\n- [8] M. Kassner, W. Patera, and A. Bulling. ''Pupil: an open source platform for pervasive eye tracking and mobile gaze-based interaction.\", In Proceedings of the 2014 ACM international joint conference on pervasive and ubiquitous computing: Adjunct publication , pp. 1151-1160, 2014.\n- [9] C. Elmadjian, P. Shukla, A. D. Tula, and C. H. Morimoto. ''3D gaze estimation in the scene volume with a head-mounted eye tracker,'' In Proceedings of the Workshop on Communcation by Gaze Interaction , pp. 1-9, 2018.\n- [10] M. Liu, Y. Li, and H. Liu. ''Robust 3-D gaze estimation via data optimization and saliency aggregation for mobile eye-tracking systems.'' IEEE Transactions on Instrumentation and Measurement , 70, pp. 1-10, 2021.\n- [11] M. Liu, Y. Li, and H. Liu. ''3d gaze estimation for head-mounted devices based on visual saliency.'' In IEEE RSJ International Conference on Intelligent Robots and Systems(IROS) , pp. 10611-10616, 2020.\n- [12] M. Lambooij, W. IJsselsteijn, M. Fortuin, and I. Heynderickx. ''Visual discomfort and visual fatigue of stereoscopic displays: a review.'' Journal of imaging science and technology , vol.53, no.3, 30201-1, 2009.\n- [13] P. Linton. ''Does vision extract absolute distance from vergence?'' Attention, Perception, & Psychophysics , vol.82, no.6, pp. 3176-3195, 2020.\n- [14] J. W. Lee, C. W. Cho, K. Y. Shin, E. C. Lee, and K. R. Park. ''3D gaze tracking method using Purkinje images on eye optimal and pupil.'' Optics and Lasers in Engineering , vol.50, no.5, pp. 736-751, 2021.\n- [15] W. Fuhl, T. Kübler, K. Sippel, W. Rosenstiel, and E. Rosenstiel. ''Excuse: Robust pupil detection in real-world scenarios.'' In Computer Analysis of Images and Patterns: 16th International Conference, CAIP 2015, Valletta, Malta, September 2-4, Proceedings, Part I , pp. 39-51, Springer International Publishing, 2015.\n- [16] Y. Lee, C. Shin, A. Plopski, Y. Itoh, T. Piumsomboon, A. Dey, ... , and M. Billinghurst, ''Estimating gaze depth using multi-layer perceptron.'' In IEEE 2017 International Symposium on Ubiquitous Virtual Reality (ISUVR) , pp. 26-29, 2017.\n- [17] J. Harel, C. Koch, and P. Perona. ''Graph-based visual saliency.'' Advances in neural information processing systems , 2006.\n- [18] D. Gálvez-López, and J. D. Tardos. ''Bags of binary words for fast place recognition in image sequences.\" IEEE Transactions on robotics , vol.28, no.5, pp. 1188-1197, 2012.\n- [19] L. Keselman, W. J., Iselin, A. G.- Jepsen, and A. Bhowmik. ''Intel realsense stereoscopic depth cameras.'' In Proceedings of the IEEE conference on computer vision and pattern recognition workshops , pp. 1-10, 2017.\nDAE-YONG CHO received his B.S. degree in Intelligent Systems from the Department of Robotics at Kwangwoon University, South Korea, in 2014. In 2016, he earned a M.S. degree from the School of Information and Communications Engineering at the Gwangju Institute of Science and Technology (GIST). He worked as a Assistant Researcher at ADAS ONE, Inc., Seoul, South Korea, for approximately two years. From 2019, he served as a researcher at the Korea Institute of Science and\nTechnology (KIST) for about thee years. Currently, he is pursuing a Ph.D. in the KHU-KIST Department of Converging Science and Technology at Kyung Hee University, South Korea. His current research interests include smart glasses, focusing on the integration of augmented reality and artificial intelligence technologies for enhanced user interaction and accessibility in various applications, such as real-time information display, health monitoring, and assistive technology for visually impaired users.\nMIN-KOO KANG received his B.S. degree in Electronic Engineering from Inha University, South Korea, in 2008, and his M.S. degree in Electrical Engineering and Ph.D. degree in Information and Communications Engineering from the Gwangju Institute of Science and Technology (GIST) in 2010 and 2015, respectively. He worked as a Postdoctoral Researcher at the Center for Imaging Media Research, Korea Institute of Science and Technology (KIST), Seoul, South\nKorea, for three years, and has been a Principal Researcher at the Intelligent · Interaction Research Center since 2018. In addition, he has been an Adjunct Professor at Korea University since 2020 and at Kyung Hee University since 2023. His current research interests include telepresence, holoporation, remote monitoring and pilot technologies that transcend space and time; integrated visual/perceptual augmentation glass technology to enhance human capabilities; and other areas such as user behavior and intent analysis, digital healthcare, and human-computer interaction technologies.",
        "original_text": "- [1] D. M Hoffman, A. R Girshick, K. Akeley, and M. S Banks. ''Vergenceaccommdation conflicts hinder visual performance and cause visual fatigue,'' Journal of vision , vol.8, no.3, pp. 33-33, 2008.\n- [2] A. K. Bhowmick, A. Jamali, D. Bryant, S. Pintz, and P. J. Bos. ''Liquid Crystal Based 5 cm Adaptive Focus Lens to Solve AccommodationConvergence (AC) Mismatch Issue of AR/VR/3D Displays.'' Society of Information Display (SID) , June 28, 2021.\n- [3] A. K. Bhowmick, A. Jamali, D. Bryant, S. Pintz, S. and P. J. Bos. ''Optical performance characterization of 5 cm aperture size continuous focus tunable liquid crystal lens for resolving Accommodation-Convergence mismatch conflict of AR/VR/3D HMDs.'' In Dig. Tech. Pap.-Soc. Inf. Disp. Int. Symp . Vol. 53, no. 1, pp. 166-169, 2022.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\nIEEE Access\n- [4] D. Dunn, C. Tippets, K. Torell, H. Fuchs, P. Kellnhofer, K. Myszkowski, ... and D. Luebke. ''Membrane AR: varifocal, wide field of view augmented reality display from deformable membranes.'' In ACM SIGGRAPH 2017 Emerging Technologies , pp. 1-2, 2017.\n- [5] S. G. Park, K. Kim, and J. Ha, ''T-Glasses: Light-Weight Hight Efficient Augmented Reality Smart Glasses.'' Proceedings of the International Display Workshops , 2022.\n- [6] Y. M. Kwon, K. W. Jeon, J. Ki, Q. M. Shahab, S. Jo, and S. K. Kim. ''3d gaze estimation and interaction to stereo display,'' International Journal of Virtual Reality , vol.5, no.3, pp. 41-45, 2006.\n- [7] E. G. Mlot, H. Bahmani, S. Wahl, and E. Kasneci. ''3d gaze estimation using eye vergence,'' International Conference on Health Informatics , vol.6, pp. 125-131, Scitepress, 2016.\n- [8] M. Kassner, W. Patera, and A. Bulling. ''Pupil: an open source platform for pervasive eye tracking and mobile gaze-based interaction.\", In Proceedings of the 2014 ACM international joint conference on pervasive and ubiquitous computing: Adjunct publication , pp. 1151-1160, 2014.\n- [9] C. Elmadjian, P. Shukla, A. D. Tula, and C. H. Morimoto. ''3D gaze estimation in the scene volume with a head-mounted eye tracker,'' In Proceedings of the Workshop on Communcation by Gaze Interaction , pp. 1-9, 2018.\n- [10] M. Liu, Y. Li, and H. Liu. ''Robust 3-D gaze estimation via data optimization and saliency aggregation for mobile eye-tracking systems.'' IEEE Transactions on Instrumentation and Measurement , 70, pp. 1-10, 2021.\n- [11] M. Liu, Y. Li, and H. Liu. ''3d gaze estimation for head-mounted devices based on visual saliency.'' In IEEE RSJ International Conference on Intelligent Robots and Systems(IROS) , pp. 10611-10616, 2020.\n- [12] M. Lambooij, W. IJsselsteijn, M. Fortuin, and I. Heynderickx. ''Visual discomfort and visual fatigue of stereoscopic displays: a review.'' Journal of imaging science and technology , vol.53, no.3, 30201-1, 2009.\n- [13] P. Linton. ''Does vision extract absolute distance from vergence?'' Attention, Perception, & Psychophysics , vol.82, no.6, pp. 3176-3195, 2020.\n- [14] J. W. Lee, C. W. Cho, K. Y. Shin, E. C. Lee, and K. R. Park. ''3D gaze tracking method using Purkinje images on eye optimal and pupil.'' Optics and Lasers in Engineering , vol.50, no.5, pp. 736-751, 2021.\n- [15] W. Fuhl, T. Kübler, K. Sippel, W. Rosenstiel, and E. Rosenstiel. ''Excuse: Robust pupil detection in real-world scenarios.'' In Computer Analysis of Images and Patterns: 16th International Conference, CAIP 2015, Valletta, Malta, September 2-4, Proceedings, Part I , pp. 39-51, Springer International Publishing, 2015.\n- [16] Y. Lee, C. Shin, A. Plopski, Y. Itoh, T. Piumsomboon, A. Dey, ... , and M. Billinghurst, ''Estimating gaze depth using multi-layer perceptron.'' In IEEE 2017 International Symposium on Ubiquitous Virtual Reality (ISUVR) , pp. 26-29, 2017.\n- [17] J. Harel, C. Koch, and P. Perona. ''Graph-based visual saliency.'' Advances in neural information processing systems , 2006.\n- [18] D. Gálvez-López, and J. D. Tardos. ''Bags of binary words for fast place recognition in image sequences.\" IEEE Transactions on robotics , vol.28, no.5, pp. 1188-1197, 2012.\n- [19] L. Keselman, W. J., Iselin, A. G.- Jepsen, and A. Bhowmik. ''Intel realsense stereoscopic depth cameras.'' In Proceedings of the IEEE conference on computer vision and pattern recognition workshops , pp. 1-10, 2017.\nDAE-YONG CHO received his B.S. degree in Intelligent Systems from the Department of Robotics at Kwangwoon University, South Korea, in 2014. In 2016, he earned a M.S. degree from the School of Information and Communications Engineering at the Gwangju Institute of Science and Technology (GIST). He worked as a Assistant Researcher at ADAS ONE, Inc., Seoul, South Korea, for approximately two years. From 2019, he served as a researcher at the Korea Institute of Science and\nTechnology (KIST) for about thee years. Currently, he is pursuing a Ph.D. in the KHU-KIST Department of Converging Science and Technology at Kyung Hee University, South Korea. His current research interests include smart glasses, focusing on the integration of augmented reality and artificial intelligence technologies for enhanced user interaction and accessibility in various applications, such as real-time information display, health monitoring, and assistive technology for visually impaired users.\nMIN-KOO KANG received his B.S. degree in Electronic Engineering from Inha University, South Korea, in 2008, and his M.S. degree in Electrical Engineering and Ph.D. degree in Information and Communications Engineering from the Gwangju Institute of Science and Technology (GIST) in 2010 and 2015, respectively. He worked as a Postdoctoral Researcher at the Center for Imaging Media Research, Korea Institute of Science and Technology (KIST), Seoul, South\nKorea, for three years, and has been a Principal Researcher at the Intelligent · Interaction Research Center since 2018. In addition, he has been an Adjunct Professor at Korea University since 2020 and at Kyung Hee University since 2023. His current research interests include telepresence, holoporation, remote monitoring and pilot technologies that transcend space and time; integrated visual/perceptual augmentation glass technology to enhance human capabilities; and other areas such as user behavior and intent analysis, digital healthcare, and human-computer interaction technologies.",
        "context": "Provides supporting evidence for the argument on economic inequality.",
        "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
        "pages": [
            9
        ],
        "id": "16346c14ededefef2fc73f5767a29c1a6464bd25b93fb05a03bda4faf3d2646a"
    }
]