[
    {
        "text": "Introduces the document’s publication details and DOI.\n\nReceived February 7, 2022, accepted February 22, 2022, date of publication March 2, 2022, date of current version March 11, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.31561 18",
        "original_text": "Received February 7, 2022, accepted February 22, 2022, date of publication March 2, 2022, date of current version March 11, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.31561 18",
        "context": "Introduces the document’s publication details and DOI.",
        "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
        "pages": [
            1
        ],
        "id": "ced7da322a4c089bc6e250f16b3cce510dc9cafef75170bb71275124b8a4ec10"
    },
    {
        "text": "Introduces a machine-learning model (HS-BNN) designed to predict FinFET electrical characteristics affected by process-induced line-edge roughness, highlighting its ability to reduce simulation time and perform automatic model selection.\n\nSANGHO YU 1 , SANG MIN WON 1 , HYOUNG WON BAAC 1 , DONGHEE SON 1 , AND CHANGHWAN SHIN 2 , (Senior Member, IEEE)\n1 Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon 16419, Republic of Korea 2 School of Electrical Engineering, Korea University, Seoul 02841, Republic of Korea\nCorresponding authors: Changhwan Shin (cshin@korea.ac.kr) and Donghee Son (daniel3600@g.skku.edu)\nThis work was supported in part by the National Research Foundation of Korea (NRF) Grant funded by the Korea Government Ministry of Science and ICT (MSIT) under Grant 2020R1A2C1009063; and in part by the Korea Medical Device Development Fund Grant funded by the Korean Government (the Ministry of Trade, Industry and Energy, the Ministry of Health & Welfare, and the Ministry of Food and Drug Safety) under Grant 202012D28.\n- ABSTRACT To design a device that is robust to process-induced random variation, this study proposes a machine-learning-based predictive model that can simulate the electrical characteristics of FinFETs with process-induced line-edge roughness. This model, i.e., a Bayesian neural network (BNN) model with horseshoe priors (Horseshoe-BNN), can signiGLYPH<28>cantly reduce the simulation time (as compared to the conventional technology computer-aided design (TCAD) simulation method) in a sufGLYPH<28>ciently accurate manner. Moreover, this model can perform autonomous model selection over the most compact layer size, which is necessary when the amount of data must be limited. The mean absolute percentage error for the mean and standard deviation of the drain-to-source current . IDS / were GLYPH<24> 0.5% and GLYPH<24> 6%, respectively. By estimating the distribution of the current-voltage characteristics, the distributions of the other device metrics, such as off-state leakage current and threshold voltage, can be estimated as well.\nINDEX TERMS Line edge roughness (LER), process-induced random variation, Bayesian neural network, automatic model selection.",
        "original_text": "SANGHO YU 1 , SANG MIN WON 1 , HYOUNG WON BAAC 1 , DONGHEE SON 1 , AND CHANGHWAN SHIN 2 , (Senior Member, IEEE)\n1 Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon 16419, Republic of Korea 2 School of Electrical Engineering, Korea University, Seoul 02841, Republic of Korea\nCorresponding authors: Changhwan Shin (cshin@korea.ac.kr) and Donghee Son (daniel3600@g.skku.edu)\nThis work was supported in part by the National Research Foundation of Korea (NRF) Grant funded by the Korea Government Ministry of Science and ICT (MSIT) under Grant 2020R1A2C1009063; and in part by the Korea Medical Device Development Fund Grant funded by the Korean Government (the Ministry of Trade, Industry and Energy, the Ministry of Health & Welfare, and the Ministry of Food and Drug Safety) under Grant 202012D28.\n- ABSTRACT To design a device that is robust to process-induced random variation, this study proposes a machine-learning-based predictive model that can simulate the electrical characteristics of FinFETs with process-induced line-edge roughness. This model, i.e., a Bayesian neural network (BNN) model with horseshoe priors (Horseshoe-BNN), can signiGLYPH<28>cantly reduce the simulation time (as compared to the conventional technology computer-aided design (TCAD) simulation method) in a sufGLYPH<28>ciently accurate manner. Moreover, this model can perform autonomous model selection over the most compact layer size, which is necessary when the amount of data must be limited. The mean absolute percentage error for the mean and standard deviation of the drain-to-source current . IDS / were GLYPH<24> 0.5% and GLYPH<24> 6%, respectively. By estimating the distribution of the current-voltage characteristics, the distributions of the other device metrics, such as off-state leakage current and threshold voltage, can be estimated as well.\nINDEX TERMS Line edge roughness (LER), process-induced random variation, Bayesian neural network, automatic model selection.",
        "context": "Introduces a machine-learning model (HS-BNN) designed to predict FinFET electrical characteristics affected by process-induced line-edge roughness, highlighting its ability to reduce simulation time and perform automatic model selection.",
        "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
        "pages": [
            1
        ],
        "id": "e9078adc0f19121c9bdf19b95484326ad38c44565abca94f0181966dca6de015"
    },
    {
        "text": "Introduces the central thesis about climate policy reform.\n\nAs the lateral/vertical dimension of transistors in integrated circuits (ICs) (e.g., channel length, channel width, junction depth, etc.) has been scaled down, the process-induced randomvariation of device parameters becomes signiGLYPH<28>cant. This would signiGLYPH<28>cantly damage the yield of devices. Processinduced line-edge-roughness (LER) is one type of random variation sources, and it induces a signiGLYPH<28>cant amount of variation in aggressively scaled transistors. Because the amplitude of LER did not shrink as much as the feature size shrinkage, the portion of LER in the nominal/physical channel length and width became signiGLYPH<28>cant, resulting in a larger variation in IDS GLYPH<0> VGS characteristics of devices. Hence, the LER in device has become a primary limit when the physical dimensions of device has been scaled down. In this regard, precise proGLYPH<28>ling of the impact of LER on the device performance has become\nThe associate editor coordinating the review of this manuscript and approving it for publication was Anisul Haque.\nan indispensable requisite for designing a variation-immune device. To date, technology computer-aided design (TCAD) simulation has been used to evaluate the impact of LER on device characteristics; however, it takes a signiGLYPH<28>cant amount of time. Thousands of devices should be simulated to determine the exact amount of LER-induced variation (in real, a few weeks even for a single LER proGLYPH<28>le). For this reason, a novel approach for less time-consuming simulation is necessary. In this context, we proposed a linear regression model in [1]; however, this linear regression model was limited in dealing with nonlinearity. Conversely, artiGLYPH<28>cial neural network (ANN) is the most popular machine-learning model because it shows a powerful ability to GLYPH<28>nd patterns as well as to detect trends in complex real-world data. By stacking multiple layers (in detail, neurons/nodes aggregations that receive inputs and compute/give outputs, based on their predeGLYPH<28>ned simple non-linear functions), an ANN can well deGLYPH<28>ne highly nonlinear functions and capture non-linear dependencies in data. However, standard ANN models are prone\nto overGLYPH<28>tting and exhibit poor generalization performance. In addition, they tend to make unjustiGLYPH<28>ed over-conGLYPH<28>dent predictions for the inputs far away from the training data, which can sometimes result in suboptimal predictions [2]. In particular, in the regime of small data (where the uncertainty of data is severe), ANNs are prone to poorly specify their weights. To solve these problems, the Bayesian neural network (BNN) has received lots of attention. The BNNs treat their weights as random variables (described by distributions), and thereby capturing uncertainties in the training data and their weights. This study employed BNN models to predict the IDS GLYPH<0> VGS characteristics in various FinFETs with different device structures. In real, there is a demand for FinFET devices with multiple heights [3], [4]. The BNN model proposed in this work assigns horseshoe priors [5] as a prior distribution, which we call the Horseshoe-BNN (HS-BNN). The prior (i.e., prior distribution) is a prior belief of the weight distribution's conGLYPH<28>guration before observing the data. According to the Bayesian theorem, the beliefs are updated after the data observance [6]. Herein, depending on which prior distribution is to be speciGLYPH<28>ed, it leads to a different model training process. While the Gaussian prior is the most commonly-used prior, the horseshoe distribution has been chosen to introduce sparsity and shrinkage over the weights. The horseshoe prior makes it available the model selection over a number of nodes and can reduce arduous/tiresome work in optimizing the layer sizes.",
        "original_text": "As the lateral/vertical dimension of transistors in integrated circuits (ICs) (e.g., channel length, channel width, junction depth, etc.) has been scaled down, the process-induced randomvariation of device parameters becomes signiGLYPH<28>cant. This would signiGLYPH<28>cantly damage the yield of devices. Processinduced line-edge-roughness (LER) is one type of random variation sources, and it induces a signiGLYPH<28>cant amount of variation in aggressively scaled transistors. Because the amplitude of LER did not shrink as much as the feature size shrinkage, the portion of LER in the nominal/physical channel length and width became signiGLYPH<28>cant, resulting in a larger variation in IDS GLYPH<0> VGS characteristics of devices. Hence, the LER in device has become a primary limit when the physical dimensions of device has been scaled down. In this regard, precise proGLYPH<28>ling of the impact of LER on the device performance has become\nThe associate editor coordinating the review of this manuscript and approving it for publication was Anisul Haque.\nan indispensable requisite for designing a variation-immune device. To date, technology computer-aided design (TCAD) simulation has been used to evaluate the impact of LER on device characteristics; however, it takes a signiGLYPH<28>cant amount of time. Thousands of devices should be simulated to determine the exact amount of LER-induced variation (in real, a few weeks even for a single LER proGLYPH<28>le). For this reason, a novel approach for less time-consuming simulation is necessary. In this context, we proposed a linear regression model in [1]; however, this linear regression model was limited in dealing with nonlinearity. Conversely, artiGLYPH<28>cial neural network (ANN) is the most popular machine-learning model because it shows a powerful ability to GLYPH<28>nd patterns as well as to detect trends in complex real-world data. By stacking multiple layers (in detail, neurons/nodes aggregations that receive inputs and compute/give outputs, based on their predeGLYPH<28>ned simple non-linear functions), an ANN can well deGLYPH<28>ne highly nonlinear functions and capture non-linear dependencies in data. However, standard ANN models are prone\nto overGLYPH<28>tting and exhibit poor generalization performance. In addition, they tend to make unjustiGLYPH<28>ed over-conGLYPH<28>dent predictions for the inputs far away from the training data, which can sometimes result in suboptimal predictions [2]. In particular, in the regime of small data (where the uncertainty of data is severe), ANNs are prone to poorly specify their weights. To solve these problems, the Bayesian neural network (BNN) has received lots of attention. The BNNs treat their weights as random variables (described by distributions), and thereby capturing uncertainties in the training data and their weights. This study employed BNN models to predict the IDS GLYPH<0> VGS characteristics in various FinFETs with different device structures. In real, there is a demand for FinFET devices with multiple heights [3], [4]. The BNN model proposed in this work assigns horseshoe priors [5] as a prior distribution, which we call the Horseshoe-BNN (HS-BNN). The prior (i.e., prior distribution) is a prior belief of the weight distribution's conGLYPH<28>guration before observing the data. According to the Bayesian theorem, the beliefs are updated after the data observance [6]. Herein, depending on which prior distribution is to be speciGLYPH<28>ed, it leads to a different model training process. While the Gaussian prior is the most commonly-used prior, the horseshoe distribution has been chosen to introduce sparsity and shrinkage over the weights. The horseshoe prior makes it available the model selection over a number of nodes and can reduce arduous/tiresome work in optimizing the layer sizes.",
        "context": "Introduces the central thesis about climate policy reform.",
        "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
        "pages": [
            1,
            2
        ],
        "id": "975d97ababdbc33169d57b2a71bc113ba3f2811d21281758d0961d84c45ea489"
    },
    {
        "text": "Describes the simulation process, detailing the use of MATLAB and Sentaurus TCAD to generate 3D LER profiles from MATLAB parameters (RMS amplitude, x-axis correlation length, and roughness exponent) and their subsequent import into the TCAD tool for device simulation.\n\nVarious FinFETs with different structures were designed on the basis of [7]GLYPH<21>[10], and thereafter, 3D LER proGLYPH<28>les on those FinFETs were applied/simulated using the MATLAB model (which was proposed in [11]) and Sentaurus TCAD. Specifically, the 3D LER sequences (i.e., points of the randomly rough surfaces) were generated by the MATLAB model with three main parameters for LER [i.e., RMS amplitude ( GLYPH<27> ), x(y)-axis correlation length .GLYPH<24> X ( GLYPH<24> Y / ), and roughness exponent ( GLYPH<11> )], and the sequences were imported into the TCAD tool. Herein, the RMS amplitude ( GLYPH<27> ) indicates the standard deviation of the LER amplitudes, and it is often referred to as the LER or LER magnitude. Note that GLYPH<27> is the major factor for LER. The correlation length ( GLYPH<24> ) is corresponding to the wavelength of LER proGLYPH<28>le, and the roughness exponent ( GLYPH<11> ) quantitatively indicates the way how high-frequency components in LER proGLYPH<28>le diminishes. The device parameters for FinFET (including the parameters for LER and device structure) are summarized in Table 1. Figure 1(a) shows an isometric view and current density distribution of a FinFET with Lg D 20 nm ; WGLYPH<28>n D 7 nm ; HGLYPH<28>n D 42 nm . Note that the feature parameters for LER used in the FinFET is as follows: GLYPH<27> D 0 : 5 nm ; GLYPH<24> X D 20 nm ; GLYPH<24> Y D 50 nm ; GLYPH<11> D 1. Figure 1(b) shows the IDS GLYPH<0> VGS characteristics with 100 sample devices. Herein, the GLYPH<28>n width variation by LER causes the variations in the IDS GLYPH<0> VGS characteristics [12]. It is noteworthy that the 3D TCAD device simulations were run using various\nTABLE 1. Device parameters used in this work.\nCorrelation length along y direction (5x), 1 = 1~500 nm. Roughness exponent, 1 = 0.1~1.3\n(a)\nFIGURE 1. (a) Isometric view of FinFET with LER (left) and its current density distribution (right), and (b) simulated I DS -V GS of nominal device and 100 sample devices with LER. The parameters for LER and FinFET device structure is GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1, and Lg D 20 nm, W fin D 7 nm, H fin D 42 nm, respectively.\nphysics models, i.e., the ShockleyGLYPH<21>ReadGLYPH<21>Hall model for carrier generation and recombination, the Old Slotboom model for bandgap narrowing, the Lombardi model for thin-layer mobility, and the density gradient quantization model for quantum mechanics effects.\nWe have noted that creating a number of data is very time-consuming because of a long TCAD simulation run\nFIGURE 2. Probability density of Horseshoe prior and Gaussian prior.\ntime. This led to a dilemma when satisfying (i) the number of LER proGLYPH<28>les or (ii) the number of sample devices with each LER proGLYPH<28>le. In other words, to appropriately estimate the distribution of electrical characteristics, the number of sample devices should be sufGLYPH<28>cient, but at the same time, the number of LER proGLYPH<28>les is quite important. In considering those two aspects, 169 different datasets were composed for training and testing the model, and each dataset consisted of 50 different sample FinFETs with identical LER and device parameters. Eighteen types of FinFET device structure were chosen based on [7]GLYPH<21>[10], and the LER parameters were randomly selected in the relevant ranges shown in Table 1.\nAs mentioned in the previous study [1], the distribution of log . IDS / for a given VGS can be considered as a normal distribution in terms of small kurtosis and skewness values. From this perspective, the mean and standard deviation of log . IDS / (denoted as GLYPH<22> IDS and GLYPH<27> IDS , respectively) were selected as target variables. And gate voltage . VGS / , LER parameters (i.e., RMS amplitude ( GLYPH<27> ), correlation length along the x-axis ( GLYPH<24> X ), correlation length along the y-axis .GLYPH<24> Y / , and roughness exponent ( GLYPH<11> )) and device structure parameters (i.e., gate length ( Lg ), GLYPH<28>n width ( WGLYPH<28>n ), and GLYPH<28>n height ( HGLYPH<28>n )) were selected as feature variables to predict the LER-induced variation in the IDS-VGS characteristics of various FinFET structures.",
        "original_text": "Various FinFETs with different structures were designed on the basis of [7]GLYPH<21>[10], and thereafter, 3D LER proGLYPH<28>les on those FinFETs were applied/simulated using the MATLAB model (which was proposed in [11]) and Sentaurus TCAD. Specifically, the 3D LER sequences (i.e., points of the randomly rough surfaces) were generated by the MATLAB model with three main parameters for LER [i.e., RMS amplitude ( GLYPH<27> ), x(y)-axis correlation length .GLYPH<24> X ( GLYPH<24> Y / ), and roughness exponent ( GLYPH<11> )], and the sequences were imported into the TCAD tool. Herein, the RMS amplitude ( GLYPH<27> ) indicates the standard deviation of the LER amplitudes, and it is often referred to as the LER or LER magnitude. Note that GLYPH<27> is the major factor for LER. The correlation length ( GLYPH<24> ) is corresponding to the wavelength of LER proGLYPH<28>le, and the roughness exponent ( GLYPH<11> ) quantitatively indicates the way how high-frequency components in LER proGLYPH<28>le diminishes. The device parameters for FinFET (including the parameters for LER and device structure) are summarized in Table 1. Figure 1(a) shows an isometric view and current density distribution of a FinFET with Lg D 20 nm ; WGLYPH<28>n D 7 nm ; HGLYPH<28>n D 42 nm . Note that the feature parameters for LER used in the FinFET is as follows: GLYPH<27> D 0 : 5 nm ; GLYPH<24> X D 20 nm ; GLYPH<24> Y D 50 nm ; GLYPH<11> D 1. Figure 1(b) shows the IDS GLYPH<0> VGS characteristics with 100 sample devices. Herein, the GLYPH<28>n width variation by LER causes the variations in the IDS GLYPH<0> VGS characteristics [12]. It is noteworthy that the 3D TCAD device simulations were run using various\nTABLE 1. Device parameters used in this work.\nCorrelation length along y direction (5x), 1 = 1~500 nm. Roughness exponent, 1 = 0.1~1.3\n(a)\nFIGURE 1. (a) Isometric view of FinFET with LER (left) and its current density distribution (right), and (b) simulated I DS -V GS of nominal device and 100 sample devices with LER. The parameters for LER and FinFET device structure is GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1, and Lg D 20 nm, W fin D 7 nm, H fin D 42 nm, respectively.\nphysics models, i.e., the ShockleyGLYPH<21>ReadGLYPH<21>Hall model for carrier generation and recombination, the Old Slotboom model for bandgap narrowing, the Lombardi model for thin-layer mobility, and the density gradient quantization model for quantum mechanics effects.\nWe have noted that creating a number of data is very time-consuming because of a long TCAD simulation run\nFIGURE 2. Probability density of Horseshoe prior and Gaussian prior.\ntime. This led to a dilemma when satisfying (i) the number of LER proGLYPH<28>les or (ii) the number of sample devices with each LER proGLYPH<28>le. In other words, to appropriately estimate the distribution of electrical characteristics, the number of sample devices should be sufGLYPH<28>cient, but at the same time, the number of LER proGLYPH<28>les is quite important. In considering those two aspects, 169 different datasets were composed for training and testing the model, and each dataset consisted of 50 different sample FinFETs with identical LER and device parameters. Eighteen types of FinFET device structure were chosen based on [7]GLYPH<21>[10], and the LER parameters were randomly selected in the relevant ranges shown in Table 1.\nAs mentioned in the previous study [1], the distribution of log . IDS / for a given VGS can be considered as a normal distribution in terms of small kurtosis and skewness values. From this perspective, the mean and standard deviation of log . IDS / (denoted as GLYPH<22> IDS and GLYPH<27> IDS , respectively) were selected as target variables. And gate voltage . VGS / , LER parameters (i.e., RMS amplitude ( GLYPH<27> ), correlation length along the x-axis ( GLYPH<24> X ), correlation length along the y-axis .GLYPH<24> Y / , and roughness exponent ( GLYPH<11> )) and device structure parameters (i.e., gate length ( Lg ), GLYPH<28>n width ( WGLYPH<28>n ), and GLYPH<28>n height ( HGLYPH<28>n )) were selected as feature variables to predict the LER-induced variation in the IDS-VGS characteristics of various FinFET structures.",
        "context": "Describes the simulation process, detailing the use of MATLAB and Sentaurus TCAD to generate 3D LER profiles from MATLAB parameters (RMS amplitude, x-axis correlation length, and roughness exponent) and their subsequent import into the TCAD tool for device simulation.",
        "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
        "pages": [
            2,
            3
        ],
        "id": "48dad3f022235b00c7e6e3980ea3e5af9bc9b1bcdb345fc023f80f9f8a477a8c"
    },
    {
        "text": "Introduces the central thesis about a machine-learning-based predictive model for simulating electrical characteristics of FinFETs with process-induced line-edge roughness.\n\nPredictive models are just a function in the form of y D f ( x ) that GLYPH<28>ts the given data D D GLYPH<8>GLYPH<0> xn ; y n GLYPH<1>GLYPH<9> N 1 , and they are generalized/extended to the cases that have not been seen in training. For example, the function form of a linear regression model is y D P wixi C b . However, this model has its own limit for determining the nonlinear relationship between x and y . Conversely, an ANN with an L hidden layer is in the form of y D W . L C 1 / h . L / C b . L C 1 / , where W l is a matrix of weights between layer ( l GLYPH<0> 1 ) and layer ( l ) in the size of R . Kl GLYPH<0> 1 C 1 / × Kl , and h . l / is a matrix of\nFIGURE 3. Architecture of the HS-BNN, predicting the mean and standard deviation of log(I DS ) in various FinFET structures with arbitrary LER profiles.\nFIGURE 4. Diagram of K-fold cross-validation.\nFIGURE 5. MAPE, RMSE, MAE, and PLL values of each model along different numbers of nodes, showing model selection over the number of HS-BNN nodes.\noutputs from layer ( l ) . Herein, Kl is the number of units in layer l , h . l / D a GLYPH<0> W . l / h . l GLYPH<0> 1 / C b GLYPH<1> ; h . 0 / D x ; and a ( · ) is an activation function for each hidden layer. In the case of the ANN with three hidden layers, the function is\nFIGURE 6. (a) Predicted standard deviation and (b) predicted mean of log(I OFF ) for FinFET structure of Lg D 20 nm, H fin D 42 nm, W fin D 7 nm with the LER profile of GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1. Note that each parameter (i.e., GLYPH<27> , GLYPH<24> X , GLYPH<24> Y , GLYPH<11> , Lg, H fin , and W fin ) is varied in relevant ranges.\ny D W 4 a GLYPH<0> W 3 a GLYPH<0> W 2 a GLYPH<0> W 1 x C b 1 GLYPH<1> C b 2 GLYPH<1> C b 3 GLYPH<1> C b 4 . The ANN model is a composite function of multiple activation functions (i.e., activation integral) [13], which can make it complex nonlinear regression predictions.\nA Bayesian neural network is an extended standard ANN with Bayesian/posterior inference [14]. While the ANN is a deterministic model, the BNN is a stochastic/probabilistic model. The BNN incorporates uncertainty in modeling tasks by introducing distributions over the weights W GLYPH<24> p ( W ) (i.e., W D GLYPH<8> W l GLYPH<9> L C 1 1 , a set of weight matrices over all the layers) [15]. By doing so, BNNs can demonstrate/reconGLYPH<28>gure epistemic uncertainty when the amount of data is limited. However, at the same time, there are inherent neural network problems like over\n(under)-parameterization. The larger(smaller) the number of data points is, the larger(smaller) the number of required nodes is. When the number of nodes is much larger(smaller) than that estimated from the data, the variance around the BNN's predictions would be too large(small). The layer sizes should be as large as necessary. Especially, in the case of estimating the LER-induced variation, data were bound to be small, so that over-parameterization had to be prevented. In this regard, this study assigns horseshoe priors over weights using Bayesian inference. The horseshoe prior is given by: GLYPH<16> w . l / i ; j j GLYPH<28> i ; v GLYPH<17> ∼ N GLYPH<0> 0 ;GLYPH<28> 2 i v 2 GLYPH<1> where GLYPH<28> i ∼ C C . 0 ; b 0 / and v ∼ C C GLYPH<0> 0 ; bg GLYPH<1> . Herein, w . l / i ; j is the weight connecting the i th node of layer ( l -1) and j th node of\nFIGURE 7. (a) Predicted standard deviation and (b) mean of V TH distribution for FinFET structure of Lg D 20 nm, H fin D 42 nm, W fin D 7 nm with the LER profile of GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1. Note that each parameter (i.e., GLYPH<27> , GLYPH<24> X , GLYPH<24> Y , GLYPH<11> , Lg, H fin , and W fin ) is varied in relevant ranges.\nlayer . l / , and C C . 0 ; b / is a half-Cauchy distribution, and b 0 and bg are shrinkage parameters. The horseshoe prior can introduce shrinkage and sparsity over the weights and bring model selection that automatically GLYPH<28>nds the most compact neural network structure (i.e., GLYPH<28>nding the best number of nodes). This is due to the two features of the horseshoe distribution: (1) a tall spike at zero and (2) heavy (relatively GLYPH<29>at) tails (see Figure 2). The tall spike at zero promotes shrinkage of the weights, which seems unnecessary in making predictions. However, its heavy tail allows large weights to avoid shrinkage.\nThe complete architecture of the HS-BNN is illustrated in Figure 3: Three hidden layers with a RELU activation function, horseshoe priors for the weights into the hidden layers, and a Gaussian prior for the weights into the output layer. The Gaussian prior was used to prevent overGLYPH<28>tting [16].",
        "original_text": "Predictive models are just a function in the form of y D f ( x ) that GLYPH<28>ts the given data D D GLYPH<8>GLYPH<0> xn ; y n GLYPH<1>GLYPH<9> N 1 , and they are generalized/extended to the cases that have not been seen in training. For example, the function form of a linear regression model is y D P wixi C b . However, this model has its own limit for determining the nonlinear relationship between x and y . Conversely, an ANN with an L hidden layer is in the form of y D W . L C 1 / h . L / C b . L C 1 / , where W l is a matrix of weights between layer ( l GLYPH<0> 1 ) and layer ( l ) in the size of R . Kl GLYPH<0> 1 C 1 / × Kl , and h . l / is a matrix of\nFIGURE 3. Architecture of the HS-BNN, predicting the mean and standard deviation of log(I DS ) in various FinFET structures with arbitrary LER profiles.\nFIGURE 4. Diagram of K-fold cross-validation.\nFIGURE 5. MAPE, RMSE, MAE, and PLL values of each model along different numbers of nodes, showing model selection over the number of HS-BNN nodes.\noutputs from layer ( l ) . Herein, Kl is the number of units in layer l , h . l / D a GLYPH<0> W . l / h . l GLYPH<0> 1 / C b GLYPH<1> ; h . 0 / D x ; and a ( · ) is an activation function for each hidden layer. In the case of the ANN with three hidden layers, the function is\nFIGURE 6. (a) Predicted standard deviation and (b) predicted mean of log(I OFF ) for FinFET structure of Lg D 20 nm, H fin D 42 nm, W fin D 7 nm with the LER profile of GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1. Note that each parameter (i.e., GLYPH<27> , GLYPH<24> X , GLYPH<24> Y , GLYPH<11> , Lg, H fin , and W fin ) is varied in relevant ranges.\ny D W 4 a GLYPH<0> W 3 a GLYPH<0> W 2 a GLYPH<0> W 1 x C b 1 GLYPH<1> C b 2 GLYPH<1> C b 3 GLYPH<1> C b 4 . The ANN model is a composite function of multiple activation functions (i.e., activation integral) [13], which can make it complex nonlinear regression predictions.\nA Bayesian neural network is an extended standard ANN with Bayesian/posterior inference [14]. While the ANN is a deterministic model, the BNN is a stochastic/probabilistic model. The BNN incorporates uncertainty in modeling tasks by introducing distributions over the weights W GLYPH<24> p ( W ) (i.e., W D GLYPH<8> W l GLYPH<9> L C 1 1 , a set of weight matrices over all the layers) [15]. By doing so, BNNs can demonstrate/reconGLYPH<28>gure epistemic uncertainty when the amount of data is limited. However, at the same time, there are inherent neural network problems like over\n(under)-parameterization. The larger(smaller) the number of data points is, the larger(smaller) the number of required nodes is. When the number of nodes is much larger(smaller) than that estimated from the data, the variance around the BNN's predictions would be too large(small). The layer sizes should be as large as necessary. Especially, in the case of estimating the LER-induced variation, data were bound to be small, so that over-parameterization had to be prevented. In this regard, this study assigns horseshoe priors over weights using Bayesian inference. The horseshoe prior is given by: GLYPH<16> w . l / i ; j j GLYPH<28> i ; v GLYPH<17> ∼ N GLYPH<0> 0 ;GLYPH<28> 2 i v 2 GLYPH<1> where GLYPH<28> i ∼ C C . 0 ; b 0 / and v ∼ C C GLYPH<0> 0 ; bg GLYPH<1> . Herein, w . l / i ; j is the weight connecting the i th node of layer ( l -1) and j th node of\nFIGURE 7. (a) Predicted standard deviation and (b) mean of V TH distribution for FinFET structure of Lg D 20 nm, H fin D 42 nm, W fin D 7 nm with the LER profile of GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1. Note that each parameter (i.e., GLYPH<27> , GLYPH<24> X , GLYPH<24> Y , GLYPH<11> , Lg, H fin , and W fin ) is varied in relevant ranges.\nlayer . l / , and C C . 0 ; b / is a half-Cauchy distribution, and b 0 and bg are shrinkage parameters. The horseshoe prior can introduce shrinkage and sparsity over the weights and bring model selection that automatically GLYPH<28>nds the most compact neural network structure (i.e., GLYPH<28>nding the best number of nodes). This is due to the two features of the horseshoe distribution: (1) a tall spike at zero and (2) heavy (relatively GLYPH<29>at) tails (see Figure 2). The tall spike at zero promotes shrinkage of the weights, which seems unnecessary in making predictions. However, its heavy tail allows large weights to avoid shrinkage.\nThe complete architecture of the HS-BNN is illustrated in Figure 3: Three hidden layers with a RELU activation function, horseshoe priors for the weights into the hidden layers, and a Gaussian prior for the weights into the output layer. The Gaussian prior was used to prevent overGLYPH<28>tting [16].",
        "context": "Introduces the central thesis about a machine-learning-based predictive model for simulating electrical characteristics of FinFETs with process-induced line-edge roughness.",
        "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
        "pages": [
            3,
            4,
            5
        ],
        "id": "aad5d978d36fc9f91ba5ca5bf315bf6e3d4b31aa73ac15df9c8fb7c64dd98ca0"
    },
    {
        "text": "Evaluates the HS-BNN predictions using K-fold crossvalidation, calculating MAPEs, RMSEs, MAEs, and PLLs to determine overall prediction performance compared to the Bayesian linear regression (BLR) model.\n\nThe HS-BNN predictions were evaluated using K-fold crossvalidation [splitting data in a K number of sections, and then, iteratively using one of the sections as a test set and the others as a training set (see Figure 4)]. This method is expected to be a proper choice when data are limited, by ensuring all data to be used as a test set at least once, which makes less biased evaluation and well estimates how a model will perform in general cases [17], [18]. Mean absolute percentage errors (MAPEs), root-mean-squared errors (RMSEs), mean absolute errors (MAEs) (herein, note that the lower they are,\nTABLE 2. MAPE and RMSE values of each model.\nTABLE 3. MAPE, RMSE, MAE, and PLL values of each model.\nthe better they are), and predictive log likelihoods (PLLs) (herein, note that the higher it is, the better it is) are calculated in each test set. Afterwards, the averages of those are the representative evaluation process values.\nThe overall prediction performance was improved, as compared against the previous work [1] which is the Bayesian linear regression (BLR) model (see Table 2 ). Note that the HS-BNN model for this comparison is only for the FinFET with Lg D 14 nm, WGLYPH<28>n D 7 nm, HGLYPH<28>n D 30 nm because the BLR model made predictions only for the corresponding FinFET structure. Table 2 summarizes the MAPE and RMSEof the two target variables (i.e., GLYPH<22> IDS and GLYPH<27> IDS) of the HS-BNN and BLR. The predictions of both GLYPH<22> IDS and GLYPH<27> IDS were much improved over those in the previous work. The prediction for GLYPH<22> IDS was improved (i.e., 0.55% vs. 0.81%), but there was signiGLYPH<28>cant prediction improvement for GLYPH<27> IDS (i.e., 6.66% vs. 19.59%).\nTable 3 and Figure 5 show the results of HS-BNN model for various FinFET structures. To verify the beneGLYPH<28>t of the HS-BNN model, we compared it with the Gaussian-BNN model. Table 3 summarizes the predictive performance, showing that the HS-BNN showed much better results. Figure 5 demonstrates the performance of the model selection with respect to the layer sizes. For the number of nodes over 200, the HS-BNN's prediction for GLYPH<27> IDS showed almost the same results (MAPEs GLYPH<24> 7%), while the Gaussian-BNN showed totally different results for different numbers of nodes. It is noteworthy that it is a log-scale for the MAPE, RMSE, and MAE in Figure 5. This indicates that, as mentioned in [19], even if the number of nodes was excessively overestimated, the horseshoe prior made it possible to GLYPH<28>nd the most compact layer sizes. However, it is meaningful, not to merely make a simple comparison of predictive performance, but to focus on the fact that the HS-BNN achieved the model selection while making a GLYPH<28>ne predictive performance. This is because the number of nodes for every hidden layer is speciGLYPH<28>ed as the same. Manual layer size optimization was not performed, and the optimization was performed automatically\nin the HS-BNN. Thus, it is an open question whether the HS-BNNis better than all the Gaussian-BNNs. However, it is certain that the HS-BNN has saved a signiGLYPH<28>cant optimization time.\nFigures 6 and 7 show the predictions of the mean and standard deviation of log off-state current [log(IOFF)] and threshold voltage (VTH) of the FinFET with Lg D 20 nm, WGLYPH<28>n D 7 nm, HGLYPH<28>n D 42 nm, and an LER proGLYPH<28>le [i.e., GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1]. By GLYPH<28>xing all the parameters other than a single parameter, GLYPH<27> log(IOFF), GLYPH<22> log(IOFF), GLYPH<27> VTH, and GLYPH<22> VTH were predicted.\nThe device performance trends shown in Figures 6 and 7 seem to be well matched to the general trends, when considering several studies shown in [20]GLYPH<21>[25]. Using this HS-BNN model, we can show how both the standard deviation and mean of the electrical characteristics vary with arbitrary LER proGLYPH<28>les in various FinFET structures.",
        "original_text": "The HS-BNN predictions were evaluated using K-fold crossvalidation [splitting data in a K number of sections, and then, iteratively using one of the sections as a test set and the others as a training set (see Figure 4)]. This method is expected to be a proper choice when data are limited, by ensuring all data to be used as a test set at least once, which makes less biased evaluation and well estimates how a model will perform in general cases [17], [18]. Mean absolute percentage errors (MAPEs), root-mean-squared errors (RMSEs), mean absolute errors (MAEs) (herein, note that the lower they are,\nTABLE 2. MAPE and RMSE values of each model.\nTABLE 3. MAPE, RMSE, MAE, and PLL values of each model.\nthe better they are), and predictive log likelihoods (PLLs) (herein, note that the higher it is, the better it is) are calculated in each test set. Afterwards, the averages of those are the representative evaluation process values.\nThe overall prediction performance was improved, as compared against the previous work [1] which is the Bayesian linear regression (BLR) model (see Table 2 ). Note that the HS-BNN model for this comparison is only for the FinFET with Lg D 14 nm, WGLYPH<28>n D 7 nm, HGLYPH<28>n D 30 nm because the BLR model made predictions only for the corresponding FinFET structure. Table 2 summarizes the MAPE and RMSEof the two target variables (i.e., GLYPH<22> IDS and GLYPH<27> IDS) of the HS-BNN and BLR. The predictions of both GLYPH<22> IDS and GLYPH<27> IDS were much improved over those in the previous work. The prediction for GLYPH<22> IDS was improved (i.e., 0.55% vs. 0.81%), but there was signiGLYPH<28>cant prediction improvement for GLYPH<27> IDS (i.e., 6.66% vs. 19.59%).\nTable 3 and Figure 5 show the results of HS-BNN model for various FinFET structures. To verify the beneGLYPH<28>t of the HS-BNN model, we compared it with the Gaussian-BNN model. Table 3 summarizes the predictive performance, showing that the HS-BNN showed much better results. Figure 5 demonstrates the performance of the model selection with respect to the layer sizes. For the number of nodes over 200, the HS-BNN's prediction for GLYPH<27> IDS showed almost the same results (MAPEs GLYPH<24> 7%), while the Gaussian-BNN showed totally different results for different numbers of nodes. It is noteworthy that it is a log-scale for the MAPE, RMSE, and MAE in Figure 5. This indicates that, as mentioned in [19], even if the number of nodes was excessively overestimated, the horseshoe prior made it possible to GLYPH<28>nd the most compact layer sizes. However, it is meaningful, not to merely make a simple comparison of predictive performance, but to focus on the fact that the HS-BNN achieved the model selection while making a GLYPH<28>ne predictive performance. This is because the number of nodes for every hidden layer is speciGLYPH<28>ed as the same. Manual layer size optimization was not performed, and the optimization was performed automatically\nin the HS-BNN. Thus, it is an open question whether the HS-BNNis better than all the Gaussian-BNNs. However, it is certain that the HS-BNN has saved a signiGLYPH<28>cant optimization time.\nFigures 6 and 7 show the predictions of the mean and standard deviation of log off-state current [log(IOFF)] and threshold voltage (VTH) of the FinFET with Lg D 20 nm, WGLYPH<28>n D 7 nm, HGLYPH<28>n D 42 nm, and an LER proGLYPH<28>le [i.e., GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1]. By GLYPH<28>xing all the parameters other than a single parameter, GLYPH<27> log(IOFF), GLYPH<22> log(IOFF), GLYPH<27> VTH, and GLYPH<22> VTH were predicted.\nThe device performance trends shown in Figures 6 and 7 seem to be well matched to the general trends, when considering several studies shown in [20]GLYPH<21>[25]. Using this HS-BNN model, we can show how both the standard deviation and mean of the electrical characteristics vary with arbitrary LER proGLYPH<28>les in various FinFET structures.",
        "context": "Evaluates the HS-BNN predictions using K-fold crossvalidation, calculating MAPEs, RMSEs, MAEs, and PLLs to determine overall prediction performance compared to the Bayesian linear regression (BLR) model.",
        "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
        "pages": [
            5,
            6
        ],
        "id": "907ee6bd1249aa8cfc54c14f66a6290d0804ea2687ca417b39b85f6825d3e492"
    },
    {
        "text": "Quantitatively estimates LER-induced random variation in FinFET characteristics within seconds, significantly faster than TCAD simulations. Improves prediction performance compared to Bayesian linear regression and verifies optimal layer size selection for limited data.\n\nA Bayesian neural network model with horseshoe priors (HS-BNN) has been proposed for the design of LERimmune FinFETs. This model can quantitatively estimate the LER-induced random variation of IDS-VGS characteristics in various FinFET structures within a few seconds, which is much shorter than the conventional TCAD simulation running time. With the capability of neural networks to deGLYPH<28>ne highly nonlinear functions, the prediction has been improved [vs. the Bayesian linear regression model [1] (i.e., GLYPH<24> 6 % vs. GLYPH<24> 19 %)]. Moreover, the model selection performance over the most compact layer size was veriGLYPH<28>ed, which is essential when the amount of data is very limited.",
        "original_text": "A Bayesian neural network model with horseshoe priors (HS-BNN) has been proposed for the design of LERimmune FinFETs. This model can quantitatively estimate the LER-induced random variation of IDS-VGS characteristics in various FinFET structures within a few seconds, which is much shorter than the conventional TCAD simulation running time. With the capability of neural networks to deGLYPH<28>ne highly nonlinear functions, the prediction has been improved [vs. the Bayesian linear regression model [1] (i.e., GLYPH<24> 6 % vs. GLYPH<24> 19 %)]. Moreover, the model selection performance over the most compact layer size was veriGLYPH<28>ed, which is essential when the amount of data is very limited.",
        "context": "Quantitatively estimates LER-induced random variation in FinFET characteristics within seconds, significantly faster than TCAD simulations. Improves prediction performance compared to Bayesian linear regression and verifies optimal layer size selection for limited data.",
        "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
        "pages": [
            6
        ],
        "id": "a46e0e09053ca021f82200a107eaef751089702af5e42cd98622737900981c6e"
    },
    {
        "text": "Provides supporting evidence for the argument on process-induced random variation.\n\n- [1] S. Yu and C. Shin, ''Quantitative evaluation of process-induced lineedge roughness in FinFET: Bayesian regression model,'' Semicond. Sci. Technol. , vol. 32, no. 2, 2021, Art. no. 025020.\n- [2] L. Valentin Jospin, W. Buntine, F. Boussaid, H. Laga, and M. Bennamoun, ''Hands-on Bayesian neural networksGLYPH<21>A tutorial for deep learning users,'' 2020, arXiv:2007.06823 .\n- [3] R. Courtland, ''3-D transistors with different in heights make better memory cells,'' IEEE Spectr. , Jun. 2013. [Online]. Available: \n- [4] I. Aller and B. Rainey, ''Multi-height FinFETs,'' U.S. Patent 6 909 147 B2, Jun. 21, 2005.\n- [5] C. M. Carvalho, N. G. Polson, and J. G. Scott, ''Handling sparsity via the horseshoe,'' presented at the Artif. Intell. Statist., 2009. [Online]. Available: ings.mlr.press/v5/carvalho09a\n- [6] J. Miguel HernÆndez-Lobato and R. P. Adams, ''Probabilistic backpropagation for scalable learning of Bayesian neural networks,'' 2015, arXiv:1502.05336 .\n- [7] M. Badaroglu. (2018). International Roadmap for Device and Systems More Moore . [Online]. Available: \n- [8] A. Allan. (2016). International Roadmap for Device and Systems More Moore . [Online]. Available: \n- [9] WikiChip. Technology Node . Accessed: Mar. 15, 2021. [Online]. Available: \n- [10] Wikipedia. Semiconductor Device Fabrication . Accessed: Mar. 15, 2021. [Online]. Available: device_fabrication\n- [11] S. Oh and C. Shin, ''3-D quasi-atomistic model for line edge roughness in nonplanar MOSFETs,'' IEEE Trans. Electron Devices , vol. 63, no. 12, pp. 4617GLYPH<21>4623, Dec. 2016.\n- [12] C.-Y. Chen, W.-T. Huang, and Y. Li, ''Electrical characteristic and power consumption GLYPH<29>uctuations of trapezoidal bulk FinFET devices and circuits induced by random line edge roughness,'' in Proc. 16th Int. Symp. Quality Electron. Design , Mar. 2015, pp. 61GLYPH<21>64, doi: 10.1109/ISQED.2015.7085399.\n- [13] Z. Ma, ''The function representation of artiGLYPH<28>cial neural network,'' 2019, arXiv:1908.10493 .\n- [14] J. Gordon and J. Miguel HernÆndez-Lobato, ''Bayesian semisupervised learning with deep generative models,'' 2017, arXiv:1706.09751 .\n- [15] H. Overweg, A.-L. Popkes, A. Ercole, Y. Li, J. Miguel HernÆndez-Lobato, Y. Zaykov, and C. Zhang, ''Interpretable outcome prediction with sparse Bayesian neural networks in intensive care,'' 2019, arXiv:1905.02599 .\n- [16] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, ''Weight uncertainty in neural networks,'' 2015, arXiv:1505.05424 .\n- [17] S. Shinmura, ''Comparison of linear discriminant function by K-fold cross validation,'' Data Anal. , vol. 2014, pp. 1GLYPH<21>6, Sep. 2014.\n- [18] J. Brownlee. (2018). A Gentle Introduction to K-Fold CrossValidation . Machine Learning Mastery. [Online]. Available: machinelearningmastery.com/k-fold-cross-validation/\n- [19] S. Ghosh, J. Yao, and F. Doshi-Velez, ''Model selection in Bayesian neural networks via horseshoe priors,'' J. Mach. Learn. Res. , vol. 20, no. 182, pp. 1GLYPH<21>46, 2019.\n- [20] A. Asenov, S. Kaya, and A. R. Brown, ''Intrinsic parameter GLYPH<29>uctuations in decananometer MOSFETs introduced by gate line edge roughness,'' IEEE Trans. Electron Devices , vol. 50, no. 5, pp. 1254GLYPH<21>1260, May 2003.\n- [21] M. Wong, K. D. Holland, S. Anderson, and S. Rizw, ''Impact of shortwavelength and long-wavelength line-edge roughness on the variability of ultrascaled FinFETs,'' IEEE Trans. Electron Devices , vol. 64, no. 3, pp. 1231GLYPH<21>1238, Mar. 2017.\n- [22] Y. Ma, H. J. Levinson, and T. Wallow, ''Line edge roughness impact on critical dimension variation,'' Proc. SPIE , vol. 6518, Apr. 2007, Art. no. 651824.\n- [23] K. Patel, T. Wallow, H. J. Levinson, and C. J. Spanos, ''Comparative study of line width roughness (LWR) in next-generation lithography (NGL) processes,'' Proc. SPIE , vol. 7640, Mar. 2010, Art. no. 76400T.\n- [24] Y. Ban, ''Electrical impact of line-edge roughness on sub-45 nm node standard cell,'' J. Micro/Nanolithography, MEMS, MOEMS , vol. 9, no. 4, 2009, Art. no. 041206.\n- [25] E. Liu, K. Lutker-Lee, Q. Lou, Y.-M. Chen, A. Raley, and P. Biolsi, ''Line edge roughness (LER) reduction strategies for EUV self-aligned double patterning (SADP),'' Proc. SPIE , vol. 11615, Apr. 2021, Art. no. 1161506.\nSANGHO YU received the M.S. degree in electronic and electrical engineering from Sungkyunkwan University (SKKU), Suwon, Republic of Korea, in 2021. He is currently working as a Researcher with SKKU. His current research interests include process-induced random variation and machine learning model.\nSANG MIN WON received the B.S., M.S., and Ph.D. degrees in electrical and computer engineering from the University of Illinois at UrbanaGLYPH<21> Champaign. He is currently an Assistant Professor with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include sensors/stimulators with unique applications in advanced biomedical and/or health monitoring systems.\nHYOUNG WON BAAC received the B.S. degree (Hons.) in electronic engineering from Sungkyunkwan University, Suwon, Republic of Korea, in 1999, and the Ph.D. degree in electrical engineering and computer sciences from the University of Michigan, Ann Arbor, MI, USA, in 2011. He is currently an Associate Professor with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include opti- cal/acoustic/electrical sensors and systems for biomedical therapy, healthcare, and non-destructive evaluation.\nDONGHEE SON received the Ph.D. degree in chemical and biological engineering from Seoul National University, in 2015. He is currently an Assistant Professor with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include soft GLYPH<29>exible electronic devices and systems.",
        "original_text": "- [1] S. Yu and C. Shin, ''Quantitative evaluation of process-induced lineedge roughness in FinFET: Bayesian regression model,'' Semicond. Sci. Technol. , vol. 32, no. 2, 2021, Art. no. 025020.\n- [2] L. Valentin Jospin, W. Buntine, F. Boussaid, H. Laga, and M. Bennamoun, ''Hands-on Bayesian neural networksGLYPH<21>A tutorial for deep learning users,'' 2020, arXiv:2007.06823 .\n- [3] R. Courtland, ''3-D transistors with different in heights make better memory cells,'' IEEE Spectr. , Jun. 2013. [Online]. Available: \n- [4] I. Aller and B. Rainey, ''Multi-height FinFETs,'' U.S. Patent 6 909 147 B2, Jun. 21, 2005.\n- [5] C. M. Carvalho, N. G. Polson, and J. G. Scott, ''Handling sparsity via the horseshoe,'' presented at the Artif. Intell. Statist., 2009. [Online]. Available: ings.mlr.press/v5/carvalho09a\n- [6] J. Miguel HernÆndez-Lobato and R. P. Adams, ''Probabilistic backpropagation for scalable learning of Bayesian neural networks,'' 2015, arXiv:1502.05336 .\n- [7] M. Badaroglu. (2018). International Roadmap for Device and Systems More Moore . [Online]. Available: \n- [8] A. Allan. (2016). International Roadmap for Device and Systems More Moore . [Online]. Available: \n- [9] WikiChip. Technology Node . Accessed: Mar. 15, 2021. [Online]. Available: \n- [10] Wikipedia. Semiconductor Device Fabrication . Accessed: Mar. 15, 2021. [Online]. Available: device_fabrication\n- [11] S. Oh and C. Shin, ''3-D quasi-atomistic model for line edge roughness in nonplanar MOSFETs,'' IEEE Trans. Electron Devices , vol. 63, no. 12, pp. 4617GLYPH<21>4623, Dec. 2016.\n- [12] C.-Y. Chen, W.-T. Huang, and Y. Li, ''Electrical characteristic and power consumption GLYPH<29>uctuations of trapezoidal bulk FinFET devices and circuits induced by random line edge roughness,'' in Proc. 16th Int. Symp. Quality Electron. Design , Mar. 2015, pp. 61GLYPH<21>64, doi: 10.1109/ISQED.2015.7085399.\n- [13] Z. Ma, ''The function representation of artiGLYPH<28>cial neural network,'' 2019, arXiv:1908.10493 .\n- [14] J. Gordon and J. Miguel HernÆndez-Lobato, ''Bayesian semisupervised learning with deep generative models,'' 2017, arXiv:1706.09751 .\n- [15] H. Overweg, A.-L. Popkes, A. Ercole, Y. Li, J. Miguel HernÆndez-Lobato, Y. Zaykov, and C. Zhang, ''Interpretable outcome prediction with sparse Bayesian neural networks in intensive care,'' 2019, arXiv:1905.02599 .\n- [16] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, ''Weight uncertainty in neural networks,'' 2015, arXiv:1505.05424 .\n- [17] S. Shinmura, ''Comparison of linear discriminant function by K-fold cross validation,'' Data Anal. , vol. 2014, pp. 1GLYPH<21>6, Sep. 2014.\n- [18] J. Brownlee. (2018). A Gentle Introduction to K-Fold CrossValidation . Machine Learning Mastery. [Online]. Available: machinelearningmastery.com/k-fold-cross-validation/\n- [19] S. Ghosh, J. Yao, and F. Doshi-Velez, ''Model selection in Bayesian neural networks via horseshoe priors,'' J. Mach. Learn. Res. , vol. 20, no. 182, pp. 1GLYPH<21>46, 2019.\n- [20] A. Asenov, S. Kaya, and A. R. Brown, ''Intrinsic parameter GLYPH<29>uctuations in decananometer MOSFETs introduced by gate line edge roughness,'' IEEE Trans. Electron Devices , vol. 50, no. 5, pp. 1254GLYPH<21>1260, May 2003.\n- [21] M. Wong, K. D. Holland, S. Anderson, and S. Rizw, ''Impact of shortwavelength and long-wavelength line-edge roughness on the variability of ultrascaled FinFETs,'' IEEE Trans. Electron Devices , vol. 64, no. 3, pp. 1231GLYPH<21>1238, Mar. 2017.\n- [22] Y. Ma, H. J. Levinson, and T. Wallow, ''Line edge roughness impact on critical dimension variation,'' Proc. SPIE , vol. 6518, Apr. 2007, Art. no. 651824.\n- [23] K. Patel, T. Wallow, H. J. Levinson, and C. J. Spanos, ''Comparative study of line width roughness (LWR) in next-generation lithography (NGL) processes,'' Proc. SPIE , vol. 7640, Mar. 2010, Art. no. 76400T.\n- [24] Y. Ban, ''Electrical impact of line-edge roughness on sub-45 nm node standard cell,'' J. Micro/Nanolithography, MEMS, MOEMS , vol. 9, no. 4, 2009, Art. no. 041206.\n- [25] E. Liu, K. Lutker-Lee, Q. Lou, Y.-M. Chen, A. Raley, and P. Biolsi, ''Line edge roughness (LER) reduction strategies for EUV self-aligned double patterning (SADP),'' Proc. SPIE , vol. 11615, Apr. 2021, Art. no. 1161506.\nSANGHO YU received the M.S. degree in electronic and electrical engineering from Sungkyunkwan University (SKKU), Suwon, Republic of Korea, in 2021. He is currently working as a Researcher with SKKU. His current research interests include process-induced random variation and machine learning model.\nSANG MIN WON received the B.S., M.S., and Ph.D. degrees in electrical and computer engineering from the University of Illinois at UrbanaGLYPH<21> Champaign. He is currently an Assistant Professor with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include sensors/stimulators with unique applications in advanced biomedical and/or health monitoring systems.\nHYOUNG WON BAAC received the B.S. degree (Hons.) in electronic engineering from Sungkyunkwan University, Suwon, Republic of Korea, in 1999, and the Ph.D. degree in electrical engineering and computer sciences from the University of Michigan, Ann Arbor, MI, USA, in 2011. He is currently an Associate Professor with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include opti- cal/acoustic/electrical sensors and systems for biomedical therapy, healthcare, and non-destructive evaluation.\nDONGHEE SON received the Ph.D. degree in chemical and biological engineering from Seoul National University, in 2015. He is currently an Assistant Professor with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include soft GLYPH<29>exible electronic devices and systems.",
        "context": "Provides supporting evidence for the argument on process-induced random variation.",
        "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
        "pages": [
            6,
            7
        ],
        "id": "d61b1e84bbd10628d8e07a14b2a5ed79e36bb9d3f10d736ee162b5c23697f66b"
    },
    {
        "text": "Provides background information on the author's academic credentials and research interests.\n\nCHANGHWAN SHIN (Senior Member, IEEE) received the B.S. degree (Hons.) in electrical engineering from Korea University, Seoul, South Korea, in 2006, and the Ph.D. degree in electrical engineering and computer sciences from the University of California at Berkeley, Berkeley, CA, USA, in 2011. Since 2017, he has been the Board of Directors with SK Hynix. He is currently Professor with the School of Electrical Engineering, Korea University. His current research interests include advanced CMOS device designs and their applications to memory/logic devices.",
        "original_text": "CHANGHWAN SHIN (Senior Member, IEEE) received the B.S. degree (Hons.) in electrical engineering from Korea University, Seoul, South Korea, in 2006, and the Ph.D. degree in electrical engineering and computer sciences from the University of California at Berkeley, Berkeley, CA, USA, in 2011. Since 2017, he has been the Board of Directors with SK Hynix. He is currently Professor with the School of Electrical Engineering, Korea University. His current research interests include advanced CMOS device designs and their applications to memory/logic devices.",
        "context": "Provides background information on the author's academic credentials and research interests.",
        "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
        "pages": [
            7
        ],
        "id": "bb12d010210110042d07877954fddc21c85dcbc9145af6a0edc2c6a844c89b57"
    }
]