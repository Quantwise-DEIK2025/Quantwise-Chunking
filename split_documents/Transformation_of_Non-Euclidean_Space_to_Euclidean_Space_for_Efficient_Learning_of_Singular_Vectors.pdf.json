[
    {
        "text": "The chunk introduces the paper's focus on utilizing singular value decomposition (SVD) in convolutional neural networks, addressing challenges like sign ambiguity and manifold features through a transformation to Euclidean space and a novel pooling method called singular vector pooling (SVP).\n\nReceived June 20, 2020, accepted July 6, 2020, date of publication July 9, 2020, date of current version July 22, 2020.\nDigital Object Identifier 10.1 109/ACCESS.2020.3008195",
        "original_text": "Received June 20, 2020, accepted July 6, 2020, date of publication July 9, 2020, date of current version July 22, 2020.\nDigital Object Identifier 10.1 109/ACCESS.2020.3008195",
        "context": "The chunk introduces the paper's focus on utilizing singular value decomposition (SVD) in convolutional neural networks, addressing challenges like sign ambiguity and manifold features through a transformation to Euclidean space and a novel pooling method called singular vector pooling (SVP).",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            1
        ],
        "id": "8a95b9951edb4fadfa691dd448ba0164867ec693e842d3e52c37ef242dcd62d8"
    },
    {
        "text": "This section details the funding sources and institutional affiliations for the research, and introduces the core problem: the difficulty of using singular vectors in CNNs due to their inherent properties, leading to a proposed solution involving transforming them into Euclidean space via singular vector pooling (SVP).\n\nDepartment of Electronic Engineering, Inha University, Incheon 22212, South Korea\nCorresponding author: Byung Cheol Song (bcsong@inha.ac.kr)\nThis work was supported in part by the Industrial Technology Innovation Program funded by the Ministry of Trade, Industry & Energy (MI, South Korea) (Development on Deep Learning based 4K30P Edge Computing enabled IP Camera System) under Grant 20006483, in part by the Institute of Information & Communications Technology Planning & Evaluation (IITP) funded by the Korea Government (MSIT) (ArtiGLYPH<28>cial Intelligence Convergence Research Center, Inha University) under Grant 2020-0-01389, and in part by the Industrial Technology Innovation Program through the Ministry of Trade, Industry, and Energy (MI, South Korea) (Development of Human-Friendly Human-Robot Interaction Technologies Using Human Internal Emotional States) under Grant 10073154.\n- ABSTRACT Singular value decomposition (SVD) is a popular technique to extract essential information by reducing the dimension of a feature set. SVD is able to analyze a vast matrix in spite of a relatively low computational cost. However, singular vectors produced by SVD have been seldom used in convolutional neural networks (CNNs). This is because the inherent properties of singular vectors such as sign ambiguity and manifold features make CNNs difGLYPH<28>cult to learn singular vectors. In order to overcome the limitations, this paper analyzes the undesirable properties of singular vectors and presents the transformation of singular vectors into Euclidean space as a smart solution. If the singular vectors are transformed to follow Euclidean geometry, SVD can be used for pooling to maintain the feature information well, which is called singular vector pooling (SVP). Since SVP can extract essential information from a feature map, it is robust against adversarial attacks in comparison to global average pooling. Thus, SVP shows a quantitative performance improvement of about 36% for the CIFAR10 dataset. In addition, we applied SVP to a knowledge distillation scheme that uses singular vectors in a restricted manner. As a result, SVP improved the performance by up to 1.7% for the CIFAR100 dataset.\nINDEX TERMS Neural networks, pattern analysis, principal component analysis.",
        "original_text": "Department of Electronic Engineering, Inha University, Incheon 22212, South Korea\nCorresponding author: Byung Cheol Song (bcsong@inha.ac.kr)\nThis work was supported in part by the Industrial Technology Innovation Program funded by the Ministry of Trade, Industry & Energy (MI, South Korea) (Development on Deep Learning based 4K30P Edge Computing enabled IP Camera System) under Grant 20006483, in part by the Institute of Information & Communications Technology Planning & Evaluation (IITP) funded by the Korea Government (MSIT) (ArtiGLYPH<28>cial Intelligence Convergence Research Center, Inha University) under Grant 2020-0-01389, and in part by the Industrial Technology Innovation Program through the Ministry of Trade, Industry, and Energy (MI, South Korea) (Development of Human-Friendly Human-Robot Interaction Technologies Using Human Internal Emotional States) under Grant 10073154.\n- ABSTRACT Singular value decomposition (SVD) is a popular technique to extract essential information by reducing the dimension of a feature set. SVD is able to analyze a vast matrix in spite of a relatively low computational cost. However, singular vectors produced by SVD have been seldom used in convolutional neural networks (CNNs). This is because the inherent properties of singular vectors such as sign ambiguity and manifold features make CNNs difGLYPH<28>cult to learn singular vectors. In order to overcome the limitations, this paper analyzes the undesirable properties of singular vectors and presents the transformation of singular vectors into Euclidean space as a smart solution. If the singular vectors are transformed to follow Euclidean geometry, SVD can be used for pooling to maintain the feature information well, which is called singular vector pooling (SVP). Since SVP can extract essential information from a feature map, it is robust against adversarial attacks in comparison to global average pooling. Thus, SVP shows a quantitative performance improvement of about 36% for the CIFAR10 dataset. In addition, we applied SVP to a knowledge distillation scheme that uses singular vectors in a restricted manner. As a result, SVP improved the performance by up to 1.7% for the CIFAR100 dataset.\nINDEX TERMS Neural networks, pattern analysis, principal component analysis.",
        "context": "This section details the funding sources and institutional affiliations for the research, and introduces the core problem: the difficulty of using singular vectors in CNNs due to their inherent properties, leading to a proposed solution involving transforming them into Euclidean space via singular vector pooling (SVP).",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            1
        ],
        "id": "e48cea70177b6e462537498a97a4f63745db576d881284fe22cb5a1f7aeb5815"
    },
    {
        "text": "Analyzes the problematic properties of singular vectors and proposes solutions for direct, learnable singular vector utilization in deep learning.\n\nPrincipal component analysis (PCA) is a technique used for the identiGLYPH<28>cation of a smaller number of uncorrelated variables known as principal components from a larger set of data. Several principal components have most information of a given data, so the dimension of the data can be signiGLYPH<28>cantly reduced or compressed by choosing only some of the principal components. Therefore, PCA is a beneGLYPH<28>cial technique for analyzing high-dimensional data. Among PCA techniques, singular value decomposition (SVD) is most popular because it can produce global solutions to various problems in computer vision, pattern recognition, and machine learning [1]GLYPH<21>[3]. As the derivative function of SVD was developed [4], SVD has been applied even to convolutional neural networks (CNNs) that can deal with very high-dimensional data. For instance, SVD was employed to obtain a unique square root of a symmetric positive deGLYPH<28>nite (SPD) matrix for subspace analysis [5]GLYPH<21>[7] or higher-order pooling [4], [8]GLYPH<21>[10].\nThe associate editor coordinating the review of this manuscript and approving it for publication was Claudio Cusano .\nPrevious works show that SVD can effectively extract essential information from feature maps. However, since singular vectors have undesirable properties such as sign ambiguity [11] and manifold features, they have been seldom studied to be directly learned in spite that they have feature map's essential information. In order to avoid undesirable properties, conventional approaches adopted an SPD matrix consisting of singular vectors. However, since they should expand the compressed feature's dimension again, they require additional computational and memory cost.\nLee et al. proposed a knowledge distillation method using SVD (KD-SVD) the harmful properties of singular vectors and deGLYPH<28>ned singular vectors as knowledge [12]. But this algorithm has a limitation that it needs guidance from a teacher network.\nIn this paper, we deGLYPH<28>ne a singular vector as a feature vector and make its information directly learnable without re-composition and guidance. To do this, we analyze two problematic properties of singular vectors and proposes solutions to effectively remove the unfavorable properties based on the analysis. The sign ambiguity is eliminated so that singular vectors with similar information are gathered, and then singular vectors are converted from manifold to Euclidean space. Here, Euclidean geometry that is useful for ordinary learning schemes is employed. This procedure does not simply make singular vectors learnable but transform them to follow Euclidean geometry. Thus, SVD can be easily analyzed with layers based on Euclidean geometry.\nAlso, we propose a method called singular vector pooling (SVP), which is the only pooling method using SVD to our knowledge. SVP is basically robust to noise due to the nature of SVD. For example, in adversarial leaning with the CIFAR10 dataset, SVP provides about 36% better performance than global average pooling (GAP). As shown in the feature distributions of Fig. 1, SVP has smaller intra-class variance and larger inter-class variance than GAP, which is a great advantage of SVP. In addition, when the SVP is applied to KD-SVD, which has learned singular vectors relatively naively, the performance of a student network can be improved by about 1.7% for the CIFAR100 dataset. The contribution of this paper is summarized as follows:\n- GLYPH<15> This paper analyzes the properties of disturbing learning of singular vectors. Then, we propose skills to eliminate the unfavorable properties and present a method to transform singular vectors on non-Euclidean space to Euclidean space for effective learning.\n- GLYPH<15> We propose SVP as a novel pooling method using SVD, and prove that applying SVP to CNN's feature maps is very useful for obtaining essential information.\n- GLYPH<15> In addition, this paper shows that since singular vectors preserve feature map's information well, they are not only robust to adversarial attack, but also effective in knowledge distillation.",
        "original_text": "Principal component analysis (PCA) is a technique used for the identiGLYPH<28>cation of a smaller number of uncorrelated variables known as principal components from a larger set of data. Several principal components have most information of a given data, so the dimension of the data can be signiGLYPH<28>cantly reduced or compressed by choosing only some of the principal components. Therefore, PCA is a beneGLYPH<28>cial technique for analyzing high-dimensional data. Among PCA techniques, singular value decomposition (SVD) is most popular because it can produce global solutions to various problems in computer vision, pattern recognition, and machine learning [1]GLYPH<21>[3]. As the derivative function of SVD was developed [4], SVD has been applied even to convolutional neural networks (CNNs) that can deal with very high-dimensional data. For instance, SVD was employed to obtain a unique square root of a symmetric positive deGLYPH<28>nite (SPD) matrix for subspace analysis [5]GLYPH<21>[7] or higher-order pooling [4], [8]GLYPH<21>[10].\nThe associate editor coordinating the review of this manuscript and approving it for publication was Claudio Cusano .\nPrevious works show that SVD can effectively extract essential information from feature maps. However, since singular vectors have undesirable properties such as sign ambiguity [11] and manifold features, they have been seldom studied to be directly learned in spite that they have feature map's essential information. In order to avoid undesirable properties, conventional approaches adopted an SPD matrix consisting of singular vectors. However, since they should expand the compressed feature's dimension again, they require additional computational and memory cost.\nLee et al. proposed a knowledge distillation method using SVD (KD-SVD) the harmful properties of singular vectors and deGLYPH<28>ned singular vectors as knowledge [12]. But this algorithm has a limitation that it needs guidance from a teacher network.\nIn this paper, we deGLYPH<28>ne a singular vector as a feature vector and make its information directly learnable without re-composition and guidance. To do this, we analyze two problematic properties of singular vectors and proposes solutions to effectively remove the unfavorable properties based on the analysis. The sign ambiguity is eliminated so that singular vectors with similar information are gathered, and then singular vectors are converted from manifold to Euclidean space. Here, Euclidean geometry that is useful for ordinary learning schemes is employed. This procedure does not simply make singular vectors learnable but transform them to follow Euclidean geometry. Thus, SVD can be easily analyzed with layers based on Euclidean geometry.\nAlso, we propose a method called singular vector pooling (SVP), which is the only pooling method using SVD to our knowledge. SVP is basically robust to noise due to the nature of SVD. For example, in adversarial leaning with the CIFAR10 dataset, SVP provides about 36% better performance than global average pooling (GAP). As shown in the feature distributions of Fig. 1, SVP has smaller intra-class variance and larger inter-class variance than GAP, which is a great advantage of SVP. In addition, when the SVP is applied to KD-SVD, which has learned singular vectors relatively naively, the performance of a student network can be improved by about 1.7% for the CIFAR100 dataset. The contribution of this paper is summarized as follows:\n- GLYPH<15> This paper analyzes the properties of disturbing learning of singular vectors. Then, we propose skills to eliminate the unfavorable properties and present a method to transform singular vectors on non-Euclidean space to Euclidean space for effective learning.\n- GLYPH<15> We propose SVP as a novel pooling method using SVD, and prove that applying SVP to CNN's feature maps is very useful for obtaining essential information.\n- GLYPH<15> In addition, this paper shows that since singular vectors preserve feature map's information well, they are not only robust to adversarial attack, but also effective in knowledge distillation.",
        "context": "Analyzes the problematic properties of singular vectors and proposes solutions for direct, learnable singular vector utilization in deep learning.",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            1,
            2
        ],
        "id": "9af4ca68d2a7af19771c7857a1d2a536d09fdac354acc2005e4b4b3354785181"
    },
    {
        "text": "Contextualizes the chunk’s role in establishing the paper’s focus on utilizing singular vectors in deep learning, specifically addressing limitations of previous approaches and introducing a novel pooling method (SVP).\n\nAs one of the most popular PCA techniques, SVD is often used to reduce the dimension of a matrix or to extract essential information from a matrix or dataset. Even in the GLYPH<28>eld of machine learning, the use of SVD is expanding more and more [4], [5], [8]GLYPH<21>[10], [12]GLYPH<21>[15]. For example, SVD has been applied to a typical CNN as the derivative of SVD has been mathematically derived recently. The most common example is the computation of a unique square root of an SPD matrix. This method is used for various purposes, such as\nFIGURE 1. Comparison of feature vectors obtained by global average pooling and singular vector pooling.\ncovariance pooling [4], [8], [14], [15] and manifold embedding [5], [8]GLYPH<21>[10]. In addition, there are several cases where a good property that singular vector(s) obtained from SVD has the compressed information is used for global pooling.\nSince global pooling was GLYPH<28>rst introduced in [16], it has evolved in various ways, such as employing trainable variables [17] and adopting non-linear functions [18], [19]. On the other hand, a global pooling method using SVD was proposed [20], which was based on covariance pooling. Covariance pooling obtains the covariance matrix FF T of a feature matrix F and extracts meaningful information from covariance. Reference [4] showed that if matrix logarithm is obtained by SVD, tangent space mapping is applicable. Recently, various covariance pooling methods have been proposed, inspired by [4]. For instance, [14] presented a method to solve the instability of SVD gradients, and [8] reduced the computational cost of SVD through Newton-Schulz iteration. Also, [15] showed that covariance pooling can be effectively applied in the middle of the network.\nHowever, very few studies directly use singular vectors in deep learning. As far as we know, KD-SVD [12] may be the GLYPH<28>rst deep learning technique that uses essential information of singular vectors directly. In KD-SVD, singular vectors are extracted from two feature maps by SVD, a correlation between two singular vectors is computed, and the correlation is distilled as knowledge. To facilitate the learning of singular vectors, KD-SVD uses a teacher network as a guide. However, such an approach is not applicable to general architectures other than the student-teacher network where the teacher network plays a guide role.\nAccording to our survey, the use of SVD in deep learning is continuously being studied, but there is no way to learn singular vectors directly. On the other hand, it is also true that many methods using singular vectors for various computer vision tasks have been studied before the deep learning era [11], [21], [22]. We argue that the reason for this recent research trend is that although singular vectors have good information, there are some bad properties making difGLYPH<28>cult to learn them with common deep learning schemes. Therefore, by analyzing and removing these properties, we intend to propose a way that singular vectors can be utilized in deep learning scheme. Ultimately, the proposed algorithm will provide an opportunity to use SVD in various machine learning applications.\nFIGURE 2. An example of CNN architecture using SVP. At the bottom of the figure, we show the process of transforming the singular vectors obtained through SVD prior to learning and the process of changing the feature distribution through each process.",
        "original_text": "As one of the most popular PCA techniques, SVD is often used to reduce the dimension of a matrix or to extract essential information from a matrix or dataset. Even in the GLYPH<28>eld of machine learning, the use of SVD is expanding more and more [4], [5], [8]GLYPH<21>[10], [12]GLYPH<21>[15]. For example, SVD has been applied to a typical CNN as the derivative of SVD has been mathematically derived recently. The most common example is the computation of a unique square root of an SPD matrix. This method is used for various purposes, such as\nFIGURE 1. Comparison of feature vectors obtained by global average pooling and singular vector pooling.\ncovariance pooling [4], [8], [14], [15] and manifold embedding [5], [8]GLYPH<21>[10]. In addition, there are several cases where a good property that singular vector(s) obtained from SVD has the compressed information is used for global pooling.\nSince global pooling was GLYPH<28>rst introduced in [16], it has evolved in various ways, such as employing trainable variables [17] and adopting non-linear functions [18], [19]. On the other hand, a global pooling method using SVD was proposed [20], which was based on covariance pooling. Covariance pooling obtains the covariance matrix FF T of a feature matrix F and extracts meaningful information from covariance. Reference [4] showed that if matrix logarithm is obtained by SVD, tangent space mapping is applicable. Recently, various covariance pooling methods have been proposed, inspired by [4]. For instance, [14] presented a method to solve the instability of SVD gradients, and [8] reduced the computational cost of SVD through Newton-Schulz iteration. Also, [15] showed that covariance pooling can be effectively applied in the middle of the network.\nHowever, very few studies directly use singular vectors in deep learning. As far as we know, KD-SVD [12] may be the GLYPH<28>rst deep learning technique that uses essential information of singular vectors directly. In KD-SVD, singular vectors are extracted from two feature maps by SVD, a correlation between two singular vectors is computed, and the correlation is distilled as knowledge. To facilitate the learning of singular vectors, KD-SVD uses a teacher network as a guide. However, such an approach is not applicable to general architectures other than the student-teacher network where the teacher network plays a guide role.\nAccording to our survey, the use of SVD in deep learning is continuously being studied, but there is no way to learn singular vectors directly. On the other hand, it is also true that many methods using singular vectors for various computer vision tasks have been studied before the deep learning era [11], [21], [22]. We argue that the reason for this recent research trend is that although singular vectors have good information, there are some bad properties making difGLYPH<28>cult to learn them with common deep learning schemes. Therefore, by analyzing and removing these properties, we intend to propose a way that singular vectors can be utilized in deep learning scheme. Ultimately, the proposed algorithm will provide an opportunity to use SVD in various machine learning applications.\nFIGURE 2. An example of CNN architecture using SVP. At the bottom of the figure, we show the process of transforming the singular vectors obtained through SVD prior to learning and the process of changing the feature distribution through each process.",
        "context": "Contextualizes the chunk’s role in establishing the paper’s focus on utilizing singular vectors in deep learning, specifically addressing limitations of previous approaches and introducing a novel pooling method (SVP).",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            2,
            3
        ],
        "id": "0c33a10f3144f7d81394a5d861d7c37943e24f9e62bf73be57f536e0e140bc41"
    },
    {
        "text": "Analysis of singular vector learning challenges and introduction of singular vector pooling (SVP) as a solution to address these difficulties, specifically focusing on mitigating sign ambiguity and mapping non-Euclidean space to Euclidean space for improved learning.\n\nWe theoretically analyze the undesirable properties that generally make the learning of singular vectors difGLYPH<28>cult. Based on the theoretical analysis, we propose SVP as a solution to overcome the limitations of using singular vectors. Fig. 2 shows the overall structure of SVP, which is a sort of post-processing for learning singular vectors. Feature maps obtained from CNN are GLYPH<28>rst decomposed by SVD, and then the output singular vectors are transformed to facilitate learning by SVP. Finally, the transformed information is classiGLYPH<28>ed.",
        "original_text": "We theoretically analyze the undesirable properties that generally make the learning of singular vectors difGLYPH<28>cult. Based on the theoretical analysis, we propose SVP as a solution to overcome the limitations of using singular vectors. Fig. 2 shows the overall structure of SVP, which is a sort of post-processing for learning singular vectors. Feature maps obtained from CNN are GLYPH<28>rst decomposed by SVD, and then the output singular vectors are transformed to facilitate learning by SVP. Finally, the transformed information is classiGLYPH<28>ed.",
        "context": "Analysis of singular vector learning challenges and introduction of singular vector pooling (SVP) as a solution to address these difficulties, specifically focusing on mitigating sign ambiguity and mapping non-Euclidean space to Euclidean space for improved learning.",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            3
        ],
        "id": "8412b8c408d036af00a80def43cbc48e74cabc894234da49bdc924f902cee7dd"
    },
    {
        "text": "Details the decomposition of a CNN feature map using SVD, highlighting the issues of sign ambiguity and the difficulty of learning singular vectors with standard neural networks.\n\nAssume that a feature map obtained by a CNN has a spatial shape of H GLYPH<2> W and a feature depth of D , and it can be transformed into a matrix form. Then, the feature map of matrix form can be interpreted as a set composed of HWD -dimensional vectors. We deGLYPH<28>ne this as F . Decomposing F using SVD results in:\n<!-- formula-not-decoded -->\nLeft-hand matrix U has feature speciGLYPH<28>c information, right-hand matrix V has global feature information, and the central matrix 6 indicates singular value magnitudes [1]. Since the singular values are decomposed in descending order, SVD of the same matrices always yields the same 6 . On the other hand, U and V are created in pairs as follows:\n<!-- formula-not-decoded -->\nwhere\n<!-- formula-not-decoded -->\nEq. (2) indicates that singular vectors with identical information can be randomly distributed in two different positions due to random variable pk . This phenomenon is called sign ambiguity [11]. This is an undesirable property that must be removed because it makes learning singular vectors difGLYPH<28>cult. Meanwhile, each singular vector is a unit vector with a norm of 1. In other words, singular vectors are the points on a unit hypersphereGLYPH<22>i.e., a manifold. Therefore, learning singular vectors with a generic neural network and Euclidean geometry can be very inefGLYPH<28>cient or impossible. This is the second problem. In summary, singular vectors have two fundamental problems as follows.\n- GLYPH<15> Singular vectors with similar information are randomly distributed in two areas due to sign ambiguity.\n- GLYPH<15> It is inefGLYPH<28>cient to learn singular vectors on manifold space through a general neural network scheme.",
        "original_text": "Assume that a feature map obtained by a CNN has a spatial shape of H GLYPH<2> W and a feature depth of D , and it can be transformed into a matrix form. Then, the feature map of matrix form can be interpreted as a set composed of HWD -dimensional vectors. We deGLYPH<28>ne this as F . Decomposing F using SVD results in:\n<!-- formula-not-decoded -->\nLeft-hand matrix U has feature speciGLYPH<28>c information, right-hand matrix V has global feature information, and the central matrix 6 indicates singular value magnitudes [1]. Since the singular values are decomposed in descending order, SVD of the same matrices always yields the same 6 . On the other hand, U and V are created in pairs as follows:\n<!-- formula-not-decoded -->\nwhere\n<!-- formula-not-decoded -->\nEq. (2) indicates that singular vectors with identical information can be randomly distributed in two different positions due to random variable pk . This phenomenon is called sign ambiguity [11]. This is an undesirable property that must be removed because it makes learning singular vectors difGLYPH<28>cult. Meanwhile, each singular vector is a unit vector with a norm of 1. In other words, singular vectors are the points on a unit hypersphereGLYPH<22>i.e., a manifold. Therefore, learning singular vectors with a generic neural network and Euclidean geometry can be very inefGLYPH<28>cient or impossible. This is the second problem. In summary, singular vectors have two fundamental problems as follows.\n- GLYPH<15> Singular vectors with similar information are randomly distributed in two areas due to sign ambiguity.\n- GLYPH<15> It is inefGLYPH<28>cient to learn singular vectors on manifold space through a general neural network scheme.",
        "context": "Details the decomposition of a CNN feature map using SVD, highlighting the issues of sign ambiguity and the difficulty of learning singular vectors with standard neural networks.",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            3
        ],
        "id": "4a5cab8b87efe1ae58e0ae6e270551604be45019f31cf289d3709867f5afcc9a"
    },
    {
        "text": "Addresses challenges in learning singular vectors by mapping the unit hypersphere (manifold) to Euclidean space through coordinate conversion and rotation to mitigate discontinuity and sign ambiguity.\n\nWe GLYPH<28>rst describe a solution for learning singular vectors with the manifold property. Since the manifold is a unit hypersphere, it can be mapped to Euclidean space by disentangling each coordinate through a speciGLYPH<28>c coordinate conversion. As a result, it becomes possible to use SVD as a layer of CNN because it is easier to learn singular vectors based on Euclidean geometry and to transform them. (see Sec. III-E).\nHowever, since each coordinate in the spherical space is bounded, the singular vector distribution can be discontinuous near the boundary. This phenomenon can negatively affect the feature embedding of CNN. Therefore, rotation is applied so that the singular vectors are not located near the boundary before coordinate conversion. (see Sec. III-C).\nNext, we present a method to remove the sign ambiguity phenomenon. Suppose that singular vectors are distributed in two regions around a speciGLYPH<28>c center vector due to sign ambiguity. Similarity of each singular vector is calculated based on the center vector. By reversing the sign of a singular vector with negative similarity, the sign ambiguity can be resolved. This method is very simple and efGLYPH<28>cient as in Sec. III-D.\nOn the other hand, it is not easy to determine an accurate center vector because the singular vector distribution can change as learning progresses. Thus, we propose a way to approximate the center vector during learning. This process is performed after coordinate conversion for convenient calculation. (see Sec. III-F)",
        "original_text": "We GLYPH<28>rst describe a solution for learning singular vectors with the manifold property. Since the manifold is a unit hypersphere, it can be mapped to Euclidean space by disentangling each coordinate through a speciGLYPH<28>c coordinate conversion. As a result, it becomes possible to use SVD as a layer of CNN because it is easier to learn singular vectors based on Euclidean geometry and to transform them. (see Sec. III-E).\nHowever, since each coordinate in the spherical space is bounded, the singular vector distribution can be discontinuous near the boundary. This phenomenon can negatively affect the feature embedding of CNN. Therefore, rotation is applied so that the singular vectors are not located near the boundary before coordinate conversion. (see Sec. III-C).\nNext, we present a method to remove the sign ambiguity phenomenon. Suppose that singular vectors are distributed in two regions around a speciGLYPH<28>c center vector due to sign ambiguity. Similarity of each singular vector is calculated based on the center vector. By reversing the sign of a singular vector with negative similarity, the sign ambiguity can be resolved. This method is very simple and efGLYPH<28>cient as in Sec. III-D.\nOn the other hand, it is not easy to determine an accurate center vector because the singular vector distribution can change as learning progresses. Thus, we propose a way to approximate the center vector during learning. This process is performed after coordinate conversion for convenient calculation. (see Sec. III-F)",
        "context": "Addresses challenges in learning singular vectors by mapping the unit hypersphere (manifold) to Euclidean space through coordinate conversion and rotation to mitigate discontinuity and sign ambiguity.",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            3
        ],
        "id": "98ef740ce6a0ea099f42e369f568298bba0b935dd31045ef14496c77d77c9a0c"
    },
    {
        "text": "Addresses the discontinuity problem arising from mapping spherical coordinates to Euclidean space, proposing a rotation strategy to center singular vectors and minimize path lengths during learning.\n\nMapping of singular vectors in the spherical coordinate system is equivalent to converting them from non-Euclidean space to Euclidean space. However, when singular vectors\nFIGURE 3. Discontinuity and rotation effects in the spherical coordinate system. Red, green, yellow, and blue lines indicate a discontinuity boundary, SP1, SP2, and rotation, respectively. (a) Discontinuity in spherical coordinates (b) SP1 and SP2 before rotation (c) SP1 and SP2 after rotation.\nare directly mapped to spherical coordinates, a discontinuity problem may occur.\nLet . r ; GLYPH<30> 1 ; GLYPH<30> 2 ; : : : ; GLYPH<30> N GLYPH<0> 1 / denote N -dimensional spherical coordinates. Each component of .GLYPH<30> 1 ; GLYPH<30> 2 ; : : : ; GLYPH<30> N GLYPH<0> 2 / has a bounded range of [0 ; GLYPH<25> ], and GLYPH<30> N GLYPH<0> 1 is limited to [ GLYPH<0> GLYPH<25> ; GLYPH<25> ], which means that discontinuity may occur at the boundary of each component, as shown in Fig. 3(a). For convenience, the three-dimensional space is assumed in the GLYPH<28>gure. If two feature vectors v and v t are present near the discontinuity boundary, as in Fig. 3(b), we can consider two different shortest paths: one through the discontinuity boundary (SP1) and another to avoid the discontinuity boundary (SP2). Therefore, the learning efGLYPH<28>ciency may be reduced because singular vectors can practically move to longer paths such as SP2 rather than the real shortest path (SP1) during the learning process. Thus, singular vectors should be kept as distant from the discontinuity boundary as possible. In other words, the singular vector distribution must be intentionally centered in the spherical coordinate system. The center of the spherical coordinate system GLYPH<8> GLYPH<25> 2 ; GLYPH<25> 2 ; : : : ; GLYPH<25> 2 ; 0 GLYPH<9> is converted to f 0 ; : : : ; 0 ; 1 ; 0 g of the Cartesian coordinate using spherical to Cartesian coordinates conversion equation in [23].\nAs a result, the center of the singular vector distribution v c must be rotated so that it becomes the coordinate center, e v c which is f 0 ; : : : ; 0 ; 1 ; 0 g . How to obtain v c will be given in Sec. III-F. Then, the rotation matrix R is computed by using the well-known Rodrigues rotation as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere k is the direction of the rotation axis. Using the rotation matrix of Eq. (6), e V is obtained by:\n<!-- formula-not-decoded -->\nWe can obtain e U by applying the same process to U .\nAs shown in Fig. 3(c), the shortest path and the real path now coincide. However, if singular vector variance is too high, some singular vectors still exist near the discontinuity boundary. Fortunately, SVD computes a least-square solution which is not sparse, and regularization skills such as weight\ndecay and batch normalization [24] also control feature map's variance to some extent. So, we can ignore this high variance problem. Therefore, the discontinuity problem is eliminated so that a singular vector can be effectively learned. However, the two problems of singular vectors mentioned in Section 3.1 still remain unsolved.",
        "original_text": "Mapping of singular vectors in the spherical coordinate system is equivalent to converting them from non-Euclidean space to Euclidean space. However, when singular vectors\nFIGURE 3. Discontinuity and rotation effects in the spherical coordinate system. Red, green, yellow, and blue lines indicate a discontinuity boundary, SP1, SP2, and rotation, respectively. (a) Discontinuity in spherical coordinates (b) SP1 and SP2 before rotation (c) SP1 and SP2 after rotation.\nare directly mapped to spherical coordinates, a discontinuity problem may occur.\nLet . r ; GLYPH<30> 1 ; GLYPH<30> 2 ; : : : ; GLYPH<30> N GLYPH<0> 1 / denote N -dimensional spherical coordinates. Each component of .GLYPH<30> 1 ; GLYPH<30> 2 ; : : : ; GLYPH<30> N GLYPH<0> 2 / has a bounded range of [0 ; GLYPH<25> ], and GLYPH<30> N GLYPH<0> 1 is limited to [ GLYPH<0> GLYPH<25> ; GLYPH<25> ], which means that discontinuity may occur at the boundary of each component, as shown in Fig. 3(a). For convenience, the three-dimensional space is assumed in the GLYPH<28>gure. If two feature vectors v and v t are present near the discontinuity boundary, as in Fig. 3(b), we can consider two different shortest paths: one through the discontinuity boundary (SP1) and another to avoid the discontinuity boundary (SP2). Therefore, the learning efGLYPH<28>ciency may be reduced because singular vectors can practically move to longer paths such as SP2 rather than the real shortest path (SP1) during the learning process. Thus, singular vectors should be kept as distant from the discontinuity boundary as possible. In other words, the singular vector distribution must be intentionally centered in the spherical coordinate system. The center of the spherical coordinate system GLYPH<8> GLYPH<25> 2 ; GLYPH<25> 2 ; : : : ; GLYPH<25> 2 ; 0 GLYPH<9> is converted to f 0 ; : : : ; 0 ; 1 ; 0 g of the Cartesian coordinate using spherical to Cartesian coordinates conversion equation in [23].\nAs a result, the center of the singular vector distribution v c must be rotated so that it becomes the coordinate center, e v c which is f 0 ; : : : ; 0 ; 1 ; 0 g . How to obtain v c will be given in Sec. III-F. Then, the rotation matrix R is computed by using the well-known Rodrigues rotation as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere k is the direction of the rotation axis. Using the rotation matrix of Eq. (6), e V is obtained by:\n<!-- formula-not-decoded -->\nWe can obtain e U by applying the same process to U .\nAs shown in Fig. 3(c), the shortest path and the real path now coincide. However, if singular vector variance is too high, some singular vectors still exist near the discontinuity boundary. Fortunately, SVD computes a least-square solution which is not sparse, and regularization skills such as weight\ndecay and batch normalization [24] also control feature map's variance to some extent. So, we can ignore this high variance problem. Therefore, the discontinuity problem is eliminated so that a singular vector can be effectively learned. However, the two problems of singular vectors mentioned in Section 3.1 still remain unsolved.",
        "context": "Addresses the discontinuity problem arising from mapping spherical coordinates to Euclidean space, proposing a rotation strategy to center singular vectors and minimize path lengths during learning.",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            3,
            4
        ],
        "id": "e473d704fd3f305edc8be733a6418f2acc3ab0dd7ae8cc7d20a6a947ca7c68be"
    },
    {
        "text": "Eliminates sign ambiguity by aligning singular vectors and establishing a reference, resulting in a more stable and analyzable representation of singular vectors.\n\nU and V in Eq. (1) exist as a pair, and one of them is used as a reference. We adopt an approach of aligning e V based on e U as a reference. Let e u k be the k -th singular vector. Then e u k and GLYPH<0> e u k exist concurrently with the same information but opposite signs. Therefore, a domain in which a singular vector exists can be divided into a positive half-sphere that is decomposed in the forward direction and a negative half-sphere that is decomposed in the reverse direction. Thus, we eliminate the randomness property by multiplying a singular vector on the negative half-sphere by GLYPH<0> 1. In the previous section, the center vector of the singular vector distribution is rotated to e u c , which is f 0 ; 0 ; : : : ; 0 ; 1 ; 0 g . So we can verify the hyper-sphere where e u k is in according to the sign of e uHW GLYPH<0> 1 ; k . Finally, O U and O V that are arranged based on e u c are deGLYPH<28>ned as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nSince the sign ambiguity phenomenon in O U and O V has been removed, O U and O V are easier to analyze than the original e U and e V .",
        "original_text": "U and V in Eq. (1) exist as a pair, and one of them is used as a reference. We adopt an approach of aligning e V based on e U as a reference. Let e u k be the k -th singular vector. Then e u k and GLYPH<0> e u k exist concurrently with the same information but opposite signs. Therefore, a domain in which a singular vector exists can be divided into a positive half-sphere that is decomposed in the forward direction and a negative half-sphere that is decomposed in the reverse direction. Thus, we eliminate the randomness property by multiplying a singular vector on the negative half-sphere by GLYPH<0> 1. In the previous section, the center vector of the singular vector distribution is rotated to e u c , which is f 0 ; 0 ; : : : ; 0 ; 1 ; 0 g . So we can verify the hyper-sphere where e u k is in according to the sign of e uHW GLYPH<0> 1 ; k . Finally, O U and O V that are arranged based on e u c are deGLYPH<28>ned as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nSince the sign ambiguity phenomenon in O U and O V has been removed, O U and O V are easier to analyze than the original e U and e V .",
        "context": "Eliminates sign ambiguity by aligning singular vectors and establishing a reference, resulting in a more stable and analyzable representation of singular vectors.",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            4
        ],
        "id": "5af1b979401dce08bea1c289fa93b9ce0c0c34adbd78c050d2bc01dfa47edc8e"
    },
    {
        "text": "Transforms the coordinate system of O V to spherical coordinates, addressing a potential divergence issue through a first-order Taylor expansion and zero-centering, ultimately enabling learning by a generic neural network.\n\nTo convert the coordinate system of O V , it is rewritten as:\n<!-- formula-not-decoded -->\nThen, the D -dimensional transformation of O V in Cartesian coordinates into spherical coordinates is deGLYPH<28>ned as follows:\n<!-- formula-not-decoded -->\nwhere\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nIn Eq. (13), GLYPH<15> is a constant to prevent zero-division and is set to 10 GLYPH<0> 3 . So, N V on the spherical coordinate system is obtained. However, the 1 st derivative of the arccosine in Eq. (12) can easily diverge, as shown in Eq. (14).\n<!-- formula-not-decoded -->\nSo, we approximate the arccosine to GLYPH<0> z C GLYPH<25> 2 by the GLYPH<28>rst order Taylor expansion. Next, we add a constant term for keeping a zero-centered feature. Finally, Eq. (12) is re-written by\n<!-- formula-not-decoded -->\nAs a result, O V is converted to N V that can be learned by a generic neural network. At the same manner, N U is derived.",
        "original_text": "To convert the coordinate system of O V , it is rewritten as:\n<!-- formula-not-decoded -->\nThen, the D -dimensional transformation of O V in Cartesian coordinates into spherical coordinates is deGLYPH<28>ned as follows:\n<!-- formula-not-decoded -->\nwhere\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nIn Eq. (13), GLYPH<15> is a constant to prevent zero-division and is set to 10 GLYPH<0> 3 . So, N V on the spherical coordinate system is obtained. However, the 1 st derivative of the arccosine in Eq. (12) can easily diverge, as shown in Eq. (14).\n<!-- formula-not-decoded -->\nSo, we approximate the arccosine to GLYPH<0> z C GLYPH<25> 2 by the GLYPH<28>rst order Taylor expansion. Next, we add a constant term for keeping a zero-centered feature. Finally, Eq. (12) is re-written by\n<!-- formula-not-decoded -->\nAs a result, O V is converted to N V that can be learned by a generic neural network. At the same manner, N U is derived.",
        "context": "Transforms the coordinate system of O V to spherical coordinates, addressing a potential divergence issue through a first-order Taylor expansion and zero-centering, ultimately enabling learning by a generic neural network.",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            4,
            5
        ],
        "id": "b7043fe9d636a147413ffa99723c6cc58e39d76fe6924560d1cb8d2206781bef"
    },
    {
        "text": "Describes the process of estimating the center vector, highlighting the issue of inaccurate center vector estimation and proposing a learning-based approach to correct it.\n\nIn this section, the process of estimating the center vector mentioned in Sec. III-C is described. Since this process has to be explained in connection with Sec. III-D, it is described with u c instead of v c . The proposed method rotates u around u c , moves the center of the distribution to f 0 ; 0 ; : : : ; 0 ; 1 ; 0 g , and determines which half-sphere it belongs to through the sign of e uHW GLYPH<0> 1 ; k . If u c is inaccurate, this process will be erroneous. Thereby, the sign ambiguity problem cannot be solved properly. To observe this phenomenon, the distribution of N u obtained from u c is shown in the GLYPH<28>rst row of Fig. 4. Here u c is experimentally set to a unit vector with all components of the same size. We can GLYPH<28>nd that the distributions of the two half-spheres are not aligned correctly because of the sign ambiguity problem caused by inaccurate u c . To obtain the precise u c , we suggest learning of u c . Since we are hard to understand the statistical characteristics of singular vectors due to two inherent bad properties, we learn u c based on N u . Note that each component of features produced by neural networks follows the Gaussian distribution in general. So, assuming that each component of the aligned N u has the Gaussian distribution, u c is learned through KL-divergence [25] of Eq. (16) so that the assumption is correct.\n<!-- formula-not-decoded -->\nwhere Norm GLYPH<16> GLYPH<22> N u b ; k ; GLYPH<27> 2 N u b ; k GLYPH<17> indicates a Gaussian distribution. Furthermore, GLYPH<27> 2 N u b ; k D var GLYPH<0> N u b ; k GLYPH<1> , and mean GLYPH<22> N u b ; k is empirically set to 0. B is the batch size. As shown in Fig. 4, in order to minimize L KL N u b ; k , two distributions of N u get merged and form a single Gaussian distribution, which means that the sign ambiguity problem is solved. Therefore, the proposed method can estimate u c overcoming two bad properties of singular vectors. In the same manner, v c is estimated by minimizing L KL N v b ; k . Note that L KL N u b ; k and L KL N v b ; k are loss functions for learning u c and v c only and do not affect the network parameters.",
        "original_text": "In this section, the process of estimating the center vector mentioned in Sec. III-C is described. Since this process has to be explained in connection with Sec. III-D, it is described with u c instead of v c . The proposed method rotates u around u c , moves the center of the distribution to f 0 ; 0 ; : : : ; 0 ; 1 ; 0 g , and determines which half-sphere it belongs to through the sign of e uHW GLYPH<0> 1 ; k . If u c is inaccurate, this process will be erroneous. Thereby, the sign ambiguity problem cannot be solved properly. To observe this phenomenon, the distribution of N u obtained from u c is shown in the GLYPH<28>rst row of Fig. 4. Here u c is experimentally set to a unit vector with all components of the same size. We can GLYPH<28>nd that the distributions of the two half-spheres are not aligned correctly because of the sign ambiguity problem caused by inaccurate u c . To obtain the precise u c , we suggest learning of u c . Since we are hard to understand the statistical characteristics of singular vectors due to two inherent bad properties, we learn u c based on N u . Note that each component of features produced by neural networks follows the Gaussian distribution in general. So, assuming that each component of the aligned N u has the Gaussian distribution, u c is learned through KL-divergence [25] of Eq. (16) so that the assumption is correct.\n<!-- formula-not-decoded -->\nwhere Norm GLYPH<16> GLYPH<22> N u b ; k ; GLYPH<27> 2 N u b ; k GLYPH<17> indicates a Gaussian distribution. Furthermore, GLYPH<27> 2 N u b ; k D var GLYPH<0> N u b ; k GLYPH<1> , and mean GLYPH<22> N u b ; k is empirically set to 0. B is the batch size. As shown in Fig. 4, in order to minimize L KL N u b ; k , two distributions of N u get merged and form a single Gaussian distribution, which means that the sign ambiguity problem is solved. Therefore, the proposed method can estimate u c overcoming two bad properties of singular vectors. In the same manner, v c is estimated by minimizing L KL N v b ; k . Note that L KL N u b ; k and L KL N v b ; k are loss functions for learning u c and v c only and do not affect the network parameters.",
        "context": "Describes the process of estimating the center vector, highlighting the issue of inaccurate center vector estimation and proposing a learning-based approach to correct it.",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            5
        ],
        "id": "cc76a4baade3dbd9df09debf62c0bad7df09f12d10c1ffa7c8ac66858786ce11"
    },
    {
        "text": "Evaluates the proposed singular vector pooling method (SVP) in CNNs using SVD, demonstrating its improved performance compared to GAP and GMP in KD-SVD and highlighting its effectiveness in extracting relevant information from feature maps through ablation studies.\n\nWe performed three experiments to verify the proposed method. First, we evaluated the performance of SVP in a CNN structure when using SVD. Because classiGLYPH<28>cation task outputs only class information, we use one singular vector\nFIGURE 4. Learning process of singular vectors which are converted to the spherical coordinate. The left column is plotting the last two components and the right column is the distribution. The black line shows a Gaussian distribution, and the blue and yellow lines indicate the distributions of singular vectors in two half-spheres obtained by applying the proposed method. During the learning process, it gradually gets closer to the Gaussian distribution, indicating that the basis is well learned.\nfor the classiGLYPH<28>cation task. Second, we applied the proposed method to KD-SVD, which is a representative network using singular vectors. Since KD-SVD compresses CNN's feature maps with several dominant singular vectors, the performance of the student network improves. In this experiment, weadditionally show an effect of using multiple singular vectors for extracting important information from feature maps. Third, the performance of the proposed method is analyzed through an ablation study.",
        "original_text": "We performed three experiments to verify the proposed method. First, we evaluated the performance of SVP in a CNN structure when using SVD. Because classiGLYPH<28>cation task outputs only class information, we use one singular vector\nFIGURE 4. Learning process of singular vectors which are converted to the spherical coordinate. The left column is plotting the last two components and the right column is the distribution. The black line shows a Gaussian distribution, and the blue and yellow lines indicate the distributions of singular vectors in two half-spheres obtained by applying the proposed method. During the learning process, it gradually gets closer to the Gaussian distribution, indicating that the basis is well learned.\nfor the classiGLYPH<28>cation task. Second, we applied the proposed method to KD-SVD, which is a representative network using singular vectors. Since KD-SVD compresses CNN's feature maps with several dominant singular vectors, the performance of the student network improves. In this experiment, weadditionally show an effect of using multiple singular vectors for extracting important information from feature maps. Third, the performance of the proposed method is analyzed through an ablation study.",
        "context": "Evaluates the proposed singular vector pooling method (SVP) in CNNs using SVD, demonstrating its improved performance compared to GAP and GMP in KD-SVD and highlighting its effectiveness in extracting relevant information from feature maps through ablation studies.",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            5
        ],
        "id": "00c79b643eb8046e6f72a9958c56c406901c14c40df1cde35c960dc4f4e2bef2"
    },
    {
        "text": "Describes the datasets and augmentation techniques used in the experiments.\n\nThe datasets used in the following experiments are CIFAR10, CIFAR100 [26], Tiny-ImageNet, and ImageNet2012 [27], which are all normalized to have values of [-0.5, 0.5]. CIFAR10 and CIFAR100 have 10 and 100 labels, respectively, and are compact datasets consisting of 50,000 color images of 32 GLYPH<2> 32 pixels. Because these datasets require relatively little cost, they are used for basic veriGLYPH<28>cation of the proposed method. Tiny-ImageNet is a medium-sized dataset with 100,000 color images of 64 GLYPH<2> 64 pixels and has 200 labels. ImageNet2012 is a large dataset with more than 1.5 million high-resolution color images and 1,000 labels. The last two datasets are used to prove that the proposed method works well even for huge datasets with large resolutions.\nA particular augmentation is applied to each dataset. Horizontal random GLYPH<29>ip is applied to all the datasets. Also, the images of CIFAR10 and CIFAR100 are zero-padded by 4 pixels, and the images of Tiny-ImageNet [27] are zero-padded by 8 pixels. Then, the zero-padded images are randomly cropped to the original size. In case of ImageNet2012, all\nFIGURE 5. The block diagrams of the networks introduced in this paper.\nthe images are resized to 256 GLYPH<2> 256, and then the training image are produced by randomly cropping the resized images to 224 GLYPH<2> 224. The test set is constructed by cutting the central area of each image to 224 GLYPH<2> 224.\nAll algorithms are implemented using TensorGLYPH<29>ow [28], and their network architectures are shown in Fig. 5. 'CONV' and 'Max Pool' indicate convolutional layer and max pooling layer, respectively. 'FC' stands for fully connected layer. Also H GLYPH<2> W is the kernel size, D is the output depth. s means stride and the same layer repeats by N . (Label) indicates the number of labels in the target dataset. The activation function for convolution layers and all FC layers except the end of the network is ReLU [29]. In Fig. 5(a), 5(b) and 5(c), one of SVP, global average pooling (GAP) [16], global max pooling (GMP) [18], and matrix power normalized covariance pooling (MPN) [8] is adopted for pooling block. Also, in Fig. 5(a), 5(b), 5(c) and 5(d), batch normalization [24] is used next to the convolutional layer. The dotted gray boxes in Fig. 5(e) and 5(f) are the layer modules for knowledge distillation. The input and output feature maps of this module are sensed and applied to both the authentic KD-SVD [12] and the KD-SVP where SVP was applied to KD-SVD.\nThe weights of all networks are initialized with He's initialization [30], and L 2 regularization is applied. A stochastic gradient descent (SGD) [31] is used as the optimizer, and a Nesterov accelerated gradient [32] is applied. All numerical values in the tables and GLYPH<28>gures are the averages of all GLYPH<28>ve trials.",
        "original_text": "The datasets used in the following experiments are CIFAR10, CIFAR100 [26], Tiny-ImageNet, and ImageNet2012 [27], which are all normalized to have values of [-0.5, 0.5]. CIFAR10 and CIFAR100 have 10 and 100 labels, respectively, and are compact datasets consisting of 50,000 color images of 32 GLYPH<2> 32 pixels. Because these datasets require relatively little cost, they are used for basic veriGLYPH<28>cation of the proposed method. Tiny-ImageNet is a medium-sized dataset with 100,000 color images of 64 GLYPH<2> 64 pixels and has 200 labels. ImageNet2012 is a large dataset with more than 1.5 million high-resolution color images and 1,000 labels. The last two datasets are used to prove that the proposed method works well even for huge datasets with large resolutions.\nA particular augmentation is applied to each dataset. Horizontal random GLYPH<29>ip is applied to all the datasets. Also, the images of CIFAR10 and CIFAR100 are zero-padded by 4 pixels, and the images of Tiny-ImageNet [27] are zero-padded by 8 pixels. Then, the zero-padded images are randomly cropped to the original size. In case of ImageNet2012, all\nFIGURE 5. The block diagrams of the networks introduced in this paper.\nthe images are resized to 256 GLYPH<2> 256, and then the training image are produced by randomly cropping the resized images to 224 GLYPH<2> 224. The test set is constructed by cutting the central area of each image to 224 GLYPH<2> 224.\nAll algorithms are implemented using TensorGLYPH<29>ow [28], and their network architectures are shown in Fig. 5. 'CONV' and 'Max Pool' indicate convolutional layer and max pooling layer, respectively. 'FC' stands for fully connected layer. Also H GLYPH<2> W is the kernel size, D is the output depth. s means stride and the same layer repeats by N . (Label) indicates the number of labels in the target dataset. The activation function for convolution layers and all FC layers except the end of the network is ReLU [29]. In Fig. 5(a), 5(b) and 5(c), one of SVP, global average pooling (GAP) [16], global max pooling (GMP) [18], and matrix power normalized covariance pooling (MPN) [8] is adopted for pooling block. Also, in Fig. 5(a), 5(b), 5(c) and 5(d), batch normalization [24] is used next to the convolutional layer. The dotted gray boxes in Fig. 5(e) and 5(f) are the layer modules for knowledge distillation. The input and output feature maps of this module are sensed and applied to both the authentic KD-SVD [12] and the KD-SVP where SVP was applied to KD-SVD.\nThe weights of all networks are initialized with He's initialization [30], and L 2 regularization is applied. A stochastic gradient descent (SGD) [31] is used as the optimizer, and a Nesterov accelerated gradient [32] is applied. All numerical values in the tables and GLYPH<28>gures are the averages of all GLYPH<28>ve trials.",
        "context": "Describes the datasets and augmentation techniques used in the experiments.",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            5,
            6
        ],
        "id": "4c75524b7630f60d6eec171cd1306599bed278b713001658dd306f2fe4fa6113"
    },
    {
        "text": "Analyzes the impact of SVP on CIFAR10 and ImageNet2012 datasets, demonstrating improved accuracy and silhouette scores compared to GAP and GMP, while also exhibiting lower forward time.\n\nMost CNNs increase the size of the receptive GLYPH<28>eld and reduce the size of the feature map through pooling. GMP and GAP are the most popular pooling methods, but both methods suffer from the inherent loss of information. SVP can improve the performance of a given network because it can reduce the size of feature maps while keeping as much of their information as possible.\nThe base network used for this experiment is ResNet32 [30] which described in Fig. 5(a). We analyzed the results by replacing GAP of ResNet-32 with GMP [18] and SVP, respectively. In case of CIFAR10, learning is proceeded for 200 epochs and the initial learning rate is equal to 10 GLYPH<0> 2 , which is reduced by 0.1 times at 100 and 150 epochs. The batch size is set to 128, and the weight decay of L 2 regularization is GLYPH<28>xed to 10 GLYPH<0> 4 . ImageNet2012 is trained for 90 epochs, and the initial learning rate is set to 10 GLYPH<0> 2 , and the learning rate is reduced by 0.1 times at 30 and 60 epochs. The batch size in this experiment is 128 and the weight decay of L 2 regularization is GLYPH<28>xed to 5 GLYPH<2> 10 GLYPH<0> 4 .\nTABLE 1. The comparison of the proposed SVP with GAP and GMP for CIFAR10 dataset. 'ACC' indicates accuracy, 'Silh' indicates silhouette score, and T indicates forward time for each pooling layer.\nTable 1 shows the experimental results for CIFAR10. We can GLYPH<28>nd the validation accuracy of SVP is 0.67% and 0.91% better than that of GAP and GMP, respectively. However, this is not a big improvement and almost same with MPN. Nevertheless, the silhouette score [33] of SVP on training dataset is better by 0.444, 0.510, and 0.551 than that of GAP, GMP, and MPN, respectively. SVP shows excellent silhouette scores that are more than twice those of the\nFIGURE 6. The distribution of feature vectors obtained by several pooling techniques for CIFAR10 training set.\nother pooling methods. This indicate that SVP makes feature vectors get a high inter-class variance and a low intra-class variance. Also, the proposed method has a lower forward time than MPN, which is a light-weight second-order pooling method. Therefore, we can GLYPH<28>nd that the proposed method can provide better clustering performance with less computation time.\nAlso, to illustrate the clustering performance clearly, we performed the same experiment through a network with an additional FC layer having a two-dimensional output as shown in Fig. 5(b). The network is learned for CIFAR10 dataset until overGLYPH<28>tting occurs in order to discriminate clusters clearly. The feature distributions are shown in Fig. 6. The feature distributions of GAP, GMP, and MPN have very large intra-class variation, and the features tend to gather near the origin. That is, their inter-class variation is low. However, the feature distribution of SVP shows better separation than those of GAP, GMP, and MPN. This means that SVP makes a given network operate more robustly to noise and attack. In addition, we show the evaluation results for ImageNet2012 through ResNet-18 of Fig. 5(c) to generalize the proposed method (see Table 2). We could observe a trend similar to the results in CIFAR10 dataset, which means that SVP extracts essential information well even with large feature maps.\nTABLE 2. The comparison of SVP with GAP for ImageNet2012 dataset.",
        "original_text": "Most CNNs increase the size of the receptive GLYPH<28>eld and reduce the size of the feature map through pooling. GMP and GAP are the most popular pooling methods, but both methods suffer from the inherent loss of information. SVP can improve the performance of a given network because it can reduce the size of feature maps while keeping as much of their information as possible.\nThe base network used for this experiment is ResNet32 [30] which described in Fig. 5(a). We analyzed the results by replacing GAP of ResNet-32 with GMP [18] and SVP, respectively. In case of CIFAR10, learning is proceeded for 200 epochs and the initial learning rate is equal to 10 GLYPH<0> 2 , which is reduced by 0.1 times at 100 and 150 epochs. The batch size is set to 128, and the weight decay of L 2 regularization is GLYPH<28>xed to 10 GLYPH<0> 4 . ImageNet2012 is trained for 90 epochs, and the initial learning rate is set to 10 GLYPH<0> 2 , and the learning rate is reduced by 0.1 times at 30 and 60 epochs. The batch size in this experiment is 128 and the weight decay of L 2 regularization is GLYPH<28>xed to 5 GLYPH<2> 10 GLYPH<0> 4 .\nTABLE 1. The comparison of the proposed SVP with GAP and GMP for CIFAR10 dataset. 'ACC' indicates accuracy, 'Silh' indicates silhouette score, and T indicates forward time for each pooling layer.\nTable 1 shows the experimental results for CIFAR10. We can GLYPH<28>nd the validation accuracy of SVP is 0.67% and 0.91% better than that of GAP and GMP, respectively. However, this is not a big improvement and almost same with MPN. Nevertheless, the silhouette score [33] of SVP on training dataset is better by 0.444, 0.510, and 0.551 than that of GAP, GMP, and MPN, respectively. SVP shows excellent silhouette scores that are more than twice those of the\nFIGURE 6. The distribution of feature vectors obtained by several pooling techniques for CIFAR10 training set.\nother pooling methods. This indicate that SVP makes feature vectors get a high inter-class variance and a low intra-class variance. Also, the proposed method has a lower forward time than MPN, which is a light-weight second-order pooling method. Therefore, we can GLYPH<28>nd that the proposed method can provide better clustering performance with less computation time.\nAlso, to illustrate the clustering performance clearly, we performed the same experiment through a network with an additional FC layer having a two-dimensional output as shown in Fig. 5(b). The network is learned for CIFAR10 dataset until overGLYPH<28>tting occurs in order to discriminate clusters clearly. The feature distributions are shown in Fig. 6. The feature distributions of GAP, GMP, and MPN have very large intra-class variation, and the features tend to gather near the origin. That is, their inter-class variation is low. However, the feature distribution of SVP shows better separation than those of GAP, GMP, and MPN. This means that SVP makes a given network operate more robustly to noise and attack. In addition, we show the evaluation results for ImageNet2012 through ResNet-18 of Fig. 5(c) to generalize the proposed method (see Table 2). We could observe a trend similar to the results in CIFAR10 dataset, which means that SVP extracts essential information well even with large feature maps.\nTABLE 2. The comparison of SVP with GAP for ImageNet2012 dataset.",
        "context": "Analyzes the impact of SVP on CIFAR10 and ImageNet2012 datasets, demonstrating improved accuracy and silhouette scores compared to GAP and GMP, while also exhibiting lower forward time.",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            6,
            7
        ],
        "id": "33456d172209fdb82cdd35e5857d51a327b14ebb670d68fc25f3c43e07fe4de2"
    },
    {
        "text": "Analyzes the robustness of SVP against adversarial attacks, specifically using FGSM and BIM to assess its performance on CIFAR10 with ResNet-32.  Demonstrates SVP’s superior accuracy compared to GAP, GMP, and MPN under both natural and adversarial training conditions, highlighting its robustness to noise.\n\nWe next examined the robustness of SVP against adversarial attacks. The attack methods include the fast gradient sign method (FGSM) [34] and basic iterative method (BIM) [35]. FGSM is a gradient-based adversarial attack and one of the simplest and most effective attack methods. The image X adv generated by FGSM is deGLYPH<28>ned by:\n<!-- formula-not-decoded -->\nwhere X is an input image, ytrue is the label, and J stands for the Jacobian. The attack rate GLYPH<11> is set to 0.01 in this experiment.\nSince BIM performs FGSM iteratively, BIM generally attacks more strongly than FGSM. The I -th image generated by BIM is deGLYPH<28>ned by Eqs. (18) and (19):\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere GLYPH<12> is the clipping threshold. The following values are used in this experiment: GLYPH<11> D 0 : 01, GLYPH<12> D 0 : 002, and T D 5. All hyper-parameters are set as in Sec. IV-B.\nTABLE 3. Experimental result on white-box adversarial attack. The first three rows correspond to natural training, and the second three rows are the result of adversarial training.\nTo compare the results of Sec. IV-B, the CIFAR10 dataset was employed in this experiment, using ResNet-32 in Fig. 5(a) and ResNet-32-plot in Fig. 5(b) Table 3 shows the white-box attack results for several pooling methods. For FGSM with natural and adversarial training, SVP shows 36.07%, 28.18%, and 34.21% higher accuracies than GAP, GMP, and MPN, respectively. Overall, SVP provides the best performance for all other cases. The performance improvement is interpreted through the high silhouette score of the feature vector obtained by SVP. Fig 7 plots the feature vector distributions by employing the network used in Fig. 6. In the other pooling methods, the clusters are scattered due to the attack, but SVP maintains a relatively strong inter-cluster\nFIGURE 7. The distribution of feature vectors obtained by several pooling techniques for an adversarial attacked CIFAR10 training set.\nTABLE 4. Experimental results on black-box adversarial attack. The first three rows represent the natural training, and the second three rows represent the adversarial training results.\ndistance. Furthermore, Fig. 8 shows the performance changes according to the attack rates. As the attack rate increases, the gaps between SVP and the other pooling methods increase. For example, when GLYPH<11> is 0.02, SVP shows 40.99%, 33.60%, and 40.62% better than GAP, GMP, and MPN, respectively.\nA black-box attack is a method of training and testing a target network using attacked data generated by an arbitrary network. The network used for attacking is VGG-16 [36], and the experimental results are shown in Table 4. SVP still outperforms GAP, GMP, and MPN, but the gap is somewhat reduced. This occurs because SVP is located at the end of the network and it is difGLYPH<28>cult for it to extract essential information from a feature map that has already been destroyed. The experiments with adversarial attacks prove that the proposed method is robust to noise. Despite the limitations, the proposed method makes the given network robust against noise without requiring additional techniques for singular vectors.",
        "original_text": "We next examined the robustness of SVP against adversarial attacks. The attack methods include the fast gradient sign method (FGSM) [34] and basic iterative method (BIM) [35]. FGSM is a gradient-based adversarial attack and one of the simplest and most effective attack methods. The image X adv generated by FGSM is deGLYPH<28>ned by:\n<!-- formula-not-decoded -->\nwhere X is an input image, ytrue is the label, and J stands for the Jacobian. The attack rate GLYPH<11> is set to 0.01 in this experiment.\nSince BIM performs FGSM iteratively, BIM generally attacks more strongly than FGSM. The I -th image generated by BIM is deGLYPH<28>ned by Eqs. (18) and (19):\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere GLYPH<12> is the clipping threshold. The following values are used in this experiment: GLYPH<11> D 0 : 01, GLYPH<12> D 0 : 002, and T D 5. All hyper-parameters are set as in Sec. IV-B.\nTABLE 3. Experimental result on white-box adversarial attack. The first three rows correspond to natural training, and the second three rows are the result of adversarial training.\nTo compare the results of Sec. IV-B, the CIFAR10 dataset was employed in this experiment, using ResNet-32 in Fig. 5(a) and ResNet-32-plot in Fig. 5(b) Table 3 shows the white-box attack results for several pooling methods. For FGSM with natural and adversarial training, SVP shows 36.07%, 28.18%, and 34.21% higher accuracies than GAP, GMP, and MPN, respectively. Overall, SVP provides the best performance for all other cases. The performance improvement is interpreted through the high silhouette score of the feature vector obtained by SVP. Fig 7 plots the feature vector distributions by employing the network used in Fig. 6. In the other pooling methods, the clusters are scattered due to the attack, but SVP maintains a relatively strong inter-cluster\nFIGURE 7. The distribution of feature vectors obtained by several pooling techniques for an adversarial attacked CIFAR10 training set.\nTABLE 4. Experimental results on black-box adversarial attack. The first three rows represent the natural training, and the second three rows represent the adversarial training results.\ndistance. Furthermore, Fig. 8 shows the performance changes according to the attack rates. As the attack rate increases, the gaps between SVP and the other pooling methods increase. For example, when GLYPH<11> is 0.02, SVP shows 40.99%, 33.60%, and 40.62% better than GAP, GMP, and MPN, respectively.\nA black-box attack is a method of training and testing a target network using attacked data generated by an arbitrary network. The network used for attacking is VGG-16 [36], and the experimental results are shown in Table 4. SVP still outperforms GAP, GMP, and MPN, but the gap is somewhat reduced. This occurs because SVP is located at the end of the network and it is difGLYPH<28>cult for it to extract essential information from a feature map that has already been destroyed. The experiments with adversarial attacks prove that the proposed method is robust to noise. Despite the limitations, the proposed method makes the given network robust against noise without requiring additional techniques for singular vectors.",
        "context": "Analyzes the robustness of SVP against adversarial attacks, specifically using FGSM and BIM to assess its performance on CIFAR10 with ResNet-32.  Demonstrates SVP’s superior accuracy compared to GAP, GMP, and MPN under both natural and adversarial training conditions, highlighting its robustness to noise.",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            8,
            7
        ],
        "id": "4b1097296e4f342c435ec2f7e4ac692fedb233cfec826260aa3bf380d0cb1371"
    },
    {
        "text": "Demonstrates how singular vectors are learned more effectively by KD-SVP, highlighting the method’s impact on student network performance as the number of singular vectors increases.\n\nThe feature vectors obtained by KD-SVP are easier to analyze than original singular vectors. In KD-SVD, the sign ambiguity is removed by aligning the singular vectors of the student network based on the singular vector of the teacher network. In addition, KD-SVD learns only similar features to learn the manifold features and deGLYPH<28>nes the correlation by a radial basis function (RBF), which has smaller values as the distance becomes larger. In other words, singular vectors\nFIGURE 8. The performance change according to the intensity of adversarial attack.\nare post-processed in a very naive manner in KD-SVD. Therefore, if SVP plays a post-processing role, the learning accuracy will improve. The correlation between feature vectors obtained by SVP is computed by RBF, similarly to KD-SVD.TheCIFAR100andTiny-ImageNet are used in this experiment. The teacher network is VGG-16, and the student network is a condensed network that uses only one convolution of the same depth as VGG-16, which are described in Fig. 5(e) and 5(f). KD-SVD and KD-SVP are all learned with the same hyper-parameters. Their learning progressed by 200 epochs, with an initial learning rate of 10 GLYPH<0> 2 and a reduction of 0.1 in 100 and 150 epochs. Also, the batch size is set to 128 and the weight decay of L 2 regularization is 10 GLYPH<0> 4 .\nTABLE 5. Performance of KD-SVP according to the number of singular vectors.\nTable 5 shows that when the proposed method is applied to KD-SVD, the performance is improved by 1.69% for CIFAR100 and 0.97% for Tiny-ImageNet. This experiment shows that singular vectors are learned much better by KD-SVP. Thus, the experiment demonstrates that the\nTABLE 6. Performance of KD-SVP according to the number of singular vectors.\nTABLE 7. Performance change whenever each step of SVP is removed.\nFIGURE 9. Performance comparison whenever removing each step of SVP. Red triangle means NaN.\napplication of SVP signiGLYPH<28>cantly improves the performance of a given network. Next, we analyze the performance of KD-SVP according to the number of used singular vectors. Table 6 shows that since more singular vectors give more information, they improve the performance of the student network.",
        "original_text": "The feature vectors obtained by KD-SVP are easier to analyze than original singular vectors. In KD-SVD, the sign ambiguity is removed by aligning the singular vectors of the student network based on the singular vector of the teacher network. In addition, KD-SVD learns only similar features to learn the manifold features and deGLYPH<28>nes the correlation by a radial basis function (RBF), which has smaller values as the distance becomes larger. In other words, singular vectors\nFIGURE 8. The performance change according to the intensity of adversarial attack.\nare post-processed in a very naive manner in KD-SVD. Therefore, if SVP plays a post-processing role, the learning accuracy will improve. The correlation between feature vectors obtained by SVP is computed by RBF, similarly to KD-SVD.TheCIFAR100andTiny-ImageNet are used in this experiment. The teacher network is VGG-16, and the student network is a condensed network that uses only one convolution of the same depth as VGG-16, which are described in Fig. 5(e) and 5(f). KD-SVD and KD-SVP are all learned with the same hyper-parameters. Their learning progressed by 200 epochs, with an initial learning rate of 10 GLYPH<0> 2 and a reduction of 0.1 in 100 and 150 epochs. Also, the batch size is set to 128 and the weight decay of L 2 regularization is 10 GLYPH<0> 4 .\nTABLE 5. Performance of KD-SVP according to the number of singular vectors.\nTable 5 shows that when the proposed method is applied to KD-SVD, the performance is improved by 1.69% for CIFAR100 and 0.97% for Tiny-ImageNet. This experiment shows that singular vectors are learned much better by KD-SVP. Thus, the experiment demonstrates that the\nTABLE 6. Performance of KD-SVP according to the number of singular vectors.\nTABLE 7. Performance change whenever each step of SVP is removed.\nFIGURE 9. Performance comparison whenever removing each step of SVP. Red triangle means NaN.\napplication of SVP signiGLYPH<28>cantly improves the performance of a given network. Next, we analyze the performance of KD-SVP according to the number of used singular vectors. Table 6 shows that since more singular vectors give more information, they improve the performance of the student network.",
        "context": "Demonstrates how singular vectors are learned more effectively by KD-SVP, highlighting the method’s impact on student network performance as the number of singular vectors increases.",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            8,
            9
        ],
        "id": "4ad93a66dfa9bc84888159ae9a90b07e99098a28137029112b4eda0dd9f3805e"
    },
    {
        "text": "Analysis of the impact of each component of the proposed SVP on network performance, demonstrating that omitting any step leads to a decrease in overall performance, highlighting the necessity of each step for effective singular vector learning.\n\nThe effect of each part of the proposed SVP on the overall performance of a given network is analyzed. SVP consists of rotation (step 1), removing sign ambiguity (step 2), and coordinate conversion (step 3). The model used for this experiment is ResNet-18, and CIFAR100 is chosen as the dataset. Wemeasuredthe performance changes by removing each part one by one. Table 7 shows the results, and Fig. 9 shows the training plots.\nOf course, whenever each step is omitted, the overall performance deteriorates. If step 1 is omitted (rotation), the sign ambiguity cannot be eliminated accurately because the singular vectors on the two half-spheres are not superimposed on each other. The performance decreases by about 1.46% because the error increases as the distance from the center of the coordinates deviates. However, the performance degradation is smaller than when omitting other steps. When the coordinate conversion step is removed, manifold features are not sufGLYPH<28>ciently learned, which resulted in performance degradation by 5.01%. This occurred because singular vectors that are deGLYPH<28>ned in non-Euclidean space are hard to learn by Euclidean geometry. Finally, if the sign ambiguity removal of step 2 is absent, learning is impossible. This experiment shows that each step of SVP is indispensable for proper learning of singular vectors.",
        "original_text": "The effect of each part of the proposed SVP on the overall performance of a given network is analyzed. SVP consists of rotation (step 1), removing sign ambiguity (step 2), and coordinate conversion (step 3). The model used for this experiment is ResNet-18, and CIFAR100 is chosen as the dataset. Wemeasuredthe performance changes by removing each part one by one. Table 7 shows the results, and Fig. 9 shows the training plots.\nOf course, whenever each step is omitted, the overall performance deteriorates. If step 1 is omitted (rotation), the sign ambiguity cannot be eliminated accurately because the singular vectors on the two half-spheres are not superimposed on each other. The performance decreases by about 1.46% because the error increases as the distance from the center of the coordinates deviates. However, the performance degradation is smaller than when omitting other steps. When the coordinate conversion step is removed, manifold features are not sufGLYPH<28>ciently learned, which resulted in performance degradation by 5.01%. This occurred because singular vectors that are deGLYPH<28>ned in non-Euclidean space are hard to learn by Euclidean geometry. Finally, if the sign ambiguity removal of step 2 is absent, learning is impossible. This experiment shows that each step of SVP is indispensable for proper learning of singular vectors.",
        "context": "Analysis of the impact of each component of the proposed SVP on network performance, demonstrating that omitting any step leads to a decrease in overall performance, highlighting the necessity of each step for effective singular vector learning.",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            9
        ],
        "id": "7f8eccd03d9917c715b580e59a70274cf9cf3c98461677fe6fd0aa6074778a08"
    },
    {
        "text": "Highlights the advantages of the proposed method, including its ability to leverage Euclidean geometry for CNN learning and scalability compared to other techniques, while acknowledging its higher computational complexity.\n\nOne advantage of the proposed method is that converting singular vectors into Euclidean space enables to use Euclidean geometry. The merit makes a generic CNN learn the singular vectors. As another advantage, the proposed SVP is superior to other techniques that treat singular vectors as manifold features in terms of scalability. SVP outperforms conventional pooling methods such as GAP and GMP, and also it is more robust against noise. In addition, SVP is experimentally proven to provide further performance enhancement when it is applied to an existing knowledge distillation scheme such as [17]. On the other hand, SVP has a disadvantage that its computational complexity is somewhat higher than that of general pooling methods. Also, SVD requires burdensome computations, so it may be difGLYPH<28>cult to apply it directly to an embedded system or a mobile environment.",
        "original_text": "One advantage of the proposed method is that converting singular vectors into Euclidean space enables to use Euclidean geometry. The merit makes a generic CNN learn the singular vectors. As another advantage, the proposed SVP is superior to other techniques that treat singular vectors as manifold features in terms of scalability. SVP outperforms conventional pooling methods such as GAP and GMP, and also it is more robust against noise. In addition, SVP is experimentally proven to provide further performance enhancement when it is applied to an existing knowledge distillation scheme such as [17]. On the other hand, SVP has a disadvantage that its computational complexity is somewhat higher than that of general pooling methods. Also, SVD requires burdensome computations, so it may be difGLYPH<28>cult to apply it directly to an embedded system or a mobile environment.",
        "context": "Highlights the advantages of the proposed method, including its ability to leverage Euclidean geometry for CNN learning and scalability compared to other techniques, while acknowledging its higher computational complexity.",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            9
        ],
        "id": "c935dde40e31dd09914acd77f8120dacf1ebdd556e807b16008ccd49b22cc8bd"
    },
    {
        "text": "Introduces the importance of SVD in deep learning and highlights the study’s contribution as a starting point for effectively utilizing singular vectors in CNNs.\n\nSVDisone of the most important techniques in many areas of high-level data manipulation. Therefore, if SVD can be used in deep learning methods such as CNN, it could be useful for various purposes. However, singular vectors decomposed by SVD have not been widely used in CNNs yet, as they are generally difGLYPH<28>cult to handle. This study presented a starting point for effectively using singular vectors with essential information in deep learning. Although the proposed SVP has a simple form, it is very useful because it is about 36% more robust to adversarial attacks than GAP and produces 1.69% more improvement in knowledge distillation than the naGLYPH<239>ve solution [12]. Future work should be done to improve the performance of CNNs further by devising ways of more effectively using the information of singular vectors.",
        "original_text": "SVDisone of the most important techniques in many areas of high-level data manipulation. Therefore, if SVD can be used in deep learning methods such as CNN, it could be useful for various purposes. However, singular vectors decomposed by SVD have not been widely used in CNNs yet, as they are generally difGLYPH<28>cult to handle. This study presented a starting point for effectively using singular vectors with essential information in deep learning. Although the proposed SVP has a simple form, it is very useful because it is about 36% more robust to adversarial attacks than GAP and produces 1.69% more improvement in knowledge distillation than the naGLYPH<239>ve solution [12]. Future work should be done to improve the performance of CNNs further by devising ways of more effectively using the information of singular vectors.",
        "context": "Introduces the importance of SVD in deep learning and highlights the study’s contribution as a starting point for effectively utilizing singular vectors in CNNs.",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            9
        ],
        "id": "273675854b71eb18e7ee710ed2c3006c09a73416d18c5410cc47473b7721e415"
    },
    {
        "text": "Introduces foundational research in singular value decomposition (SVD) and its applications, including genomics, efficient convolutional network evaluation, and matrix backpropagation for deep networks.\n\n- [1] O. Alter, P. O. Brown, and D. Botstein, ''Singular value decomposition for genome-wide expression data processing and modeling,'' Proc. Nat. Acad. Sci. USA , vol. 97, no. 18, pp. 10101GLYPH<21>10106, Aug. 2000.\n- [2] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus, ''Exploiting linear structure within convolutional networks for efGLYPH<28>cient evaluation,'' in Proc. Adv. Neural Inf. Process. Syst. , 2014, pp. 1269GLYPH<21>1277.\n- [3] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl, ''Incremental singular value decomposition algorithms for highly scalable recommender systems,'' in Proc. 5th Int. Conf. Comput. Inf. Sci. , 2002, pp. 27GLYPH<21>28.\n- [4] C. Ionescu, O. Vantzos, and C. Sminchisescu, ''Matrix backpropagation for deep networks with structured layers,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , Dec. 2015, pp. 2965GLYPH<21>2973.\n- [5] Z. Huang, J. Wu, and L. Van Gool, ''Building deep networks on Grassmann manifolds,'' 2016, arXiv:1611.05742 . [Online]. Available: \n- [6] R. Ranftl and V. Koltun, ''Deep fundamental matrix estimation,'' in Proc. Eur. Conf. Comput. Vis. (ECCV) , Sep. 2018, pp. 284GLYPH<21>299.",
        "original_text": "- [1] O. Alter, P. O. Brown, and D. Botstein, ''Singular value decomposition for genome-wide expression data processing and modeling,'' Proc. Nat. Acad. Sci. USA , vol. 97, no. 18, pp. 10101GLYPH<21>10106, Aug. 2000.\n- [2] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus, ''Exploiting linear structure within convolutional networks for efGLYPH<28>cient evaluation,'' in Proc. Adv. Neural Inf. Process. Syst. , 2014, pp. 1269GLYPH<21>1277.\n- [3] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl, ''Incremental singular value decomposition algorithms for highly scalable recommender systems,'' in Proc. 5th Int. Conf. Comput. Inf. Sci. , 2002, pp. 27GLYPH<21>28.\n- [4] C. Ionescu, O. Vantzos, and C. Sminchisescu, ''Matrix backpropagation for deep networks with structured layers,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , Dec. 2015, pp. 2965GLYPH<21>2973.\n- [5] Z. Huang, J. Wu, and L. Van Gool, ''Building deep networks on Grassmann manifolds,'' 2016, arXiv:1611.05742 . [Online]. Available: \n- [6] R. Ranftl and V. Koltun, ''Deep fundamental matrix estimation,'' in Proc. Eur. Conf. Comput. Vis. (ECCV) , Sep. 2018, pp. 284GLYPH<21>299.",
        "context": "Introduces foundational research in singular value decomposition (SVD) and its applications, including genomics, efficient convolutional network evaluation, and matrix backpropagation for deep networks.",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            9
        ],
        "id": "76747e2d591af96a865b45c30b8daf936399ffbb34c26a6eaa493fae91f880be"
    },
    {
        "text": "Provides a collection of research papers exploring various techniques for image processing and deep learning, specifically focusing on methods related to singular value decomposition and its applications in areas like image denoising, feature extraction, and knowledge distillation.\n\n- [7] S. Suwajanakorn, N. Snavely, J. J. Tompson, and M. Norouzi, ''Discovery of latent 3d keypoints via end-to-end geometric reasoning,'' in Proc. Adv. Neural Inf. Process. Syst. , 2018, pp. 2059GLYPH<21>2070.\n- [8] P. Li, J. Xie, Q. Wang, and Z. Gao, ''Towards faster training of global covariance pooling networks by iterative matrix square root normalization,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , Jun. 2018, pp. 947GLYPH<21>955.\n- [9] Q. Wang, P. Li, and L. Zhang, ''G2DeNet: Global Gaussian distribution embedding network and its application to visual recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jul. 2017, pp. 2730GLYPH<21>2739.\n- [10] X. Wei, Y. Zhang, Y. Gong, J. Zhang, and N. Zheng, ''Grassmann pooling as compact homogeneous bilinear pooling for GLYPH<28>ne-grained visual classiGLYPH<28>cation,'' in Proc. Eur. Conf. Comput. Vis. (ECCV) , Sep. 2018, pp. 355GLYPH<21>370.\n- [11] R. Bro, E. Acar, and T. G. Kolda, ''Resolving the sign ambiguity in the singular value decomposition,'' J. Chemometrics, A J. Chemometrics Soc. , vol. 22, no. 2, pp. 135GLYPH<21>140, 2008.\n- [12] S. H. Lee, D. H. Kim, and B. C. Song, ''Self-supervised knowledge distillation using singular value decomposition,'' in Proc. Eur. Conf. Comput. Vis. Springer, 2018, pp. 339GLYPH<21>354.\n- [13] L. Song, B. Du, L. Zhang, L. Zhang, J. Wu, and X. Li, ''Nonlocal patch based T-SVD for image inpainting: Algorithm and error analysis,'' in Proc. 32nd AAAI Conf. Artif. Intell. , 2018, pp. 2419GLYPH<21>2426.\n- [14] T.-Y. Lin and S. Maji, ''Improved bilinear pooling with CNNs,'' 2017, arXiv:1707.06772 . [Online]. Available: \n- [15] Z. Gao, J. Xie, Q. Wang, and P. Li, ''Global second-order pooling convolutional networks,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2019, pp. 3024GLYPH<21>3033.\n- [16] M. Lin, Q. Chen, and S. Yan, ''Network in network,'' 2013, arXiv:1312.4400 . [Online]. Available: \n- [17] X. Zhang and X. Zhang, ''Global learnable pooling with enhancing distinctive feature for image classiGLYPH<28>cation,'' IEEE Access , vol. 8, pp. 98539GLYPH<21>98547, 2020.\n- [18] H. Azizpour, A. S. Razavian, J. Sullivan, A. Maki, and S. Carlsson, ''From generic to speciGLYPH<28>c deep representations for visual recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW) , Jun. 2015, pp. 36GLYPH<21>45.\n- [19] B. Zhang, Q. Zhao, W. Feng, and S. Lyu, ''AlphaMEX: A smarter global pooling method for convolutional neural networks,'' Neurocomputing , vol. 321, pp. 36GLYPH<21>48, Dec. 2018.\n- [20] T.-Y. Lin, A. RoyChowdhury, and S. Maji, ''Bilinear CNN models for GLYPH<28>negrained visual recognition,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , Dec. 2015, pp. 1449GLYPH<21>1457.\n- [21] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, ''Bm3D image denoising with shape-adaptive principal component analysis,'' Tech. Rep., 2009.\n- [22] L. Hazelhoff, J. Han, and P. H. N. de With, ''Video-based fall detection in the home using principal component analysis,'' in Proc. Int. Conf. Adv. Concepts Intell. Vis. Syst. , Springer, 2008, pp. 298GLYPH<21>309.\n- [23] L. E. Blumenson, ''A derivation of n-dimensional spherical coordinates,'' Amer. Math. Monthly , vol. 67, no. 1, pp. 63GLYPH<21>66, Jan. 1960. [Online]. Available: \n- [24] S. Ioffe and C. Szegedy, ''Batch normalization: Accelerating deep network training by reducing internal covariate shift,'' in Int. Conf. Mach. Learn. , 2015, pp. 448GLYPH<21>456.\n- [25] S. Kullback and R. A. Leibler, ''On information and sufGLYPH<28>ciency,'' Ann. Math. Statist. , vol. 22, no. 1, pp. 79GLYPH<21>86, 1951.\n- [26] A. Krizhevsky and G. Hinton, ''Learning multiple layers of features from tiny images,'' Citeseer, Tech. Rep., 2009.\n- [27] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ''ImageNet classiGLYPH<28>cation with deep convolutional neural networks,'' in Proc. Adv. Neural Inf. Process. Syst. , 2012, pp. 1097GLYPH<21>1105.\n- [28] M. Abadi et al. , ''TensorFlow: A system for large-scale machine learning,'' in Proc. OSDI , vol. 16. 2016, pp. 265GLYPH<21>283.\n- [29] V. Nair and G. E. Hinton, ''RectiGLYPH<28>ed linear units improve restricted Boltzmann machines,'' in Proc. 27th Int. Conf. Mach. Learn. (ICML) , 2010, pp. 807GLYPH<21>814.\n- [30] K. He, X. Zhang, S. Ren, and J. Sun, ''Deep residual learning for image recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2016, pp. 770GLYPH<21>778.\n- [31] J. Kiefer and J. Wolfowitz, ''Stochastic estimation of the maximum of a regression function,'' Ann. Math. Statist. , vol. 23, no. 3, pp. 462GLYPH<21>466, Sep. 1952.\n- [32] Y. Nesterov, ''A method for unconstrained convex minimization problem with the rate of convergence o (1/k ^ 2),'' in Proc. Doklady AN USSR , vol. 269, 1983, pp. 543GLYPH<21>547.\n- [33] P. J. Rousseeuw, ''Silhouettes: A graphical aid to the interpretation and validation of cluster analysis,'' J. Comput. Appl. Math. , vol. 20, pp. 53GLYPH<21>65, Nov. 1987.",
        "original_text": "- [7] S. Suwajanakorn, N. Snavely, J. J. Tompson, and M. Norouzi, ''Discovery of latent 3d keypoints via end-to-end geometric reasoning,'' in Proc. Adv. Neural Inf. Process. Syst. , 2018, pp. 2059GLYPH<21>2070.\n- [8] P. Li, J. Xie, Q. Wang, and Z. Gao, ''Towards faster training of global covariance pooling networks by iterative matrix square root normalization,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , Jun. 2018, pp. 947GLYPH<21>955.\n- [9] Q. Wang, P. Li, and L. Zhang, ''G2DeNet: Global Gaussian distribution embedding network and its application to visual recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jul. 2017, pp. 2730GLYPH<21>2739.\n- [10] X. Wei, Y. Zhang, Y. Gong, J. Zhang, and N. Zheng, ''Grassmann pooling as compact homogeneous bilinear pooling for GLYPH<28>ne-grained visual classiGLYPH<28>cation,'' in Proc. Eur. Conf. Comput. Vis. (ECCV) , Sep. 2018, pp. 355GLYPH<21>370.\n- [11] R. Bro, E. Acar, and T. G. Kolda, ''Resolving the sign ambiguity in the singular value decomposition,'' J. Chemometrics, A J. Chemometrics Soc. , vol. 22, no. 2, pp. 135GLYPH<21>140, 2008.\n- [12] S. H. Lee, D. H. Kim, and B. C. Song, ''Self-supervised knowledge distillation using singular value decomposition,'' in Proc. Eur. Conf. Comput. Vis. Springer, 2018, pp. 339GLYPH<21>354.\n- [13] L. Song, B. Du, L. Zhang, L. Zhang, J. Wu, and X. Li, ''Nonlocal patch based T-SVD for image inpainting: Algorithm and error analysis,'' in Proc. 32nd AAAI Conf. Artif. Intell. , 2018, pp. 2419GLYPH<21>2426.\n- [14] T.-Y. Lin and S. Maji, ''Improved bilinear pooling with CNNs,'' 2017, arXiv:1707.06772 . [Online]. Available: \n- [15] Z. Gao, J. Xie, Q. Wang, and P. Li, ''Global second-order pooling convolutional networks,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2019, pp. 3024GLYPH<21>3033.\n- [16] M. Lin, Q. Chen, and S. Yan, ''Network in network,'' 2013, arXiv:1312.4400 . [Online]. Available: \n- [17] X. Zhang and X. Zhang, ''Global learnable pooling with enhancing distinctive feature for image classiGLYPH<28>cation,'' IEEE Access , vol. 8, pp. 98539GLYPH<21>98547, 2020.\n- [18] H. Azizpour, A. S. Razavian, J. Sullivan, A. Maki, and S. Carlsson, ''From generic to speciGLYPH<28>c deep representations for visual recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW) , Jun. 2015, pp. 36GLYPH<21>45.\n- [19] B. Zhang, Q. Zhao, W. Feng, and S. Lyu, ''AlphaMEX: A smarter global pooling method for convolutional neural networks,'' Neurocomputing , vol. 321, pp. 36GLYPH<21>48, Dec. 2018.\n- [20] T.-Y. Lin, A. RoyChowdhury, and S. Maji, ''Bilinear CNN models for GLYPH<28>negrained visual recognition,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , Dec. 2015, pp. 1449GLYPH<21>1457.\n- [21] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, ''Bm3D image denoising with shape-adaptive principal component analysis,'' Tech. Rep., 2009.\n- [22] L. Hazelhoff, J. Han, and P. H. N. de With, ''Video-based fall detection in the home using principal component analysis,'' in Proc. Int. Conf. Adv. Concepts Intell. Vis. Syst. , Springer, 2008, pp. 298GLYPH<21>309.\n- [23] L. E. Blumenson, ''A derivation of n-dimensional spherical coordinates,'' Amer. Math. Monthly , vol. 67, no. 1, pp. 63GLYPH<21>66, Jan. 1960. [Online]. Available: \n- [24] S. Ioffe and C. Szegedy, ''Batch normalization: Accelerating deep network training by reducing internal covariate shift,'' in Int. Conf. Mach. Learn. , 2015, pp. 448GLYPH<21>456.\n- [25] S. Kullback and R. A. Leibler, ''On information and sufGLYPH<28>ciency,'' Ann. Math. Statist. , vol. 22, no. 1, pp. 79GLYPH<21>86, 1951.\n- [26] A. Krizhevsky and G. Hinton, ''Learning multiple layers of features from tiny images,'' Citeseer, Tech. Rep., 2009.\n- [27] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ''ImageNet classiGLYPH<28>cation with deep convolutional neural networks,'' in Proc. Adv. Neural Inf. Process. Syst. , 2012, pp. 1097GLYPH<21>1105.\n- [28] M. Abadi et al. , ''TensorFlow: A system for large-scale machine learning,'' in Proc. OSDI , vol. 16. 2016, pp. 265GLYPH<21>283.\n- [29] V. Nair and G. E. Hinton, ''RectiGLYPH<28>ed linear units improve restricted Boltzmann machines,'' in Proc. 27th Int. Conf. Mach. Learn. (ICML) , 2010, pp. 807GLYPH<21>814.\n- [30] K. He, X. Zhang, S. Ren, and J. Sun, ''Deep residual learning for image recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2016, pp. 770GLYPH<21>778.\n- [31] J. Kiefer and J. Wolfowitz, ''Stochastic estimation of the maximum of a regression function,'' Ann. Math. Statist. , vol. 23, no. 3, pp. 462GLYPH<21>466, Sep. 1952.\n- [32] Y. Nesterov, ''A method for unconstrained convex minimization problem with the rate of convergence o (1/k ^ 2),'' in Proc. Doklady AN USSR , vol. 269, 1983, pp. 543GLYPH<21>547.\n- [33] P. J. Rousseeuw, ''Silhouettes: A graphical aid to the interpretation and validation of cluster analysis,'' J. Comput. Appl. Math. , vol. 20, pp. 53GLYPH<21>65, Nov. 1987.",
        "context": "Provides a collection of research papers exploring various techniques for image processing and deep learning, specifically focusing on methods related to singular value decomposition and its applications in areas like image denoising, feature extraction, and knowledge distillation.",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            10
        ],
        "id": "3ccffb27b8dd9889859681163668aa92efd04db9ac5df2a0c8a274b01e5d1782"
    },
    {
        "text": "Introduces the concept of adversarial examples and their study.\n\n- [34] I. J. Goodfellow, J. Shlens, and C. Szegedy, ''Explaining and harnessing adversarial examples,'' 2014, arXiv:1412.6572 . [Online]. Available: \n- [35] A. Kurakin, I. Goodfellow, and S. Bengio, ''Adversarial examples in the physical world,'' 2016, arXiv:1607.02533 . [Online]. Available: \n- [36] K. Simonyan and A. Zisserman, ''Very deep convolutional networks for large-scale image recognition,'' 2014, arXiv:1409.1556 . [Online]. Available: \nSEUNGHYUN LEE (Associate Member, IEEE) received the B.S. degree in electronic engineering from Inha University, Incheon, South Korea, in 2017, where he is currently pursuing the Ph.D. degree. His research interests include computer vision and machine learning.\nBYUNG CHEOL SONG (Senior Member, IEEE) received the B.S., M.S., and Ph.D. degrees in electrical engineering from the Korea Advanced Institute of Science and Technology, Daejeon, South Korea, in 1994, 1996, and 2001, respectively. From 2001 to 2008, he was a Senior Engineer with the Digital Media Research and Development Center, Samsung Electronics Company Ltd., Suwon, South Korea. He joined the Department of Electronic Engineering, Inha University, Incheon,\nSouth Korea, in 2008, where he is currently a Professor. His research interests include general areas of image processing and computer vision.",
        "original_text": "- [34] I. J. Goodfellow, J. Shlens, and C. Szegedy, ''Explaining and harnessing adversarial examples,'' 2014, arXiv:1412.6572 . [Online]. Available: \n- [35] A. Kurakin, I. Goodfellow, and S. Bengio, ''Adversarial examples in the physical world,'' 2016, arXiv:1607.02533 . [Online]. Available: \n- [36] K. Simonyan and A. Zisserman, ''Very deep convolutional networks for large-scale image recognition,'' 2014, arXiv:1409.1556 . [Online]. Available: \nSEUNGHYUN LEE (Associate Member, IEEE) received the B.S. degree in electronic engineering from Inha University, Incheon, South Korea, in 2017, where he is currently pursuing the Ph.D. degree. His research interests include computer vision and machine learning.\nBYUNG CHEOL SONG (Senior Member, IEEE) received the B.S., M.S., and Ph.D. degrees in electrical engineering from the Korea Advanced Institute of Science and Technology, Daejeon, South Korea, in 1994, 1996, and 2001, respectively. From 2001 to 2008, he was a Senior Engineer with the Digital Media Research and Development Center, Samsung Electronics Company Ltd., Suwon, South Korea. He joined the Department of Electronic Engineering, Inha University, Incheon,\nSouth Korea, in 2008, where he is currently a Professor. His research interests include general areas of image processing and computer vision.",
        "context": "Introduces the concept of adversarial examples and their study.",
        "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
        "pages": [
            10
        ],
        "id": "734330b411b838aa0cb5aac0c31af453ec3fb2d68998ac6227069762cdb9d4a0"
    }
]