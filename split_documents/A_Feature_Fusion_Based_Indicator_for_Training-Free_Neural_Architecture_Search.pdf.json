[
    {
        "text": "Introduces the context of the paper: a study of training-free neural architecture search indicators, focusing on improving upon previous methods that relied on single indicators.\n\nReceived August 2, 2021, accepted September 16, 2021, date of publication September 27, 2021, date of current version October 6, 2021.\nDigital Object Identifier 10.1 109/ACCESS.2021.31 1591 1",
        "original_text": "Received August 2, 2021, accepted September 16, 2021, date of publication September 27, 2021, date of current version October 6, 2021.\nDigital Object Identifier 10.1 109/ACCESS.2021.31 1591 1",
        "context": "Introduces the context of the paper: a study of training-free neural architecture search indicators, focusing on improving upon previous methods that relied on single indicators.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            1
        ],
        "id": "eef640352621390927f15e0344fc031b0d8eaf10d5d13cafa32dac406b49703c"
    },
    {
        "text": "This section details the paper's funding and affiliation, and introduces the core research: a review of indicators for training-free neural architecture search, highlighting the limitations of single-indicator approaches and motivating the development of a fusion indicator.\n\nDepartment of Computer Science and Engineering, Kyung Hee University, Yongin 17104, Republic of Korea\nCorresponding author: Sung-Ho Bae (shbae@khu.ac.kr)\nThis work was supported by the Technology Innovation Program or the Industrial Strategic Technology Development Program funded by the Ministry of Trade, Industry and Energy (MOTIE), South Korea (Memristor Fault-Aware Neuromorphic System for 3D Memristor Array) under Grant 10085646.\nABSTRACT Neural Architecture Search Without Training (NASWOT) has been proposed recently to replace the conventional Neural Architecture Search (NAS). Pioneer works only deploy one or two indicator(s) to search. Nevertheless, the quantitative assessment for indicators is not fully studied and evaluated. In this paper, we GLYPH<28>rst review several indicators, which are used to evaluate the network in a training-free manner, including the correlation of Jacobian, the output sensitivity, the number of linear regions, and the condition number of the neural tangent kernel. Our observation is that each indicator is responsible for characterizing a network in a speciGLYPH<28>c aspect and there is no single indicator that achieves good performance in all cases, e.g. highly correlated with the test accuracy. This motivated us to develop a novel indicator where all properties of a network are taken into account. To obtain better indicator that can consider various characteristics of networks in a harmonized form, we propose a Fusion Indicator (FI). SpeciGLYPH<28>cally, the proposed FI is formed by combining multiple indicators in a weighted sum manner. We minimize the mean squared error loss between the predicted and actual accuracy of networks to acquire the weights. Moreover, as the conventional training-free NAS researches used limited metrics to evaluate the quality of indicators, we introduce more desirable metrics that can evaluate the quality of training-free NAS indicator in terms of GLYPH<28>delity, correlation and rank-order similarity between the predicted quality value and actual accuracy of networks. That is, we introduce the Pearson Linear CoefGLYPH<28>cient Correlation (PLCC), the Root Mean Square Error (RMSE), the Spearman Rank-Order Correlation CoefGLYPH<28>cient (SROCC), and Kendall Rank-Order Correlation CoefGLYPH<28>cient (KROCC). Extensive experiments on NAS-Bench-101 and NAS-Bench201 demonstrate the effectiveness of our FI, outperforming existing methods by a large margin.\nINDEX TERMS Neural architecture search, training-free neural architecture search, fusion indicator, evaluation metrics.",
        "original_text": "Department of Computer Science and Engineering, Kyung Hee University, Yongin 17104, Republic of Korea\nCorresponding author: Sung-Ho Bae (shbae@khu.ac.kr)\nThis work was supported by the Technology Innovation Program or the Industrial Strategic Technology Development Program funded by the Ministry of Trade, Industry and Energy (MOTIE), South Korea (Memristor Fault-Aware Neuromorphic System for 3D Memristor Array) under Grant 10085646.\nABSTRACT Neural Architecture Search Without Training (NASWOT) has been proposed recently to replace the conventional Neural Architecture Search (NAS). Pioneer works only deploy one or two indicator(s) to search. Nevertheless, the quantitative assessment for indicators is not fully studied and evaluated. In this paper, we GLYPH<28>rst review several indicators, which are used to evaluate the network in a training-free manner, including the correlation of Jacobian, the output sensitivity, the number of linear regions, and the condition number of the neural tangent kernel. Our observation is that each indicator is responsible for characterizing a network in a speciGLYPH<28>c aspect and there is no single indicator that achieves good performance in all cases, e.g. highly correlated with the test accuracy. This motivated us to develop a novel indicator where all properties of a network are taken into account. To obtain better indicator that can consider various characteristics of networks in a harmonized form, we propose a Fusion Indicator (FI). SpeciGLYPH<28>cally, the proposed FI is formed by combining multiple indicators in a weighted sum manner. We minimize the mean squared error loss between the predicted and actual accuracy of networks to acquire the weights. Moreover, as the conventional training-free NAS researches used limited metrics to evaluate the quality of indicators, we introduce more desirable metrics that can evaluate the quality of training-free NAS indicator in terms of GLYPH<28>delity, correlation and rank-order similarity between the predicted quality value and actual accuracy of networks. That is, we introduce the Pearson Linear CoefGLYPH<28>cient Correlation (PLCC), the Root Mean Square Error (RMSE), the Spearman Rank-Order Correlation CoefGLYPH<28>cient (SROCC), and Kendall Rank-Order Correlation CoefGLYPH<28>cient (KROCC). Extensive experiments on NAS-Bench-101 and NAS-Bench201 demonstrate the effectiveness of our FI, outperforming existing methods by a large margin.\nINDEX TERMS Neural architecture search, training-free neural architecture search, fusion indicator, evaluation metrics.",
        "context": "This section details the paper's funding and affiliation, and introduces the core research: a review of indicators for training-free neural architecture search, highlighting the limitations of single-indicator approaches and motivating the development of a fusion indicator.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            1
        ],
        "id": "3a20bec0f0c0a43137a4edcdf0312d663f5f24b9040f95b11d5c4a76f39dac71"
    },
    {
        "text": "Introduces the concept of Neural Architecture Search (NAS) and its limitations, specifically the high search cost and difficulty in applying it to changing environments.\n\nDeep neural networks (DNNs) have shown remarkable performance on various computer vision tasks. Since the success of AlexNet on ImageNet [1] classiGLYPH<28>cation task in 2012 [2], many high-performance networks have been introduced [3]GLYPH<21>[5] where these networks have been designed by experts. However, manual design is not an optimal choice especially when the network goes deeper. Moreover, the process for designing these networks requires immense time and effort. To reduce the cost of designing the network, researchers have studied to automate the process, leading to Neural Architecture Search (NAS). Instead of designing the architecture, experts design the search algorithms that GLYPH<28>nd good candidates (e.g., the number of layers, GLYPH<28>lters and types of activation, etc.) on a given search space.\nThe associate editor coordinating the review of this manuscript and approving it for publication was Danilo Pelusi .\nNAS is able to discover superior architecture on various computer vision tasks such as image classiGLYPH<28>cation [6]GLYPH<21>[8], object detection [9], [10]. However, it suffers from several limitations. Firstly, the search cost is extremely high. It takes years of reinforcement learning (RL) [6] to search for networks which achieve state-of-the-art accuracy on largescale dataset such as ImageNet. The most expensive part in RL approach is the training from scratch of child networks. To alleviate this limitation, subsequent works have suggested to search on smaller conGLYPH<28>guration (e.g., cell) [11], performed with shared weights [12], search on a continuous search space for NAS [7], or incorporate Bi-layer parallel training [13].\nSecondly, there has a barrier to apply NAS to practical applications. That is, we need to search for a new network when the environment (e.g target task or hardware) is changed. For example, the architecture found on this dataset may not work well on others [14], or the latency for a same network is different when being executed on different hardware devices [15]. Moreover, the budget for searching is still high (e.g hours of GPU).\nIn order to search for best architecture in a short period time (e.g., a few minutes), NAS without training (NASWOT) [16] is introduced. SpeciGLYPH<28>cally, NASWOT proposed a trainingfree indicator that predicts the score which is highly correlated with the actual accuracy of a network, guiding the search process. Because the step to calculate this indicator does not require any training of networks, the total cost for NAS reduces signiGLYPH<28>cantly. With its simplicity, they have GLYPH<28>rst demonstrated the possibility of performing NAS without involving any training.\nMeanwhile, there have been growing works on deep learning theory that enables us to understand the behavior of DNNs [17]GLYPH<21>[23]. TE-NAS [24] has made the GLYPH<28>rst attempt to apply the indicators derived from theoretical works for revealing network characteristics, namely trainability and the expressivity, for training-free NAS. Different from [16], the search algorithm of TE-NAS is inspired by pruning-fromscratch.\nTraining-free NAS can replace the conventional NAS algorithms. However, prior works suffer from several limitations. [16] only uses one indicator, namely correlation Jacobian of a batch of images augmented with Cutout [25], by taking one characteristic (robustness to the input perturbation) of the network into account. As [16] uses only one indicator, it cannot represent other aspects of the network such as trainability or expressivity. So, it turns out to have limited performance in correlation between predicted score and actual accuracy. Reference [24] deals with this problem by using two indicators. However, the score (criterion for selecting the best network) is calculated with the sum of two ranks from each indicator, meaning that the two indicators have the same importance. However, this does not guarantee the optimal solution because different indicators may have different behaviors and contributions to the GLYPH<28>nal scores in different importance.\nTo solve this problem, this paper investigate several training-free indicators and harmonizes them in a fusion framework. The FI is capable of measuring the performance of networks under various aspects. Instead of treating all indicators equally, we propose a simple training approach to GLYPH<28>nd the appropriate weights for each indicator. The main contributions of the paper can be summarized as follows V\n- GLYPH<15> We GLYPH<28>rst collect and analyze several indicators that can be used to estimate the network's performance without training. The collected four indicators are the correlation of Jacobian (CJ), output sensitivity (OS), condition number of Neural Tangent Kernel (CNNTK), and the number of Linear Regions (NLR).\n- GLYPH<15> We propose a FI which is a combination of output from multiple indicators with learned weights. The proposed indicator beneGLYPH<28>ts from various properties of a network such as trainability, expressivity, generalibility and robustness against perturbation.\n- GLYPH<15> We introduce new quantitative metrics to measure goodness of indicators for training-free NAS.\nThe rest of the paper is organized as follows. Section II introduces several related works. Section III presents the FI. Section IV demonstrates the experimental results. Section V is the conclusion of the paper.",
        "original_text": "Deep neural networks (DNNs) have shown remarkable performance on various computer vision tasks. Since the success of AlexNet on ImageNet [1] classiGLYPH<28>cation task in 2012 [2], many high-performance networks have been introduced [3]GLYPH<21>[5] where these networks have been designed by experts. However, manual design is not an optimal choice especially when the network goes deeper. Moreover, the process for designing these networks requires immense time and effort. To reduce the cost of designing the network, researchers have studied to automate the process, leading to Neural Architecture Search (NAS). Instead of designing the architecture, experts design the search algorithms that GLYPH<28>nd good candidates (e.g., the number of layers, GLYPH<28>lters and types of activation, etc.) on a given search space.\nThe associate editor coordinating the review of this manuscript and approving it for publication was Danilo Pelusi .\nNAS is able to discover superior architecture on various computer vision tasks such as image classiGLYPH<28>cation [6]GLYPH<21>[8], object detection [9], [10]. However, it suffers from several limitations. Firstly, the search cost is extremely high. It takes years of reinforcement learning (RL) [6] to search for networks which achieve state-of-the-art accuracy on largescale dataset such as ImageNet. The most expensive part in RL approach is the training from scratch of child networks. To alleviate this limitation, subsequent works have suggested to search on smaller conGLYPH<28>guration (e.g., cell) [11], performed with shared weights [12], search on a continuous search space for NAS [7], or incorporate Bi-layer parallel training [13].\nSecondly, there has a barrier to apply NAS to practical applications. That is, we need to search for a new network when the environment (e.g target task or hardware) is changed. For example, the architecture found on this dataset may not work well on others [14], or the latency for a same network is different when being executed on different hardware devices [15]. Moreover, the budget for searching is still high (e.g hours of GPU).\nIn order to search for best architecture in a short period time (e.g., a few minutes), NAS without training (NASWOT) [16] is introduced. SpeciGLYPH<28>cally, NASWOT proposed a trainingfree indicator that predicts the score which is highly correlated with the actual accuracy of a network, guiding the search process. Because the step to calculate this indicator does not require any training of networks, the total cost for NAS reduces signiGLYPH<28>cantly. With its simplicity, they have GLYPH<28>rst demonstrated the possibility of performing NAS without involving any training.\nMeanwhile, there have been growing works on deep learning theory that enables us to understand the behavior of DNNs [17]GLYPH<21>[23]. TE-NAS [24] has made the GLYPH<28>rst attempt to apply the indicators derived from theoretical works for revealing network characteristics, namely trainability and the expressivity, for training-free NAS. Different from [16], the search algorithm of TE-NAS is inspired by pruning-fromscratch.\nTraining-free NAS can replace the conventional NAS algorithms. However, prior works suffer from several limitations. [16] only uses one indicator, namely correlation Jacobian of a batch of images augmented with Cutout [25], by taking one characteristic (robustness to the input perturbation) of the network into account. As [16] uses only one indicator, it cannot represent other aspects of the network such as trainability or expressivity. So, it turns out to have limited performance in correlation between predicted score and actual accuracy. Reference [24] deals with this problem by using two indicators. However, the score (criterion for selecting the best network) is calculated with the sum of two ranks from each indicator, meaning that the two indicators have the same importance. However, this does not guarantee the optimal solution because different indicators may have different behaviors and contributions to the GLYPH<28>nal scores in different importance.\nTo solve this problem, this paper investigate several training-free indicators and harmonizes them in a fusion framework. The FI is capable of measuring the performance of networks under various aspects. Instead of treating all indicators equally, we propose a simple training approach to GLYPH<28>nd the appropriate weights for each indicator. The main contributions of the paper can be summarized as follows V\n- GLYPH<15> We GLYPH<28>rst collect and analyze several indicators that can be used to estimate the network's performance without training. The collected four indicators are the correlation of Jacobian (CJ), output sensitivity (OS), condition number of Neural Tangent Kernel (CNNTK), and the number of Linear Regions (NLR).\n- GLYPH<15> We propose a FI which is a combination of output from multiple indicators with learned weights. The proposed indicator beneGLYPH<28>ts from various properties of a network such as trainability, expressivity, generalibility and robustness against perturbation.\n- GLYPH<15> We introduce new quantitative metrics to measure goodness of indicators for training-free NAS.\nThe rest of the paper is organized as follows. Section II introduces several related works. Section III presents the FI. Section IV demonstrates the experimental results. Section V is the conclusion of the paper.",
        "context": "Introduces the concept of Neural Architecture Search (NAS) and its limitations, specifically the high search cost and difficulty in applying it to changing environments.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            1,
            2
        ],
        "id": "57d2720cfd1b80e36eb5ede1aa5f2016b951d05a4c3423cd9ea426e7ed1e6cac"
    },
    {
        "text": "Summarizes the initial challenges and approaches to NAS, highlighting the resource-intensive nature of early RL-based methods and the emergence of cell-based and differentiable NAS techniques.\n\nNeural architecture search (NAS) has attracted much attentions nowadays because of the ability to discover superior architectures automatically. However, most NAS algorithms require a huge amount of resources.\nThe earliest works on NAS were based on reinforcement learning [6], [11], [26], [27]. In [6], a controller was trained to generate the network's conGLYPH<28>guration which were used to construct the network. The exhaustive training and evaluation of child-network and the macro search (e.g searching the entire network) made this method unaffordable for practical applications. Particularly, the method in [6] used 800 GPUs and GLYPH<28>nished the searching phase in 28 days. To deal with this limitation, [11] searched for cells (i.e., normal cell and reduction cell). These were stacked to build the complete network. The method achieved 11.2x less search cost than [6]. Another type of NAS algorithm was based on evolutionary algorithms [28]GLYPH<21>[30], reduced the search cost to weeks of GPUs.\nMany attempts have been made to perform NAS just in a few hours [7], [12], [14], [31]. Especially ENAS [12] allowed sharing the weights among candidates. Thus, the most expensive part in the searching phase was eliminated. DARTS [7] proposed to search in a differentiable manner. During searching, a supernet was trained and the network was obtained by removing operators which has a low weight.",
        "original_text": "Neural architecture search (NAS) has attracted much attentions nowadays because of the ability to discover superior architectures automatically. However, most NAS algorithms require a huge amount of resources.\nThe earliest works on NAS were based on reinforcement learning [6], [11], [26], [27]. In [6], a controller was trained to generate the network's conGLYPH<28>guration which were used to construct the network. The exhaustive training and evaluation of child-network and the macro search (e.g searching the entire network) made this method unaffordable for practical applications. Particularly, the method in [6] used 800 GPUs and GLYPH<28>nished the searching phase in 28 days. To deal with this limitation, [11] searched for cells (i.e., normal cell and reduction cell). These were stacked to build the complete network. The method achieved 11.2x less search cost than [6]. Another type of NAS algorithm was based on evolutionary algorithms [28]GLYPH<21>[30], reduced the search cost to weeks of GPUs.\nMany attempts have been made to perform NAS just in a few hours [7], [12], [14], [31]. Especially ENAS [12] allowed sharing the weights among candidates. Thus, the most expensive part in the searching phase was eliminated. DARTS [7] proposed to search in a differentiable manner. During searching, a supernet was trained and the network was obtained by removing operators which has a low weight.",
        "context": "Summarizes the initial challenges and approaches to NAS, highlighting the resource-intensive nature of early RL-based methods and the emergence of cell-based and differentiable NAS techniques.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            2
        ],
        "id": "fd6a3fa010405c0ef17f8e884eeaa0a7d2225b407673fe0605d22dfbe91e4401"
    },
    {
        "text": "Highlights the challenges of conventional NAS and the initial demonstration of training-free NAS using a single indicator, emphasizing the limitation of relying on a single metric and paving the way for more sophisticated approaches.\n\nConventional NAS algorithms required a heavy search cost which is unaffordable to most applications especially for training the candidate networks. Therefore, if the performance of architectures is predicted without any training, the budget for NAS can be reduced signiGLYPH<28>cantly. Reference [16] was the GLYPH<28>rst to demonstrate the feasibility of performing NAS without any training. The authors in [16] empirically found the positive correlation between the correlation of Jacobian matrix among augmented input images and the network's performance. Thus, they suggested using this indicator to score the quality of the networks. Finally, NAS in [16] was performed with a simple search strategy based on Random Search where the indicator is used to replace the training process with simple prediction of the quality in a network. This work opened a new direction\nfor NAS where one can utilize some indicators to estimate the network's performance. However, the major drawback of this method is that the whole framework relies on a single indicator which only captures one characteristic of a network.\nTE-NAS [24] leveraged two training-free indicators, that is, NLR and CNNTK. The NLR is used to measure the expressivity of DNNs [21], [22]. The CNNTK measures the trainability of DNNs [23]. Based on these two indicators, they proposed a pruning-based algorithm to perform the search. Particularly, the criterion for pruning was based on the sum of two ranks measured by NLR and of CNNTK. The process was repeated until a certain stopping criteria was met.\nIn summary, the above works [16], [24] focus on performing NAS without involving any training by leveraging several training-free indicators. The search algorithm is based on random or pruning approach. Although training-free NAS has demonstrated a potential alternative for conventional NAS algorithms, naively summing the ranks in [24] does not reGLYPH<29>ect the importance of each indicator. In order to solve the aforementioned limitation, GLYPH<28>rst, we consider multiple indicators where each indicator can express different characteristic of a network. Overall, there are four indicators in our Fusion method. Second, instead of assigning an equal weight for each indicator in a weighted sum manner, we adopt a training approach to GLYPH<28>nd appropriate weights. This way we can highlight the important of each indicator. Furthermore, we perform extensive evaluation for the indicators in a systematic manner.",
        "original_text": "Conventional NAS algorithms required a heavy search cost which is unaffordable to most applications especially for training the candidate networks. Therefore, if the performance of architectures is predicted without any training, the budget for NAS can be reduced signiGLYPH<28>cantly. Reference [16] was the GLYPH<28>rst to demonstrate the feasibility of performing NAS without any training. The authors in [16] empirically found the positive correlation between the correlation of Jacobian matrix among augmented input images and the network's performance. Thus, they suggested using this indicator to score the quality of the networks. Finally, NAS in [16] was performed with a simple search strategy based on Random Search where the indicator is used to replace the training process with simple prediction of the quality in a network. This work opened a new direction\nfor NAS where one can utilize some indicators to estimate the network's performance. However, the major drawback of this method is that the whole framework relies on a single indicator which only captures one characteristic of a network.\nTE-NAS [24] leveraged two training-free indicators, that is, NLR and CNNTK. The NLR is used to measure the expressivity of DNNs [21], [22]. The CNNTK measures the trainability of DNNs [23]. Based on these two indicators, they proposed a pruning-based algorithm to perform the search. Particularly, the criterion for pruning was based on the sum of two ranks measured by NLR and of CNNTK. The process was repeated until a certain stopping criteria was met.\nIn summary, the above works [16], [24] focus on performing NAS without involving any training by leveraging several training-free indicators. The search algorithm is based on random or pruning approach. Although training-free NAS has demonstrated a potential alternative for conventional NAS algorithms, naively summing the ranks in [24] does not reGLYPH<29>ect the importance of each indicator. In order to solve the aforementioned limitation, GLYPH<28>rst, we consider multiple indicators where each indicator can express different characteristic of a network. Overall, there are four indicators in our Fusion method. Second, instead of assigning an equal weight for each indicator in a weighted sum manner, we adopt a training approach to GLYPH<28>nd appropriate weights. This way we can highlight the important of each indicator. Furthermore, we perform extensive evaluation for the indicators in a systematic manner.",
        "context": "Highlights the challenges of conventional NAS and the initial demonstration of training-free NAS using a single indicator, emphasizing the limitation of relying on a single metric and paving the way for more sophisticated approaches.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            2,
            3
        ],
        "id": "17c8cdee8474906adcc1fc36d75362f1bf58e89c46de53e5233e1cbe7306af8c"
    },
    {
        "text": "Highlights the importance of benchmark datasets like NAS-Bench-101 and NAS-Bench-201 for comparing NAS algorithms and details their construction, including the number of architectures and training procedures.\n\nReproducible and benchmarking NAS datasets are the most important factors for comparing the algorithms. There have been a lot of efforts to develop a benchmark dataset [32], [33]. Particularly, NAS-Bench-101 [32] has made the GLYPH<28>rst effort to build a dataset for benchmarking NAS. There are 423k architectures in this dataset. Each architecture is trained on CIFAR-10 under the same hyper-parameters. NAS-Bench101 provides the test accuracy of all architectures on this search space.\nNAS-Bench-201 [33] extends NAS-Bench-101 [32] by adding more operators (i.e., None and skip-connect), supporting more NAS algorithms (i.e., differentiable NAS), and evaluating on more datasets (i.e., CIFAR-100 and ImageNet16-120 [34]). The network in NAS-Bench-201 is a cell-based structure where the cell is deGLYPH<28>ned as a densely-connected directed acyclic graph with 4 nodes. With this conGLYPH<28>guration, the total architecture in NAS-Bench-201 is 15625. All networks are trained under the same settings. SpeciGLYPH<28>cally, they are trained from scratch with Nesterov momentum SGD for 200 epochs. The initial learning rate is 0.1 and is decayed with cosine annealing. The weight decay is set to 0.0005 and the batch size is 256.\nIn our work, we utilise NAS-Bench-101 and NAS-Bench201 to demonstrate the effectiveness of the proposed FI on all classiGLYPH<28>cation tasks in the dataset.",
        "original_text": "Reproducible and benchmarking NAS datasets are the most important factors for comparing the algorithms. There have been a lot of efforts to develop a benchmark dataset [32], [33]. Particularly, NAS-Bench-101 [32] has made the GLYPH<28>rst effort to build a dataset for benchmarking NAS. There are 423k architectures in this dataset. Each architecture is trained on CIFAR-10 under the same hyper-parameters. NAS-Bench101 provides the test accuracy of all architectures on this search space.\nNAS-Bench-201 [33] extends NAS-Bench-101 [32] by adding more operators (i.e., None and skip-connect), supporting more NAS algorithms (i.e., differentiable NAS), and evaluating on more datasets (i.e., CIFAR-100 and ImageNet16-120 [34]). The network in NAS-Bench-201 is a cell-based structure where the cell is deGLYPH<28>ned as a densely-connected directed acyclic graph with 4 nodes. With this conGLYPH<28>guration, the total architecture in NAS-Bench-201 is 15625. All networks are trained under the same settings. SpeciGLYPH<28>cally, they are trained from scratch with Nesterov momentum SGD for 200 epochs. The initial learning rate is 0.1 and is decayed with cosine annealing. The weight decay is set to 0.0005 and the batch size is 256.\nIn our work, we utilise NAS-Bench-101 and NAS-Bench201 to demonstrate the effectiveness of the proposed FI on all classiGLYPH<28>cation tasks in the dataset.",
        "context": "Highlights the importance of benchmark datasets like NAS-Bench-101 and NAS-Bench-201 for comparing NAS algorithms and details their construction, including the number of architectures and training procedures.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            3
        ],
        "id": "e0ccb61d92da0c93645e597bea34c68f02b954cbfff1675c4e336d05db1294ee"
    },
    {
        "text": "Summarizes methods for evaluating networks before training: CJ, OS, NLR, and CNNTK.\n\nThe motivation of training-free NAS is to select highperformance potential networks from a search space without any training. In order to design a better indicator, we study several methods that can evaluate some characteristics of the network before training, i.e., CJ, OS, NLR, and CNNTK. In this section, we will GLYPH<28>rst summarize these methods and then explain our proposed FI.",
        "original_text": "The motivation of training-free NAS is to select highperformance potential networks from a search space without any training. In order to design a better indicator, we study several methods that can evaluate some characteristics of the network before training, i.e., CJ, OS, NLR, and CNNTK. In this section, we will GLYPH<28>rst summarize these methods and then explain our proposed FI.",
        "context": "Summarizes methods for evaluating networks before training: CJ, OS, NLR, and CNNTK.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            3
        ],
        "id": "86055ab056b47a33e8badcdb3e3f17a11a774aab744f64316c4277828d244376"
    },
    {
        "text": "Introduces and summarizes four methods (CJ, OS, NLR, and CNNTK) used to characterize neural networks.\n\nTo understand the neural network (NN) behavior, we use CJ, OS, NLR, and CNNTK. These methods provide essential knowledge for characterizing NNs.",
        "original_text": "To understand the neural network (NN) behavior, we use CJ, OS, NLR, and CNNTK. These methods provide essential knowledge for characterizing NNs.",
        "context": "Introduces and summarizes four methods (CJ, OS, NLR, and CNNTK) used to characterize neural networks.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            3
        ],
        "id": "4504d1b07c3b94f06bd5f783370d2f239b2bd8ad7fe98d20fd55297badebb9a8"
    },
    {
        "text": "Introduces CJ, a method for scoring a network's initial state by calculating the correlation of activations with augmented images and their Jacobian matrix.\n\nIn order to score a network at an initial state, [16] designs CJ that computes the correlation of activations of a network with a mini-batch of n augmented images. SpeciGLYPH<28>cally, to compute the score, CJ GLYPH<28>rst calculates the derivative of output y with respect to input x of this batch V\n<!-- formula-not-decoded -->\nThen we calculate the correlation for this Jacobian matrix P J and count the number of entries that are smaller than a predeGLYPH<28>ned threshold ( GLYPH<12> ). Thus, the score for Jacobian is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->",
        "original_text": "In order to score a network at an initial state, [16] designs CJ that computes the correlation of activations of a network with a mini-batch of n augmented images. SpeciGLYPH<28>cally, to compute the score, CJ GLYPH<28>rst calculates the derivative of output y with respect to input x of this batch V\n<!-- formula-not-decoded -->\nThen we calculate the correlation for this Jacobian matrix P J and count the number of entries that are smaller than a predeGLYPH<28>ned threshold ( GLYPH<12> ). Thus, the score for Jacobian is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->",
        "context": "Introduces CJ, a method for scoring a network's initial state by calculating the correlation of activations with augmented images and their Jacobian matrix.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            3
        ],
        "id": "44a1097de93f96d31fbb349e8974f7662d854f3f43a2189b477c4adf329aebb3"
    },
    {
        "text": "Introduces methods for evaluating network characteristics before training (CJ, OS, NLR, CNNTK) to assess generalization ability and sensitivity.\n\nFor improving the network's generalization ability, authors in [35] proposed an ensemble approach called OS that can estimate the degree of generalization power of a network. Forouzesh et al. [17] further extended the results of [35] where the goal is to study the relation between the sensitivity and generalization in NNs. For determining the sensitivity of the network, external noise is added to the input. Let x be the input vector, GLYPH<18> be the parameters of a NN f GLYPH<18> , and \" be the noise which is sampled from a uniform distribution, then the error err is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->\nThe averaged error is calculated as below V\n<!-- formula-not-decoded -->\nwhere N is the number of classes and err n y indicates the n -th value of erry . The sensitivity of an NN can be measured by computing the variance of the output error. To this end, the score of sensitivity is formulated as V\n<!-- formula-not-decoded -->\nwhere X is the vector of averaged error for M samples and Var is the variance.",
        "original_text": "For improving the network's generalization ability, authors in [35] proposed an ensemble approach called OS that can estimate the degree of generalization power of a network. Forouzesh et al. [17] further extended the results of [35] where the goal is to study the relation between the sensitivity and generalization in NNs. For determining the sensitivity of the network, external noise is added to the input. Let x be the input vector, GLYPH<18> be the parameters of a NN f GLYPH<18> , and \" be the noise which is sampled from a uniform distribution, then the error err is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->\nThe averaged error is calculated as below V\n<!-- formula-not-decoded -->\nwhere N is the number of classes and err n y indicates the n -th value of erry . The sensitivity of an NN can be measured by computing the variance of the output error. To this end, the score of sensitivity is formulated as V\n<!-- formula-not-decoded -->\nwhere X is the vector of averaged error for M samples and Var is the variance.",
        "context": "Introduces methods for evaluating network characteristics before training (CJ, OS, NLR, CNNTK) to assess generalization ability and sensitivity.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            3,
            4
        ],
        "id": "33f6e16f0dd01f603c4c35fadab13cb7d34047e8eecba40fab852577b77d9de4"
    },
    {
        "text": "Analyzes deep ReLU networks' complexity, demonstrating their ability to divide the input space into more regions than shallow networks.\n\nTo answer why deep networks outperform a shallow network, [36] analyzes the deep ReLU networks with respect to their complexity. They have shown that, given the same computational resources, a deep network can divide the input space into many regions than a shallow one. Motivated by [21], [36] provided an in-depth analysis on NLR for convolution neural networks (CNNs). Following [24], NLR can be used to measure the expressivity of NNs. Let N be a ReLU CNN and GLYPH<18> be the parameters of N sampled from some distributions. The score for NLR can then be calculated as V\n<!-- formula-not-decoded -->\nwhere R ( GLYPH<1> ) is the region corresponding to P and GLYPH<18> , and is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->\nwhere z ( x 0 I GLYPH<18> ) is the pre-activation of a neuron z and P is an activation pattern such that P ( z ) 2 fGLYPH<0> 1 ; 1 g for each neuron z in N .",
        "original_text": "To answer why deep networks outperform a shallow network, [36] analyzes the deep ReLU networks with respect to their complexity. They have shown that, given the same computational resources, a deep network can divide the input space into many regions than a shallow one. Motivated by [21], [36] provided an in-depth analysis on NLR for convolution neural networks (CNNs). Following [24], NLR can be used to measure the expressivity of NNs. Let N be a ReLU CNN and GLYPH<18> be the parameters of N sampled from some distributions. The score for NLR can then be calculated as V\n<!-- formula-not-decoded -->\nwhere R ( GLYPH<1> ) is the region corresponding to P and GLYPH<18> , and is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->\nwhere z ( x 0 I GLYPH<18> ) is the pre-activation of a neuron z and P is an activation pattern such that P ( z ) 2 fGLYPH<0> 1 ; 1 g for each neuron z in N .",
        "context": "Analyzes deep ReLU networks' complexity, demonstrating their ability to divide the input space into more regions than shallow networks.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            4
        ],
        "id": "46cc18d0fddfa8d1888e76d0442c88e7ce51d14a44d8ceaab34b61c6029a1cc6"
    },
    {
        "text": "Introduces NTK and its use for analyzing the time evolution of linearized NNs during training, specifically outlining how it allows for obtaining the time evolution without gradient descent.\n\nIn [20] a new tool was proposed to help understand the behavior of DNNs during training, which is called NTK. It has been proved that using NTK, we can obtain the time evolution of linearized NNs at time t without running gradient descent V\n<!-- formula-not-decoded -->\nwhere GLYPH<22> t ( x ) D E [ z L i ( x )] is the expected outputs of inGLYPH<28>nitely wide network, z L i is the output of i -th neuron in the last layer L , GLYPH<17> is the learning rate, and O 2 train,train is the NTK between two training inputs. X train and Y train are the input and target which are drawn from the training set. Id is a constant. The trainability of NNs is studied in [23]. Let GLYPH<21> i be the i -th eigenvalue in the D diagonal matrix and U be the unitary matrix of O 2 train,train , i.e., O 2 train,train D UDU GLYPH<0> 1 . Then Eq. (8) can be rewritten as V\n<!-- formula-not-decoded -->\nwhere Q GLYPH<22> t ( X train ) i D U GLYPH<22> t ( X train) and Q Y train ; i D UY train .\nLet GLYPH<21> 0 and GLYPH<21> m be the minimum and maximum eigenvalues of O 2 train,train. In Eq. (9), the maximum learning rate scales as GLYPH<17> GLYPH<24> 2 =GLYPH<21> 0 in [37]. Thus, the smallest eigenvalue will converge exponentially at a rate given by 1 = k , where k D GLYPH<21> 0 =GLYPH<21> m and is the condition number. If the condition number of the NTK diverges, the network is untrainable. In our work, we inverse the condition number of NTK such that with a higher value,\nFIGURE 1. The illustration of the proposed fusion indicator. CJ, NLR, CNNTK, OS are the four training-free indicators. Each indicator represents a single characteristic of a network. In the fusion indicator, each standalone indicator contributes a certain property for ranking the neural network.\nwe can get better trainability. Thus, the score for trainability can be written as V\n<!-- formula-not-decoded -->",
        "original_text": "In [20] a new tool was proposed to help understand the behavior of DNNs during training, which is called NTK. It has been proved that using NTK, we can obtain the time evolution of linearized NNs at time t without running gradient descent V\n<!-- formula-not-decoded -->\nwhere GLYPH<22> t ( x ) D E [ z L i ( x )] is the expected outputs of inGLYPH<28>nitely wide network, z L i is the output of i -th neuron in the last layer L , GLYPH<17> is the learning rate, and O 2 train,train is the NTK between two training inputs. X train and Y train are the input and target which are drawn from the training set. Id is a constant. The trainability of NNs is studied in [23]. Let GLYPH<21> i be the i -th eigenvalue in the D diagonal matrix and U be the unitary matrix of O 2 train,train , i.e., O 2 train,train D UDU GLYPH<0> 1 . Then Eq. (8) can be rewritten as V\n<!-- formula-not-decoded -->\nwhere Q GLYPH<22> t ( X train ) i D U GLYPH<22> t ( X train) and Q Y train ; i D UY train .\nLet GLYPH<21> 0 and GLYPH<21> m be the minimum and maximum eigenvalues of O 2 train,train. In Eq. (9), the maximum learning rate scales as GLYPH<17> GLYPH<24> 2 =GLYPH<21> 0 in [37]. Thus, the smallest eigenvalue will converge exponentially at a rate given by 1 = k , where k D GLYPH<21> 0 =GLYPH<21> m and is the condition number. If the condition number of the NTK diverges, the network is untrainable. In our work, we inverse the condition number of NTK such that with a higher value,\nFIGURE 1. The illustration of the proposed fusion indicator. CJ, NLR, CNNTK, OS are the four training-free indicators. Each indicator represents a single characteristic of a network. In the fusion indicator, each standalone indicator contributes a certain property for ranking the neural network.\nwe can get better trainability. Thus, the score for trainability can be written as V\n<!-- formula-not-decoded -->",
        "context": "Introduces NTK and its use for analyzing the time evolution of linearized NNs during training, specifically outlining how it allows for obtaining the time evolution without gradient descent.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            4
        ],
        "id": "4e1c60ae351aea95d72b48cffe5614b403536c02f2eaff323385a9495bf5e211"
    },
    {
        "text": "Motivates the design of a fusion indicator combining multiple training-free network indicators to overcome the limitations of individual indicators and improve the reliability of the training-free framework.\n\nIn order to evaluate NNs without any training, the process requires a powerful indicator for correctly characterizing the network. Each indicator has its purpose and signiGLYPH<28>cance in specifying our model. Using these indicators separately makes them ineffective and weak since they only characterize the network with a speciGLYPH<28>c factor hence inculcating a certain bias in the network. Thus, this motivates us to design the FI where all indicators are combined. Let S be the set of training free indicators (e.g., S CJ ; S NLR ; S CNNTK ; S OS), our score for FI is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->\nwhere n is the total indicators, b is the bias, wi and Si are the weight and the score using i -th indicator in S . The overview of the proposed Fusion Indicator is demonstrated in Figure 1. We hypothesize that our proposed FI can take all indicators' advantage and make our training-free framework more reliable. We want our FI to reGLYPH<29>ect our network's accuracy closely. We are interested in minimizing the difference between the score and the accuracy. SpeciGLYPH<28>cally, we minimize the following loss function V\n<!-- formula-not-decoded -->\nwhere MSE is the mean squared error. S FI and Acc in Eq. (12) denote the score and the accuracy of a network. Thus, our S FI is the predicted accuracy of a network. The model parameters in Eq. (11) are trained with the stochastic gradient descent (SGD) method. Note that it is prohibitive to use support vector regression or linear regression in this case\nsince there are a huge numbers of samples in NAS-Bench101/202 datasets.",
        "original_text": "In order to evaluate NNs without any training, the process requires a powerful indicator for correctly characterizing the network. Each indicator has its purpose and signiGLYPH<28>cance in specifying our model. Using these indicators separately makes them ineffective and weak since they only characterize the network with a speciGLYPH<28>c factor hence inculcating a certain bias in the network. Thus, this motivates us to design the FI where all indicators are combined. Let S be the set of training free indicators (e.g., S CJ ; S NLR ; S CNNTK ; S OS), our score for FI is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->\nwhere n is the total indicators, b is the bias, wi and Si are the weight and the score using i -th indicator in S . The overview of the proposed Fusion Indicator is demonstrated in Figure 1. We hypothesize that our proposed FI can take all indicators' advantage and make our training-free framework more reliable. We want our FI to reGLYPH<29>ect our network's accuracy closely. We are interested in minimizing the difference between the score and the accuracy. SpeciGLYPH<28>cally, we minimize the following loss function V\n<!-- formula-not-decoded -->\nwhere MSE is the mean squared error. S FI and Acc in Eq. (12) denote the score and the accuracy of a network. Thus, our S FI is the predicted accuracy of a network. The model parameters in Eq. (11) are trained with the stochastic gradient descent (SGD) method. Note that it is prohibitive to use support vector regression or linear regression in this case\nsince there are a huge numbers of samples in NAS-Bench101/202 datasets.",
        "context": "Motivates the design of a fusion indicator combining multiple training-free network indicators to overcome the limitations of individual indicators and improve the reliability of the training-free framework.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            4,
            5
        ],
        "id": "c692d2409119c8cfa90f190b170bf4e62cc901ce2edc02432df5992a5aca3ae7"
    },
    {
        "text": "Evaluates the indicator's performance using Spearman and Kendall correlation, Pearson correlation, and RMSE, employing 5-fold cross-validation to assess generalizability and prevent overfitting.\n\nWe use four metrics to evaluate the performance of the indicator. The GLYPH<28>rst two metrics are the Spearman Rank-Order Correlation CoefGLYPH<28>cient (SROCC) and the Kendall RankOrder Correlation CoefGLYPH<28>cient (KROCC). The third metric and the fourth metric are the Pearson Linear Correlation CoefGLYPH<28>cient (PLCC) and the Root Mean Square Error (RMSE) between the score and the accuracy after nonlinear regression. The GLYPH<28>rst two metrics measure the prediction monotonicity of the indicator while the third one measures the linear correlation between the actual accuracy and the predicted one. These quality assessment metrics are made for evaluating such characteristics (GLYPH<28>delity and correlation) and are widely used in practice [38]GLYPH<21>[40]. To compute PLCC and RMSE, we apply the following logistic function as suggested in [41]:\n<!-- formula-not-decoded -->\nwhere x is the score (predicted accuracy) and GLYPH<12> i ( i D 1 ; 2 ; 3 ; 4 ; 5) are the set of parameters that minimize the least squares error between the output from indicator and the network's accuracy.\nSince our FI is a learning-based method, the optimal w may not generalize well on other test data. To prevent overGLYPH<28>tting, we use n-fold cross-validation. SpeciGLYPH<28>cally, we use 5-fold cross-validation and average the results obtained from 5 folds to get the overall performance. In all experiments, we run the 5-fold cross-validation 10 times and average the results to obtain the GLYPH<28>nal performance.",
        "original_text": "We use four metrics to evaluate the performance of the indicator. The GLYPH<28>rst two metrics are the Spearman Rank-Order Correlation CoefGLYPH<28>cient (SROCC) and the Kendall RankOrder Correlation CoefGLYPH<28>cient (KROCC). The third metric and the fourth metric are the Pearson Linear Correlation CoefGLYPH<28>cient (PLCC) and the Root Mean Square Error (RMSE) between the score and the accuracy after nonlinear regression. The GLYPH<28>rst two metrics measure the prediction monotonicity of the indicator while the third one measures the linear correlation between the actual accuracy and the predicted one. These quality assessment metrics are made for evaluating such characteristics (GLYPH<28>delity and correlation) and are widely used in practice [38]GLYPH<21>[40]. To compute PLCC and RMSE, we apply the following logistic function as suggested in [41]:\n<!-- formula-not-decoded -->\nwhere x is the score (predicted accuracy) and GLYPH<12> i ( i D 1 ; 2 ; 3 ; 4 ; 5) are the set of parameters that minimize the least squares error between the output from indicator and the network's accuracy.\nSince our FI is a learning-based method, the optimal w may not generalize well on other test data. To prevent overGLYPH<28>tting, we use n-fold cross-validation. SpeciGLYPH<28>cally, we use 5-fold cross-validation and average the results obtained from 5 folds to get the overall performance. In all experiments, we run the 5-fold cross-validation 10 times and average the results to obtain the GLYPH<28>nal performance.",
        "context": "Evaluates the indicator's performance using Spearman and Kendall correlation, Pearson correlation, and RMSE, employing 5-fold cross-validation to assess generalizability and prevent overfitting.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            5
        ],
        "id": "43cb228980e28ab7fa936d3ed19b5cef8541a9763e26f7c113e20dbfcc14f369"
    },
    {
        "text": "Compares the performance of the proposed Fusion Indicator (FI) and standalone indicators (CJ, NLR, CNNTK, OS) on CIFAR-10 of NAS-Bench-101, demonstrating that the FI significantly outperforms the individual indicators in terms of KROCC and SROCC, achieving a 5% higher KROCC and SROCC than the CJ-based single indicator.\n\nWe compare the performance of our FI and standalone indicators, i.e. CJ, NLR, CNNTK, OS on CIFAR-10 of NASBench-101 search space. Because we have four indicators, there are 11 combinations for our FI. FI2 means two indicators are used and so on. We use f CJ, OS g for FI2, f CJ, CNNTK, OS g for FI3, and f CJ, NLR, CNNTK, OS g for FI4. We refer the reader to the Ablation section for the performance of other combinations. The results are shown in Table 1.\nAs shown in Table 1, the proposed FI outperforms other indicators signiGLYPH<28>cantly in all performance assessment methods. Notably, our FI achieves 5% higher KROCC and SROCCthan the CJ-based single indicator [16]. Compared to NLR, CNNTK and OS, the proposed FI has around 24% and 30% higher KROCC and SROCC, respectively. Additionally, it is worth noting that increasing the number of indicator does not improve the performance on NAS-Bench-101. One possible reason for this is that there are only three operations (i.e., 3 GLYPH<2> 3 convolution, 1 GLYPH<2> 1 convolution, 3 GLYPH<2> 3 max pool) in the search space. This reduces the diversity of the network architectures even though NAS-Bench-101 contains\nTABLE 1. Performance of several training-free indicators measured on CIFAR-10 of NAS-Bench-101. The first, second, and third ranked performances are highlighted in blue, red, and black bold, respectively.\nmore architectures than NAS-Bench-201. As a result, several indicators do not contribute to the overall performance.",
        "original_text": "We compare the performance of our FI and standalone indicators, i.e. CJ, NLR, CNNTK, OS on CIFAR-10 of NASBench-101 search space. Because we have four indicators, there are 11 combinations for our FI. FI2 means two indicators are used and so on. We use f CJ, OS g for FI2, f CJ, CNNTK, OS g for FI3, and f CJ, NLR, CNNTK, OS g for FI4. We refer the reader to the Ablation section for the performance of other combinations. The results are shown in Table 1.\nAs shown in Table 1, the proposed FI outperforms other indicators signiGLYPH<28>cantly in all performance assessment methods. Notably, our FI achieves 5% higher KROCC and SROCCthan the CJ-based single indicator [16]. Compared to NLR, CNNTK and OS, the proposed FI has around 24% and 30% higher KROCC and SROCC, respectively. Additionally, it is worth noting that increasing the number of indicator does not improve the performance on NAS-Bench-101. One possible reason for this is that there are only three operations (i.e., 3 GLYPH<2> 3 convolution, 1 GLYPH<2> 1 convolution, 3 GLYPH<2> 3 max pool) in the search space. This reduces the diversity of the network architectures even though NAS-Bench-101 contains\nTABLE 1. Performance of several training-free indicators measured on CIFAR-10 of NAS-Bench-101. The first, second, and third ranked performances are highlighted in blue, red, and black bold, respectively.\nmore architectures than NAS-Bench-201. As a result, several indicators do not contribute to the overall performance.",
        "context": "Compares the performance of the proposed Fusion Indicator (FI) and standalone indicators (CJ, NLR, CNNTK, OS) on CIFAR-10 of NAS-Bench-101, demonstrating that the FI significantly outperforms the individual indicators in terms of KROCC and SROCC, achieving a 5% higher KROCC and SROCC than the CJ-based single indicator.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            5
        ],
        "id": "28d263abe193d001cdcc1bb3c45c1ba0c213c48199be31515d797bda4f2be284"
    },
    {
        "text": "Evaluates the performance of training-free indicators on NAS-Bench-201, demonstrating that adding more indicators to the fusion indicator consistently improves performance across datasets and metrics like SROCC and KROCC.\n\nWe further evaluate the performance of training-free indicators on NAS-Bench-201. We use f CNNTK, OS g for FI2, f NLR, CNNTK, OS g for FI3, and all indicators for FI4. The performance measured by PLCC, RMSE, SROCC, and KROCC for all competitors on this dataset are shown in Table 2.\nFrom Table 2, it can be seen that the proposed FI outperforms standalone indicators in all performance assessment methods for all datasets on NAS-Bench-201. It is worth noting that OS performs the best compared to CJ, NLR, and CNNTK. On CIFAR-10, FI2 has 7.29% and 8.58% higher SROCC and KROCC than OS. Adding more indicators to our FI further increases the performance. SpeciGLYPH<28>cally, FI3 has 7.43% and 8.85% SROCC and KROCC improvement over OS. When using all indicators, FI4 reaches its peak with 0.88 and 0.7017 SROCC and KROCC, respectively.\nOn CIFAR-100, our FI performs consistently well. Standalone indicator achieves less than 0.8 SROCC and 0.6 KROCC. By contrast, the proposed FI obtains more than 0.86 SROCC and 0.68 KROCC. On ImageNet-16-120, we can see that FI4 has the best performance for all assessment metrics. Compared to OS, FI4 has 9% and 11% higher SROCC and KROCC.\nAdditionally, we illustrate the scatter plots of the predicted scores versus the accuracy measured by CJ, NLR, CNNTK, and OS in Figure 2. We show the test accuracy with respect to the score of CJ, NLR, CNNTK, and OS in the GLYPH<28>rst, second, third, and fourth column of Figure 2, respectively. We also show the scatter plots of FI4 for 5-fold cross-validation (CV) in Figure 3. Each column in Figure 3 represents the test accuracy with respect to the score of each fold, namely Fold-1, Fold-2, Fold-3, Fold-4, and Fold-5 in CV. In both GLYPH<28>gures, the green line is the best GLYPH<28>tting curve. Figure 2 demonstrates that the OS indicator has a higher correlation than others. From Figure 3, we can see that there is a very strong correlation between the predicted score and the accuracy when using the proposed FI.\nIn summary, the performance of our FI improves consistently when adding more indicators. This may come from the diversity of the search space. Besides the three operations\nTABLE 2. Performance of several training-free indicators measured on CIFAR-10, CIFAR-100, and ImageNet-16-120 of NAS-Bench-201 search space. The first, second, and third ranked performances are highlighted in blue, red, and black bold, respectively.\nFIGURE 2. The plots of the score for all architectures in NAS-Bench-201 against the test accuracies on CIFAR-10, CIFAR-100, and ImageNet-16-120. For better visualization, the score is scaled to the range of 0 and 1. The best fitting curve is shown in green line. The Kendall Tau values show a strong correlation for all indicators.\nused in [32], NAS-Bench-201 uses two more operators, zero and skip-connect, which makes the search space richer. Thus, the proposed FI can fully utilize all characteristics of a network to evaluate the network correctly.",
        "original_text": "We further evaluate the performance of training-free indicators on NAS-Bench-201. We use f CNNTK, OS g for FI2, f NLR, CNNTK, OS g for FI3, and all indicators for FI4. The performance measured by PLCC, RMSE, SROCC, and KROCC for all competitors on this dataset are shown in Table 2.\nFrom Table 2, it can be seen that the proposed FI outperforms standalone indicators in all performance assessment methods for all datasets on NAS-Bench-201. It is worth noting that OS performs the best compared to CJ, NLR, and CNNTK. On CIFAR-10, FI2 has 7.29% and 8.58% higher SROCC and KROCC than OS. Adding more indicators to our FI further increases the performance. SpeciGLYPH<28>cally, FI3 has 7.43% and 8.85% SROCC and KROCC improvement over OS. When using all indicators, FI4 reaches its peak with 0.88 and 0.7017 SROCC and KROCC, respectively.\nOn CIFAR-100, our FI performs consistently well. Standalone indicator achieves less than 0.8 SROCC and 0.6 KROCC. By contrast, the proposed FI obtains more than 0.86 SROCC and 0.68 KROCC. On ImageNet-16-120, we can see that FI4 has the best performance for all assessment metrics. Compared to OS, FI4 has 9% and 11% higher SROCC and KROCC.\nAdditionally, we illustrate the scatter plots of the predicted scores versus the accuracy measured by CJ, NLR, CNNTK, and OS in Figure 2. We show the test accuracy with respect to the score of CJ, NLR, CNNTK, and OS in the GLYPH<28>rst, second, third, and fourth column of Figure 2, respectively. We also show the scatter plots of FI4 for 5-fold cross-validation (CV) in Figure 3. Each column in Figure 3 represents the test accuracy with respect to the score of each fold, namely Fold-1, Fold-2, Fold-3, Fold-4, and Fold-5 in CV. In both GLYPH<28>gures, the green line is the best GLYPH<28>tting curve. Figure 2 demonstrates that the OS indicator has a higher correlation than others. From Figure 3, we can see that there is a very strong correlation between the predicted score and the accuracy when using the proposed FI.\nIn summary, the performance of our FI improves consistently when adding more indicators. This may come from the diversity of the search space. Besides the three operations\nTABLE 2. Performance of several training-free indicators measured on CIFAR-10, CIFAR-100, and ImageNet-16-120 of NAS-Bench-201 search space. The first, second, and third ranked performances are highlighted in blue, red, and black bold, respectively.\nFIGURE 2. The plots of the score for all architectures in NAS-Bench-201 against the test accuracies on CIFAR-10, CIFAR-100, and ImageNet-16-120. For better visualization, the score is scaled to the range of 0 and 1. The best fitting curve is shown in green line. The Kendall Tau values show a strong correlation for all indicators.\nused in [32], NAS-Bench-201 uses two more operators, zero and skip-connect, which makes the search space richer. Thus, the proposed FI can fully utilize all characteristics of a network to evaluate the network correctly.",
        "context": "Evaluates the performance of training-free indicators on NAS-Bench-201, demonstrating that adding more indicators to the fusion indicator consistently improves performance across datasets and metrics like SROCC and KROCC.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            5,
            6
        ],
        "id": "2a0165a2a3aaa6d62e3fd315101d8e2eea31bb5747d36937586887d56f3d194e"
    },
    {
        "text": "This section demonstrates the generalizability of the proposed FI across different datasets by using weights trained on one dataset to evaluate performance on others, showing high KROCC values (greater than 0.67) and compatibility of weights between datasets.\n\nWe perform cross dataset tests to verify the generalizability of our FI for different datasets. To do this, we obtain the weights for the FI trained on one dataset (e.g., CIFAR-10) and evaluate it on other datasets. For example, we use the score evaluated on CIFAR-10 to train the weight for the FI and use the same weights to perform testing on CIFAR-100 and ImageNet-16-120. We denote the weight obtained from CIFAR-10, CIFAR-100, and ImageNet-16-120 as w CIFAR-10, w CIFAR-100, and w ImageNet-16-120, respectively. The performance results are listed in Table 3.\nTABLE 3. Performance of FI 4 on NAS-Bench-201, measured by KROCC.\nAs shown in Table 3, we can see that the proposed FI achieves a high KROCC. For all datasets, the value of KROCC is greater than 0.67. The weights obtained on one dataset are highly compatible with other datasets. This shows the generality and robustness of our approach.",
        "original_text": "We perform cross dataset tests to verify the generalizability of our FI for different datasets. To do this, we obtain the weights for the FI trained on one dataset (e.g., CIFAR-10) and evaluate it on other datasets. For example, we use the score evaluated on CIFAR-10 to train the weight for the FI and use the same weights to perform testing on CIFAR-100 and ImageNet-16-120. We denote the weight obtained from CIFAR-10, CIFAR-100, and ImageNet-16-120 as w CIFAR-10, w CIFAR-100, and w ImageNet-16-120, respectively. The performance results are listed in Table 3.\nTABLE 3. Performance of FI 4 on NAS-Bench-201, measured by KROCC.\nAs shown in Table 3, we can see that the proposed FI achieves a high KROCC. For all datasets, the value of KROCC is greater than 0.67. The weights obtained on one dataset are highly compatible with other datasets. This shows the generality and robustness of our approach.",
        "context": "This section demonstrates the generalizability of the proposed FI across different datasets by using weights trained on one dataset to evaluate performance on others, showing high KROCC values (greater than 0.67) and compatibility of weights between datasets.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            6
        ],
        "id": "fceabdefb5548ecb215a2e03d2581a9689472226f748e9e3ab1770589dcfad9f"
    },
    {
        "text": "Demonstrates the effectiveness of training-free indicators by integrating them into an evolutionary algorithm (AE) for NAS, replacing network accuracy with indicator scores to guide network selection during mutation.\n\nThe ultimate goal of NAS without Training is to replace the heavy cost of training candidate networks with inexpensive\nFIGURE 3. Scatter plots of predicted score, FI 4 , against the accuracy for each fold on CIFAR-10, CIFAR-100, and ImageNet-16-120 of NAS-Bench-201 search space. The best fitting curve is shown in green line.\nones (e.g., CJ, NLR...). In order to demonstrate the effectiveness of training-free indicators, we incorporate the score from training-free indicator into Aging Evolution (AE), an evolutionary algorithm for NAS [42]. We replace the network accuracy obtained from the network training step ( train and evaluation in [42]) with the predicted score from the indicator. For the mutation phase, the network which has the highest predicted score is selected as a parent. After mutation, the child network is scored using the indicator. Finally, the output is the network which has the highest predicted score. We choose the AE algorithm due to its simplicity.\nWe compare the performance of AE using CJ, NLR, CNNTK,OS,andtheproposedFI4 on CIFAR-100 with NASBench-201 search space. We evaluate around 300 networks. Werunthe experiment 100 times and show the average results in Figure 4. As demonstrated in Figure 4, it is clear that the proposed FI achieves higher test accuracy for the network with the highest predicted score than others. SpeciGLYPH<28>cally, NLR has the lowest test accuracy for the network with the highest predicted score. The three indicators CJ, CNNTK, and OS achieve comparable test accuracy. It is noticeable that the proposed FI achieves much higher accuracy than others, where we perform experiments 100 times for each case and take an average of the 100 test accuracies.",
        "original_text": "The ultimate goal of NAS without Training is to replace the heavy cost of training candidate networks with inexpensive\nFIGURE 3. Scatter plots of predicted score, FI 4 , against the accuracy for each fold on CIFAR-10, CIFAR-100, and ImageNet-16-120 of NAS-Bench-201 search space. The best fitting curve is shown in green line.\nones (e.g., CJ, NLR...). In order to demonstrate the effectiveness of training-free indicators, we incorporate the score from training-free indicator into Aging Evolution (AE), an evolutionary algorithm for NAS [42]. We replace the network accuracy obtained from the network training step ( train and evaluation in [42]) with the predicted score from the indicator. For the mutation phase, the network which has the highest predicted score is selected as a parent. After mutation, the child network is scored using the indicator. Finally, the output is the network which has the highest predicted score. We choose the AE algorithm due to its simplicity.\nWe compare the performance of AE using CJ, NLR, CNNTK,OS,andtheproposedFI4 on CIFAR-100 with NASBench-201 search space. We evaluate around 300 networks. Werunthe experiment 100 times and show the average results in Figure 4. As demonstrated in Figure 4, it is clear that the proposed FI achieves higher test accuracy for the network with the highest predicted score than others. SpeciGLYPH<28>cally, NLR has the lowest test accuracy for the network with the highest predicted score. The three indicators CJ, CNNTK, and OS achieve comparable test accuracy. It is noticeable that the proposed FI achieves much higher accuracy than others, where we perform experiments 100 times for each case and take an average of the 100 test accuracies.",
        "context": "Demonstrates the effectiveness of training-free indicators by integrating them into an evolutionary algorithm (AE) for NAS, replacing network accuracy with indicator scores to guide network selection during mutation.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            6,
            7
        ],
        "id": "42727955d1ac6f37c331024abe82ee090bc504b0a27ab55cb80aff99c55662f0"
    },
    {
        "text": "The study investigates the impact of weight initialization and the addition of more training-free indicators on the performance of a fusion indicator (FI) within the Aging Evolution (AE) algorithm for NAS. Results demonstrate consistent performance improvements with increased indicator usage, particularly when combining CNNTK and OS, and highlight the statistical significance of the FI's performance compared to standalone indicators.\n\nIn this section, we study the behavior of the training-free indicator. We investigate how weight initialization affects performance. We also study the behavior of the proposed FI when more training-free indicators are added.\nFIGURE 4. Performance of Aging Evolution using different training-free indicators on CIFAR-100 with NAS-Bench-201 search space. The black dot horizontal line is the best test accuracy on this benchmark.",
        "original_text": "In this section, we study the behavior of the training-free indicator. We investigate how weight initialization affects performance. We also study the behavior of the proposed FI when more training-free indicators are added.\nFIGURE 4. Performance of Aging Evolution using different training-free indicators on CIFAR-100 with NAS-Bench-201 search space. The black dot horizontal line is the best test accuracy on this benchmark.",
        "context": "The study investigates the impact of weight initialization and the addition of more training-free indicators on the performance of a fusion indicator (FI) within the Aging Evolution (AE) algorithm for NAS. Results demonstrate consistent performance improvements with increased indicator usage, particularly when combining CNNTK and OS, and highlight the statistical significance of the FI's performance compared to standalone indicators.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            7
        ],
        "id": "9ddc8b72473ad24147642d6495d392b254a079c69b557a3861590ab537bfb63c"
    },
    {
        "text": "Highlights the importance of indicator robustness and the dependence of performance on weight initialization methods, specifically noting the impact on the Jacobian indicator and the strategy of selecting the optimal initialization method for each indicator.\n\nWhen we develop an indicator for training-free NAS framework, the most critical factor is how robust our indicator is. At the initial state, our network is unstable. There are several initialization methodologies for a network. However, in this study, we assess the performance of training-free indicators using the following initialization methods V\n- GLYPH<15> Uniform distribution: For uniform distribution, the weights are initialized from U ( GLYPH<0> p k ; p k ), where k D groups = (C in GLYPH<3> Q 1 i D 0 k [ i ]).\n- GLYPH<15> Normal distribution: The weights are sampled from N (0 ; std 2 ). For OS indicator, we only use the standard\nTABLE 4. KROCC of indicators with different weight sampling methods on CIFAR-10 of NAS-Bench-201.\nnormal distribution without any batch normalization as mentioned in [17].\n- GLYPH<15> Kaiming He: Following [43], the weights are sampled from N (0 ; std 2 ), where std D gain = fanmode 1 = 2 .\nWe compute KROCC for each indicator with different weight sampling methods. Table 4 demonstrates the quantitative performance of each indicator for different weight sampling methods. As shown in Table 4, the performance of training-free indicators depends on how the weights are sampled, especially for CJ. One possible explanation for this is that the hypothesis of the Jacobian indicator requires the same local linear operators, where the linear maps are the Jacobian of augmented input x , should have low correlation. This means that if any weight initialization method violates the hypothesis, the Jacobian indicator will not work (e.g., strong correlation with different augmented input x ).\nTo avoid exhaustively computing all possible weight sampling combinations for FI, we use a simple technique that selects the best sampling method for each indicator. Thus, uniform distribution is used for computing CJ, NLR, CNNTK and normal distribution is used for OS.",
        "original_text": "When we develop an indicator for training-free NAS framework, the most critical factor is how robust our indicator is. At the initial state, our network is unstable. There are several initialization methodologies for a network. However, in this study, we assess the performance of training-free indicators using the following initialization methods V\n- GLYPH<15> Uniform distribution: For uniform distribution, the weights are initialized from U ( GLYPH<0> p k ; p k ), where k D groups = (C in GLYPH<3> Q 1 i D 0 k [ i ]).\n- GLYPH<15> Normal distribution: The weights are sampled from N (0 ; std 2 ). For OS indicator, we only use the standard\nTABLE 4. KROCC of indicators with different weight sampling methods on CIFAR-10 of NAS-Bench-201.\nnormal distribution without any batch normalization as mentioned in [17].\n- GLYPH<15> Kaiming He: Following [43], the weights are sampled from N (0 ; std 2 ), where std D gain = fanmode 1 = 2 .\nWe compute KROCC for each indicator with different weight sampling methods. Table 4 demonstrates the quantitative performance of each indicator for different weight sampling methods. As shown in Table 4, the performance of training-free indicators depends on how the weights are sampled, especially for CJ. One possible explanation for this is that the hypothesis of the Jacobian indicator requires the same local linear operators, where the linear maps are the Jacobian of augmented input x , should have low correlation. This means that if any weight initialization method violates the hypothesis, the Jacobian indicator will not work (e.g., strong correlation with different augmented input x ).\nTo avoid exhaustively computing all possible weight sampling combinations for FI, we use a simple technique that selects the best sampling method for each indicator. Thus, uniform distribution is used for computing CJ, NLR, CNNTK and normal distribution is used for OS.",
        "context": "Highlights the importance of indicator robustness and the dependence of performance on weight initialization methods, specifically noting the impact on the Jacobian indicator and the strategy of selecting the optimal initialization method for each indicator.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            8,
            7
        ],
        "id": "8c2ab019548833124e6b86c4671090fd547cf4c239561027ab13a9918f55cc11"
    },
    {
        "text": "Evaluates the performance of combining different indicators on CIFAR-10, demonstrating that combining indicators further improves NAS performance, with the CJ, OS combination yielding the best results on NAS-Bench-201.\n\nWe also investigate the performance of our FI by combining different indicators on CIFAR-10. There are 11 combinations in total. The weights are sampled following the above analysis. KROCC is used to measure performance. We list the results in Table 5.\nFrom Table 5, it can be seen that on NAS-Bench-101 combining indicators further improves the performance. For example, NLR, CNNTK, OS achieves around 0.27 KROCC but combining them enhances the performance to 0.30 GLYPH<24> 0.34 KROCC. The best combination is f CJ, OS g .\nOn NAS-Bench-201, we observe that when two indicators are used, the performance measured by KROCC has a high standard deviation (e.g., the value of KROCC is ranging from 0.5610 to 0.6965). On a search space that has higher diversity (e.g., more operators), fusing more indicators helps improve the performance of the network. SpeciGLYPH<28>cally, our best KROCC increases from 0.6965 to 0.6993 and 0.7016 when three, and four indicators are used respectively. We GLYPH<28>nd that when using two indicators, the combination of CNNTK and OS outperforms other combinations. It is natural since CNNTK and OS are the top-two standalone indicator. The overall performance increases when more indicators are used. The behaviour is consistent for all quality assessment methods. We can see that the best correlation is obtained when all indicators are used. Figure 5 shows the trend of\nTABLE 5. Comparison with different combination of indicators for FI on CIFAR-10, measured by KROCC.\nFIGURE 5. Overall comparison for different combination of indicators measured on CIFAR-10, NAS-Bench-201.\nincreasing the number of indicators for FI on NAS-Bench201 improves the performance.\nFor comparing the difference in performance, we conduct the statistical tests between the proposed Fusion Indicator and the standalone one. We GLYPH<28>rst deGLYPH<28>ne the absolute difference values between the predicted score O y (after nonlinear regression) and the actual accuracy y as 1 D j y GLYPH<0> O y j . The GLYPH<28>rst test veriGLYPH<28>es the normality of 1 and the second test determines whether the 1 from one indicator are statistically indistinguishable from another indicator. The signiGLYPH<28>cance level is 5% for both tests. We use Shapiro-Wilk test for normality and the results ( p -value) are shown in Table 6. For the second test, Wilcoxon rank-sum is performed because there is no normal distribution case as in Table 6. The results for the second test are shown in Table 7. In most cases, the indicators are statistically different from each other, except for CNNTK versus NLR on NAS-Bench-101, which is not different. In general, the proposed FI statistically improves the performance.",
        "original_text": "We also investigate the performance of our FI by combining different indicators on CIFAR-10. There are 11 combinations in total. The weights are sampled following the above analysis. KROCC is used to measure performance. We list the results in Table 5.\nFrom Table 5, it can be seen that on NAS-Bench-101 combining indicators further improves the performance. For example, NLR, CNNTK, OS achieves around 0.27 KROCC but combining them enhances the performance to 0.30 GLYPH<24> 0.34 KROCC. The best combination is f CJ, OS g .\nOn NAS-Bench-201, we observe that when two indicators are used, the performance measured by KROCC has a high standard deviation (e.g., the value of KROCC is ranging from 0.5610 to 0.6965). On a search space that has higher diversity (e.g., more operators), fusing more indicators helps improve the performance of the network. SpeciGLYPH<28>cally, our best KROCC increases from 0.6965 to 0.6993 and 0.7016 when three, and four indicators are used respectively. We GLYPH<28>nd that when using two indicators, the combination of CNNTK and OS outperforms other combinations. It is natural since CNNTK and OS are the top-two standalone indicator. The overall performance increases when more indicators are used. The behaviour is consistent for all quality assessment methods. We can see that the best correlation is obtained when all indicators are used. Figure 5 shows the trend of\nTABLE 5. Comparison with different combination of indicators for FI on CIFAR-10, measured by KROCC.\nFIGURE 5. Overall comparison for different combination of indicators measured on CIFAR-10, NAS-Bench-201.\nincreasing the number of indicators for FI on NAS-Bench201 improves the performance.\nFor comparing the difference in performance, we conduct the statistical tests between the proposed Fusion Indicator and the standalone one. We GLYPH<28>rst deGLYPH<28>ne the absolute difference values between the predicted score O y (after nonlinear regression) and the actual accuracy y as 1 D j y GLYPH<0> O y j . The GLYPH<28>rst test veriGLYPH<28>es the normality of 1 and the second test determines whether the 1 from one indicator are statistically indistinguishable from another indicator. The signiGLYPH<28>cance level is 5% for both tests. We use Shapiro-Wilk test for normality and the results ( p -value) are shown in Table 6. For the second test, Wilcoxon rank-sum is performed because there is no normal distribution case as in Table 6. The results for the second test are shown in Table 7. In most cases, the indicators are statistically different from each other, except for CNNTK versus NLR on NAS-Bench-101, which is not different. In general, the proposed FI statistically improves the performance.",
        "context": "Evaluates the performance of combining different indicators on CIFAR-10, demonstrating that combining indicators further improves NAS performance, with the CJ, OS combination yielding the best results on NAS-Bench-201.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            8
        ],
        "id": "d42f8336a49eaf8d7d0634bf686ffb2a26214feb4a708e6c3af60943fcbc5534"
    },
    {
        "text": "Identifies potential threats to the research's validity, including limited search spaces, small datasets, and bias due to pre-training evaluation.\n\nThere can be a few factors that threaten the validity of this research. We brieGLYPH<29>y list several potential threats V\n- GLYPH<15> Search space: There are several search spaces which are used in other NAS algorithms such as AmoebaNet [42],\nTABLE 6. The results from Shapiro-Wilk tests on CIFAR-10.\nTABLE 7. The statistical significance matrix on CIFAR-10 with 95% confidence. Each element in the table is a codeword for 2 symbols. The first and second position in the symbol indicate the result of the hypothesis test on NAS-Bench-201 and NAS-Bench-101.\nFBNet [44]. The search space of NAS-Bench-101/201 is much smaller than the aforementioned search space.\n- GLYPH<15> Dataset: Current NAS benchmark only uses small dataset such as CIFAR-10, CIFAR-100, and reduced ImageNet-16-120. Thus, it is common to ask whether the training-free indicator works well on large-scale datasets (e.g., ImageNet [1]).\n- GLYPH<15> Bias in the experiment: Since the training-free indicator evaluates the network before training, the score is affected by the value of the network's parameters.\nRegarding the GLYPH<28>rst and the second threats, there is no benchmark dataset for these search spaces at the current state. It is because training on a large-scale dataset requires a lot of computational costs, as mentioned in [33]. Due to this reason, most NAS benchmark datasets use smaller search spaces and train the networks on a small dataset such as CIFAR-10. However, we will validate the performance of training-free indicators as well as the Fusion indicator once they are available. In order to counter the last threat, we conduct the experiments multiple times (i.e., 10 times) with different random seeds and take the average as the GLYPH<28>nal results. This mitigates the inGLYPH<29>uence of random weights on the results. Extensive experiments on two recently released NAS-Bench-101/201 conGLYPH<28>rm that the Fusion Indicator brings many beneGLYPH<28>ts in ranking the networks.",
        "original_text": "There can be a few factors that threaten the validity of this research. We brieGLYPH<29>y list several potential threats V\n- GLYPH<15> Search space: There are several search spaces which are used in other NAS algorithms such as AmoebaNet [42],\nTABLE 6. The results from Shapiro-Wilk tests on CIFAR-10.\nTABLE 7. The statistical significance matrix on CIFAR-10 with 95% confidence. Each element in the table is a codeword for 2 symbols. The first and second position in the symbol indicate the result of the hypothesis test on NAS-Bench-201 and NAS-Bench-101.\nFBNet [44]. The search space of NAS-Bench-101/201 is much smaller than the aforementioned search space.\n- GLYPH<15> Dataset: Current NAS benchmark only uses small dataset such as CIFAR-10, CIFAR-100, and reduced ImageNet-16-120. Thus, it is common to ask whether the training-free indicator works well on large-scale datasets (e.g., ImageNet [1]).\n- GLYPH<15> Bias in the experiment: Since the training-free indicator evaluates the network before training, the score is affected by the value of the network's parameters.\nRegarding the GLYPH<28>rst and the second threats, there is no benchmark dataset for these search spaces at the current state. It is because training on a large-scale dataset requires a lot of computational costs, as mentioned in [33]. Due to this reason, most NAS benchmark datasets use smaller search spaces and train the networks on a small dataset such as CIFAR-10. However, we will validate the performance of training-free indicators as well as the Fusion indicator once they are available. In order to counter the last threat, we conduct the experiments multiple times (i.e., 10 times) with different random seeds and take the average as the GLYPH<28>nal results. This mitigates the inGLYPH<29>uence of random weights on the results. Extensive experiments on two recently released NAS-Bench-101/201 conGLYPH<28>rm that the Fusion Indicator brings many beneGLYPH<28>ts in ranking the networks.",
        "context": "Identifies potential threats to the research's validity, including limited search spaces, small datasets, and bias due to pre-training evaluation.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            8,
            9
        ],
        "id": "a4de586ab7e0ebefe179cfc9aa7fd2d8cd4432e25be12b8cc4aff1a170cde144"
    },
    {
        "text": "Summarizes the core motivation and key findings of the proposed training-free NAS method (FI), highlighting its superior performance compared to standalone indicators and its potential for future development in query-based NAS algorithms.\n\nIn this paper, we proposed a simple yet effective method for training-free NAS, called FI. The core motivation of our work is to harmonize several characteristics, including correlation of Jacobian, the number of linear regions, the condition number of the neural tangent kernel, and the output sensitivity of a network in a weighted sum manner. Through extensive experiments, our work demonstrated that the proposed FI achieves a higher correlation between the predicted score and the network's accuracy than standalone indicator in all quality assessment methods. The best combination on NAS-Bench-101 and NAS-Bench-201 is f CJ, OS g and f CJ, NLR, CNNTK, OS g , respectively. The large search space such as NAS-Bench-201 can fully use the characteristics effectively because of the diversity in their search space. Interestingly, we GLYPH<28>nd that fusing only two indicators (e.g., f CNNTK, OS g on NAS-Bench-201) achieved comparable performance to fuse four indicators.\nWe encourage the researchers in this GLYPH<28>eld to discover more characteristics of a network so that we can develop a powerful training-free indicator. The proposed FI can be applied to query-based NAS algorithms such as random search or evolutionary search easily. We plan to develop an efGLYPH<28>cient search method that uses the proposed FI in our future work.",
        "original_text": "In this paper, we proposed a simple yet effective method for training-free NAS, called FI. The core motivation of our work is to harmonize several characteristics, including correlation of Jacobian, the number of linear regions, the condition number of the neural tangent kernel, and the output sensitivity of a network in a weighted sum manner. Through extensive experiments, our work demonstrated that the proposed FI achieves a higher correlation between the predicted score and the network's accuracy than standalone indicator in all quality assessment methods. The best combination on NAS-Bench-101 and NAS-Bench-201 is f CJ, OS g and f CJ, NLR, CNNTK, OS g , respectively. The large search space such as NAS-Bench-201 can fully use the characteristics effectively because of the diversity in their search space. Interestingly, we GLYPH<28>nd that fusing only two indicators (e.g., f CNNTK, OS g on NAS-Bench-201) achieved comparable performance to fuse four indicators.\nWe encourage the researchers in this GLYPH<28>eld to discover more characteristics of a network so that we can develop a powerful training-free indicator. The proposed FI can be applied to query-based NAS algorithms such as random search or evolutionary search easily. We plan to develop an efGLYPH<28>cient search method that uses the proposed FI in our future work.",
        "context": "Summarizes the core motivation and key findings of the proposed training-free NAS method (FI), highlighting its superior performance compared to standalone indicators and its potential for future development in query-based NAS algorithms.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            9
        ],
        "id": "fe5207211383b3333adb2a41e8f489d3fc080a9e8a46cf53449feff449f26bb6"
    },
    {
        "text": "Introduces the core concept of training-free NAS and highlights the importance of indicator robustness.\n\n- [1] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ''ImageNet: A large-scale hierarchical image database,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , Jun. 2009, pp. 248GLYPH<21>255.\n- [2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ''ImageNet classiGLYPH<28>cation with deep convolutional neural networks,'' in Proc. Adv. Neural Inf. Process. Syst. , vol. 25, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, Eds. Red Hook, NY, USA: Curran Associates, 2012, pp. 1097GLYPH<21>1105.\n- [3] K. He, X. Zhang, S. Ren, and J. Sun, ''Deep residual learning for image recognition,'' 2015, arXiv:1512.03385 . [Online]. Available: \n- [4] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, ''Densely connected convolutional networks,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jul. 2017, pp. 4700GLYPH<21>4708.\n- [5] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, ''MobileNets: EfGLYPH<28>cient convolutional neural networks for mobile vision applications,'' 2017, arXiv:1704.04861 . [Online]. Available: \n- [6] B. Zoph and Q. V. Le, ''Neural architecture search with reinforcement learning,'' 2016, arXiv:1611.01578 . [Online]. Available: org/abs/1611.01578\n- [7] H. Liu, K. Simonyan, and Y. Yang, ''DARTS: Differentiable architecture search,'' in Proc. Int. Conf. Learn. Represent. , 2019, pp. 1GLYPH<21>12.\n- [8] M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard, and Q. V. Le, ''MnasNet: Platform-aware neural architecture search for mobile,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2019, pp. 2820GLYPH<21>2828.\n- [9] B. Chen, G. Ghiasi, H. Liu, T.-Y. Lin, D. Kalenichenko, H. Adam, and Q. V. Le, ''MnasFPN: Learning latency-aware pyramid architecture for object detection on mobile devices,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2020, pp. 2820GLYPH<21>2828.\n- [10] N. Wang, Y. Gao, H. Chen, P. Wang, Z. Tian, C. Shen, and Y. Zhang, ''NAS-FCOS: Fast neural architecture search for object detection,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2020, pp. 11943GLYPH<21>11951.\n- [11] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, ''Learning transferable architectures for scalable image recognition,'' 2017, arXiv:1707.07012 . [Online]. Available: \n- [12] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean, ''EfGLYPH<28>cient neural architecture search via parameter sharing,'' 2018, arXiv:1802.03268 . [Online]. Available:",
        "original_text": "- [1] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ''ImageNet: A large-scale hierarchical image database,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , Jun. 2009, pp. 248GLYPH<21>255.\n- [2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ''ImageNet classiGLYPH<28>cation with deep convolutional neural networks,'' in Proc. Adv. Neural Inf. Process. Syst. , vol. 25, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, Eds. Red Hook, NY, USA: Curran Associates, 2012, pp. 1097GLYPH<21>1105.\n- [3] K. He, X. Zhang, S. Ren, and J. Sun, ''Deep residual learning for image recognition,'' 2015, arXiv:1512.03385 . [Online]. Available: \n- [4] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, ''Densely connected convolutional networks,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jul. 2017, pp. 4700GLYPH<21>4708.\n- [5] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, ''MobileNets: EfGLYPH<28>cient convolutional neural networks for mobile vision applications,'' 2017, arXiv:1704.04861 . [Online]. Available: \n- [6] B. Zoph and Q. V. Le, ''Neural architecture search with reinforcement learning,'' 2016, arXiv:1611.01578 . [Online]. Available: org/abs/1611.01578\n- [7] H. Liu, K. Simonyan, and Y. Yang, ''DARTS: Differentiable architecture search,'' in Proc. Int. Conf. Learn. Represent. , 2019, pp. 1GLYPH<21>12.\n- [8] M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard, and Q. V. Le, ''MnasNet: Platform-aware neural architecture search for mobile,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2019, pp. 2820GLYPH<21>2828.\n- [9] B. Chen, G. Ghiasi, H. Liu, T.-Y. Lin, D. Kalenichenko, H. Adam, and Q. V. Le, ''MnasFPN: Learning latency-aware pyramid architecture for object detection on mobile devices,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2020, pp. 2820GLYPH<21>2828.\n- [10] N. Wang, Y. Gao, H. Chen, P. Wang, Z. Tian, C. Shen, and Y. Zhang, ''NAS-FCOS: Fast neural architecture search for object detection,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2020, pp. 11943GLYPH<21>11951.\n- [11] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, ''Learning transferable architectures for scalable image recognition,'' 2017, arXiv:1707.07012 . [Online]. Available: \n- [12] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean, ''EfGLYPH<28>cient neural architecture search via parameter sharing,'' 2018, arXiv:1802.03268 . [Online]. Available:",
        "context": "Introduces the core concept of training-free NAS and highlights the importance of indicator robustness.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            9
        ],
        "id": "9baff0c67f69639b19f1e6b6fad665e0e21686f8ecac9f19099f2c535bd8b63a"
    },
    {
        "text": "Provides supporting evidence for the argument on economic inequality; transitions from background information to proposed methodology.\n\n- [13] J. Chen, K. Li, K. Bilal, X. Zhou, K. Li, and P. S. Yu, ''A bi-layered parallel training architecture for large-scale convolutional neural networks,'' IEEE Trans. Parallel Distrib. Syst. , vol. 30, no. 5, pp. 965GLYPH<21>976, May 2019.\n- [14] Y. Xu, L. Xie, X. Zhang, X. Chen, G. Qi, Q. Tian, and H. Xiong, ''PC-DARTS: Partial channel connections for memory-efGLYPH<28>cient differentiable architecture search,'' 2019, arXiv:1907.05737 . [Online]. Available: \n- [15] C. Li, Z. Yu, Y. Fu, Y. Zhang, Y. Zhao, H. You, Q. Yu, Y. Wang, C. Hao, and Y. Lin, ''HW-NAS-Bench: Hardware-aware neural architecture search benchmark,'' in Proc. Int. Conf. Learn. Represent. , 2021. [Online]. Available: \n- [16] J. Mellor, J. Turner, A. Storkey, and E. J. Crowley, ''Neural architecture search without training,'' 2020, arXiv:2006.04647 . [Online]. Available: \n- [17] M. Forouzesh, F. Salehi, and P. Thiran, ''Generalization comparison of deep neural networks via output sensitivity,'' in Proc. 25th Int. Conf. Pattern Recognit. (ICPR) , Jan. 2021, pp. 7411GLYPH<21>7418.\n- [18] R. Novak, Y. Bahri, D. A. AbolaGLYPH<28>a, J. Pennington, and J. Sohl-Dickstein, ''Sensitivity and generalization in neural networks: An empirical study,'' 2018, arXiv:1802.08760 . [Online]. Available: \n- [19] K. Kawaguchi, L. P. Kaelbling, and Y. Bengio, ''Generalization in deep learning,'' 2017, arXiv:1710.05468 . [Online]. Available: \n- [20] A. Jacot, F. Gabriel, and C. Hongler, ''Neural tangent kernel: Convergence and generalization in neural networks,'' 2018, arXiv:1806.07572 . [Online]. Available: \n- [21] H. Xiong, L. Huang, M. Yu, L. Liu, F. Zhu, and L. Shao, ''On the number of linear regions of convolutional neural networks,'' in Proc. 37th Int. Conf. Mach. Learn. , vol. 119, H. D. III and A. Singh, Eds. Jul. 2020, pp. 10514GLYPH<21>10523.\n- [22] M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, and J. Sohl-Dickstein, ''On the expressive power of deep neural networks,'' 2017, arXiv:1606.05336 . [Online]. Available: \n- [23] L. Xiao, J. Pennington, and S. S. Schoenholz, ''Disentangling trainability and generalization in deep learning,'' 2019, arXiv:1912.13053 . [Online]. Available: \n- [24] W. Chen, X. Gong, and Z. Wang, ''Neural architecture search on ImageNet in four GPU hours: A theoretically inspired perspective,'' in Proc. Int. Conf. Learn. Represent. , 2021. [Online]. Available: \n- [25] T. Devries and G. W. Taylor, ''Improved regularization of convolutional neural networks with cutout,'' 2017, arXiv:1708.04552 . [Online]. Available: \n- [26] B. Baker, O. Gupta, N. Naik, and R. Raskar, ''Designing neural network architectures using reinforcement learning,'' 2016, arXiv:1611.02167 . [Online]. Available: \n- [27] Z. Zhong, J. Yan, W. Wu, J. Shao, and C.-L. Liu, ''Practical blockwise neural network architecture generation,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , Jun. 2018, pp. 2423GLYPH<21>2432.\n- [28] L. Xie and A. Yuille, ''Genetic CNN,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , Oct. 2017, pp. 1379GLYPH<21>1388.\n- [29] E. Real, S. Moore, A. Selle, S. Saxena, Y . L. Suematsu, J. Tan, Q. V . Le, and A. Kurakin, ''Large-scale evolution of image classiGLYPH<28>ers,'' in Proc. 34th Int. Conf. Mach. Learn. (ICML) , Sydney, NSW, Australia, vol. 70, Aug. 2017, pp. 2902GLYPH<21>2911.\n- [30] T. Elsken, J. H. Metzen, and F. Hutter, ''EfGLYPH<28>cient multi-objective neural architecture search via Lamarckian evolution,'' in Proc. Int. Conf. Learn. Represent. , Sep. 2019. [Online]. Available: openreview.net/forum?id=ByME42AqK7\n- [31] X. Chen, L. Xie, J. Wu, and Q. Tian, ''Progressive differentiable architecture search: Bridging the depth gap between search and evaluation,'' 2019, arXiv:1904.12760 . [Online]. Available: \n- [32] C. Ying, A. Klein, E. Christiansen, E. Real, K. Murphy, and F. Hutter, ''Nas-bench-101: Towards reproducible neural architecture search,'' in Proc. ICML , 2019, pp. 7105GLYPH<21>7114.\n- [33] X. Dong and Y. Yang, ''NAS-Bench-201: Extending the scope of reproducible neural architecture search,'' in Proc. Int. Conf. Learn. Represent. , 2020, pp. 2GLYPH<21>17.\n- [34] P. Chrabaszcz, I. Loshchilov, and F. Hutter, ''A downsampled variant of ImageNet as an alternative to the CIFAR datasets,'' 2017, arXiv:1707.08819 . [Online]. Available: \n- [35] J. Yang, X. Zeng, S. Zhong, and S. Wu, ''Effective neural network ensemble approach for improving generalization performance,'' IEEE Trans. Neural Netw. Learn. Syst. , vol. 24, no. 6, pp. 878GLYPH<21>887, Jun. 2013.\n- [36] R. Pascanu, G. Montufar, and Y. Bengio, ''On the number of response regions of deep feed forward networks with piece-wise linear activations,'' 2014, arXiv:1312.6098 . [Online]. Available:",
        "original_text": "- [13] J. Chen, K. Li, K. Bilal, X. Zhou, K. Li, and P. S. Yu, ''A bi-layered parallel training architecture for large-scale convolutional neural networks,'' IEEE Trans. Parallel Distrib. Syst. , vol. 30, no. 5, pp. 965GLYPH<21>976, May 2019.\n- [14] Y. Xu, L. Xie, X. Zhang, X. Chen, G. Qi, Q. Tian, and H. Xiong, ''PC-DARTS: Partial channel connections for memory-efGLYPH<28>cient differentiable architecture search,'' 2019, arXiv:1907.05737 . [Online]. Available: \n- [15] C. Li, Z. Yu, Y. Fu, Y. Zhang, Y. Zhao, H. You, Q. Yu, Y. Wang, C. Hao, and Y. Lin, ''HW-NAS-Bench: Hardware-aware neural architecture search benchmark,'' in Proc. Int. Conf. Learn. Represent. , 2021. [Online]. Available: \n- [16] J. Mellor, J. Turner, A. Storkey, and E. J. Crowley, ''Neural architecture search without training,'' 2020, arXiv:2006.04647 . [Online]. Available: \n- [17] M. Forouzesh, F. Salehi, and P. Thiran, ''Generalization comparison of deep neural networks via output sensitivity,'' in Proc. 25th Int. Conf. Pattern Recognit. (ICPR) , Jan. 2021, pp. 7411GLYPH<21>7418.\n- [18] R. Novak, Y. Bahri, D. A. AbolaGLYPH<28>a, J. Pennington, and J. Sohl-Dickstein, ''Sensitivity and generalization in neural networks: An empirical study,'' 2018, arXiv:1802.08760 . [Online]. Available: \n- [19] K. Kawaguchi, L. P. Kaelbling, and Y. Bengio, ''Generalization in deep learning,'' 2017, arXiv:1710.05468 . [Online]. Available: \n- [20] A. Jacot, F. Gabriel, and C. Hongler, ''Neural tangent kernel: Convergence and generalization in neural networks,'' 2018, arXiv:1806.07572 . [Online]. Available: \n- [21] H. Xiong, L. Huang, M. Yu, L. Liu, F. Zhu, and L. Shao, ''On the number of linear regions of convolutional neural networks,'' in Proc. 37th Int. Conf. Mach. Learn. , vol. 119, H. D. III and A. Singh, Eds. Jul. 2020, pp. 10514GLYPH<21>10523.\n- [22] M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, and J. Sohl-Dickstein, ''On the expressive power of deep neural networks,'' 2017, arXiv:1606.05336 . [Online]. Available: \n- [23] L. Xiao, J. Pennington, and S. S. Schoenholz, ''Disentangling trainability and generalization in deep learning,'' 2019, arXiv:1912.13053 . [Online]. Available: \n- [24] W. Chen, X. Gong, and Z. Wang, ''Neural architecture search on ImageNet in four GPU hours: A theoretically inspired perspective,'' in Proc. Int. Conf. Learn. Represent. , 2021. [Online]. Available: \n- [25] T. Devries and G. W. Taylor, ''Improved regularization of convolutional neural networks with cutout,'' 2017, arXiv:1708.04552 . [Online]. Available: \n- [26] B. Baker, O. Gupta, N. Naik, and R. Raskar, ''Designing neural network architectures using reinforcement learning,'' 2016, arXiv:1611.02167 . [Online]. Available: \n- [27] Z. Zhong, J. Yan, W. Wu, J. Shao, and C.-L. Liu, ''Practical blockwise neural network architecture generation,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , Jun. 2018, pp. 2423GLYPH<21>2432.\n- [28] L. Xie and A. Yuille, ''Genetic CNN,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , Oct. 2017, pp. 1379GLYPH<21>1388.\n- [29] E. Real, S. Moore, A. Selle, S. Saxena, Y . L. Suematsu, J. Tan, Q. V . Le, and A. Kurakin, ''Large-scale evolution of image classiGLYPH<28>ers,'' in Proc. 34th Int. Conf. Mach. Learn. (ICML) , Sydney, NSW, Australia, vol. 70, Aug. 2017, pp. 2902GLYPH<21>2911.\n- [30] T. Elsken, J. H. Metzen, and F. Hutter, ''EfGLYPH<28>cient multi-objective neural architecture search via Lamarckian evolution,'' in Proc. Int. Conf. Learn. Represent. , Sep. 2019. [Online]. Available: openreview.net/forum?id=ByME42AqK7\n- [31] X. Chen, L. Xie, J. Wu, and Q. Tian, ''Progressive differentiable architecture search: Bridging the depth gap between search and evaluation,'' 2019, arXiv:1904.12760 . [Online]. Available: \n- [32] C. Ying, A. Klein, E. Christiansen, E. Real, K. Murphy, and F. Hutter, ''Nas-bench-101: Towards reproducible neural architecture search,'' in Proc. ICML , 2019, pp. 7105GLYPH<21>7114.\n- [33] X. Dong and Y. Yang, ''NAS-Bench-201: Extending the scope of reproducible neural architecture search,'' in Proc. Int. Conf. Learn. Represent. , 2020, pp. 2GLYPH<21>17.\n- [34] P. Chrabaszcz, I. Loshchilov, and F. Hutter, ''A downsampled variant of ImageNet as an alternative to the CIFAR datasets,'' 2017, arXiv:1707.08819 . [Online]. Available: \n- [35] J. Yang, X. Zeng, S. Zhong, and S. Wu, ''Effective neural network ensemble approach for improving generalization performance,'' IEEE Trans. Neural Netw. Learn. Syst. , vol. 24, no. 6, pp. 878GLYPH<21>887, Jun. 2013.\n- [36] R. Pascanu, G. Montufar, and Y. Bengio, ''On the number of response regions of deep feed forward networks with piece-wise linear activations,'' 2014, arXiv:1312.6098 . [Online]. Available:",
        "context": "Provides supporting evidence for the argument on economic inequality; transitions from background information to proposed methodology.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            10
        ],
        "id": "5799d8649edfe3219919099d5b288c6be3781663b0c654a104a4032853f2ed07"
    },
    {
        "text": "Provides supporting evidence for the argument on image quality assessment, specifically detailing metrics and algorithms used to evaluate image quality.\n\n- [37] J. Lee, L. Xiao, S. S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, and J. Pennington, ''Wide neural networks of any depth evolve as linear models under gradient descent,'' J. Stat. Mech. Theory Exp. , vol. 2020, no. 12, Dec. 2020, Art. no. 124002.\n- [38] L. Zhang, Y. Shen, and H. Li, ''VSI: A visual saliency-induced index for perceptual image quality assessment,'' IEEE Trans. Image Process. , vol. 23, no. 10, pp. 4270GLYPH<21>4281, Aug. 2014.\n- [39] S.-H. Bae and M. Kim, ''DCT-QM: A DCT-based quality degradation metric for image quality optimization problems,'' IEEE Trans. Image Process. , vol. 25, no. 10, pp. 4916GLYPH<21>4930, Oct. 2016.\n- [40] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, ''Image quality assessment: From error visibility to structural similarity,'' IEEE Trans. Image Process. , vol. 13, no. 4, pp. 600GLYPH<21>612, Apr. 2004.\n- [41] H. R. Sheikh, M. F. Sabir, and A. C. Bovik, ''A statistical evaluation of recent full reference image quality assessment algorithms,'' IEEE Trans. Image Process. , vol. 15, no. 11, pp. 3440GLYPH<21>3451, Nov. 2006.\n- [42] E. Real, A. Aggarwal, Y. Huang, and Q. V. Le, ''Regularized evolution for image classiGLYPH<28>er architecture search,'' in Proc. AAAI Conf. Artif. Intell. , vol. 33, Jul. 2019, pp. 4780GLYPH<21>4789.\n- [43] K. He, X. Zhang, S. Ren, and J. Sun, ''Delving deep into rectiGLYPH<28>ers: Surpassing human-level performance on imagenet classiGLYPH<28>cation,'' 2015, arXiv:1502.01852 . [Online]. Available: \n- [44] B. Wu, K. Keutzer, X. Dai, P. Zhang, Y. Wang, F. Sun, Y. Wu, Y. Tian, P. Vajda, and Y. Jia, ''FBNet: Hardware-aware efGLYPH<28>cient ConvNet design via differentiable neural architecture search,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2019, pp. 10734GLYPH<21>10742.\nLINH-TAM TRAN received the bachelor's degree from the Department of Computer Science and Engineering, Ho Chi Minh City University of Technology, Vietnam, in 2014, and the M.S. degree from Hongik University, Seoul, South Korea, in 2018. He is currently pursuing the Ph.D. degree with the Department of Computer Science and Engineering, Kyung Hee University, Suwon, South Korea. His research interest includes neural architecture search for practical applications.\nMUHAMMAD SALMAN ALI received the bachelor's degree in computer science from the National University of Sciences and Technology (NUST), Islamabad, Pakistan, in 2018. He is currently pursuing the M.S. degree leading to the Ph.D. degree with Kyung Hee University, South Korea. His research interests include deep learning interpretation and the effect of soft errors on deep neural networks. He was a recipient of the gold medal for being a high-achiever during his UG studies.\nSUNG-HO BAE (Member, IEEE) received the B.S. degree from Kyung Hee University, South Korea, in 2011, and the M.S. and Ph.D. degrees from Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea, in 2012 and 2016, respectively. From 2016 to 2017, he was a Postdoctoral Associate with the Computer Science and ArtiGLYPH<28>cial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT), MA, USA. Since 2017, he has been an Assistant Professor with the Department of Computer Science and Engineering, Kyung Hee University. He has been involved in model compression/interpretation for deep neural networks and inverse problems in image processing and computer vision.",
        "original_text": "- [37] J. Lee, L. Xiao, S. S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, and J. Pennington, ''Wide neural networks of any depth evolve as linear models under gradient descent,'' J. Stat. Mech. Theory Exp. , vol. 2020, no. 12, Dec. 2020, Art. no. 124002.\n- [38] L. Zhang, Y. Shen, and H. Li, ''VSI: A visual saliency-induced index for perceptual image quality assessment,'' IEEE Trans. Image Process. , vol. 23, no. 10, pp. 4270GLYPH<21>4281, Aug. 2014.\n- [39] S.-H. Bae and M. Kim, ''DCT-QM: A DCT-based quality degradation metric for image quality optimization problems,'' IEEE Trans. Image Process. , vol. 25, no. 10, pp. 4916GLYPH<21>4930, Oct. 2016.\n- [40] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, ''Image quality assessment: From error visibility to structural similarity,'' IEEE Trans. Image Process. , vol. 13, no. 4, pp. 600GLYPH<21>612, Apr. 2004.\n- [41] H. R. Sheikh, M. F. Sabir, and A. C. Bovik, ''A statistical evaluation of recent full reference image quality assessment algorithms,'' IEEE Trans. Image Process. , vol. 15, no. 11, pp. 3440GLYPH<21>3451, Nov. 2006.\n- [42] E. Real, A. Aggarwal, Y. Huang, and Q. V. Le, ''Regularized evolution for image classiGLYPH<28>er architecture search,'' in Proc. AAAI Conf. Artif. Intell. , vol. 33, Jul. 2019, pp. 4780GLYPH<21>4789.\n- [43] K. He, X. Zhang, S. Ren, and J. Sun, ''Delving deep into rectiGLYPH<28>ers: Surpassing human-level performance on imagenet classiGLYPH<28>cation,'' 2015, arXiv:1502.01852 . [Online]. Available: \n- [44] B. Wu, K. Keutzer, X. Dai, P. Zhang, Y. Wang, F. Sun, Y. Wu, Y. Tian, P. Vajda, and Y. Jia, ''FBNet: Hardware-aware efGLYPH<28>cient ConvNet design via differentiable neural architecture search,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2019, pp. 10734GLYPH<21>10742.\nLINH-TAM TRAN received the bachelor's degree from the Department of Computer Science and Engineering, Ho Chi Minh City University of Technology, Vietnam, in 2014, and the M.S. degree from Hongik University, Seoul, South Korea, in 2018. He is currently pursuing the Ph.D. degree with the Department of Computer Science and Engineering, Kyung Hee University, Suwon, South Korea. His research interest includes neural architecture search for practical applications.\nMUHAMMAD SALMAN ALI received the bachelor's degree in computer science from the National University of Sciences and Technology (NUST), Islamabad, Pakistan, in 2018. He is currently pursuing the M.S. degree leading to the Ph.D. degree with Kyung Hee University, South Korea. His research interests include deep learning interpretation and the effect of soft errors on deep neural networks. He was a recipient of the gold medal for being a high-achiever during his UG studies.\nSUNG-HO BAE (Member, IEEE) received the B.S. degree from Kyung Hee University, South Korea, in 2011, and the M.S. and Ph.D. degrees from Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea, in 2012 and 2016, respectively. From 2016 to 2017, he was a Postdoctoral Associate with the Computer Science and ArtiGLYPH<28>cial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT), MA, USA. Since 2017, he has been an Assistant Professor with the Department of Computer Science and Engineering, Kyung Hee University. He has been involved in model compression/interpretation for deep neural networks and inverse problems in image processing and computer vision.",
        "context": "Provides supporting evidence for the argument on image quality assessment, specifically detailing metrics and algorithms used to evaluate image quality.",
        "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
        "pages": [
            10
        ],
        "id": "c71ffd430ac993f6ee836862cd0725f08ea008c04cc6c31ef803ae61000bf376"
    }
]