[
    {
        "text": "This chunk provides the publication details and acknowledges funding sources for the research, outlining the paper's origin and support.\n\nReceived July 29, 2018, accepted August 27, 2018, date of publication September 13, 2018, date of current version October 8, 2018.\nDigital Object Identifier 10.1 109/ACCESS.2018.2869735",
        "original_text": "Received July 29, 2018, accepted August 27, 2018, date of publication September 13, 2018, date of current version October 8, 2018.\nDigital Object Identifier 10.1 109/ACCESS.2018.2869735",
        "context": "This chunk provides the publication details and acknowledges funding sources for the research, outlining the paper's origin and support.",
        "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
        "pages": [
            1
        ],
        "id": "990434da769b3b976cb9daaf6357417afdaad3ec0ad97db94bbba4a92877bdc2"
    },
    {
        "text": "This section introduces the authors and funding sources, and outlines the paper's focus on improving stock market prediction by integrating multiple data sources and utilizing a multi-source multiple instance model.\n\nXI ZHANG 1 , (Member, IEEE), SIYU QU 1 , JIEYUN HUANG 1 , BINXING FANG 1 , AND PHILIP YU 2 , (Fellow, IEEE)\n1 Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, Beijing University of Posts and Telecommunications,\nBeijing 100876, China\n2 Department of Computer Science, The University of Illinois at Chicago, Chicago, IL 60607, USA\nCorresponding author: Xi Zhang (zhangx@bupt.edu.cn)\nThis work was supported in part by the State Key Development Program of Basic Research of China under Grant 2013CB329605, in part by the Natural Science Foundation of China under Grant 61300014, in part by the NSF under Grant IIS-1526499, Grant IIS-1763325, and Grant CNS-1626432, and in part by the DongGuan Innovative Research Team Program under Grant 201636000100038.\nABSTRACT Forecasting the stock market movements is an important and challenging task. As the Web information grows, researchers begin to extract effective indicators (e.g., the events and sentiments) from the Web to facilitate the prediction. However, the indicators obtained in previous studies are usually based on only one data source and thus may not fully cover the factors that can affect the stock market movements. In this paper, to improve the prediction for stock market composite index movements, we exploit the consistencies among different data sources, and develop a multi-source multiple instance model that can effectively combine events, sentiments, as well as the quantitative data into a comprehensive framework. To effectively capture the news events, we successfully apply a novel event extraction and representation method. Evaluations on the data from the year 2015 and 2016 demonstrate the effectiveness of our model. In addition, our approach is able to automatically determine the importance of each data source and identify the crucial input information that is considered to drive the movements, making the predictions interpretable.\nINDEX TERMS Stock prediction, multiple instance, event extraction, sentiment analysis.",
        "original_text": "XI ZHANG 1 , (Member, IEEE), SIYU QU 1 , JIEYUN HUANG 1 , BINXING FANG 1 , AND PHILIP YU 2 , (Fellow, IEEE)\n1 Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, Beijing University of Posts and Telecommunications,\nBeijing 100876, China\n2 Department of Computer Science, The University of Illinois at Chicago, Chicago, IL 60607, USA\nCorresponding author: Xi Zhang (zhangx@bupt.edu.cn)\nThis work was supported in part by the State Key Development Program of Basic Research of China under Grant 2013CB329605, in part by the Natural Science Foundation of China under Grant 61300014, in part by the NSF under Grant IIS-1526499, Grant IIS-1763325, and Grant CNS-1626432, and in part by the DongGuan Innovative Research Team Program under Grant 201636000100038.\nABSTRACT Forecasting the stock market movements is an important and challenging task. As the Web information grows, researchers begin to extract effective indicators (e.g., the events and sentiments) from the Web to facilitate the prediction. However, the indicators obtained in previous studies are usually based on only one data source and thus may not fully cover the factors that can affect the stock market movements. In this paper, to improve the prediction for stock market composite index movements, we exploit the consistencies among different data sources, and develop a multi-source multiple instance model that can effectively combine events, sentiments, as well as the quantitative data into a comprehensive framework. To effectively capture the news events, we successfully apply a novel event extraction and representation method. Evaluations on the data from the year 2015 and 2016 demonstrate the effectiveness of our model. In addition, our approach is able to automatically determine the importance of each data source and identify the crucial input information that is considered to drive the movements, making the predictions interpretable.\nINDEX TERMS Stock prediction, multiple instance, event extraction, sentiment analysis.",
        "context": "This section introduces the authors and funding sources, and outlines the paper's focus on improving stock market prediction by integrating multiple data sources and utilizing a multi-source multiple instance model.",
        "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
        "pages": [
            1
        ],
        "id": "9ed6907515207f4a5e5cd48f826d4d5e8cc95ce11d0377e9b4f08ae801c7d000"
    },
    {
        "text": "Introduces the central thesis about stock prediction by integrating multiple data sources and focusing on event representation and sentiment analysis.\n\nStock markets play important roles in the economic operations of modern society. The estimation of the stock market index is of clear interest to various stakeholders in the market. According to the EfGLYPH<28>cient Market Hypothesis (EMH) [1], the stock market prices reGLYPH<29>ect all available information, and thus the prediction naturally relies on information from multiple sources, which can be roughly categorized into (1) quantitative data, e.g., historical prices, turnover rate, and (2) qualitative descriptions, such as the annual reports, announcements, news and social media posts. It is challenging to deal with qualitative data as they are usually unstructured and thus extracting useful signals from them is not trivial.\nAlong with the growing Web information and the advance of Natural Language Processing (NLP) techniques, recent works begin to explore Web news for market prediction. A number of existing studies have shown that the events reported in news are important signals that can drive market\nGLYPH<29>uctuations [2]GLYPH<21>[4]. However, most of the previous works represent news documents using simple features (e.g., bagwords, noun phrases, named entities) [5], [6], which may discard syntax information. Due to the large volume and diverse expressions of the events, how to represent them as useful features, and how to identify the crucial events that have signiGLYPH<28>cant impacts on the stock market are not trivial problems. In addition to events, a line of studies has shown that the investors' opinions can also largely inGLYPH<29>uence the market volatility [7], [8]. With the prosperity of Web 2.0, the sentiments extracted from social media can be beneGLYPH<28>cial to predictions. Since both events and sentiments can drive the GLYPH<29>uctuations of the market, it is natural to investigate how to effectively fuse them together to make a better prediction. The improvement may come from the correlations among different sources, and the consensus prediction with multisource information can potentially outperform each prediction relying on a single source. This problem is analogous to the multi-labeler learning problem in crowdsourcing [9], [10],\nVOLUME 6, 2018\nbut different from those studies that usually assume a labeler conducts classiGLYPH<28>cation with full information, each ''labeler'' (i.e., classiGLYPH<28>er) in this study is source-speciGLYPH<28>c and only provided with limited information from its own source, making the consensus among labelers even more challenging.\nIn this work, we aim to learn a predictive model for describing the GLYPH<29>uctuations in the stock market index by utilizing various sources of data, involving the historical quantitative data, the social media and Web news. The essential features we extract include the event representations from news articles and the sentiments from social media. Firstly, we propose a novel method to capture the event information. SpeciGLYPH<28>cally, structured events are extracted from news texts and then used as the inputs for Restricted Boltzmann Machines (RBMs) to do the pre-training. After that, the output vectors from RBMs are used as the inputs to a recently proposed sentence2vec framework [11], in order to achieve effective event embeddings. Secondly, we exploit the latent relationships among different data sources with carefully designed loss terms, and propose an extension of the Multiple Instance Learning (MIL) model that can effectively integrate the features from multiple sources to make more accurate predictions. One beneGLYPH<28>t of our method is that we can determine source-speciGLYPH<28>c weights and identify the speciGLYPH<28>c factors that incur the changes in the composite index. Figure 1 shows an example of the news precursors identiGLYPH<28>ed by our model, and the dots with numbers denote the probabilistic estimates for the events leading to the index change on Jan. 26, 2016.\nFIGURE 1. An example of the news events that are responsible for the Shanghai Composite Index change on Jan. 26, 2016. The x-axis is the timeline. The left y-axis is the probability of each event leading to the index change. The right y-axis is the composite index in Shanghai Stock Exchange.\nThe summary of the contributions is as follows:\n- 1) To provide robust and accurate predictions for stock market movements, we extend the Multiple Instance Learning model to integrate the heterogeneous information including Web news, social media posts, and quantitative data.\n- 2) The latent consistencies among different data sources are modeled in our framework by sharing the common estimated true label among the hinge losses of different data sources at the instance level.\n- 3) A novel event representation model is proposed by GLYPH<28>rst extracting structured events from news text, and then training them with deep learning methods involving RBM and sentence2vec to obtain dense vectors.\n- 4) Evaluation results on two-year datasets show that our proposal can outperform the state-of-art baselines. Moreover, the impacts of different sources and the key factors that drive the movements can be obtained.",
        "original_text": "Stock markets play important roles in the economic operations of modern society. The estimation of the stock market index is of clear interest to various stakeholders in the market. According to the EfGLYPH<28>cient Market Hypothesis (EMH) [1], the stock market prices reGLYPH<29>ect all available information, and thus the prediction naturally relies on information from multiple sources, which can be roughly categorized into (1) quantitative data, e.g., historical prices, turnover rate, and (2) qualitative descriptions, such as the annual reports, announcements, news and social media posts. It is challenging to deal with qualitative data as they are usually unstructured and thus extracting useful signals from them is not trivial.\nAlong with the growing Web information and the advance of Natural Language Processing (NLP) techniques, recent works begin to explore Web news for market prediction. A number of existing studies have shown that the events reported in news are important signals that can drive market\nGLYPH<29>uctuations [2]GLYPH<21>[4]. However, most of the previous works represent news documents using simple features (e.g., bagwords, noun phrases, named entities) [5], [6], which may discard syntax information. Due to the large volume and diverse expressions of the events, how to represent them as useful features, and how to identify the crucial events that have signiGLYPH<28>cant impacts on the stock market are not trivial problems. In addition to events, a line of studies has shown that the investors' opinions can also largely inGLYPH<29>uence the market volatility [7], [8]. With the prosperity of Web 2.0, the sentiments extracted from social media can be beneGLYPH<28>cial to predictions. Since both events and sentiments can drive the GLYPH<29>uctuations of the market, it is natural to investigate how to effectively fuse them together to make a better prediction. The improvement may come from the correlations among different sources, and the consensus prediction with multisource information can potentially outperform each prediction relying on a single source. This problem is analogous to the multi-labeler learning problem in crowdsourcing [9], [10],\nVOLUME 6, 2018\nbut different from those studies that usually assume a labeler conducts classiGLYPH<28>cation with full information, each ''labeler'' (i.e., classiGLYPH<28>er) in this study is source-speciGLYPH<28>c and only provided with limited information from its own source, making the consensus among labelers even more challenging.\nIn this work, we aim to learn a predictive model for describing the GLYPH<29>uctuations in the stock market index by utilizing various sources of data, involving the historical quantitative data, the social media and Web news. The essential features we extract include the event representations from news articles and the sentiments from social media. Firstly, we propose a novel method to capture the event information. SpeciGLYPH<28>cally, structured events are extracted from news texts and then used as the inputs for Restricted Boltzmann Machines (RBMs) to do the pre-training. After that, the output vectors from RBMs are used as the inputs to a recently proposed sentence2vec framework [11], in order to achieve effective event embeddings. Secondly, we exploit the latent relationships among different data sources with carefully designed loss terms, and propose an extension of the Multiple Instance Learning (MIL) model that can effectively integrate the features from multiple sources to make more accurate predictions. One beneGLYPH<28>t of our method is that we can determine source-speciGLYPH<28>c weights and identify the speciGLYPH<28>c factors that incur the changes in the composite index. Figure 1 shows an example of the news precursors identiGLYPH<28>ed by our model, and the dots with numbers denote the probabilistic estimates for the events leading to the index change on Jan. 26, 2016.\nFIGURE 1. An example of the news events that are responsible for the Shanghai Composite Index change on Jan. 26, 2016. The x-axis is the timeline. The left y-axis is the probability of each event leading to the index change. The right y-axis is the composite index in Shanghai Stock Exchange.\nThe summary of the contributions is as follows:\n- 1) To provide robust and accurate predictions for stock market movements, we extend the Multiple Instance Learning model to integrate the heterogeneous information including Web news, social media posts, and quantitative data.\n- 2) The latent consistencies among different data sources are modeled in our framework by sharing the common estimated true label among the hinge losses of different data sources at the instance level.\n- 3) A novel event representation model is proposed by GLYPH<28>rst extracting structured events from news text, and then training them with deep learning methods involving RBM and sentence2vec to obtain dense vectors.\n- 4) Evaluation results on two-year datasets show that our proposal can outperform the state-of-art baselines. Moreover, the impacts of different sources and the key factors that drive the movements can be obtained.",
        "context": "Introduces the central thesis about stock prediction by integrating multiple data sources and focusing on event representation and sentiment analysis.",
        "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
        "pages": [
            1,
            2
        ],
        "id": "7ad78330122f8822c806a60689a342ee4a4dc34abf7a776d73dcd0f131fad1ed"
    },
    {
        "text": "Summarizes existing event-driven stock prediction models and highlights a common limitation: reliance on a single data source, prompting the need for a more comprehensive approach.\n\nThere is a line of research works using event-driven stock prediction models. Hogenboom et al . [12] give an overview of event extraction methods. Akita et al . [13] convert newspaper articles into distributed representations via Paragraph Vector and model the temporal effects of past events with LSTM on opening prices of stocks in Tokyo Stock Exchange. Nguyen et al . [14] formulated a temporal sentiment index function, which is used to extract signiGLYPH<28>cant events. Then the corresponding blog posts are analyzed using topic modeling to understand the contents. Ding et al . [15] applied the Open IE tool to extract structured events from texts, and this event extraction method is also implemented as a baseline and compared with our proposal. Ding et al . [16] then trained event embeddings with a neural tensor network and then used the deep convolutional neural network to model inGLYPH<29>uences of events.\nIn addition to events, investors' emotions also have great impacts on the stock market index. Bollen et al . [17] revealed that the public moods derived from Twitter have impacts on stock indicators. Si et al . [3] proposed a technique to leverage topic based sentiments from Twitter to predict the stock market. Makrehchi et al . [18] assigned a positive or negative label for each tweet according to stock movements. The aggregate sentiment per day shows predictive power for stock market prediction. Topic-speciGLYPH<28>c sentiments are learned in [19] to facilitate the stock prediction. However, this method is not suitable to short texts in social media.\nThe common limitation of the aforementioned methods is that they rely only on a single data source and thus may limit the predictive power. In [20], events and sentiments are integrated into a tensor framework together with GLYPH<28>rm-speciGLYPH<28>c features (e.g., P/B, P/E), to model the joint impacts on the stock volatility. We also implement it as a baseline. However, it uses a simple event extraction method which may not fully capture sufGLYPH<28>cient event information.",
        "original_text": "There is a line of research works using event-driven stock prediction models. Hogenboom et al . [12] give an overview of event extraction methods. Akita et al . [13] convert newspaper articles into distributed representations via Paragraph Vector and model the temporal effects of past events with LSTM on opening prices of stocks in Tokyo Stock Exchange. Nguyen et al . [14] formulated a temporal sentiment index function, which is used to extract signiGLYPH<28>cant events. Then the corresponding blog posts are analyzed using topic modeling to understand the contents. Ding et al . [15] applied the Open IE tool to extract structured events from texts, and this event extraction method is also implemented as a baseline and compared with our proposal. Ding et al . [16] then trained event embeddings with a neural tensor network and then used the deep convolutional neural network to model inGLYPH<29>uences of events.\nIn addition to events, investors' emotions also have great impacts on the stock market index. Bollen et al . [17] revealed that the public moods derived from Twitter have impacts on stock indicators. Si et al . [3] proposed a technique to leverage topic based sentiments from Twitter to predict the stock market. Makrehchi et al . [18] assigned a positive or negative label for each tweet according to stock movements. The aggregate sentiment per day shows predictive power for stock market prediction. Topic-speciGLYPH<28>c sentiments are learned in [19] to facilitate the stock prediction. However, this method is not suitable to short texts in social media.\nThe common limitation of the aforementioned methods is that they rely only on a single data source and thus may limit the predictive power. In [20], events and sentiments are integrated into a tensor framework together with GLYPH<28>rm-speciGLYPH<28>c features (e.g., P/B, P/E), to model the joint impacts on the stock volatility. We also implement it as a baseline. However, it uses a simple event extraction method which may not fully capture sufGLYPH<28>cient event information.",
        "context": "Summarizes existing event-driven stock prediction models and highlights a common limitation: reliance on a single data source, prompting the need for a more comprehensive approach.",
        "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
        "pages": [
            2
        ],
        "id": "63466cd1c3215b2b14348f9a75e130ab740dfbf5849f70b156789da93e4a08bc"
    },
    {
        "text": "Provides a detailed explanation of the multiple instance learning (MIL) paradigm and its applications, highlighting its limitations in the context of stock market prediction due to reliance on a single data source and simple event features.\n\nThe multiple instance learning (MIL) paradigm is a form of weakly supervised learning. Training instances arranged in sets are called bags or groups. A label is provided for entire groups instead of individual instances. Negative groups don't contain any positive instances, while positive groups contain at least one positive instance [21]. Various applications and the comparisons of different methods in MIL\nwere given in [22]. The common MIL approach is used to predict the group-level label, Liu et al . [23], however, proposed an approach to identify the instance-level labels, especially the labels of key instances in groups based on K nearest neighbors ( K -NN). Kotzias et al . [24] predicted the labels for sentences given labels for reviews, which can be used to detect sentiments. A multiple-instance multiple-label learning framework with deep neutral network formation is proposed in [25]. An event forecasting framework via the nested multiple instance learning is proposed in [26]. However, it only uses one data source and simple event features, which may not be sufGLYPH<28>cient in the stock market application. We have implemented this algorithm as a baseline for comparison.",
        "original_text": "The multiple instance learning (MIL) paradigm is a form of weakly supervised learning. Training instances arranged in sets are called bags or groups. A label is provided for entire groups instead of individual instances. Negative groups don't contain any positive instances, while positive groups contain at least one positive instance [21]. Various applications and the comparisons of different methods in MIL\nwere given in [22]. The common MIL approach is used to predict the group-level label, Liu et al . [23], however, proposed an approach to identify the instance-level labels, especially the labels of key instances in groups based on K nearest neighbors ( K -NN). Kotzias et al . [24] predicted the labels for sentences given labels for reviews, which can be used to detect sentiments. A multiple-instance multiple-label learning framework with deep neutral network formation is proposed in [25]. An event forecasting framework via the nested multiple instance learning is proposed in [26]. However, it only uses one data source and simple event features, which may not be sufGLYPH<28>cient in the stock market application. We have implemented this algorithm as a baseline for comparison.",
        "context": "Provides a detailed explanation of the multiple instance learning (MIL) paradigm and its applications, highlighting its limitations in the context of stock market prediction due to reliance on a single data source and simple event features.",
        "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
        "pages": [
            2,
            3
        ],
        "id": "0428f89847fd011bab3d275aa32980711a132649d362258d11276dd5b09751f7"
    },
    {
        "text": "Introduces the multi-source multiple instance (M-MI) framework and defines key notations for the proposed model.\n\nIn this section, we GLYPH<28>rst state and formulate the problem, and then propose the multi-source multiple instance (M-MI) framework. Before going into details of our framework, we deGLYPH<28>ne some important notations as shown in Table 1.\nTABLE 1. Notations in our model.",
        "original_text": "In this section, we GLYPH<28>rst state and formulate the problem, and then propose the multi-source multiple instance (M-MI) framework. Before going into details of our framework, we deGLYPH<28>ne some important notations as shown in Table 1.\nTABLE 1. Notations in our model.",
        "context": "Introduces the multi-source multiple instance (M-MI) framework and defines key notations for the proposed model.",
        "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
        "pages": [
            3
        ],
        "id": "af3c341f1413d2c6e010922625c3ede6cb8e26fd77851bd0b8e952a0d8c9cf5c"
    },
    {
        "text": "This chunk introduces the multi-source data integration approach for stock market prediction, specifically outlining the representation of news articles, sentiments, and quantitative data as a multi-source super group and the forecasting function mapping this group to a future stock market movement indicator.\n\nStock markets are impacted by various factors, such as the trading volume, news events and the investors' emotions. Thus, relying on a single data source may not be sufGLYPH<28>cient to make accurate predictions. The object of our study is to develop a multi-source data integration approach to predict the stock market trends. SpeciGLYPH<28>cally, given a collection of economic news, social network posts and historical trading data, we aim to forecast the stock market index movements. Moreover, we also try to obtain the impacts of each data source and identify the key factors that have decisive inGLYPH<29>uences, which may be inGLYPH<29>uential news, collective sentiments or some important quantitative index in the trading data. These key factors are supporting evidence for further analysis and can make our prediction interpretable.\nFormally, according to Table 1, a news article j on day i is denoted as a V -dimensional vector x ij 2 R V GLYPH<2> 1 (please note that the process of representing a news article as a vector will be illustrated in the next section). In order to predict the stock market movement on day t C k , we assume that there are a group of news articles for each day i ( i < t ), which is denoted as X i , and thus X i D f x ij g ; j 2 f 1 ; GLYPH<1> GLYPH<1> GLYPH<1> ; ni g . In addition to the news articles, the sentiment and quantitative indices on day i (denoted as si and di respectively) are also taken into account. Then the temporal ordered collection of news articles, sentiments and quantitative indices across t days can be represented as a multi-source super group, that is, G D f Ci g ; i 2 f 1 ; GLYPH<1> GLYPH<1> GLYPH<1> ; t g , where Ci D f X i ; di ; si g . The change in the stock market movement on day t C k can be denoted as Yt C k 2 fC 1 ; GLYPH<0> 1 g , where C 1 denotes the index rise and -1 denotes the index decline. Then the forecasting problem can be modeled as a mathematical function f ( G ) ! Yt C k , indicating that we map the multi-source information to an indicator (i.e., label) k days in the future from the day t , where k is number of the lead days that we aim to forecast.\nFIGURE 2. The system framework of our proposed model.",
        "original_text": "Stock markets are impacted by various factors, such as the trading volume, news events and the investors' emotions. Thus, relying on a single data source may not be sufGLYPH<28>cient to make accurate predictions. The object of our study is to develop a multi-source data integration approach to predict the stock market trends. SpeciGLYPH<28>cally, given a collection of economic news, social network posts and historical trading data, we aim to forecast the stock market index movements. Moreover, we also try to obtain the impacts of each data source and identify the key factors that have decisive inGLYPH<29>uences, which may be inGLYPH<29>uential news, collective sentiments or some important quantitative index in the trading data. These key factors are supporting evidence for further analysis and can make our prediction interpretable.\nFormally, according to Table 1, a news article j on day i is denoted as a V -dimensional vector x ij 2 R V GLYPH<2> 1 (please note that the process of representing a news article as a vector will be illustrated in the next section). In order to predict the stock market movement on day t C k , we assume that there are a group of news articles for each day i ( i < t ), which is denoted as X i , and thus X i D f x ij g ; j 2 f 1 ; GLYPH<1> GLYPH<1> GLYPH<1> ; ni g . In addition to the news articles, the sentiment and quantitative indices on day i (denoted as si and di respectively) are also taken into account. Then the temporal ordered collection of news articles, sentiments and quantitative indices across t days can be represented as a multi-source super group, that is, G D f Ci g ; i 2 f 1 ; GLYPH<1> GLYPH<1> GLYPH<1> ; t g , where Ci D f X i ; di ; si g . The change in the stock market movement on day t C k can be denoted as Yt C k 2 fC 1 ; GLYPH<0> 1 g , where C 1 denotes the index rise and -1 denotes the index decline. Then the forecasting problem can be modeled as a mathematical function f ( G ) ! Yt C k , indicating that we map the multi-source information to an indicator (i.e., label) k days in the future from the day t , where k is number of the lead days that we aim to forecast.\nFIGURE 2. The system framework of our proposed model.",
        "context": "This chunk introduces the multi-source data integration approach for stock market prediction, specifically outlining the representation of news articles, sentiments, and quantitative data as a multi-source super group and the forecasting function mapping this group to a future stock market movement indicator.",
        "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
        "pages": [
            3
        ],
        "id": "39c825d2635eebb324ba7686fde1bb602ed93eea99b7c365141b1c4f4dc38a2e"
    },
    {
        "text": "Describes the framework for predicting stock market trends, utilizing multiple data sources like news, social media, and quantitative data. It employs multiple instance learning, distinguishing instance-level, group-level, and super group-level labels. The model estimates instance-level probabilities and source-specific weights to combine information from various data sources, aiming for a consensus prediction of market index movement.\n\nThe framework of our proposal is shown in Fig. 2. The inputs of the framework are the stock quantitative data, the social media and Web news. We GLYPH<28>rst use the sentiment analyzer to obtain the collective sentiments from social media, and extract effective event representations from the Web news. Then the extracted sentiments, events as well as the stock quantitative data are fed into the M-MI model. The M-MI model is proposed based on the Multiple Instance Learning algorithm, that is, a group of instances are given group labels, which are assumed to be an association function (e.g., OR, average) of the instance-level labels. Our work further distinguishes the instance-level labels, multi-source group-level labels, and multi-source super group-level labels. The primary goal is to predict the label for the multi-source super group that indicates the rise or decline of the stock market index. In addition, we also try to estimate the instancelevel probabilities indicating how related a speciGLYPH<28>c instance is to the index movement (i.e., target label), as well as the\nsource-speciGLYPH<28>c weights that reveal how related a speciGLYPH<28>c source is to the index movement.\nTo this end, for a given day, we GLYPH<28>rst model the instancelevel probability pij for a news article j on day i to the target label with a logistic function, that is\n<!-- formula-not-decoded -->\nwhere w m denotes the weight vector of the news articles. The higher the probability pij , the more related the article j is to the target label. The probability of all the news articles for a given day i can be computed as the average of probabilities of each news article, that is\n<!-- formula-not-decoded -->\nIn addition to news articles, we also model the probability p d GLYPH<0> i for stock quantitative data and p s GLYPH<0> i for sentiments on day i , that is\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere w d and w s denote the weight vector of d i and s i respectively. We then model the probability Pi for multisource information on day i as\n<!-- formula-not-decoded -->\nwhere GLYPH<18> 0, GLYPH<18> 1 and GLYPH<18> 2 denote the source-speciGLYPH<28>c weights of p m GLYPH<0> i , p d GLYPH<0> i and p s GLYPH<0> i respectively, and GLYPH<18> 0 C GLYPH<18> 1 C GLYPH<18> 2 D 1. It is obvious that Pi 2 [0 ; 1]. We use GLYPH<18> D ( GLYPH<18> 0 ; GLYPH<18> 1 ; GLYPH<18> 2) to denote the source weight vector, and then model the probability of the multi-source super group as the average of the probabilities in t days, that is\n<!-- formula-not-decoded -->\nThen, we start with a log-likelihood loss function:\n<!-- formula-not-decoded -->\n■ As the inGLYPH<29>uences of the multi-source information usually last for a number of days, we assume the probabilities on two consecutive days are essentially similar, which can be represented by minimizing the cost where G is a multi-source super group, n is the number of multi-source super groups, and Y denotes the set of true labels. ( GLYPH<1> ) is the indicator function.\n<!-- formula-not-decoded -->\nwhere Ci denotes the multi-source group for the day i . By introducing this loss term, Eq. 7 can be rewritten as:\n<!-- formula-not-decoded -->\nwhere GLYPH<12> is a constant to control the contribution of the GLYPH<28>rst term. Eq. 9 aggregates the costs at the super group level and the group level. However, the instance-level loss has not been considered yet, which is challenging to be designed due to two reasons: (1) it lacks of true labels at the instance level; (2) the instances from different sources are heterogeneous but intrinsically correlated. The instances can be categorized into three types according to their sources, and each type leads to a distinct loss term. Inspired by the hinge loss used in Support Vector Machines (SVMs), the classiGLYPH<28>cation loss term for the instances of news article instance xij is\n<!-- formula-not-decoded -->\nHere, sgn ( GLYPH<1> ) is the sign function, m 0 is a margin parameter used to separate the positive and negative instances from the hyperplane in the feature space. w T m x ij denotes the prediction with article xij . As the true label for each instance is unknown during the classiGLYPH<28>er training, we replace it with the estimated true label sgn ( Pi -P 0 ), where P 0 is a threshold parameter to determine the positiveness of the prediction. If ( Pi -P 0 ) > 0, the prediction with multiple-source information on day i would be positive. Otherwise, it would be negative. Similarly, we can derive the instance-level loss terms for quantitative data and sentiments respectively,\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nBased on Eq. 10, 11 and 12, the classiGLYPH<28>cation loss at the instance level for each data source has been obtained. We then explain why they share a common estimated true label (i.e., sgn ( Pi -P 0 )). As predictions from different sources are commonly correlated with each other, instead of treating the loss of each source independently, we need to consider their intrinsic consistencies. The intuition behind is that according to EfGLYPH<28>cient Market Hypothesis, different data sources would keep up to date with the latest stock market information, and they commonly indicate the same sign (index rise or fall). Thus, through sharing the same estimated true label, we are able to combine the indications from different data sources to learn a consensus label. This can potentially provide more robust and conGLYPH<28>dent predictions.\nWe then give several cases to illustrate the consensus among sources. The three source-speciGLYPH<28>c predictions are denoted as l 0 , l 1 and l 2 respectively. Firstly, if l 0 , l 1 and l 2 all make very positive predictions, i.e., large values of pm -i , pd -i and ps -i , it would be conGLYPH<28>dent to make a positive group-level prediction due to Pi > P 0 and l 0 , l 1 and l 2\nall agree with the label without costs. Secondly, if only l 0 disagrees with the estimated true label, l 0 will be penalized as l 1 and l 2 agree with this label and make Pi approach their predictions. Thirdly, if l 0 disagrees with l 1 and l 2 , but l 0 is very conGLYPH<28>dent (and thus far from hyperplane) while l 1 and l 2 are not conGLYPH<28>dent enough (and thus close to hyperplane), this may make Pi approach l 0 , resulting in that l 0 agrees with the estimated true label while l 1 and l 2 disagree with it and thus are penalized. Our proposed instance-level loss terms are consistent with these cases and thus make sense.\nThen we try to minimize the overall instance-level loss, that is, h 1( x ij ; w m ) C h 2( d i ; w d ) C h 3( s i ; w s ). By introducing this summation and other regularization terms, the objective function Eq. 9 can be reformulated as\n<!-- formula-not-decoded -->\nEq. 13 is the ultimate objective function to optimize. To summarize, it consists of losses at three levels: the super group level, the group level and the instance level. In addition, it includes the regularization terms, that is, R ( w m ), R ( w d ), R ( w s ) and R ( GLYPH<18> ), and GLYPH<12> , GLYPH<21> m , GLYPH<21> d , GLYPH<21> s and GLYPH<21> GLYPH<18> are constants to control the trade-offs among multiple terms. The model learning goal is to estimate the parameters w m , w d , w s and GLYPH<18> to minimize L ( w m ; w d ; w s ; GLYPH<18> ). We randomly choose a set ( G ; Y ) from S , and the online stochastic gradient descent optimization is adopted to GLYPH<28>t the model.",
        "original_text": "The framework of our proposal is shown in Fig. 2. The inputs of the framework are the stock quantitative data, the social media and Web news. We GLYPH<28>rst use the sentiment analyzer to obtain the collective sentiments from social media, and extract effective event representations from the Web news. Then the extracted sentiments, events as well as the stock quantitative data are fed into the M-MI model. The M-MI model is proposed based on the Multiple Instance Learning algorithm, that is, a group of instances are given group labels, which are assumed to be an association function (e.g., OR, average) of the instance-level labels. Our work further distinguishes the instance-level labels, multi-source group-level labels, and multi-source super group-level labels. The primary goal is to predict the label for the multi-source super group that indicates the rise or decline of the stock market index. In addition, we also try to estimate the instancelevel probabilities indicating how related a speciGLYPH<28>c instance is to the index movement (i.e., target label), as well as the\nsource-speciGLYPH<28>c weights that reveal how related a speciGLYPH<28>c source is to the index movement.\nTo this end, for a given day, we GLYPH<28>rst model the instancelevel probability pij for a news article j on day i to the target label with a logistic function, that is\n<!-- formula-not-decoded -->\nwhere w m denotes the weight vector of the news articles. The higher the probability pij , the more related the article j is to the target label. The probability of all the news articles for a given day i can be computed as the average of probabilities of each news article, that is\n<!-- formula-not-decoded -->\nIn addition to news articles, we also model the probability p d GLYPH<0> i for stock quantitative data and p s GLYPH<0> i for sentiments on day i , that is\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere w d and w s denote the weight vector of d i and s i respectively. We then model the probability Pi for multisource information on day i as\n<!-- formula-not-decoded -->\nwhere GLYPH<18> 0, GLYPH<18> 1 and GLYPH<18> 2 denote the source-speciGLYPH<28>c weights of p m GLYPH<0> i , p d GLYPH<0> i and p s GLYPH<0> i respectively, and GLYPH<18> 0 C GLYPH<18> 1 C GLYPH<18> 2 D 1. It is obvious that Pi 2 [0 ; 1]. We use GLYPH<18> D ( GLYPH<18> 0 ; GLYPH<18> 1 ; GLYPH<18> 2) to denote the source weight vector, and then model the probability of the multi-source super group as the average of the probabilities in t days, that is\n<!-- formula-not-decoded -->\nThen, we start with a log-likelihood loss function:\n<!-- formula-not-decoded -->\n■ As the inGLYPH<29>uences of the multi-source information usually last for a number of days, we assume the probabilities on two consecutive days are essentially similar, which can be represented by minimizing the cost where G is a multi-source super group, n is the number of multi-source super groups, and Y denotes the set of true labels. ( GLYPH<1> ) is the indicator function.\n<!-- formula-not-decoded -->\nwhere Ci denotes the multi-source group for the day i . By introducing this loss term, Eq. 7 can be rewritten as:\n<!-- formula-not-decoded -->\nwhere GLYPH<12> is a constant to control the contribution of the GLYPH<28>rst term. Eq. 9 aggregates the costs at the super group level and the group level. However, the instance-level loss has not been considered yet, which is challenging to be designed due to two reasons: (1) it lacks of true labels at the instance level; (2) the instances from different sources are heterogeneous but intrinsically correlated. The instances can be categorized into three types according to their sources, and each type leads to a distinct loss term. Inspired by the hinge loss used in Support Vector Machines (SVMs), the classiGLYPH<28>cation loss term for the instances of news article instance xij is\n<!-- formula-not-decoded -->\nHere, sgn ( GLYPH<1> ) is the sign function, m 0 is a margin parameter used to separate the positive and negative instances from the hyperplane in the feature space. w T m x ij denotes the prediction with article xij . As the true label for each instance is unknown during the classiGLYPH<28>er training, we replace it with the estimated true label sgn ( Pi -P 0 ), where P 0 is a threshold parameter to determine the positiveness of the prediction. If ( Pi -P 0 ) > 0, the prediction with multiple-source information on day i would be positive. Otherwise, it would be negative. Similarly, we can derive the instance-level loss terms for quantitative data and sentiments respectively,\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nBased on Eq. 10, 11 and 12, the classiGLYPH<28>cation loss at the instance level for each data source has been obtained. We then explain why they share a common estimated true label (i.e., sgn ( Pi -P 0 )). As predictions from different sources are commonly correlated with each other, instead of treating the loss of each source independently, we need to consider their intrinsic consistencies. The intuition behind is that according to EfGLYPH<28>cient Market Hypothesis, different data sources would keep up to date with the latest stock market information, and they commonly indicate the same sign (index rise or fall). Thus, through sharing the same estimated true label, we are able to combine the indications from different data sources to learn a consensus label. This can potentially provide more robust and conGLYPH<28>dent predictions.\nWe then give several cases to illustrate the consensus among sources. The three source-speciGLYPH<28>c predictions are denoted as l 0 , l 1 and l 2 respectively. Firstly, if l 0 , l 1 and l 2 all make very positive predictions, i.e., large values of pm -i , pd -i and ps -i , it would be conGLYPH<28>dent to make a positive group-level prediction due to Pi > P 0 and l 0 , l 1 and l 2\nall agree with the label without costs. Secondly, if only l 0 disagrees with the estimated true label, l 0 will be penalized as l 1 and l 2 agree with this label and make Pi approach their predictions. Thirdly, if l 0 disagrees with l 1 and l 2 , but l 0 is very conGLYPH<28>dent (and thus far from hyperplane) while l 1 and l 2 are not conGLYPH<28>dent enough (and thus close to hyperplane), this may make Pi approach l 0 , resulting in that l 0 agrees with the estimated true label while l 1 and l 2 disagree with it and thus are penalized. Our proposed instance-level loss terms are consistent with these cases and thus make sense.\nThen we try to minimize the overall instance-level loss, that is, h 1( x ij ; w m ) C h 2( d i ; w d ) C h 3( s i ; w s ). By introducing this summation and other regularization terms, the objective function Eq. 9 can be reformulated as\n<!-- formula-not-decoded -->\nEq. 13 is the ultimate objective function to optimize. To summarize, it consists of losses at three levels: the super group level, the group level and the instance level. In addition, it includes the regularization terms, that is, R ( w m ), R ( w d ), R ( w s ) and R ( GLYPH<18> ), and GLYPH<12> , GLYPH<21> m , GLYPH<21> d , GLYPH<21> s and GLYPH<21> GLYPH<18> are constants to control the trade-offs among multiple terms. The model learning goal is to estimate the parameters w m , w d , w s and GLYPH<18> to minimize L ( w m ; w d ; w s ; GLYPH<18> ). We randomly choose a set ( G ; Y ) from S , and the online stochastic gradient descent optimization is adopted to GLYPH<28>t the model.",
        "context": "Describes the framework for predicting stock market trends, utilizing multiple data sources like news, social media, and quantitative data. It employs multiple instance learning, distinguishing instance-level, group-level, and super group-level labels. The model estimates instance-level probabilities and source-specific weights to combine information from various data sources, aiming for a consensus prediction of market index movement.",
        "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
        "pages": [
            3,
            4,
            5
        ],
        "id": "d3db3e341b9b3a661356b26b5a5314024e4eaf4f7ea86141844454c45c6ac721"
    },
    {
        "text": "Identifies key data sources and their influence on market movements; estimates the probability of each input contributing to index rise or decline, allowing for the identification of critical factors driving market trends.\n\nAfter the learning process, the source weight vector GLYPH<18> is obtained, representing the impacts of different data sources on the market movements. In addition, a probability for each piece of input information can also be obtained through Eq. 1, 3 or 4, which reveals the probability of that information signifying the rise of the market index on the target day. Note that if the probability signifying the index rise is pr , the probability indicating index decline would be 1 GLYPH<0> pr . We can identify the key input information that triggers the market index movement, if the product of its probability value and its source-speciGLYPH<28>c weight is above a given threshold GLYPH<28> .",
        "original_text": "After the learning process, the source weight vector GLYPH<18> is obtained, representing the impacts of different data sources on the market movements. In addition, a probability for each piece of input information can also be obtained through Eq. 1, 3 or 4, which reveals the probability of that information signifying the rise of the market index on the target day. Note that if the probability signifying the index rise is pr , the probability indicating index decline would be 1 GLYPH<0> pr . We can identify the key input information that triggers the market index movement, if the product of its probability value and its source-speciGLYPH<28>c weight is above a given threshold GLYPH<28> .",
        "context": "Identifies key data sources and their influence on market movements; estimates the probability of each input contributing to index rise or decline, allowing for the identification of critical factors driving market trends.",
        "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
        "pages": [
            5
        ],
        "id": "25f44ed84cbffde363f0517d1dc3d35384ab567270a351db961722e8f117c30c"
    },
    {
        "text": "Describes the process of extracting structured events from news articles and sentiment from social media posts, which serve as inputs to the M-MI framework.\n\nThe quantitative features are quite simple to extract, we just collect three indices and normalize each index to form d i 2 R 3 GLYPH<2> 1 . Here we introduce how to extract event representations from news articles and extract the sentiments from posts in social media, which are used as the inputs to M-MI framework.\nFIGURE 3. Structured event extraction from texts.",
        "original_text": "The quantitative features are quite simple to extract, we just collect three indices and normalize each index to form d i 2 R 3 GLYPH<2> 1 . Here we introduce how to extract event representations from news articles and extract the sentiments from posts in social media, which are used as the inputs to M-MI framework.\nFIGURE 3. Structured event extraction from texts.",
        "context": "Describes the process of extracting structured events from news articles and sentiment from social media posts, which serve as inputs to the M-MI framework.",
        "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
        "pages": [
            5
        ],
        "id": "9b7a7361b3ab2b0a7fc7db3c3365d0c49669f69970f8755dd2003bdbc78273b3"
    },
    {
        "text": "Introduces the method for extracting structured events from news articles, utilizing syntactic analysis and an RBM for pre-training, followed by sentence2vec for final event representations.\n\nConventional methods commonly represent events using simple features such as TF-IDF, noun phrases or named entities. Recent advances in NLP techniques enable more accurate event models with structures. In this study, we GLYPH<28>rst use the syntactic analysis method to extract the main structure information of the sentences, and then use it as the input to an RBM. The output of an RBM would be a pre-trained vector used as the input to sentence2vec, and then the event representations are obtained. The process is shown in Figure 3 and described in detail as follows. Note that though we use the Chinese dataset in this study, this process can also be applied to other languages.\n- 1) Structured event extraction. With a commonly used text parser HanLP, 1 we can capture the syntactic structure of a sentence, which is depicted as a three-level tree at the top of Fig. 3. The root node denotes the core verb, and the nodes of the second layer are the subject of the verb and the object of the verb respectively. The child of the subject is the modiGLYPH<28>er who is the nearest to the subject in the sentence, and so is the child of the object. Then we connect these core words together as the structure information to represent the event information.\n- 2) Training with RBM. Wethen map the structured event into a vector. To make the vectors better reconstruct the original events, we use RBM as a pre-training module. The Restricted Boltzmann Machine (RBM) is a generative stochastic artiGLYPH<28>cial neural network, and has been applied in various applications such as dimensionality reduction [27]. RBM contains two-layer neural nets, one is the visible layer or input layer, and the\n1 \nother is the hidden layer. In our model, each event is represented as an m -dimensional vector with one-hot encoding, which is the visible layer. Our target is to estimate the n -dimensional hidden layer to approximate the input layer as much as possible. Then the hidden layer will be set as the initial vector in sentence2vec. The reason is that directly training the representations using sentence2vec without RBM may fall into the local minimum.\n- 3) Training with sentence2vec. Finally, we use sentence2vec, the neural probabilistic language model to obtain the event representations. Different from the word2vec with CBOW model, the sentence id will be added during the training process of sentence2vec, and will also be mapped into a vector, called sentence vector, which would be the GLYPH<28>nal vector that we want. In the training process, the sentence vector and the word vectors of context will be concatenated as the input to softmax. After training, the sentence vector will be obtained and used as the features for the proposed model.\nHere is an example of extracting structured events from the news. The news text is that it is expected that the Renminbi speculators will face huge losses. After the dependency parsing analysis, the core words (Renminbi, speculators, face, huge, losses) are obtained, and after one hot coding, each word is encoded into zero or one vector. Then the vector preprocesses by RBM into a 100-dimensional vector, and GLYPH<28>nally processes by the sentence2vec became the news event feature vector. Through the above steps, the news event is obtained as a feature of the M-MI model, a 100-dimensional vector.",
        "original_text": "Conventional methods commonly represent events using simple features such as TF-IDF, noun phrases or named entities. Recent advances in NLP techniques enable more accurate event models with structures. In this study, we GLYPH<28>rst use the syntactic analysis method to extract the main structure information of the sentences, and then use it as the input to an RBM. The output of an RBM would be a pre-trained vector used as the input to sentence2vec, and then the event representations are obtained. The process is shown in Figure 3 and described in detail as follows. Note that though we use the Chinese dataset in this study, this process can also be applied to other languages.\n- 1) Structured event extraction. With a commonly used text parser HanLP, 1 we can capture the syntactic structure of a sentence, which is depicted as a three-level tree at the top of Fig. 3. The root node denotes the core verb, and the nodes of the second layer are the subject of the verb and the object of the verb respectively. The child of the subject is the modiGLYPH<28>er who is the nearest to the subject in the sentence, and so is the child of the object. Then we connect these core words together as the structure information to represent the event information.\n- 2) Training with RBM. Wethen map the structured event into a vector. To make the vectors better reconstruct the original events, we use RBM as a pre-training module. The Restricted Boltzmann Machine (RBM) is a generative stochastic artiGLYPH<28>cial neural network, and has been applied in various applications such as dimensionality reduction [27]. RBM contains two-layer neural nets, one is the visible layer or input layer, and the\n1 \nother is the hidden layer. In our model, each event is represented as an m -dimensional vector with one-hot encoding, which is the visible layer. Our target is to estimate the n -dimensional hidden layer to approximate the input layer as much as possible. Then the hidden layer will be set as the initial vector in sentence2vec. The reason is that directly training the representations using sentence2vec without RBM may fall into the local minimum.\n- 3) Training with sentence2vec. Finally, we use sentence2vec, the neural probabilistic language model to obtain the event representations. Different from the word2vec with CBOW model, the sentence id will be added during the training process of sentence2vec, and will also be mapped into a vector, called sentence vector, which would be the GLYPH<28>nal vector that we want. In the training process, the sentence vector and the word vectors of context will be concatenated as the input to softmax. After training, the sentence vector will be obtained and used as the features for the proposed model.\nHere is an example of extracting structured events from the news. The news text is that it is expected that the Renminbi speculators will face huge losses. After the dependency parsing analysis, the core words (Renminbi, speculators, face, huge, losses) are obtained, and after one hot coding, each word is encoded into zero or one vector. Then the vector preprocesses by RBM into a 100-dimensional vector, and GLYPH<28>nally processes by the sentence2vec became the news event feature vector. Through the above steps, the news event is obtained as a feature of the M-MI model, a 100-dimensional vector.",
        "context": "Introduces the method for extracting structured events from news articles, utilizing syntactic analysis and an RBM for pre-training, followed by sentence2vec for final event representations.",
        "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
        "pages": [
            5,
            6
        ],
        "id": "2bd4712fb09fd04f99fce4a6fb05d2cd0bcb34457c9d523bc46ea8ebe5d726f9"
    },
    {
        "text": "Introduces the LDA-S method for sentiment extraction from social media posts, highlighting its advantage over simple sentiment extraction by considering topic dependencies.\n\nTo extract the sentiments from the posts in the social network, we use the LDA-S method [28], an extension of Latent Dirichlet Allocation (LDA) model that proposed to obtain the topic-speciGLYPH<28>c sentiments for short texts. The intuition behind is that extracting sentiments discarding topics may be not sufGLYPH<28>cient as sentiment polarities usually depend on topics or domains [29]. In other words, the exact same word mayexpress different sentiment polarities for different topics, e.g., the opinion word ''low'' in the phrase ''low speed'' in a trafGLYPH<28>c-related topic and ''low fat'' in a food-related topic. Therefore, extracting the sentiments corresponding to different topics can potentially improve the sentiment classiGLYPH<28>cation accuracy. The LDA-S model can infer sentiment distribution and topic distribution simultaneously for short texts. It consists of two steps. The GLYPH<28>rst step aims to obtain the topic distribution of each post, and then set the topic as the one with the largest probability. The second step gets the sentiment distribution of each post.\nIn this work, a sentiment word list called NTUSD [30] is adopted, which contains 4370 negative words and 4566 positive words. If a word is an adjective but not in the sentiment\nword list, the sentiment label of this word is set as neutral. If a word is a noun, it is considered as a topic word. Otherwise, it is considered as a background word. For each topic, opinion word distributions are distinguished from two polarities, that is, positive or negative.",
        "original_text": "To extract the sentiments from the posts in the social network, we use the LDA-S method [28], an extension of Latent Dirichlet Allocation (LDA) model that proposed to obtain the topic-speciGLYPH<28>c sentiments for short texts. The intuition behind is that extracting sentiments discarding topics may be not sufGLYPH<28>cient as sentiment polarities usually depend on topics or domains [29]. In other words, the exact same word mayexpress different sentiment polarities for different topics, e.g., the opinion word ''low'' in the phrase ''low speed'' in a trafGLYPH<28>c-related topic and ''low fat'' in a food-related topic. Therefore, extracting the sentiments corresponding to different topics can potentially improve the sentiment classiGLYPH<28>cation accuracy. The LDA-S model can infer sentiment distribution and topic distribution simultaneously for short texts. It consists of two steps. The GLYPH<28>rst step aims to obtain the topic distribution of each post, and then set the topic as the one with the largest probability. The second step gets the sentiment distribution of each post.\nIn this work, a sentiment word list called NTUSD [30] is adopted, which contains 4370 negative words and 4566 positive words. If a word is an adjective but not in the sentiment\nword list, the sentiment label of this word is set as neutral. If a word is a noun, it is considered as a topic word. Otherwise, it is considered as a background word. For each topic, opinion word distributions are distinguished from two polarities, that is, positive or negative.",
        "context": "Introduces the LDA-S method for sentiment extraction from social media posts, highlighting its advantage over simple sentiment extraction by considering topic dependencies.",
        "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
        "pages": [
            6
        ],
        "id": "d4e215c5fadc5fac4d170615b4265dea2c1c5e1e8211ce725c91926dd7e520fa"
    },
    {
        "text": "Details the collection and organization of data sources (quantitative, news, and social media) used for stock market prediction analysis, including the timeframe, data volume, and the method of extracting relevant information (e.g., news titles vs. full articles).\n\nWe collected stock market-related information from Jan. 1, 2015 to Dec. 31, 2016, and separate the information into two data sets, one for the year 2015 and the other for 2016. The data consist of three parts, the historical quantitative data, the news articles and the posts on the social network, which are introduced in detail as follows.\n- GLYPH<15> Quantitative data : the source of quantitative data is Wind, 2 a widely used GLYPH<28>nancial information service provider in China. The data we collect are the average prices, market index change and turnover rate of the Shanghai Composite Index in each trading day.\n- GLYPH<15> News data : we collect the news articles on the macro economy through Wind, and get 38,727 and 39,465 news articles in 2015 and 2016 respectively. The news articles are aggregated by Wind from major GLYPH<28>nancial news websites in China, such as and We process the news titles rather than the whole articles to extract the events, as the main topic of a news article is often summed up in the title.\n- GLYPH<15> Social media data : the sentiments are extracted from the posts crawled from a popular investor social network in China named Xueqiu. 3 Totally 6,163,056 postings are collected for 2015 and 2016. For each post, we get the posting time stamp and the content.\nFor each trading day, if the stock market index rises, it would be a positive instance, otherwise it is a negative instance. For each year, we use the data from the GLYPH<28>rst 10 months as the training set and the last 2 months as the testing set. We evaluate the performance of our model with varying lead days and varying historical days. Lead days refers to the number of days in advance the model makes predictions and the historical days indicates the number of days over which the multi-source information is utilized. The evaluation metrics we use are F1-score and accuracy (ACC).",
        "original_text": "We collected stock market-related information from Jan. 1, 2015 to Dec. 31, 2016, and separate the information into two data sets, one for the year 2015 and the other for 2016. The data consist of three parts, the historical quantitative data, the news articles and the posts on the social network, which are introduced in detail as follows.\n- GLYPH<15> Quantitative data : the source of quantitative data is Wind, 2 a widely used GLYPH<28>nancial information service provider in China. The data we collect are the average prices, market index change and turnover rate of the Shanghai Composite Index in each trading day.\n- GLYPH<15> News data : we collect the news articles on the macro economy through Wind, and get 38,727 and 39,465 news articles in 2015 and 2016 respectively. The news articles are aggregated by Wind from major GLYPH<28>nancial news websites in China, such as and We process the news titles rather than the whole articles to extract the events, as the main topic of a news article is often summed up in the title.\n- GLYPH<15> Social media data : the sentiments are extracted from the posts crawled from a popular investor social network in China named Xueqiu. 3 Totally 6,163,056 postings are collected for 2015 and 2016. For each post, we get the posting time stamp and the content.\nFor each trading day, if the stock market index rises, it would be a positive instance, otherwise it is a negative instance. For each year, we use the data from the GLYPH<28>rst 10 months as the training set and the last 2 months as the testing set. We evaluate the performance of our model with varying lead days and varying historical days. Lead days refers to the number of days in advance the model makes predictions and the historical days indicates the number of days over which the multi-source information is utilized. The evaluation metrics we use are F1-score and accuracy (ACC).",
        "context": "Details the collection and organization of data sources (quantitative, news, and social media) used for stock market prediction analysis, including the timeframe, data volume, and the method of extracting relevant information (e.g., news titles vs. full articles).",
        "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
        "pages": [
            6
        ],
        "id": "4cf733a743ce1b6eb4e3c41e23b999ed51cd3d280449efe83926e394f4def7ff"
    },
    {
        "text": "Compares various baseline models for stock prediction, outlining their differences and the evaluation criteria used for comparison.\n\nThe following baselines and variations of our proposed model are implemented for comparisons. The full implementation of our framework is named as Multi-source Multiple Instance (M-MI) model.\n- GLYPH<15> SVM : the standard support vector machine is used as a basic prediction method. During the training process, the label assigned to each instance and each group is the same as its multi-source super group label. During the\n2 \n3 \nprediction phase, we obtain the predicted label for each of the instance, and then average the labels as the GLYPH<28>nal label of the super group.\n- GLYPH<15> TeSIA: the tensor-based learning approach [20] utilizes multi-source information for stock prediction. SpeciGLYPH<28>cally, it uses a third-order tensor to model the GLYPH<28>rm-mode, event-mode, and sentiment-mode data.\n- GLYPH<15> nMIL : nested Multi-Instance Learning (nMIL) model [26] is the state-of-art baseline. In this model, only one data source, i.e., the news, is used to extract simple event features. It ignores the impacts of the sentiments and the historical quantitative indices.\n- GLYPH<15> O-MI : Open IE Multiple Instance (O-MI) Learning model differs from M-MI in the event extraction module. It adopts a previously proposed event extraction method [15], and uses Open IE [31] to extract event tuples from sentences. The structured event tuples are then processed by sentence2vec to obtain event representations. Please note that the sentiment data and quantitative data are also used in this model.\n- GLYPH<15> WoR-MI : Without RBM Multiple Instance (WoR-MI) Learning model is also a part of the M-MI framework. It differs M-MI in that it works without the RBM module, and therefore the sentence2vec module is fed with original structured events instead of pre-trained vectors.\n- GLYPH<15> WoH-MI : Compare to M-MI, Without Hinge loss Multiple Instance (WoH-MI) Learning model lacks the instance-level hinge loss terms (i.e., Eq. 10, 11 and 12).\nTo make a fair comparison, we use the same set of instances and the same setting of parameters to evaluate different methods. In our proposal and the baselines, we set the predicted label to GLYPH<0> 1 if the estimated probability for a multisource super group is less than 0.5; otherwise, we set the predicted label to C 1.\nTABLE 2. Prediction Results (history day=1, lead day=1).",
        "original_text": "The following baselines and variations of our proposed model are implemented for comparisons. The full implementation of our framework is named as Multi-source Multiple Instance (M-MI) model.\n- GLYPH<15> SVM : the standard support vector machine is used as a basic prediction method. During the training process, the label assigned to each instance and each group is the same as its multi-source super group label. During the\n2 \n3 \nprediction phase, we obtain the predicted label for each of the instance, and then average the labels as the GLYPH<28>nal label of the super group.\n- GLYPH<15> TeSIA: the tensor-based learning approach [20] utilizes multi-source information for stock prediction. SpeciGLYPH<28>cally, it uses a third-order tensor to model the GLYPH<28>rm-mode, event-mode, and sentiment-mode data.\n- GLYPH<15> nMIL : nested Multi-Instance Learning (nMIL) model [26] is the state-of-art baseline. In this model, only one data source, i.e., the news, is used to extract simple event features. It ignores the impacts of the sentiments and the historical quantitative indices.\n- GLYPH<15> O-MI : Open IE Multiple Instance (O-MI) Learning model differs from M-MI in the event extraction module. It adopts a previously proposed event extraction method [15], and uses Open IE [31] to extract event tuples from sentences. The structured event tuples are then processed by sentence2vec to obtain event representations. Please note that the sentiment data and quantitative data are also used in this model.\n- GLYPH<15> WoR-MI : Without RBM Multiple Instance (WoR-MI) Learning model is also a part of the M-MI framework. It differs M-MI in that it works without the RBM module, and therefore the sentence2vec module is fed with original structured events instead of pre-trained vectors.\n- GLYPH<15> WoH-MI : Compare to M-MI, Without Hinge loss Multiple Instance (WoH-MI) Learning model lacks the instance-level hinge loss terms (i.e., Eq. 10, 11 and 12).\nTo make a fair comparison, we use the same set of instances and the same setting of parameters to evaluate different methods. In our proposal and the baselines, we set the predicted label to GLYPH<0> 1 if the estimated probability for a multisource super group is less than 0.5; otherwise, we set the predicted label to C 1.\nTABLE 2. Prediction Results (history day=1, lead day=1).",
        "context": "Compares various baseline models for stock prediction, outlining their differences and the evaluation criteria used for comparison.",
        "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
        "pages": [
            6,
            7
        ],
        "id": "ff9cb801e31206f89cbc95ddcb4acb14d47e25b9946d4baa2d9717a801a11b50"
    },
    {
        "text": "Specifies the experimental settings for evaluating the model's performance, including the number of history days, lead days, and the weighting of different data sources (news events, quantitative data, and sentiments).\n\nWe set both the number of history days and the number of lead days to 1. We empirically set m 0 D 0 : 6, and set m 1 ; m 2 and P 0 all as 0.5, i.e., the default setting in hinge loss. GLYPH<12> is set as 3.0, and GLYPH<21> m , GLYPH<21> d , GLYPH<21> s and GLYPH<21> GLYPH<18> are set as 0.05 by sensitivity analysis. The dimension of event representations is set as 100. Table 2 shows the performance of M-MI and the baselines. We can observe that M-MI outperforms all the baselines in both of the metrics, while SVM method shows the worst performance, indicating that simply tagging each news article with the label of its super-group is not effective. It can also be observed that M-MI and its variations (i.e., O-MI, WoH-MI and WoR-MI) all outperform nMIL. Compared to nMIL, M-MI improves F-1 by 6.9% in 2015 and 9.2% in 2016, while it improves accuracy by 6.7% and 9.4% in 2015 and 2016 respectively. Such gains mainly come from (1) utilizing multi-source information instead of only news articles, and (2) the advanced event representations rather than simple event features. Though all using multi-source information, TeSIA performs worse than M-MI and its variations, showing the effectiveness of our proposed models and feature extraction methods. Both M-MI and WoR-MI perform better than O-MI, indicating that both the structured event extraction module and the RBM pre-training module in our framework are effective. WoH-MI performs worse than M-MI, showing the proposed instance-level hinge losses across multiple data sources are useful for accurate predictions.\nFIGURE 4. F-1 scores with varying history days. (a) 2015. (b) 2016.\nFigure 4 (a) and (b) show the F-1 scores of all the comparative models with varying history days in training for 2015 and 2016 respectively (where lead day remains 1). The number of history days (i.e., t in Eq. 13) is varied from 1 to 5 and the results show that M-MI consistently performs better than the others. We can also observe that as the number of history days keeps increasing, the F-1 scores generally GLYPH<28>rst go up and then go down. The possible reason is that the impacts of the news, sentiments and quantitative indices released on some day will quickly decay after a period of time (2 or 3 days). Thus, out-of-date information should be assigned with small weights or even discarded. Fortunately, our learning process can automatically assign small weights for information with weak impacts, alleviating the impact decaying problem.\nIn order to know how early our model can predict the index movement, we show the F-1 scores of WoR-MI and M-MI with varied lead days from 1 to 3 and history days from 1 to 5 in Table 3. We observe that as the number of lead days increases, the predictive capabilities of our models decrease. This makes sense since the stock market commonly reGLYPH<29>ects the available information in a timely manner. In other words, the up-to-date information will immediately be reGLYPH<29>ected in the index change and the impacts will decay as time goes, making it difGLYPH<28>cult for long-term predictions.\nFigure 5 shows the weights of different data sources, that is, GLYPH<18> 1, GLYPH<18> 2 and GLYPH<18> 3. It can be observed that among the\nFIGURE 5. The weights of different data sources.\nTABLE 3. F-1 scores for M-MI and WoR-MI in 2015 and 2016 with varying lead days.\nthree sources, news events contribute most to the overall prediction, while the quantitative data takes the second place. It indicates that both news events and quantitative data have larger impacts to drive stock GLYPH<29>uctuations than sentiments.",
        "original_text": "We set both the number of history days and the number of lead days to 1. We empirically set m 0 D 0 : 6, and set m 1 ; m 2 and P 0 all as 0.5, i.e., the default setting in hinge loss. GLYPH<12> is set as 3.0, and GLYPH<21> m , GLYPH<21> d , GLYPH<21> s and GLYPH<21> GLYPH<18> are set as 0.05 by sensitivity analysis. The dimension of event representations is set as 100. Table 2 shows the performance of M-MI and the baselines. We can observe that M-MI outperforms all the baselines in both of the metrics, while SVM method shows the worst performance, indicating that simply tagging each news article with the label of its super-group is not effective. It can also be observed that M-MI and its variations (i.e., O-MI, WoH-MI and WoR-MI) all outperform nMIL. Compared to nMIL, M-MI improves F-1 by 6.9% in 2015 and 9.2% in 2016, while it improves accuracy by 6.7% and 9.4% in 2015 and 2016 respectively. Such gains mainly come from (1) utilizing multi-source information instead of only news articles, and (2) the advanced event representations rather than simple event features. Though all using multi-source information, TeSIA performs worse than M-MI and its variations, showing the effectiveness of our proposed models and feature extraction methods. Both M-MI and WoR-MI perform better than O-MI, indicating that both the structured event extraction module and the RBM pre-training module in our framework are effective. WoH-MI performs worse than M-MI, showing the proposed instance-level hinge losses across multiple data sources are useful for accurate predictions.\nFIGURE 4. F-1 scores with varying history days. (a) 2015. (b) 2016.\nFigure 4 (a) and (b) show the F-1 scores of all the comparative models with varying history days in training for 2015 and 2016 respectively (where lead day remains 1). The number of history days (i.e., t in Eq. 13) is varied from 1 to 5 and the results show that M-MI consistently performs better than the others. We can also observe that as the number of history days keeps increasing, the F-1 scores generally GLYPH<28>rst go up and then go down. The possible reason is that the impacts of the news, sentiments and quantitative indices released on some day will quickly decay after a period of time (2 or 3 days). Thus, out-of-date information should be assigned with small weights or even discarded. Fortunately, our learning process can automatically assign small weights for information with weak impacts, alleviating the impact decaying problem.\nIn order to know how early our model can predict the index movement, we show the F-1 scores of WoR-MI and M-MI with varied lead days from 1 to 3 and history days from 1 to 5 in Table 3. We observe that as the number of lead days increases, the predictive capabilities of our models decrease. This makes sense since the stock market commonly reGLYPH<29>ects the available information in a timely manner. In other words, the up-to-date information will immediately be reGLYPH<29>ected in the index change and the impacts will decay as time goes, making it difGLYPH<28>cult for long-term predictions.\nFigure 5 shows the weights of different data sources, that is, GLYPH<18> 1, GLYPH<18> 2 and GLYPH<18> 3. It can be observed that among the\nFIGURE 5. The weights of different data sources.\nTABLE 3. F-1 scores for M-MI and WoR-MI in 2015 and 2016 with varying lead days.\nthree sources, news events contribute most to the overall prediction, while the quantitative data takes the second place. It indicates that both news events and quantitative data have larger impacts to drive stock GLYPH<29>uctuations than sentiments.",
        "context": "Specifies the experimental settings for evaluating the model's performance, including the number of history days, lead days, and the weighting of different data sources (news events, quantitative data, and sentiments).",
        "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
        "pages": [
            8,
            7
        ],
        "id": "7aac9256c0c8e83e2244347075e7e8b17eba7168f24a797bc72d92f03944e653"
    },
    {
        "text": "Summarizes the paper's core contribution: a multi-source model that integrates diverse data (events, sentiments, and quantitative features) to predict stock market movement, along with a new event representation learning process.\n\nIn this paper, a Multi-source Multiple Instance model is proposed which can predict the stock market movement and identify the importance of the information simultaneously. Different from previous studies that commonly exploit only one data source, our model effectively integrates heterogeneous information, that is, the events, sentiments and historical quantitative features into a comprehensive framework, and considers the consistencies among different data sources to make a better prediction. We also propose a novel event representation learning process that can effectively capture the event information. Extensive evaluations on the two-year data conGLYPH<28>rm the effectiveness of our model.",
        "original_text": "In this paper, a Multi-source Multiple Instance model is proposed which can predict the stock market movement and identify the importance of the information simultaneously. Different from previous studies that commonly exploit only one data source, our model effectively integrates heterogeneous information, that is, the events, sentiments and historical quantitative features into a comprehensive framework, and considers the consistencies among different data sources to make a better prediction. We also propose a novel event representation learning process that can effectively capture the event information. Extensive evaluations on the two-year data conGLYPH<28>rm the effectiveness of our model.",
        "context": "Summarizes the paper's core contribution: a multi-source model that integrates diverse data (events, sentiments, and quantitative features) to predict stock market movement, along with a new event representation learning process.",
        "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
        "pages": [
            8
        ],
        "id": "583c07887826aa08be6742cde96fdbbb6535a18f014a75598ef8db6dc09ab916"
    },
    {
        "text": "Provides supporting evidence for the argument on economic inequality and transitions from background information to proposed methodology.\n\n- [1] E. F. Fama, ''The behavior of stock-market prices,'' J. Bus. , vol. 38, no. 1, pp. 34GLYPH<21>105, 1965.\n- [2] S. R. Das and M. Y. Chen, ''Yahoo! for Amazon: Sentiment extraction from small talk on the Web,'' Manage. Sci. , vol. 53, no. 9, pp. 1375GLYPH<21>1388, 2007.\n- [3] J. Si, A. Mukherjee, B. Liu, Q. Li, H. Li, and X. Deng, ''Exploiting topic based twitter sentiment for stock prediction,'' in Proc. 51st Annu. Meeting Assoc. Comput. Linguistics (ACL) , 2013, pp. 24GLYPH<21>29.\n- [4] W. Y. Wang and Z. Hua, ''A semiparametric Gaussian copula regression model for predicting GLYPH<28>nancial risks from earnings calls,'' in Proc. 52nd Annu. Meeting Assoc. Comput. Linguistics (ACL) , Jun. 2014, pp. 1155GLYPH<21>1165.\n- [5] S. Kogan, D. Levin, B. R. Routledge, J. S. Sagi, and N. A. Smith, ''Predicting risk from GLYPH<28>nancial reports with regression,'' in Proc. Annu. Conf. North Amer. Chapter Assoc. Comput. Linguistics Hum. Lang. Technol. , 2009, pp. 272GLYPH<21>280.\n- [6] R. Luss and A. D'Aspremont, ''Predicting abnormal returns from news using text classiGLYPH<28>cation,'' Quant. Finance , vol. 15, no. 6, pp. 999GLYPH<21>1012, 2015.\n- [7] R. R. Prechter, The Wave Principle of Human Social Behavior and the New Science of Socionomics , vol. 1. Gainesville, GA, USA: New Classics Library, 1999.\n- [8] J. R. Nofsinger, ''Social mood and GLYPH<28>nancial economics,'' J. Behav. Finance , vol. 6, no. 3, pp. 144GLYPH<21>160, 2005.\n- [9] J. Bi and X. Wang, ''Learning classiGLYPH<28>ers from dual annotation ambiguity via a minGLYPH<21>max framework,'' Neurocomputing , vol. 151, pp. 891GLYPH<21>904, Mar. 2015.\n- [10] S. Xie, W. Fan, and P. S. Yu, ''An iterative and re-weighting framework for rejection and uncertainty resolution in crowdsourcing,'' in Proc. SIAM Int. Conf. Data Mining , 2012, pp. 1107GLYPH<21>1118.\n- [11] Q. Le and T. Mikolov, ''Distributed representations of sentences and documents,'' in Proc. 31st Int. Conf. Mach. Learn. (ICML) , 2014, pp. 1188GLYPH<21>1196.\n- [12] F. Hogenboom, F. Frasincar, U. Kaymak, and F. De Jong, ''An overview of event extraction from text,'' in Proc. Workshop Detection, Represent., Exploitation Events Semantic Web (DeRiVE), 10th Int. Semantic Web Conf. (ISWC) , vol. 779, 2011, pp. 48GLYPH<21>57.\n- [13] R. Akita, A. Yoshihara, T. Matsubara, and K. Uehara, ''Deep learning for stock prediction using numerical and textual information,'' in Proc. IEEE/ACIS 15th Int. Conf. Comput. Inf. Sci. (ICIS) , Jun. 2016, pp. 1GLYPH<21>6.\n- [14] T. Nguyen, D. Phung, B. Adams, and S. Venkatesh, ''Event extraction using behaviors of sentiment signals and burst structure in social media,'' Knowl. Inf. Syst. , vol. 37, no. 2, pp. 279GLYPH<21>304, 2013.\n- [15] X. Ding, Y. Zhang, T. Liu, and J. Duan, ''Using structured events to predict stock price movement: An empirical investigation,'' in Proc. Conf. Empirical Methods Natural Lang. Process. (EMNLP) , 2014, pp. 1415GLYPH<21>1425.\n- [16] X. Ding, Y. Zhang, T. Liu, and J. Duan, ''Deep learning for event-driven stock prediction,'' in Proc. 24th Int. Joint Conf. Artif. Intell. (IJCAI) , 2015, pp. 2327GLYPH<21>2333.\n- [17] J. Bollen, H. Mao, and X. Zeng, ''Twitter mood predicts the stock market,'' J. Comput. Sci. , vol. 2, no. 1, pp. 1GLYPH<21>8, Mar. 2011.\n- [18] M. Makrehchi, S. Shah, and W. Liao, ''Stock prediction using eventbased sentiment analysis,'' in Proc. IEEE/WIC/ACM Int. Joint Conf. Web Intell. (WI) Intell. Agent Technol. (IAT) , vol. 1, Nov. 2013, pp. 337GLYPH<21>342.\n- [19] T. H. Nguyen and K. Shirai, ''Topic modeling based sentiment analysis on social media for stock market prediction,'' in Proc. 53rd Annu. Meeting Assoc. Comput. Linguistics (ACL) , 2015, pp. 1354GLYPH<21>1364.\n- [20] Q. Li, L. Jiang, P. Li, and H. Chen, ''Tensor-based learning for predicting stock movements,'' in Proc. 29th AAAI Conf. Artif. Intell. (AAAI) , 2015, pp. 1784GLYPH<21>1790.\n- [21] T. G. Dietterich, R. H. Lathrop, and T. Lozano-PØrez, ''Solving the multiple instance problem with axis-parallel rectangles,'' Artif. Intell. , vol. 89, nos. 1GLYPH<21>2, pp. 31GLYPH<21>71, 1997.\n- [22] J. Amores, ''Multiple instance classiGLYPH<28>cation: Review, taxonomy and comparative study,'' Artif. Intell. , vol. 201, pp. 81GLYPH<21>105, Aug. 2013.\n- [23] G. Liu, J. Wu, and Z.-H. Zhou, ''Key instance detection in multiinstance learning,'' in Proc. Asian Conf. Mach. Learn. , 2012, pp. 253GLYPH<21>268.\n- [24] D. Kotzias, M. Denil, N. de Freitas, and P. Smyth, ''From group to individual labels using deep features,'' in Proc. 21st ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD) , 2015, pp. 597GLYPH<21>606.\n- [25] J. Feng and Z.-H. Zhou, ''Deep MIML network,'' in Proc. 21st AAAI Conf. Artif. Intell. (AAAI) , 2017, pp. 1884GLYPH<21>1890.\n- [26] Y. Ning, S. Muthiah, H. Rangwala, and N. Ramakrishnan, ''Modeling precursors for event forecasting via nested multi-instance learning,'' in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD) , 2016, pp. 1095GLYPH<21>1104.\n- [27] G. E. Hinton and R. R. Salakhutdinov, ''Reducing the dimensionality of data with neural networks,'' Science , vol. 313, no. 5786, pp. 504GLYPH<21>507, 2006.\n- [28] X. Zhang et al. , ''IAD: Interaction-aware diffusion framework in social networks,'' IEEE Trans. Knowl. Data Eng. , to be published.\n- [29] W. X. Zhao, J. Jiang, H. Yan, and X. Li, ''Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid,'' in Proc. Conf. Empirical Methods Natural Lang. Process. , 2010, pp. 56GLYPH<21>65.",
        "original_text": "- [1] E. F. Fama, ''The behavior of stock-market prices,'' J. Bus. , vol. 38, no. 1, pp. 34GLYPH<21>105, 1965.\n- [2] S. R. Das and M. Y. Chen, ''Yahoo! for Amazon: Sentiment extraction from small talk on the Web,'' Manage. Sci. , vol. 53, no. 9, pp. 1375GLYPH<21>1388, 2007.\n- [3] J. Si, A. Mukherjee, B. Liu, Q. Li, H. Li, and X. Deng, ''Exploiting topic based twitter sentiment for stock prediction,'' in Proc. 51st Annu. Meeting Assoc. Comput. Linguistics (ACL) , 2013, pp. 24GLYPH<21>29.\n- [4] W. Y. Wang and Z. Hua, ''A semiparametric Gaussian copula regression model for predicting GLYPH<28>nancial risks from earnings calls,'' in Proc. 52nd Annu. Meeting Assoc. Comput. Linguistics (ACL) , Jun. 2014, pp. 1155GLYPH<21>1165.\n- [5] S. Kogan, D. Levin, B. R. Routledge, J. S. Sagi, and N. A. Smith, ''Predicting risk from GLYPH<28>nancial reports with regression,'' in Proc. Annu. Conf. North Amer. Chapter Assoc. Comput. Linguistics Hum. Lang. Technol. , 2009, pp. 272GLYPH<21>280.\n- [6] R. Luss and A. D'Aspremont, ''Predicting abnormal returns from news using text classiGLYPH<28>cation,'' Quant. Finance , vol. 15, no. 6, pp. 999GLYPH<21>1012, 2015.\n- [7] R. R. Prechter, The Wave Principle of Human Social Behavior and the New Science of Socionomics , vol. 1. Gainesville, GA, USA: New Classics Library, 1999.\n- [8] J. R. Nofsinger, ''Social mood and GLYPH<28>nancial economics,'' J. Behav. Finance , vol. 6, no. 3, pp. 144GLYPH<21>160, 2005.\n- [9] J. Bi and X. Wang, ''Learning classiGLYPH<28>ers from dual annotation ambiguity via a minGLYPH<21>max framework,'' Neurocomputing , vol. 151, pp. 891GLYPH<21>904, Mar. 2015.\n- [10] S. Xie, W. Fan, and P. S. Yu, ''An iterative and re-weighting framework for rejection and uncertainty resolution in crowdsourcing,'' in Proc. SIAM Int. Conf. Data Mining , 2012, pp. 1107GLYPH<21>1118.\n- [11] Q. Le and T. Mikolov, ''Distributed representations of sentences and documents,'' in Proc. 31st Int. Conf. Mach. Learn. (ICML) , 2014, pp. 1188GLYPH<21>1196.\n- [12] F. Hogenboom, F. Frasincar, U. Kaymak, and F. De Jong, ''An overview of event extraction from text,'' in Proc. Workshop Detection, Represent., Exploitation Events Semantic Web (DeRiVE), 10th Int. Semantic Web Conf. (ISWC) , vol. 779, 2011, pp. 48GLYPH<21>57.\n- [13] R. Akita, A. Yoshihara, T. Matsubara, and K. Uehara, ''Deep learning for stock prediction using numerical and textual information,'' in Proc. IEEE/ACIS 15th Int. Conf. Comput. Inf. Sci. (ICIS) , Jun. 2016, pp. 1GLYPH<21>6.\n- [14] T. Nguyen, D. Phung, B. Adams, and S. Venkatesh, ''Event extraction using behaviors of sentiment signals and burst structure in social media,'' Knowl. Inf. Syst. , vol. 37, no. 2, pp. 279GLYPH<21>304, 2013.\n- [15] X. Ding, Y. Zhang, T. Liu, and J. Duan, ''Using structured events to predict stock price movement: An empirical investigation,'' in Proc. Conf. Empirical Methods Natural Lang. Process. (EMNLP) , 2014, pp. 1415GLYPH<21>1425.\n- [16] X. Ding, Y. Zhang, T. Liu, and J. Duan, ''Deep learning for event-driven stock prediction,'' in Proc. 24th Int. Joint Conf. Artif. Intell. (IJCAI) , 2015, pp. 2327GLYPH<21>2333.\n- [17] J. Bollen, H. Mao, and X. Zeng, ''Twitter mood predicts the stock market,'' J. Comput. Sci. , vol. 2, no. 1, pp. 1GLYPH<21>8, Mar. 2011.\n- [18] M. Makrehchi, S. Shah, and W. Liao, ''Stock prediction using eventbased sentiment analysis,'' in Proc. IEEE/WIC/ACM Int. Joint Conf. Web Intell. (WI) Intell. Agent Technol. (IAT) , vol. 1, Nov. 2013, pp. 337GLYPH<21>342.\n- [19] T. H. Nguyen and K. Shirai, ''Topic modeling based sentiment analysis on social media for stock market prediction,'' in Proc. 53rd Annu. Meeting Assoc. Comput. Linguistics (ACL) , 2015, pp. 1354GLYPH<21>1364.\n- [20] Q. Li, L. Jiang, P. Li, and H. Chen, ''Tensor-based learning for predicting stock movements,'' in Proc. 29th AAAI Conf. Artif. Intell. (AAAI) , 2015, pp. 1784GLYPH<21>1790.\n- [21] T. G. Dietterich, R. H. Lathrop, and T. Lozano-PØrez, ''Solving the multiple instance problem with axis-parallel rectangles,'' Artif. Intell. , vol. 89, nos. 1GLYPH<21>2, pp. 31GLYPH<21>71, 1997.\n- [22] J. Amores, ''Multiple instance classiGLYPH<28>cation: Review, taxonomy and comparative study,'' Artif. Intell. , vol. 201, pp. 81GLYPH<21>105, Aug. 2013.\n- [23] G. Liu, J. Wu, and Z.-H. Zhou, ''Key instance detection in multiinstance learning,'' in Proc. Asian Conf. Mach. Learn. , 2012, pp. 253GLYPH<21>268.\n- [24] D. Kotzias, M. Denil, N. de Freitas, and P. Smyth, ''From group to individual labels using deep features,'' in Proc. 21st ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD) , 2015, pp. 597GLYPH<21>606.\n- [25] J. Feng and Z.-H. Zhou, ''Deep MIML network,'' in Proc. 21st AAAI Conf. Artif. Intell. (AAAI) , 2017, pp. 1884GLYPH<21>1890.\n- [26] Y. Ning, S. Muthiah, H. Rangwala, and N. Ramakrishnan, ''Modeling precursors for event forecasting via nested multi-instance learning,'' in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD) , 2016, pp. 1095GLYPH<21>1104.\n- [27] G. E. Hinton and R. R. Salakhutdinov, ''Reducing the dimensionality of data with neural networks,'' Science , vol. 313, no. 5786, pp. 504GLYPH<21>507, 2006.\n- [28] X. Zhang et al. , ''IAD: Interaction-aware diffusion framework in social networks,'' IEEE Trans. Knowl. Data Eng. , to be published.\n- [29] W. X. Zhao, J. Jiang, H. Yan, and X. Li, ''Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid,'' in Proc. Conf. Empirical Methods Natural Lang. Process. , 2010, pp. 56GLYPH<21>65.",
        "context": "Provides supporting evidence for the argument on economic inequality and transitions from background information to proposed methodology.",
        "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
        "pages": [
            8
        ],
        "id": "b8924c479fd4638dd76359ff91183307ed7f1d5947938673a5c3298a89152683"
    },
    {
        "text": "Provides citations for related research and acknowledges the authors involved in the study.\n\n- [30] L.-W. Ku and H.-H. Chen, ''Mining opinions from the Web: Beyond relevance retrieval,'' J. Amer. Soc. Inf. Sci. Technol. , vol. 58, no. 12, pp. 1838GLYPH<21>1850, 2007.\n- [31] A. Fader, S. Soderland, and O. Etzioni, ''Identifying relations for open information extraction,'' in Proc. Conf. Empirical Methods Natural Lang. Process. , 2011, pp. 1535GLYPH<21>1545.\nXI ZHANG (M'17) received the Ph.D. degree in computer science from Tsinghua University. He was a Visiting Scholar at The University of Illinois at Chicago. He is currently an Associate Professor with the Beijing University of Posts and Telecommunications and is also the Vice Director of the Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, China. His research interests include data mining and computer architecture.\nSIYU QU received the bachelor's degree in computer science from Xidian University in 2012. She is currently pursuing the master's degree with the Key Laboratory of Trustworthy Distributed Computing and Service, Beijing University of Posts and Telecommunications, Ministry of Education, China. Her research interests include data mining and machine learning.\nJIEYUN HUANG received the bachelor's degree in information security from the Beijing University of Posts and Telecommunications in 2017, where she is currently pursuing the master's degree with the Key Laboratory of Trustworthy Distributed Computing and Service. Her research interests are in data mining and machine learning.\nBINXING FANG received the Ph.D. degree from the Harbin Institute of Technology, China, in 1989. He was the Chief Scientist of the State Key Development Program of Basic Research of China. He is currently a member of the Chinese Academy of Engineering and is also a Professor with the School of Cyberspace Security, Beijing University of Posts and Telecommunications. His current research interests include big data and cybersecurity.\nPHILIP YU (F'93) received the Ph.D. degree in electrical engineering from Stanford University. He is currently a Distinguished Professor in computer science at The University of Illinois at Chicago and is also the Wexler Chair in information technology. His research interests include big data, data mining, data stream, database, and privacy. He is a fellow of ACM. He received the Research Contributions Award from the IEEE International Conference on Data Mining in 2003, the Technical Achievement Award from the IEEE Computer Society in 2013, and the ACM SIGKDD 2016 Innovation Award. He was the Editor-in-Chief of the IEEE TRANSACTIONSON KNOWLEDGEAND DATA ENGINEERING and the ACM Transactions on Knowledge Discovery from Data .",
        "original_text": "- [30] L.-W. Ku and H.-H. Chen, ''Mining opinions from the Web: Beyond relevance retrieval,'' J. Amer. Soc. Inf. Sci. Technol. , vol. 58, no. 12, pp. 1838GLYPH<21>1850, 2007.\n- [31] A. Fader, S. Soderland, and O. Etzioni, ''Identifying relations for open information extraction,'' in Proc. Conf. Empirical Methods Natural Lang. Process. , 2011, pp. 1535GLYPH<21>1545.\nXI ZHANG (M'17) received the Ph.D. degree in computer science from Tsinghua University. He was a Visiting Scholar at The University of Illinois at Chicago. He is currently an Associate Professor with the Beijing University of Posts and Telecommunications and is also the Vice Director of the Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, China. His research interests include data mining and computer architecture.\nSIYU QU received the bachelor's degree in computer science from Xidian University in 2012. She is currently pursuing the master's degree with the Key Laboratory of Trustworthy Distributed Computing and Service, Beijing University of Posts and Telecommunications, Ministry of Education, China. Her research interests include data mining and machine learning.\nJIEYUN HUANG received the bachelor's degree in information security from the Beijing University of Posts and Telecommunications in 2017, where she is currently pursuing the master's degree with the Key Laboratory of Trustworthy Distributed Computing and Service. Her research interests are in data mining and machine learning.\nBINXING FANG received the Ph.D. degree from the Harbin Institute of Technology, China, in 1989. He was the Chief Scientist of the State Key Development Program of Basic Research of China. He is currently a member of the Chinese Academy of Engineering and is also a Professor with the School of Cyberspace Security, Beijing University of Posts and Telecommunications. His current research interests include big data and cybersecurity.\nPHILIP YU (F'93) received the Ph.D. degree in electrical engineering from Stanford University. He is currently a Distinguished Professor in computer science at The University of Illinois at Chicago and is also the Wexler Chair in information technology. His research interests include big data, data mining, data stream, database, and privacy. He is a fellow of ACM. He received the Research Contributions Award from the IEEE International Conference on Data Mining in 2003, the Technical Achievement Award from the IEEE Computer Society in 2013, and the ACM SIGKDD 2016 Innovation Award. He was the Editor-in-Chief of the IEEE TRANSACTIONSON KNOWLEDGEAND DATA ENGINEERING and the ACM Transactions on Knowledge Discovery from Data .",
        "context": "Provides citations for related research and acknowledges the authors involved in the study.",
        "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
        "pages": [
            8,
            9
        ],
        "id": "eef4cdb3ae7db3a8433e04cb2678050d9bc1d66ad48ee7d7d1387f909c3127ee"
    }
]