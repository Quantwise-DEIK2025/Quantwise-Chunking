[
  {
    "text": "Received July 29, 2018, accepted August 27, 2018, date of publication September 13, 2018, date of current version October 8, 2018.\nDigital Object Identifier 10.1 109/ACCESS.2018.2869735\n\nProvides contextual information about the publication of the research paper, including its dates and DOI.",
    "original_text": "Received July 29, 2018, accepted August 27, 2018, date of publication September 13, 2018, date of current version October 8, 2018.\nDigital Object Identifier 10.1 109/ACCESS.2018.2869735",
    "context": "Provides contextual information about the publication of the research paper, including its dates and DOI.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      1
    ],
    "id": "990434da769b3b976cb9daaf6357417afdaad3ec0ad97db94bbba4a92877bdc2"
  },
  {
    "text": "XI ZHANG 1 , (Member, IEEE), SIYU QU 1 , JIEYUN HUANG 1 , BINXING FANG 1 , AND PHILIP YU 2 , (Fellow, IEEE)\n1 Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, Beijing University of Posts and Telecommunications,\nBeijing 100876, China\n2 Department of Computer Science, The University of Illinois at Chicago, Chicago, IL 60607, USA\nCorresponding author: Xi Zhang (zhangx@bupt.edu.cn)\nThis work was supported in part by the State Key Development Program of Basic Research of China under Grant 2013CB329605, in part by the Natural Science Foundation of China under Grant 61300014, in part by the NSF under Grant IIS-1526499, Grant IIS-1763325, and Grant CNS-1626432, and in part by the DongGuan Innovative Research Team Program under Grant 201636000100038.\nABSTRACT Forecasting the stock market movements is an important and challenging task. As the Web information grows, researchers begin to extract effective indicators (e.g., the events and sentiments) from the Web to facilitate the prediction. However, the indicators obtained in previous studies are usually based on only one data source and thus may not fully cover the factors that can affect the stock market movements. In this paper, to improve the prediction for stock market composite index movements, we exploit the consistencies among different data sources, and develop a multi-source multiple instance model that can effectively combine events, sentiments, as well as the quantitative data into a comprehensive framework. To effectively capture the news events, we successfully apply a novel event extraction and representation method. Evaluations on the data from the year 2015 and 2016 demonstrate the effectiveness of our model. In addition, our approach is able to automatically determine the importance of each data source and identify the crucial input information that is considered to drive the movements, making the predictions interpretable.\nINDEX TERMS Stock prediction, multiple instance, event extraction, sentiment analysis.\n\nIntroduces the core problem and proposed solution: a multi-source multiple instance model for predicting stock market movements by integrating events, sentiments, and quantitative data, with a focus on interpretable predictions.",
    "original_text": "XI ZHANG 1 , (Member, IEEE), SIYU QU 1 , JIEYUN HUANG 1 , BINXING FANG 1 , AND PHILIP YU 2 , (Fellow, IEEE)\n1 Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, Beijing University of Posts and Telecommunications,\nBeijing 100876, China\n2 Department of Computer Science, The University of Illinois at Chicago, Chicago, IL 60607, USA\nCorresponding author: Xi Zhang (zhangx@bupt.edu.cn)\nThis work was supported in part by the State Key Development Program of Basic Research of China under Grant 2013CB329605, in part by the Natural Science Foundation of China under Grant 61300014, in part by the NSF under Grant IIS-1526499, Grant IIS-1763325, and Grant CNS-1626432, and in part by the DongGuan Innovative Research Team Program under Grant 201636000100038.\nABSTRACT Forecasting the stock market movements is an important and challenging task. As the Web information grows, researchers begin to extract effective indicators (e.g., the events and sentiments) from the Web to facilitate the prediction. However, the indicators obtained in previous studies are usually based on only one data source and thus may not fully cover the factors that can affect the stock market movements. In this paper, to improve the prediction for stock market composite index movements, we exploit the consistencies among different data sources, and develop a multi-source multiple instance model that can effectively combine events, sentiments, as well as the quantitative data into a comprehensive framework. To effectively capture the news events, we successfully apply a novel event extraction and representation method. Evaluations on the data from the year 2015 and 2016 demonstrate the effectiveness of our model. In addition, our approach is able to automatically determine the importance of each data source and identify the crucial input information that is considered to drive the movements, making the predictions interpretable.\nINDEX TERMS Stock prediction, multiple instance, event extraction, sentiment analysis.",
    "context": "Introduces the core problem and proposed solution: a multi-source multiple instance model for predicting stock market movements by integrating events, sentiments, and quantitative data, with a focus on interpretable predictions.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      1
    ],
    "id": "9ed6907515207f4a5e5cd48f826d4d5e8cc95ce11d0377e9b4f08ae801c7d000"
  },
  {
    "text": "Stock markets play important roles in the economic operations of modern society. The estimation of the stock market index is of clear interest to various stakeholders in the market. According to the EfGLYPH<28>cient Market Hypothesis (EMH) [1], the stock market prices reGLYPH<29>ect all available information, and thus the prediction naturally relies on information from multiple sources, which can be roughly categorized into (1) quantitative data, e.g., historical prices, turnover rate, and (2) qualitative descriptions, such as the annual reports, announcements, news and social media posts. It is challenging to deal with qualitative data as they are usually unstructured and thus extracting useful signals from them is not trivial.\nAlong with the growing Web information and the advance of Natural Language Processing (NLP) techniques, recent works begin to explore Web news for market prediction. A number of existing studies have shown that the events reported in news are important signals that can drive market\nGLYPH<29>uctuations [2]GLYPH<21>[4]. However, most of the previous works represent news documents using simple features (e.g., bagwords, noun phrases, named entities) [5], [6], which may discard syntax information. Due to the large volume and diverse expressions of the events, how to represent them as useful features, and how to identify the crucial events that have signiGLYPH<28>cant impacts on the stock market are not trivial problems. In addition to events, a line of studies has shown that the investors' opinions can also largely inGLYPH<29>uence the market volatility [7], [8]. With the prosperity of Web 2.0, the sentiments extracted from social media can be beneGLYPH<28>cial to predictions. Since both events and sentiments can drive the GLYPH<29>uctuations of the market, it is natural to investigate how to effectively fuse them together to make a better prediction. The improvement may come from the correlations among different sources, and the consensus prediction with multisource information can potentially outperform each prediction relying on a single source. This problem is analogous to the multi-labeler learning problem in crowdsourcing [9], [10],\nVOLUME 6, 2018\nbut different from those studies that usually assume a labeler conducts classiGLYPH<28>cation with full information, each ''labeler'' (i.e., classiGLYPH<28>er) in this study is source-speciGLYPH<28>c and only provided with limited information from its own source, making the consensus among labelers even more challenging.\nIn this work, we aim to learn a predictive model for describing the GLYPH<29>uctuations in the stock market index by utilizing various sources of data, involving the historical quantitative data, the social media and Web news. The essential features we extract include the event representations from news articles and the sentiments from social media. Firstly, we propose a novel method to capture the event information. SpeciGLYPH<28>cally, structured events are extracted from news texts and then used as the inputs for Restricted Boltzmann Machines (RBMs) to do the pre-training. After that, the output vectors from RBMs are used as the inputs to a recently proposed sentence2vec framework [11], in order to achieve effective event embeddings. Secondly, we exploit the latent relationships among different data sources with carefully designed loss terms, and propose an extension of the Multiple Instance Learning (MIL) model that can effectively integrate the features from multiple sources to make more accurate predictions. One beneGLYPH<28>t of our method is that we can determine source-speciGLYPH<28>c weights and identify the speciGLYPH<28>c factors that incur the changes in the composite index. Figure 1 shows an example of the news precursors identiGLYPH<28>ed by our model, and the dots with numbers denote the probabilistic estimates for the events leading to the index change on Jan. 26, 2016.\nFIGURE 1. An example of the news events that are responsible for the Shanghai Composite Index change on Jan. 26, 2016. The x-axis is the timeline. The left y-axis is the probability of each event leading to the index change. The right y-axis is the composite index in Shanghai Stock Exchange.\nThe summary of the contributions is as follows:\n- 1) To provide robust and accurate predictions for stock market movements, we extend the Multiple Instance Learning model to integrate the heterogeneous information including Web news, social media posts, and quantitative data.\n- 2) The latent consistencies among different data sources are modeled in our framework by sharing the common estimated true label among the hinge losses of different data sources at the instance level.\n- 3) A novel event representation model is proposed by GLYPH<28>rst extracting structured events from news text, and then training them with deep learning methods involving RBM and sentence2vec to obtain dense vectors.\n- 4) Evaluation results on two-year datasets show that our proposal can outperform the state-of-art baselines. Moreover, the impacts of different sources and the key factors that drive the movements can be obtained.\n\nIntroduces the core problem: predicting stock market movements and highlights the challenge of integrating diverse data sources (quantitative data and qualitative descriptions like news and social media). It also establishes the key approach: utilizing a multi-source multiple instance learning (M-MI) model to fuse event representations and sentiments for improved predictions.",
    "original_text": "Stock markets play important roles in the economic operations of modern society. The estimation of the stock market index is of clear interest to various stakeholders in the market. According to the EfGLYPH<28>cient Market Hypothesis (EMH) [1], the stock market prices reGLYPH<29>ect all available information, and thus the prediction naturally relies on information from multiple sources, which can be roughly categorized into (1) quantitative data, e.g., historical prices, turnover rate, and (2) qualitative descriptions, such as the annual reports, announcements, news and social media posts. It is challenging to deal with qualitative data as they are usually unstructured and thus extracting useful signals from them is not trivial.\nAlong with the growing Web information and the advance of Natural Language Processing (NLP) techniques, recent works begin to explore Web news for market prediction. A number of existing studies have shown that the events reported in news are important signals that can drive market\nGLYPH<29>uctuations [2]GLYPH<21>[4]. However, most of the previous works represent news documents using simple features (e.g., bagwords, noun phrases, named entities) [5], [6], which may discard syntax information. Due to the large volume and diverse expressions of the events, how to represent them as useful features, and how to identify the crucial events that have signiGLYPH<28>cant impacts on the stock market are not trivial problems. In addition to events, a line of studies has shown that the investors' opinions can also largely inGLYPH<29>uence the market volatility [7], [8]. With the prosperity of Web 2.0, the sentiments extracted from social media can be beneGLYPH<28>cial to predictions. Since both events and sentiments can drive the GLYPH<29>uctuations of the market, it is natural to investigate how to effectively fuse them together to make a better prediction. The improvement may come from the correlations among different sources, and the consensus prediction with multisource information can potentially outperform each prediction relying on a single source. This problem is analogous to the multi-labeler learning problem in crowdsourcing [9], [10],\nVOLUME 6, 2018\nbut different from those studies that usually assume a labeler conducts classiGLYPH<28>cation with full information, each ''labeler'' (i.e., classiGLYPH<28>er) in this study is source-speciGLYPH<28>c and only provided with limited information from its own source, making the consensus among labelers even more challenging.\nIn this work, we aim to learn a predictive model for describing the GLYPH<29>uctuations in the stock market index by utilizing various sources of data, involving the historical quantitative data, the social media and Web news. The essential features we extract include the event representations from news articles and the sentiments from social media. Firstly, we propose a novel method to capture the event information. SpeciGLYPH<28>cally, structured events are extracted from news texts and then used as the inputs for Restricted Boltzmann Machines (RBMs) to do the pre-training. After that, the output vectors from RBMs are used as the inputs to a recently proposed sentence2vec framework [11], in order to achieve effective event embeddings. Secondly, we exploit the latent relationships among different data sources with carefully designed loss terms, and propose an extension of the Multiple Instance Learning (MIL) model that can effectively integrate the features from multiple sources to make more accurate predictions. One beneGLYPH<28>t of our method is that we can determine source-speciGLYPH<28>c weights and identify the speciGLYPH<28>c factors that incur the changes in the composite index. Figure 1 shows an example of the news precursors identiGLYPH<28>ed by our model, and the dots with numbers denote the probabilistic estimates for the events leading to the index change on Jan. 26, 2016.\nFIGURE 1. An example of the news events that are responsible for the Shanghai Composite Index change on Jan. 26, 2016. The x-axis is the timeline. The left y-axis is the probability of each event leading to the index change. The right y-axis is the composite index in Shanghai Stock Exchange.\nThe summary of the contributions is as follows:\n- 1) To provide robust and accurate predictions for stock market movements, we extend the Multiple Instance Learning model to integrate the heterogeneous information including Web news, social media posts, and quantitative data.\n- 2) The latent consistencies among different data sources are modeled in our framework by sharing the common estimated true label among the hinge losses of different data sources at the instance level.\n- 3) A novel event representation model is proposed by GLYPH<28>rst extracting structured events from news text, and then training them with deep learning methods involving RBM and sentence2vec to obtain dense vectors.\n- 4) Evaluation results on two-year datasets show that our proposal can outperform the state-of-art baselines. Moreover, the impacts of different sources and the key factors that drive the movements can be obtained.",
    "context": "Introduces the core problem: predicting stock market movements and highlights the challenge of integrating diverse data sources (quantitative data and qualitative descriptions like news and social media). It also establishes the key approach: utilizing a multi-source multiple instance learning (M-MI) model to fuse event representations and sentiments for improved predictions.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      1,
      2
    ],
    "id": "7ad78330122f8822c806a60689a342ee4a4dc34abf7a776d73dcd0f131fad1ed"
  },
  {
    "text": "There is a line of research works using event-driven stock prediction models. Hogenboom et al . [12] give an overview of event extraction methods. Akita et al . [13] convert newspaper articles into distributed representations via Paragraph Vector and model the temporal effects of past events with LSTM on opening prices of stocks in Tokyo Stock Exchange. Nguyen et al . [14] formulated a temporal sentiment index function, which is used to extract signiGLYPH<28>cant events. Then the corresponding blog posts are analyzed using topic modeling to understand the contents. Ding et al . [15] applied the Open IE tool to extract structured events from texts, and this event extraction method is also implemented as a baseline and compared with our proposal. Ding et al . [16] then trained event embeddings with a neural tensor network and then used the deep convolutional neural network to model inGLYPH<29>uences of events.\nIn addition to events, investors' emotions also have great impacts on the stock market index. Bollen et al . [17] revealed that the public moods derived from Twitter have impacts on stock indicators. Si et al . [3] proposed a technique to leverage topic based sentiments from Twitter to predict the stock market. Makrehchi et al . [18] assigned a positive or negative label for each tweet according to stock movements. The aggregate sentiment per day shows predictive power for stock market prediction. Topic-speciGLYPH<28>c sentiments are learned in [19] to facilitate the stock prediction. However, this method is not suitable to short texts in social media.\nThe common limitation of the aforementioned methods is that they rely only on a single data source and thus may limit the predictive power. In [20], events and sentiments are integrated into a tensor framework together with GLYPH<28>rm-speciGLYPH<28>c features (e.g., P/B, P/E), to model the joint impacts on the stock volatility. We also implement it as a baseline. However, it uses a simple event extraction method which may not fully capture sufGLYPH<28>cient event information.\n\nSummarizes the reliance on single data sources in existing event-driven stock prediction models and highlights the need for integrating multiple sources to overcome this limitation.",
    "original_text": "There is a line of research works using event-driven stock prediction models. Hogenboom et al . [12] give an overview of event extraction methods. Akita et al . [13] convert newspaper articles into distributed representations via Paragraph Vector and model the temporal effects of past events with LSTM on opening prices of stocks in Tokyo Stock Exchange. Nguyen et al . [14] formulated a temporal sentiment index function, which is used to extract signiGLYPH<28>cant events. Then the corresponding blog posts are analyzed using topic modeling to understand the contents. Ding et al . [15] applied the Open IE tool to extract structured events from texts, and this event extraction method is also implemented as a baseline and compared with our proposal. Ding et al . [16] then trained event embeddings with a neural tensor network and then used the deep convolutional neural network to model inGLYPH<29>uences of events.\nIn addition to events, investors' emotions also have great impacts on the stock market index. Bollen et al . [17] revealed that the public moods derived from Twitter have impacts on stock indicators. Si et al . [3] proposed a technique to leverage topic based sentiments from Twitter to predict the stock market. Makrehchi et al . [18] assigned a positive or negative label for each tweet according to stock movements. The aggregate sentiment per day shows predictive power for stock market prediction. Topic-speciGLYPH<28>c sentiments are learned in [19] to facilitate the stock prediction. However, this method is not suitable to short texts in social media.\nThe common limitation of the aforementioned methods is that they rely only on a single data source and thus may limit the predictive power. In [20], events and sentiments are integrated into a tensor framework together with GLYPH<28>rm-speciGLYPH<28>c features (e.g., P/B, P/E), to model the joint impacts on the stock volatility. We also implement it as a baseline. However, it uses a simple event extraction method which may not fully capture sufGLYPH<28>cient event information.",
    "context": "Summarizes the reliance on single data sources in existing event-driven stock prediction models and highlights the need for integrating multiple sources to overcome this limitation.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      2
    ],
    "id": "63466cd1c3215b2b14348f9a75e130ab740dfbf5849f70b156789da93e4a08bc"
  },
  {
    "text": "The multiple instance learning (MIL) paradigm is a form of weakly supervised learning. Training instances arranged in sets are called bags or groups. A label is provided for entire groups instead of individual instances. Negative groups don't contain any positive instances, while positive groups contain at least one positive instance [21]. Various applications and the comparisons of different methods in MIL\nwere given in [22]. The common MIL approach is used to predict the group-level label, Liu et al . [23], however, proposed an approach to identify the instance-level labels, especially the labels of key instances in groups based on K nearest neighbors ( K -NN). Kotzias et al . [24] predicted the labels for sentences given labels for reviews, which can be used to detect sentiments. A multiple-instance multiple-label learning framework with deep neutral network formation is proposed in [25]. An event forecasting framework via the nested multiple instance learning is proposed in [26]. However, it only uses one data source and simple event features, which may not be sufGLYPH<28>cient in the stock market application. We have implemented this algorithm as a baseline for comparison.\n\nHighlights the limitations of existing MIL approaches in the context of stock prediction, specifically noting their reliance on single data sources and simple event features, and introduces the proposed M-MI model as a more robust alternative.",
    "original_text": "The multiple instance learning (MIL) paradigm is a form of weakly supervised learning. Training instances arranged in sets are called bags or groups. A label is provided for entire groups instead of individual instances. Negative groups don't contain any positive instances, while positive groups contain at least one positive instance [21]. Various applications and the comparisons of different methods in MIL\nwere given in [22]. The common MIL approach is used to predict the group-level label, Liu et al . [23], however, proposed an approach to identify the instance-level labels, especially the labels of key instances in groups based on K nearest neighbors ( K -NN). Kotzias et al . [24] predicted the labels for sentences given labels for reviews, which can be used to detect sentiments. A multiple-instance multiple-label learning framework with deep neutral network formation is proposed in [25]. An event forecasting framework via the nested multiple instance learning is proposed in [26]. However, it only uses one data source and simple event features, which may not be sufGLYPH<28>cient in the stock market application. We have implemented this algorithm as a baseline for comparison.",
    "context": "Highlights the limitations of existing MIL approaches in the context of stock prediction, specifically noting their reliance on single data sources and simple event features, and introduces the proposed M-MI model as a more robust alternative.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      2,
      3
    ],
    "id": "0428f89847fd011bab3d275aa32980711a132649d362258d11276dd5b09751f7"
  },
  {
    "text": "In this section, we GLYPH<28>rst state and formulate the problem, and then propose the multi-source multiple instance (M-MI) framework. Before going into details of our framework, we deGLYPH<28>ne some important notations as shown in Table 1.\nTABLE 1. Notations in our model.\n\nThis chunk introduces the problem being addressed and outlines the framework to be presented, including the definition of key notations.",
    "original_text": "In this section, we GLYPH<28>rst state and formulate the problem, and then propose the multi-source multiple instance (M-MI) framework. Before going into details of our framework, we deGLYPH<28>ne some important notations as shown in Table 1.\nTABLE 1. Notations in our model.",
    "context": "This chunk introduces the problem being addressed and outlines the framework to be presented, including the definition of key notations.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      3
    ],
    "id": "af3c341f1413d2c6e010922625c3ede6cb8e26fd77851bd0b8e952a0d8c9cf5c"
  },
  {
    "text": "Stock markets are impacted by various factors, such as the trading volume, news events and the investors' emotions. Thus, relying on a single data source may not be sufGLYPH<28>cient to make accurate predictions. The object of our study is to develop a multi-source data integration approach to predict the stock market trends. SpeciGLYPH<28>cally, given a collection of economic news, social network posts and historical trading data, we aim to forecast the stock market index movements. Moreover, we also try to obtain the impacts of each data source and identify the key factors that have decisive inGLYPH<29>uences, which may be inGLYPH<29>uential news, collective sentiments or some important quantitative index in the trading data. These key factors are supporting evidence for further analysis and can make our prediction interpretable.\nFormally, according to Table 1, a news article j on day i is denoted as a V -dimensional vector x ij 2 R V GLYPH<2> 1 (please note that the process of representing a news article as a vector will be illustrated in the next section). In order to predict the stock market movement on day t C k , we assume that there are a group of news articles for each day i ( i < t ), which is denoted as X i , and thus X i D f x ij g ; j 2 f 1 ; GLYPH<1> GLYPH<1> GLYPH<1> ; ni g . In addition to the news articles, the sentiment and quantitative indices on day i (denoted as si and di respectively) are also taken into account. Then the temporal ordered collection of news articles, sentiments and quantitative indices across t days can be represented as a multi-source super group, that is, G D f Ci g ; i 2 f 1 ; GLYPH<1> GLYPH<1> GLYPH<1> ; t g , where Ci D f X i ; di ; si g . The change in the stock market movement on day t C k can be denoted as Yt C k 2 fC 1 ; GLYPH<0> 1 g , where C 1 denotes the index rise and -1 denotes the index decline. Then the forecasting problem can be modeled as a mathematical function f ( G ) ! Yt C k , indicating that we map the multi-source information to an indicator (i.e., label) k days in the future from the day t , where k is number of the lead days that we aim to forecast.\nFIGURE 2. The system framework of our proposed model.\n\nProvides a framework for predicting stock market movements by integrating heterogeneous data sources (news, sentiments, and quantitative data) and modeling the consistency among them.",
    "original_text": "Stock markets are impacted by various factors, such as the trading volume, news events and the investors' emotions. Thus, relying on a single data source may not be sufGLYPH<28>cient to make accurate predictions. The object of our study is to develop a multi-source data integration approach to predict the stock market trends. SpeciGLYPH<28>cally, given a collection of economic news, social network posts and historical trading data, we aim to forecast the stock market index movements. Moreover, we also try to obtain the impacts of each data source and identify the key factors that have decisive inGLYPH<29>uences, which may be inGLYPH<29>uential news, collective sentiments or some important quantitative index in the trading data. These key factors are supporting evidence for further analysis and can make our prediction interpretable.\nFormally, according to Table 1, a news article j on day i is denoted as a V -dimensional vector x ij 2 R V GLYPH<2> 1 (please note that the process of representing a news article as a vector will be illustrated in the next section). In order to predict the stock market movement on day t C k , we assume that there are a group of news articles for each day i ( i < t ), which is denoted as X i , and thus X i D f x ij g ; j 2 f 1 ; GLYPH<1> GLYPH<1> GLYPH<1> ; ni g . In addition to the news articles, the sentiment and quantitative indices on day i (denoted as si and di respectively) are also taken into account. Then the temporal ordered collection of news articles, sentiments and quantitative indices across t days can be represented as a multi-source super group, that is, G D f Ci g ; i 2 f 1 ; GLYPH<1> GLYPH<1> GLYPH<1> ; t g , where Ci D f X i ; di ; si g . The change in the stock market movement on day t C k can be denoted as Yt C k 2 fC 1 ; GLYPH<0> 1 g , where C 1 denotes the index rise and -1 denotes the index decline. Then the forecasting problem can be modeled as a mathematical function f ( G ) ! Yt C k , indicating that we map the multi-source information to an indicator (i.e., label) k days in the future from the day t , where k is number of the lead days that we aim to forecast.\nFIGURE 2. The system framework of our proposed model.",
    "context": "Provides a framework for predicting stock market movements by integrating heterogeneous data sources (news, sentiments, and quantitative data) and modeling the consistency among them.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      3
    ],
    "id": "39c825d2635eebb324ba7686fde1bb602ed93eea99b7c365141b1c4f4dc38a2e"
  },
  {
    "text": "The framework of our proposal is shown in Fig. 2. The inputs of the framework are the stock quantitative data, the social media and Web news. We GLYPH<28>rst use the sentiment analyzer to obtain the collective sentiments from social media, and extract effective event representations from the Web news. Then the extracted sentiments, events as well as the stock quantitative data are fed into the M-MI model. The M-MI model is proposed based on the Multiple Instance Learning algorithm, that is, a group of instances are given group labels, which are assumed to be an association function (e.g., OR, average) of the instance-level labels. Our work further distinguishes the instance-level labels, multi-source group-level labels, and multi-source super group-level labels. The primary goal is to predict the label for the multi-source super group that indicates the rise or decline of the stock market index. In addition, we also try to estimate the instancelevel probabilities indicating how related a speciGLYPH<28>c instance is to the index movement (i.e., target label), as well as the\nsource-speciGLYPH<28>c weights that reveal how related a speciGLYPH<28>c source is to the index movement.\nTo this end, for a given day, we GLYPH<28>rst model the instancelevel probability pij for a news article j on day i to the target label with a logistic function, that is\n<!-- formula-not-decoded -->\nwhere w m denotes the weight vector of the news articles. The higher the probability pij , the more related the article j is to the target label. The probability of all the news articles for a given day i can be computed as the average of probabilities of each news article, that is\n<!-- formula-not-decoded -->\nIn addition to news articles, we also model the probability p d GLYPH<0> i for stock quantitative data and p s GLYPH<0> i for sentiments on day i , that is\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere w d and w s denote the weight vector of d i and s i respectively. We then model the probability Pi for multisource information on day i as\n<!-- formula-not-decoded -->\nwhere GLYPH<18> 0, GLYPH<18> 1 and GLYPH<18> 2 denote the source-speciGLYPH<28>c weights of p m GLYPH<0> i , p d GLYPH<0> i and p s GLYPH<0> i respectively, and GLYPH<18> 0 C GLYPH<18> 1 C GLYPH<18> 2 D 1. It is obvious that Pi 2 [0 ; 1]. We use GLYPH<18> D ( GLYPH<18> 0 ; GLYPH<18> 1 ; GLYPH<18> 2) to denote the source weight vector, and then model the probability of the multi-source super group as the average of the probabilities in t days, that is\n<!-- formula-not-decoded -->\nThen, we start with a log-likelihood loss function:\n<!-- formula-not-decoded -->\n■ As the inGLYPH<29>uences of the multi-source information usually last for a number of days, we assume the probabilities on two consecutive days are essentially similar, which can be represented by minimizing the cost where G is a multi-source super group, n is the number of multi-source super groups, and Y denotes the set of true labels. ( GLYPH<1> ) is the indicator function.\n<!-- formula-not-decoded -->\nwhere Ci denotes the multi-source group for the day i . By introducing this loss term, Eq. 7 can be rewritten as:\n<!-- formula-not-decoded -->\nwhere GLYPH<12> is a constant to control the contribution of the GLYPH<28>rst term. Eq. 9 aggregates the costs at the super group level and the group level. However, the instance-level loss has not been considered yet, which is challenging to be designed due to two reasons: (1) it lacks of true labels at the instance level; (2) the instances from different sources are heterogeneous but intrinsically correlated. The instances can be categorized into three types according to their sources, and each type leads to a distinct loss term. Inspired by the hinge loss used in Support Vector Machines (SVMs), the classiGLYPH<28>cation loss term for the instances of news article instance xij is\n<!-- formula-not-decoded -->\nHere, sgn ( GLYPH<1> ) is the sign function, m 0 is a margin parameter used to separate the positive and negative instances from the hyperplane in the feature space. w T m x ij denotes the prediction with article xij . As the true label for each instance is unknown during the classiGLYPH<28>er training, we replace it with the estimated true label sgn ( Pi -P 0 ), where P 0 is a threshold parameter to determine the positiveness of the prediction. If ( Pi -P 0 ) > 0, the prediction with multiple-source information on day i would be positive. Otherwise, it would be negative. Similarly, we can derive the instance-level loss terms for quantitative data and sentiments respectively,\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nBased on Eq. 10, 11 and 12, the classiGLYPH<28>cation loss at the instance level for each data source has been obtained. We then explain why they share a common estimated true label (i.e., sgn ( Pi -P 0 )). As predictions from different sources are commonly correlated with each other, instead of treating the loss of each source independently, we need to consider their intrinsic consistencies. The intuition behind is that according to EfGLYPH<28>cient Market Hypothesis, different data sources would keep up to date with the latest stock market information, and they commonly indicate the same sign (index rise or fall). Thus, through sharing the same estimated true label, we are able to combine the indications from different data sources to learn a consensus label. This can potentially provide more robust and conGLYPH<28>dent predictions.\nWe then give several cases to illustrate the consensus among sources. The three source-speciGLYPH<28>c predictions are denoted as l 0 , l 1 and l 2 respectively. Firstly, if l 0 , l 1 and l 2 all make very positive predictions, i.e., large values of pm -i , pd -i and ps -i , it would be conGLYPH<28>dent to make a positive group-level prediction due to Pi > P 0 and l 0 , l 1 and l 2\nall agree with the label without costs. Secondly, if only l 0 disagrees with the estimated true label, l 0 will be penalized as l 1 and l 2 agree with this label and make Pi approach their predictions. Thirdly, if l 0 disagrees with l 1 and l 2 , but l 0 is very conGLYPH<28>dent (and thus far from hyperplane) while l 1 and l 2 are not conGLYPH<28>dent enough (and thus close to hyperplane), this may make Pi approach l 0 , resulting in that l 0 agrees with the estimated true label while l 1 and l 2 disagree with it and thus are penalized. Our proposed instance-level loss terms are consistent with these cases and thus make sense.\nThen we try to minimize the overall instance-level loss, that is, h 1( x ij ; w m ) C h 2( d i ; w d ) C h 3( s i ; w s ). By introducing this summation and other regularization terms, the objective function Eq. 9 can be reformulated as\n<!-- formula-not-decoded -->\nEq. 13 is the ultimate objective function to optimize. To summarize, it consists of losses at three levels: the super group level, the group level and the instance level. In addition, it includes the regularization terms, that is, R ( w m ), R ( w d ), R ( w s ) and R ( GLYPH<18> ), and GLYPH<12> , GLYPH<21> m , GLYPH<21> d , GLYPH<21> s and GLYPH<21> GLYPH<18> are constants to control the trade-offs among multiple terms. The model learning goal is to estimate the parameters w m , w d , w s and GLYPH<18> to minimize L ( w m ; w d ; w s ; GLYPH<18> ). We randomly choose a set ( G ; Y ) from S , and the online stochastic gradient descent optimization is adopted to GLYPH<28>t the model.\n\nDescribes the framework for predicting stock market movements, integrating data from news, social media, and quantitative sources. It utilizes a Multiple Instance Learning (MIL) model with a novel event representation learning process and emphasizes the importance of source-specific weights to capture the consensus among different data sources.",
    "original_text": "The framework of our proposal is shown in Fig. 2. The inputs of the framework are the stock quantitative data, the social media and Web news. We GLYPH<28>rst use the sentiment analyzer to obtain the collective sentiments from social media, and extract effective event representations from the Web news. Then the extracted sentiments, events as well as the stock quantitative data are fed into the M-MI model. The M-MI model is proposed based on the Multiple Instance Learning algorithm, that is, a group of instances are given group labels, which are assumed to be an association function (e.g., OR, average) of the instance-level labels. Our work further distinguishes the instance-level labels, multi-source group-level labels, and multi-source super group-level labels. The primary goal is to predict the label for the multi-source super group that indicates the rise or decline of the stock market index. In addition, we also try to estimate the instancelevel probabilities indicating how related a speciGLYPH<28>c instance is to the index movement (i.e., target label), as well as the\nsource-speciGLYPH<28>c weights that reveal how related a speciGLYPH<28>c source is to the index movement.\nTo this end, for a given day, we GLYPH<28>rst model the instancelevel probability pij for a news article j on day i to the target label with a logistic function, that is\n<!-- formula-not-decoded -->\nwhere w m denotes the weight vector of the news articles. The higher the probability pij , the more related the article j is to the target label. The probability of all the news articles for a given day i can be computed as the average of probabilities of each news article, that is\n<!-- formula-not-decoded -->\nIn addition to news articles, we also model the probability p d GLYPH<0> i for stock quantitative data and p s GLYPH<0> i for sentiments on day i , that is\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere w d and w s denote the weight vector of d i and s i respectively. We then model the probability Pi for multisource information on day i as\n<!-- formula-not-decoded -->\nwhere GLYPH<18> 0, GLYPH<18> 1 and GLYPH<18> 2 denote the source-speciGLYPH<28>c weights of p m GLYPH<0> i , p d GLYPH<0> i and p s GLYPH<0> i respectively, and GLYPH<18> 0 C GLYPH<18> 1 C GLYPH<18> 2 D 1. It is obvious that Pi 2 [0 ; 1]. We use GLYPH<18> D ( GLYPH<18> 0 ; GLYPH<18> 1 ; GLYPH<18> 2) to denote the source weight vector, and then model the probability of the multi-source super group as the average of the probabilities in t days, that is\n<!-- formula-not-decoded -->\nThen, we start with a log-likelihood loss function:\n<!-- formula-not-decoded -->\n■ As the inGLYPH<29>uences of the multi-source information usually last for a number of days, we assume the probabilities on two consecutive days are essentially similar, which can be represented by minimizing the cost where G is a multi-source super group, n is the number of multi-source super groups, and Y denotes the set of true labels. ( GLYPH<1> ) is the indicator function.\n<!-- formula-not-decoded -->\nwhere Ci denotes the multi-source group for the day i . By introducing this loss term, Eq. 7 can be rewritten as:\n<!-- formula-not-decoded -->\nwhere GLYPH<12> is a constant to control the contribution of the GLYPH<28>rst term. Eq. 9 aggregates the costs at the super group level and the group level. However, the instance-level loss has not been considered yet, which is challenging to be designed due to two reasons: (1) it lacks of true labels at the instance level; (2) the instances from different sources are heterogeneous but intrinsically correlated. The instances can be categorized into three types according to their sources, and each type leads to a distinct loss term. Inspired by the hinge loss used in Support Vector Machines (SVMs), the classiGLYPH<28>cation loss term for the instances of news article instance xij is\n<!-- formula-not-decoded -->\nHere, sgn ( GLYPH<1> ) is the sign function, m 0 is a margin parameter used to separate the positive and negative instances from the hyperplane in the feature space. w T m x ij denotes the prediction with article xij . As the true label for each instance is unknown during the classiGLYPH<28>er training, we replace it with the estimated true label sgn ( Pi -P 0 ), where P 0 is a threshold parameter to determine the positiveness of the prediction. If ( Pi -P 0 ) > 0, the prediction with multiple-source information on day i would be positive. Otherwise, it would be negative. Similarly, we can derive the instance-level loss terms for quantitative data and sentiments respectively,\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nBased on Eq. 10, 11 and 12, the classiGLYPH<28>cation loss at the instance level for each data source has been obtained. We then explain why they share a common estimated true label (i.e., sgn ( Pi -P 0 )). As predictions from different sources are commonly correlated with each other, instead of treating the loss of each source independently, we need to consider their intrinsic consistencies. The intuition behind is that according to EfGLYPH<28>cient Market Hypothesis, different data sources would keep up to date with the latest stock market information, and they commonly indicate the same sign (index rise or fall). Thus, through sharing the same estimated true label, we are able to combine the indications from different data sources to learn a consensus label. This can potentially provide more robust and conGLYPH<28>dent predictions.\nWe then give several cases to illustrate the consensus among sources. The three source-speciGLYPH<28>c predictions are denoted as l 0 , l 1 and l 2 respectively. Firstly, if l 0 , l 1 and l 2 all make very positive predictions, i.e., large values of pm -i , pd -i and ps -i , it would be conGLYPH<28>dent to make a positive group-level prediction due to Pi > P 0 and l 0 , l 1 and l 2\nall agree with the label without costs. Secondly, if only l 0 disagrees with the estimated true label, l 0 will be penalized as l 1 and l 2 agree with this label and make Pi approach their predictions. Thirdly, if l 0 disagrees with l 1 and l 2 , but l 0 is very conGLYPH<28>dent (and thus far from hyperplane) while l 1 and l 2 are not conGLYPH<28>dent enough (and thus close to hyperplane), this may make Pi approach l 0 , resulting in that l 0 agrees with the estimated true label while l 1 and l 2 disagree with it and thus are penalized. Our proposed instance-level loss terms are consistent with these cases and thus make sense.\nThen we try to minimize the overall instance-level loss, that is, h 1( x ij ; w m ) C h 2( d i ; w d ) C h 3( s i ; w s ). By introducing this summation and other regularization terms, the objective function Eq. 9 can be reformulated as\n<!-- formula-not-decoded -->\nEq. 13 is the ultimate objective function to optimize. To summarize, it consists of losses at three levels: the super group level, the group level and the instance level. In addition, it includes the regularization terms, that is, R ( w m ), R ( w d ), R ( w s ) and R ( GLYPH<18> ), and GLYPH<12> , GLYPH<21> m , GLYPH<21> d , GLYPH<21> s and GLYPH<21> GLYPH<18> are constants to control the trade-offs among multiple terms. The model learning goal is to estimate the parameters w m , w d , w s and GLYPH<18> to minimize L ( w m ; w d ; w s ; GLYPH<18> ). We randomly choose a set ( G ; Y ) from S , and the online stochastic gradient descent optimization is adopted to GLYPH<28>t the model.",
    "context": "Describes the framework for predicting stock market movements, integrating data from news, social media, and quantitative sources. It utilizes a Multiple Instance Learning (MIL) model with a novel event representation learning process and emphasizes the importance of source-specific weights to capture the consensus among different data sources.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      3,
      4,
      5
    ],
    "id": "d3db3e341b9b3a661356b26b5a5314024e4eaf4f7ea86141844454c45c6ac721"
  },
  {
    "text": "After the learning process, the source weight vector GLYPH<18> is obtained, representing the impacts of different data sources on the market movements. In addition, a probability for each piece of input information can also be obtained through Eq. 1, 3 or 4, which reveals the probability of that information signifying the rise of the market index on the target day. Note that if the probability signifying the index rise is pr , the probability indicating index decline would be 1 GLYPH<0> pr . We can identify the key input information that triggers the market index movement, if the product of its probability value and its source-speciGLYPH<28>c weight is above a given threshold GLYPH<28> .\n\nIdentifies the relative importance of different data sources in predicting market movements and outlines how to determine key information driving index changes.",
    "original_text": "After the learning process, the source weight vector GLYPH<18> is obtained, representing the impacts of different data sources on the market movements. In addition, a probability for each piece of input information can also be obtained through Eq. 1, 3 or 4, which reveals the probability of that information signifying the rise of the market index on the target day. Note that if the probability signifying the index rise is pr , the probability indicating index decline would be 1 GLYPH<0> pr . We can identify the key input information that triggers the market index movement, if the product of its probability value and its source-speciGLYPH<28>c weight is above a given threshold GLYPH<28> .",
    "context": "Identifies the relative importance of different data sources in predicting market movements and outlines how to determine key information driving index changes.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      5
    ],
    "id": "25f44ed84cbffde363f0517d1dc3d35384ab567270a351db961722e8f117c30c"
  },
  {
    "text": "The quantitative features are quite simple to extract, we just collect three indices and normalize each index to form d i 2 R 3 GLYPH<2> 1 . Here we introduce how to extract event representations from news articles and extract the sentiments from posts in social media, which are used as the inputs to M-MI framework.\nFIGURE 3. Structured event extraction from texts.\n\nDescribes the process of extracting event representations from news articles and sentiments from social media posts, which are then used as inputs to the M-MI framework.",
    "original_text": "The quantitative features are quite simple to extract, we just collect three indices and normalize each index to form d i 2 R 3 GLYPH<2> 1 . Here we introduce how to extract event representations from news articles and extract the sentiments from posts in social media, which are used as the inputs to M-MI framework.\nFIGURE 3. Structured event extraction from texts.",
    "context": "Describes the process of extracting event representations from news articles and sentiments from social media posts, which are then used as inputs to the M-MI framework.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      5
    ],
    "id": "9b7a7361b3ab2b0a7fc7db3c3365d0c49669f69970f8755dd2003bdbc78273b3"
  },
  {
    "text": "Conventional methods commonly represent events using simple features such as TF-IDF, noun phrases or named entities. Recent advances in NLP techniques enable more accurate event models with structures. In this study, we GLYPH<28>rst use the syntactic analysis method to extract the main structure information of the sentences, and then use it as the input to an RBM. The output of an RBM would be a pre-trained vector used as the input to sentence2vec, and then the event representations are obtained. The process is shown in Figure 3 and described in detail as follows. Note that though we use the Chinese dataset in this study, this process can also be applied to other languages.\n- 1) Structured event extraction. With a commonly used text parser HanLP, 1 we can capture the syntactic structure of a sentence, which is depicted as a three-level tree at the top of Fig. 3. The root node denotes the core verb, and the nodes of the second layer are the subject of the verb and the object of the verb respectively. The child of the subject is the modiGLYPH<28>er who is the nearest to the subject in the sentence, and so is the child of the object. Then we connect these core words together as the structure information to represent the event information.\n- 2) Training with RBM. Wethen map the structured event into a vector. To make the vectors better reconstruct the original events, we use RBM as a pre-training module. The Restricted Boltzmann Machine (RBM) is a generative stochastic artiGLYPH<28>cial neural network, and has been applied in various applications such as dimensionality reduction [27]. RBM contains two-layer neural nets, one is the visible layer or input layer, and the\n1 https://github.com/hankcs/HanLP\nother is the hidden layer. In our model, each event is represented as an m -dimensional vector with one-hot encoding, which is the visible layer. Our target is to estimate the n -dimensional hidden layer to approximate the input layer as much as possible. Then the hidden layer will be set as the initial vector in sentence2vec. The reason is that directly training the representations using sentence2vec without RBM may fall into the local minimum.\n- 3) Training with sentence2vec. Finally, we use sentence2vec, the neural probabilistic language model to obtain the event representations. Different from the word2vec with CBOW model, the sentence id will be added during the training process of sentence2vec, and will also be mapped into a vector, called sentence vector, which would be the GLYPH<28>nal vector that we want. In the training process, the sentence vector and the word vectors of context will be concatenated as the input to softmax. After training, the sentence vector will be obtained and used as the features for the proposed model.\nHere is an example of extracting structured events from the news. The news text is that it is expected that the Renminbi speculators will face huge losses. After the dependency parsing analysis, the core words (Renminbi, speculators, face, huge, losses) are obtained, and after one hot coding, each word is encoded into zero or one vector. Then the vector preprocesses by RBM into a 100-dimensional vector, and GLYPH<28>nally processes by the sentence2vec became the news event feature vector. Through the above steps, the news event is obtained as a feature of the M-MI model, a 100-dimensional vector.\n\nDetails the process of extracting structured events from news text, outlining the use of HanLP for syntactic analysis, RBM training for event representation, and sentence2vec for final feature generation.",
    "original_text": "Conventional methods commonly represent events using simple features such as TF-IDF, noun phrases or named entities. Recent advances in NLP techniques enable more accurate event models with structures. In this study, we GLYPH<28>rst use the syntactic analysis method to extract the main structure information of the sentences, and then use it as the input to an RBM. The output of an RBM would be a pre-trained vector used as the input to sentence2vec, and then the event representations are obtained. The process is shown in Figure 3 and described in detail as follows. Note that though we use the Chinese dataset in this study, this process can also be applied to other languages.\n- 1) Structured event extraction. With a commonly used text parser HanLP, 1 we can capture the syntactic structure of a sentence, which is depicted as a three-level tree at the top of Fig. 3. The root node denotes the core verb, and the nodes of the second layer are the subject of the verb and the object of the verb respectively. The child of the subject is the modiGLYPH<28>er who is the nearest to the subject in the sentence, and so is the child of the object. Then we connect these core words together as the structure information to represent the event information.\n- 2) Training with RBM. Wethen map the structured event into a vector. To make the vectors better reconstruct the original events, we use RBM as a pre-training module. The Restricted Boltzmann Machine (RBM) is a generative stochastic artiGLYPH<28>cial neural network, and has been applied in various applications such as dimensionality reduction [27]. RBM contains two-layer neural nets, one is the visible layer or input layer, and the\n1 https://github.com/hankcs/HanLP\nother is the hidden layer. In our model, each event is represented as an m -dimensional vector with one-hot encoding, which is the visible layer. Our target is to estimate the n -dimensional hidden layer to approximate the input layer as much as possible. Then the hidden layer will be set as the initial vector in sentence2vec. The reason is that directly training the representations using sentence2vec without RBM may fall into the local minimum.\n- 3) Training with sentence2vec. Finally, we use sentence2vec, the neural probabilistic language model to obtain the event representations. Different from the word2vec with CBOW model, the sentence id will be added during the training process of sentence2vec, and will also be mapped into a vector, called sentence vector, which would be the GLYPH<28>nal vector that we want. In the training process, the sentence vector and the word vectors of context will be concatenated as the input to softmax. After training, the sentence vector will be obtained and used as the features for the proposed model.\nHere is an example of extracting structured events from the news. The news text is that it is expected that the Renminbi speculators will face huge losses. After the dependency parsing analysis, the core words (Renminbi, speculators, face, huge, losses) are obtained, and after one hot coding, each word is encoded into zero or one vector. Then the vector preprocesses by RBM into a 100-dimensional vector, and GLYPH<28>nally processes by the sentence2vec became the news event feature vector. Through the above steps, the news event is obtained as a feature of the M-MI model, a 100-dimensional vector.",
    "context": "Details the process of extracting structured events from news text, outlining the use of HanLP for syntactic analysis, RBM training for event representation, and sentence2vec for final feature generation.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      5,
      6
    ],
    "id": "af43596b4685514849c49cd2a5747e81ce26aab614736401713f519efbaa891f"
  },
  {
    "text": "To extract the sentiments from the posts in the social network, we use the LDA-S method [28], an extension of Latent Dirichlet Allocation (LDA) model that proposed to obtain the topic-speciGLYPH<28>c sentiments for short texts. The intuition behind is that extracting sentiments discarding topics may be not sufGLYPH<28>cient as sentiment polarities usually depend on topics or domains [29]. In other words, the exact same word mayexpress different sentiment polarities for different topics, e.g., the opinion word ''low'' in the phrase ''low speed'' in a trafGLYPH<28>c-related topic and ''low fat'' in a food-related topic. Therefore, extracting the sentiments corresponding to different topics can potentially improve the sentiment classiGLYPH<28>cation accuracy. The LDA-S model can infer sentiment distribution and topic distribution simultaneously for short texts. It consists of two steps. The GLYPH<28>rst step aims to obtain the topic distribution of each post, and then set the topic as the one with the largest probability. The second step gets the sentiment distribution of each post.\nIn this work, a sentiment word list called NTUSD [30] is adopted, which contains 4370 negative words and 4566 positive words. If a word is an adjective but not in the sentiment\nword list, the sentiment label of this word is set as neutral. If a word is a noun, it is considered as a topic word. Otherwise, it is considered as a background word. For each topic, opinion word distributions are distinguished from two polarities, that is, positive or negative.\n\nExplains the LDA-S method for sentiment extraction from social media posts, detailing its two-step process and the use of a sentiment word list to determine positive, negative, or neutral sentiment labels for each post.",
    "original_text": "To extract the sentiments from the posts in the social network, we use the LDA-S method [28], an extension of Latent Dirichlet Allocation (LDA) model that proposed to obtain the topic-speciGLYPH<28>c sentiments for short texts. The intuition behind is that extracting sentiments discarding topics may be not sufGLYPH<28>cient as sentiment polarities usually depend on topics or domains [29]. In other words, the exact same word mayexpress different sentiment polarities for different topics, e.g., the opinion word ''low'' in the phrase ''low speed'' in a trafGLYPH<28>c-related topic and ''low fat'' in a food-related topic. Therefore, extracting the sentiments corresponding to different topics can potentially improve the sentiment classiGLYPH<28>cation accuracy. The LDA-S model can infer sentiment distribution and topic distribution simultaneously for short texts. It consists of two steps. The GLYPH<28>rst step aims to obtain the topic distribution of each post, and then set the topic as the one with the largest probability. The second step gets the sentiment distribution of each post.\nIn this work, a sentiment word list called NTUSD [30] is adopted, which contains 4370 negative words and 4566 positive words. If a word is an adjective but not in the sentiment\nword list, the sentiment label of this word is set as neutral. If a word is a noun, it is considered as a topic word. Otherwise, it is considered as a background word. For each topic, opinion word distributions are distinguished from two polarities, that is, positive or negative.",
    "context": "Explains the LDA-S method for sentiment extraction from social media posts, detailing its two-step process and the use of a sentiment word list to determine positive, negative, or neutral sentiment labels for each post.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      6
    ],
    "id": "d4e215c5fadc5fac4d170615b4265dea2c1c5e1e8211ce725c91926dd7e520fa"
  },
  {
    "text": "We collected stock market-related information from Jan. 1, 2015 to Dec. 31, 2016, and separate the information into two data sets, one for the year 2015 and the other for 2016. The data consist of three parts, the historical quantitative data, the news articles and the posts on the social network, which are introduced in detail as follows.\n- GLYPH<15> Quantitative data : the source of quantitative data is Wind, 2 a widely used GLYPH<28>nancial information service provider in China. The data we collect are the average prices, market index change and turnover rate of the Shanghai Composite Index in each trading day.\n- GLYPH<15> News data : we collect the news articles on the macro economy through Wind, and get 38,727 and 39,465 news articles in 2015 and 2016 respectively. The news articles are aggregated by Wind from major GLYPH<28>nancial news websites in China, such as http://GLYPH<28>nance.sina.com.cn and http://www.hexun.com. We process the news titles rather than the whole articles to extract the events, as the main topic of a news article is often summed up in the title.\n- GLYPH<15> Social media data : the sentiments are extracted from the posts crawled from a popular investor social network in China named Xueqiu. 3 Totally 6,163,056 postings are collected for 2015 and 2016. For each post, we get the posting time stamp and the content.\nFor each trading day, if the stock market index rises, it would be a positive instance, otherwise it is a negative instance. For each year, we use the data from the GLYPH<28>rst 10 months as the training set and the last 2 months as the testing set. We evaluate the performance of our model with varying lead days and varying historical days. Lead days refers to the number of days in advance the model makes predictions and the historical days indicates the number of days over which the multi-source information is utilized. The evaluation metrics we use are F1-score and accuracy (ACC).\n\nDetails the data collection process, specifically outlining the sources and types of data used for the study’s evaluation.",
    "original_text": "We collected stock market-related information from Jan. 1, 2015 to Dec. 31, 2016, and separate the information into two data sets, one for the year 2015 and the other for 2016. The data consist of three parts, the historical quantitative data, the news articles and the posts on the social network, which are introduced in detail as follows.\n- GLYPH<15> Quantitative data : the source of quantitative data is Wind, 2 a widely used GLYPH<28>nancial information service provider in China. The data we collect are the average prices, market index change and turnover rate of the Shanghai Composite Index in each trading day.\n- GLYPH<15> News data : we collect the news articles on the macro economy through Wind, and get 38,727 and 39,465 news articles in 2015 and 2016 respectively. The news articles are aggregated by Wind from major GLYPH<28>nancial news websites in China, such as http://GLYPH<28>nance.sina.com.cn and http://www.hexun.com. We process the news titles rather than the whole articles to extract the events, as the main topic of a news article is often summed up in the title.\n- GLYPH<15> Social media data : the sentiments are extracted from the posts crawled from a popular investor social network in China named Xueqiu. 3 Totally 6,163,056 postings are collected for 2015 and 2016. For each post, we get the posting time stamp and the content.\nFor each trading day, if the stock market index rises, it would be a positive instance, otherwise it is a negative instance. For each year, we use the data from the GLYPH<28>rst 10 months as the training set and the last 2 months as the testing set. We evaluate the performance of our model with varying lead days and varying historical days. Lead days refers to the number of days in advance the model makes predictions and the historical days indicates the number of days over which the multi-source information is utilized. The evaluation metrics we use are F1-score and accuracy (ACC).",
    "context": "Details the data collection process, specifically outlining the sources and types of data used for the study’s evaluation.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      6
    ],
    "id": "08aa975b95d5c9c782b71fcb335e2ab61751bb4c4f6bb145ecf3952b96836668"
  },
  {
    "text": "The following baselines and variations of our proposed model are implemented for comparisons. The full implementation of our framework is named as Multi-source Multiple Instance (M-MI) model.\n- GLYPH<15> SVM : the standard support vector machine is used as a basic prediction method. During the training process, the label assigned to each instance and each group is the same as its multi-source super group label. During the\n2 http://www.wind.com.cn/\n3 https://xueqiu.com/\nprediction phase, we obtain the predicted label for each of the instance, and then average the labels as the GLYPH<28>nal label of the super group.\n- GLYPH<15> TeSIA: the tensor-based learning approach [20] utilizes multi-source information for stock prediction. SpeciGLYPH<28>cally, it uses a third-order tensor to model the GLYPH<28>rm-mode, event-mode, and sentiment-mode data.\n- GLYPH<15> nMIL : nested Multi-Instance Learning (nMIL) model [26] is the state-of-art baseline. In this model, only one data source, i.e., the news, is used to extract simple event features. It ignores the impacts of the sentiments and the historical quantitative indices.\n- GLYPH<15> O-MI : Open IE Multiple Instance (O-MI) Learning model differs from M-MI in the event extraction module. It adopts a previously proposed event extraction method [15], and uses Open IE [31] to extract event tuples from sentences. The structured event tuples are then processed by sentence2vec to obtain event representations. Please note that the sentiment data and quantitative data are also used in this model.\n- GLYPH<15> WoR-MI : Without RBM Multiple Instance (WoR-MI) Learning model is also a part of the M-MI framework. It differs M-MI in that it works without the RBM module, and therefore the sentence2vec module is fed with original structured events instead of pre-trained vectors.\n- GLYPH<15> WoH-MI : Compare to M-MI, Without Hinge loss Multiple Instance (WoH-MI) Learning model lacks the instance-level hinge loss terms (i.e., Eq. 10, 11 and 12).\nTo make a fair comparison, we use the same set of instances and the same setting of parameters to evaluate different methods. In our proposal and the baselines, we set the predicted label to GLYPH<0> 1 if the estimated probability for a multisource super group is less than 0.5; otherwise, we set the predicted label to C 1.\nTABLE 2. Prediction Results (history day=1, lead day=1).\n\nProvides a comparison of different baseline models for stock prediction, outlining the specific methods used and the criteria for evaluating their performance.",
    "original_text": "The following baselines and variations of our proposed model are implemented for comparisons. The full implementation of our framework is named as Multi-source Multiple Instance (M-MI) model.\n- GLYPH<15> SVM : the standard support vector machine is used as a basic prediction method. During the training process, the label assigned to each instance and each group is the same as its multi-source super group label. During the\n2 http://www.wind.com.cn/\n3 https://xueqiu.com/\nprediction phase, we obtain the predicted label for each of the instance, and then average the labels as the GLYPH<28>nal label of the super group.\n- GLYPH<15> TeSIA: the tensor-based learning approach [20] utilizes multi-source information for stock prediction. SpeciGLYPH<28>cally, it uses a third-order tensor to model the GLYPH<28>rm-mode, event-mode, and sentiment-mode data.\n- GLYPH<15> nMIL : nested Multi-Instance Learning (nMIL) model [26] is the state-of-art baseline. In this model, only one data source, i.e., the news, is used to extract simple event features. It ignores the impacts of the sentiments and the historical quantitative indices.\n- GLYPH<15> O-MI : Open IE Multiple Instance (O-MI) Learning model differs from M-MI in the event extraction module. It adopts a previously proposed event extraction method [15], and uses Open IE [31] to extract event tuples from sentences. The structured event tuples are then processed by sentence2vec to obtain event representations. Please note that the sentiment data and quantitative data are also used in this model.\n- GLYPH<15> WoR-MI : Without RBM Multiple Instance (WoR-MI) Learning model is also a part of the M-MI framework. It differs M-MI in that it works without the RBM module, and therefore the sentence2vec module is fed with original structured events instead of pre-trained vectors.\n- GLYPH<15> WoH-MI : Compare to M-MI, Without Hinge loss Multiple Instance (WoH-MI) Learning model lacks the instance-level hinge loss terms (i.e., Eq. 10, 11 and 12).\nTo make a fair comparison, we use the same set of instances and the same setting of parameters to evaluate different methods. In our proposal and the baselines, we set the predicted label to GLYPH<0> 1 if the estimated probability for a multisource super group is less than 0.5; otherwise, we set the predicted label to C 1.\nTABLE 2. Prediction Results (history day=1, lead day=1).",
    "context": "Provides a comparison of different baseline models for stock prediction, outlining the specific methods used and the criteria for evaluating their performance.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      6,
      7
    ],
    "id": "9ebe8b983f46dfb8659a23d9eca0404c35dc829d846d4b847fece42f59732aa8"
  },
  {
    "text": "We set both the number of history days and the number of lead days to 1. We empirically set m 0 D 0 : 6, and set m 1 ; m 2 and P 0 all as 0.5, i.e., the default setting in hinge loss. GLYPH<12> is set as 3.0, and GLYPH<21> m , GLYPH<21> d , GLYPH<21> s and GLYPH<21> GLYPH<18> are set as 0.05 by sensitivity analysis. The dimension of event representations is set as 100. Table 2 shows the performance of M-MI and the baselines. We can observe that M-MI outperforms all the baselines in both of the metrics, while SVM method shows the worst performance, indicating that simply tagging each news article with the label of its super-group is not effective. It can also be observed that M-MI and its variations (i.e., O-MI, WoH-MI and WoR-MI) all outperform nMIL. Compared to nMIL, M-MI improves F-1 by 6.9% in 2015 and 9.2% in 2016, while it improves accuracy by 6.7% and 9.4% in 2015 and 2016 respectively. Such gains mainly come from (1) utilizing multi-source information instead of only news articles, and (2) the advanced event representations rather than simple event features. Though all using multi-source information, TeSIA performs worse than M-MI and its variations, showing the effectiveness of our proposed models and feature extraction methods. Both M-MI and WoR-MI perform better than O-MI, indicating that both the structured event extraction module and the RBM pre-training module in our framework are effective. WoH-MI performs worse than M-MI, showing the proposed instance-level hinge losses across multiple data sources are useful for accurate predictions.\nFIGURE 4. F-1 scores with varying history days. (a) 2015. (b) 2016.\nFigure 4 (a) and (b) show the F-1 scores of all the comparative models with varying history days in training for 2015 and 2016 respectively (where lead day remains 1). The number of history days (i.e., t in Eq. 13) is varied from 1 to 5 and the results show that M-MI consistently performs better than the others. We can also observe that as the number of history days keeps increasing, the F-1 scores generally GLYPH<28>rst go up and then go down. The possible reason is that the impacts of the news, sentiments and quantitative indices released on some day will quickly decay after a period of time (2 or 3 days). Thus, out-of-date information should be assigned with small weights or even discarded. Fortunately, our learning process can automatically assign small weights for information with weak impacts, alleviating the impact decaying problem.\nIn order to know how early our model can predict the index movement, we show the F-1 scores of WoR-MI and M-MI with varied lead days from 1 to 3 and history days from 1 to 5 in Table 3. We observe that as the number of lead days increases, the predictive capabilities of our models decrease. This makes sense since the stock market commonly reGLYPH<29>ects the available information in a timely manner. In other words, the up-to-date information will immediately be reGLYPH<29>ected in the index change and the impacts will decay as time goes, making it difGLYPH<28>cult for long-term predictions.\nFigure 5 shows the weights of different data sources, that is, GLYPH<18> 1, GLYPH<18> 2 and GLYPH<18> 3. It can be observed that among the\nFIGURE 5. The weights of different data sources.\nTABLE 3. F-1 scores for M-MI and WoR-MI in 2015 and 2016 with varying lead days.\nthree sources, news events contribute most to the overall prediction, while the quantitative data takes the second place. It indicates that both news events and quantitative data have larger impacts to drive stock GLYPH<29>uctuations than sentiments.\n\nSummarizes the experimental setup and results, highlighting M-MI's superior performance compared to baselines and the importance of news events in predicting stock market movements.",
    "original_text": "We set both the number of history days and the number of lead days to 1. We empirically set m 0 D 0 : 6, and set m 1 ; m 2 and P 0 all as 0.5, i.e., the default setting in hinge loss. GLYPH<12> is set as 3.0, and GLYPH<21> m , GLYPH<21> d , GLYPH<21> s and GLYPH<21> GLYPH<18> are set as 0.05 by sensitivity analysis. The dimension of event representations is set as 100. Table 2 shows the performance of M-MI and the baselines. We can observe that M-MI outperforms all the baselines in both of the metrics, while SVM method shows the worst performance, indicating that simply tagging each news article with the label of its super-group is not effective. It can also be observed that M-MI and its variations (i.e., O-MI, WoH-MI and WoR-MI) all outperform nMIL. Compared to nMIL, M-MI improves F-1 by 6.9% in 2015 and 9.2% in 2016, while it improves accuracy by 6.7% and 9.4% in 2015 and 2016 respectively. Such gains mainly come from (1) utilizing multi-source information instead of only news articles, and (2) the advanced event representations rather than simple event features. Though all using multi-source information, TeSIA performs worse than M-MI and its variations, showing the effectiveness of our proposed models and feature extraction methods. Both M-MI and WoR-MI perform better than O-MI, indicating that both the structured event extraction module and the RBM pre-training module in our framework are effective. WoH-MI performs worse than M-MI, showing the proposed instance-level hinge losses across multiple data sources are useful for accurate predictions.\nFIGURE 4. F-1 scores with varying history days. (a) 2015. (b) 2016.\nFigure 4 (a) and (b) show the F-1 scores of all the comparative models with varying history days in training for 2015 and 2016 respectively (where lead day remains 1). The number of history days (i.e., t in Eq. 13) is varied from 1 to 5 and the results show that M-MI consistently performs better than the others. We can also observe that as the number of history days keeps increasing, the F-1 scores generally GLYPH<28>rst go up and then go down. The possible reason is that the impacts of the news, sentiments and quantitative indices released on some day will quickly decay after a period of time (2 or 3 days). Thus, out-of-date information should be assigned with small weights or even discarded. Fortunately, our learning process can automatically assign small weights for information with weak impacts, alleviating the impact decaying problem.\nIn order to know how early our model can predict the index movement, we show the F-1 scores of WoR-MI and M-MI with varied lead days from 1 to 3 and history days from 1 to 5 in Table 3. We observe that as the number of lead days increases, the predictive capabilities of our models decrease. This makes sense since the stock market commonly reGLYPH<29>ects the available information in a timely manner. In other words, the up-to-date information will immediately be reGLYPH<29>ected in the index change and the impacts will decay as time goes, making it difGLYPH<28>cult for long-term predictions.\nFigure 5 shows the weights of different data sources, that is, GLYPH<18> 1, GLYPH<18> 2 and GLYPH<18> 3. It can be observed that among the\nFIGURE 5. The weights of different data sources.\nTABLE 3. F-1 scores for M-MI and WoR-MI in 2015 and 2016 with varying lead days.\nthree sources, news events contribute most to the overall prediction, while the quantitative data takes the second place. It indicates that both news events and quantitative data have larger impacts to drive stock GLYPH<29>uctuations than sentiments.",
    "context": "Summarizes the experimental setup and results, highlighting M-MI's superior performance compared to baselines and the importance of news events in predicting stock market movements.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      8,
      7
    ],
    "id": "7aac9256c0c8e83e2244347075e7e8b17eba7168f24a797bc72d92f03944e653"
  },
  {
    "text": "In this paper, a Multi-source Multiple Instance model is proposed which can predict the stock market movement and identify the importance of the information simultaneously. Different from previous studies that commonly exploit only one data source, our model effectively integrates heterogeneous information, that is, the events, sentiments and historical quantitative features into a comprehensive framework, and considers the consistencies among different data sources to make a better prediction. We also propose a novel event representation learning process that can effectively capture the event information. Extensive evaluations on the two-year data conGLYPH<28>rm the effectiveness of our model.\n\nSummarizes the core contribution: proposes a multi-source, multi-instance model for stock market prediction, emphasizing its novel event representation learning and the validation through two-year data.",
    "original_text": "In this paper, a Multi-source Multiple Instance model is proposed which can predict the stock market movement and identify the importance of the information simultaneously. Different from previous studies that commonly exploit only one data source, our model effectively integrates heterogeneous information, that is, the events, sentiments and historical quantitative features into a comprehensive framework, and considers the consistencies among different data sources to make a better prediction. We also propose a novel event representation learning process that can effectively capture the event information. Extensive evaluations on the two-year data conGLYPH<28>rm the effectiveness of our model.",
    "context": "Summarizes the core contribution: proposes a multi-source, multi-instance model for stock market prediction, emphasizing its novel event representation learning and the validation through two-year data.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      8
    ],
    "id": "583c07887826aa08be6742cde96fdbbb6535a18f014a75598ef8db6dc09ab916"
  },
  {
    "text": "- [1] E. F. Fama, ''The behavior of stock-market prices,'' J. Bus. , vol. 38, no. 1, pp. 34GLYPH<21>105, 1965.\n- [2] S. R. Das and M. Y. Chen, ''Yahoo! for Amazon: Sentiment extraction from small talk on the Web,'' Manage. Sci. , vol. 53, no. 9, pp. 1375GLYPH<21>1388, 2007.\n- [3] J. Si, A. Mukherjee, B. Liu, Q. Li, H. Li, and X. Deng, ''Exploiting topic based twitter sentiment for stock prediction,'' in Proc. 51st Annu. Meeting Assoc. Comput. Linguistics (ACL) , 2013, pp. 24GLYPH<21>29.\n- [4] W. Y. Wang and Z. Hua, ''A semiparametric Gaussian copula regression model for predicting GLYPH<28>nancial risks from earnings calls,'' in Proc. 52nd Annu. Meeting Assoc. Comput. Linguistics (ACL) , Jun. 2014, pp. 1155GLYPH<21>1165.\n- [5] S. Kogan, D. Levin, B. R. Routledge, J. S. Sagi, and N. A. Smith, ''Predicting risk from GLYPH<28>nancial reports with regression,'' in Proc. Annu. Conf. North Amer. Chapter Assoc. Comput. Linguistics Hum. Lang. Technol. , 2009, pp. 272GLYPH<21>280.\n- [6] R. Luss and A. D'Aspremont, ''Predicting abnormal returns from news using text classiGLYPH<28>cation,'' Quant. Finance , vol. 15, no. 6, pp. 999GLYPH<21>1012, 2015.\n- [7] R. R. Prechter, The Wave Principle of Human Social Behavior and the New Science of Socionomics , vol. 1. Gainesville, GA, USA: New Classics Library, 1999.\n- [8] J. R. Nofsinger, ''Social mood and GLYPH<28>nancial economics,'' J. Behav. Finance , vol. 6, no. 3, pp. 144GLYPH<21>160, 2005.\n- [9] J. Bi and X. Wang, ''Learning classiGLYPH<28>ers from dual annotation ambiguity via a minGLYPH<21>max framework,'' Neurocomputing , vol. 151, pp. 891GLYPH<21>904, Mar. 2015.\n- [10] S. Xie, W. Fan, and P. S. Yu, ''An iterative and re-weighting framework for rejection and uncertainty resolution in crowdsourcing,'' in Proc. SIAM Int. Conf. Data Mining , 2012, pp. 1107GLYPH<21>1118.\n- [11] Q. Le and T. Mikolov, ''Distributed representations of sentences and documents,'' in Proc. 31st Int. Conf. Mach. Learn. (ICML) , 2014, pp. 1188GLYPH<21>1196.\n- [12] F. Hogenboom, F. Frasincar, U. Kaymak, and F. De Jong, ''An overview of event extraction from text,'' in Proc. Workshop Detection, Represent., Exploitation Events Semantic Web (DeRiVE), 10th Int. Semantic Web Conf. (ISWC) , vol. 779, 2011, pp. 48GLYPH<21>57.\n- [13] R. Akita, A. Yoshihara, T. Matsubara, and K. Uehara, ''Deep learning for stock prediction using numerical and textual information,'' in Proc. IEEE/ACIS 15th Int. Conf. Comput. Inf. Sci. (ICIS) , Jun. 2016, pp. 1GLYPH<21>6.\n- [14] T. Nguyen, D. Phung, B. Adams, and S. Venkatesh, ''Event extraction using behaviors of sentiment signals and burst structure in social media,'' Knowl. Inf. Syst. , vol. 37, no. 2, pp. 279GLYPH<21>304, 2013.\n- [15] X. Ding, Y. Zhang, T. Liu, and J. Duan, ''Using structured events to predict stock price movement: An empirical investigation,'' in Proc. Conf. Empirical Methods Natural Lang. Process. (EMNLP) , 2014, pp. 1415GLYPH<21>1425.\n- [16] X. Ding, Y. Zhang, T. Liu, and J. Duan, ''Deep learning for event-driven stock prediction,'' in Proc. 24th Int. Joint Conf. Artif. Intell. (IJCAI) , 2015, pp. 2327GLYPH<21>2333.\n- [17] J. Bollen, H. Mao, and X. Zeng, ''Twitter mood predicts the stock market,'' J. Comput. Sci. , vol. 2, no. 1, pp. 1GLYPH<21>8, Mar. 2011.\n- [18] M. Makrehchi, S. Shah, and W. Liao, ''Stock prediction using eventbased sentiment analysis,'' in Proc. IEEE/WIC/ACM Int. Joint Conf. Web Intell. (WI) Intell. Agent Technol. (IAT) , vol. 1, Nov. 2013, pp. 337GLYPH<21>342.\n- [19] T. H. Nguyen and K. Shirai, ''Topic modeling based sentiment analysis on social media for stock market prediction,'' in Proc. 53rd Annu. Meeting Assoc. Comput. Linguistics (ACL) , 2015, pp. 1354GLYPH<21>1364.\n- [20] Q. Li, L. Jiang, P. Li, and H. Chen, ''Tensor-based learning for predicting stock movements,'' in Proc. 29th AAAI Conf. Artif. Intell. (AAAI) , 2015, pp. 1784GLYPH<21>1790.\n- [21] T. G. Dietterich, R. H. Lathrop, and T. Lozano-PØrez, ''Solving the multiple instance problem with axis-parallel rectangles,'' Artif. Intell. , vol. 89, nos. 1GLYPH<21>2, pp. 31GLYPH<21>71, 1997.\n- [22] J. Amores, ''Multiple instance classiGLYPH<28>cation: Review, taxonomy and comparative study,'' Artif. Intell. , vol. 201, pp. 81GLYPH<21>105, Aug. 2013.\n- [23] G. Liu, J. Wu, and Z.-H. Zhou, ''Key instance detection in multiinstance learning,'' in Proc. Asian Conf. Mach. Learn. , 2012, pp. 253GLYPH<21>268.\n- [24] D. Kotzias, M. Denil, N. de Freitas, and P. Smyth, ''From group to individual labels using deep features,'' in Proc. 21st ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD) , 2015, pp. 597GLYPH<21>606.\n- [25] J. Feng and Z.-H. Zhou, ''Deep MIML network,'' in Proc. 21st AAAI Conf. Artif. Intell. (AAAI) , 2017, pp. 1884GLYPH<21>1890.\n- [26] Y. Ning, S. Muthiah, H. Rangwala, and N. Ramakrishnan, ''Modeling precursors for event forecasting via nested multi-instance learning,'' in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD) , 2016, pp. 1095GLYPH<21>1104.\n- [27] G. E. Hinton and R. R. Salakhutdinov, ''Reducing the dimensionality of data with neural networks,'' Science , vol. 313, no. 5786, pp. 504GLYPH<21>507, 2006.\n- [28] X. Zhang et al. , ''IAD: Interaction-aware diffusion framework in social networks,'' IEEE Trans. Knowl. Data Eng. , to be published.\n- [29] W. X. Zhao, J. Jiang, H. Yan, and X. Li, ''Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid,'' in Proc. Conf. Empirical Methods Natural Lang. Process. , 2010, pp. 56GLYPH<21>65.\n\nIntroduces the core methodology: a multi-source multiple instance learning model that integrates news events, sentiments, and quantitative data to predict stock market movement, emphasizing the novel event extraction and representation techniques.",
    "original_text": "- [1] E. F. Fama, ''The behavior of stock-market prices,'' J. Bus. , vol. 38, no. 1, pp. 34GLYPH<21>105, 1965.\n- [2] S. R. Das and M. Y. Chen, ''Yahoo! for Amazon: Sentiment extraction from small talk on the Web,'' Manage. Sci. , vol. 53, no. 9, pp. 1375GLYPH<21>1388, 2007.\n- [3] J. Si, A. Mukherjee, B. Liu, Q. Li, H. Li, and X. Deng, ''Exploiting topic based twitter sentiment for stock prediction,'' in Proc. 51st Annu. Meeting Assoc. Comput. Linguistics (ACL) , 2013, pp. 24GLYPH<21>29.\n- [4] W. Y. Wang and Z. Hua, ''A semiparametric Gaussian copula regression model for predicting GLYPH<28>nancial risks from earnings calls,'' in Proc. 52nd Annu. Meeting Assoc. Comput. Linguistics (ACL) , Jun. 2014, pp. 1155GLYPH<21>1165.\n- [5] S. Kogan, D. Levin, B. R. Routledge, J. S. Sagi, and N. A. Smith, ''Predicting risk from GLYPH<28>nancial reports with regression,'' in Proc. Annu. Conf. North Amer. Chapter Assoc. Comput. Linguistics Hum. Lang. Technol. , 2009, pp. 272GLYPH<21>280.\n- [6] R. Luss and A. D'Aspremont, ''Predicting abnormal returns from news using text classiGLYPH<28>cation,'' Quant. Finance , vol. 15, no. 6, pp. 999GLYPH<21>1012, 2015.\n- [7] R. R. Prechter, The Wave Principle of Human Social Behavior and the New Science of Socionomics , vol. 1. Gainesville, GA, USA: New Classics Library, 1999.\n- [8] J. R. Nofsinger, ''Social mood and GLYPH<28>nancial economics,'' J. Behav. Finance , vol. 6, no. 3, pp. 144GLYPH<21>160, 2005.\n- [9] J. Bi and X. Wang, ''Learning classiGLYPH<28>ers from dual annotation ambiguity via a minGLYPH<21>max framework,'' Neurocomputing , vol. 151, pp. 891GLYPH<21>904, Mar. 2015.\n- [10] S. Xie, W. Fan, and P. S. Yu, ''An iterative and re-weighting framework for rejection and uncertainty resolution in crowdsourcing,'' in Proc. SIAM Int. Conf. Data Mining , 2012, pp. 1107GLYPH<21>1118.\n- [11] Q. Le and T. Mikolov, ''Distributed representations of sentences and documents,'' in Proc. 31st Int. Conf. Mach. Learn. (ICML) , 2014, pp. 1188GLYPH<21>1196.\n- [12] F. Hogenboom, F. Frasincar, U. Kaymak, and F. De Jong, ''An overview of event extraction from text,'' in Proc. Workshop Detection, Represent., Exploitation Events Semantic Web (DeRiVE), 10th Int. Semantic Web Conf. (ISWC) , vol. 779, 2011, pp. 48GLYPH<21>57.\n- [13] R. Akita, A. Yoshihara, T. Matsubara, and K. Uehara, ''Deep learning for stock prediction using numerical and textual information,'' in Proc. IEEE/ACIS 15th Int. Conf. Comput. Inf. Sci. (ICIS) , Jun. 2016, pp. 1GLYPH<21>6.\n- [14] T. Nguyen, D. Phung, B. Adams, and S. Venkatesh, ''Event extraction using behaviors of sentiment signals and burst structure in social media,'' Knowl. Inf. Syst. , vol. 37, no. 2, pp. 279GLYPH<21>304, 2013.\n- [15] X. Ding, Y. Zhang, T. Liu, and J. Duan, ''Using structured events to predict stock price movement: An empirical investigation,'' in Proc. Conf. Empirical Methods Natural Lang. Process. (EMNLP) , 2014, pp. 1415GLYPH<21>1425.\n- [16] X. Ding, Y. Zhang, T. Liu, and J. Duan, ''Deep learning for event-driven stock prediction,'' in Proc. 24th Int. Joint Conf. Artif. Intell. (IJCAI) , 2015, pp. 2327GLYPH<21>2333.\n- [17] J. Bollen, H. Mao, and X. Zeng, ''Twitter mood predicts the stock market,'' J. Comput. Sci. , vol. 2, no. 1, pp. 1GLYPH<21>8, Mar. 2011.\n- [18] M. Makrehchi, S. Shah, and W. Liao, ''Stock prediction using eventbased sentiment analysis,'' in Proc. IEEE/WIC/ACM Int. Joint Conf. Web Intell. (WI) Intell. Agent Technol. (IAT) , vol. 1, Nov. 2013, pp. 337GLYPH<21>342.\n- [19] T. H. Nguyen and K. Shirai, ''Topic modeling based sentiment analysis on social media for stock market prediction,'' in Proc. 53rd Annu. Meeting Assoc. Comput. Linguistics (ACL) , 2015, pp. 1354GLYPH<21>1364.\n- [20] Q. Li, L. Jiang, P. Li, and H. Chen, ''Tensor-based learning for predicting stock movements,'' in Proc. 29th AAAI Conf. Artif. Intell. (AAAI) , 2015, pp. 1784GLYPH<21>1790.\n- [21] T. G. Dietterich, R. H. Lathrop, and T. Lozano-PØrez, ''Solving the multiple instance problem with axis-parallel rectangles,'' Artif. Intell. , vol. 89, nos. 1GLYPH<21>2, pp. 31GLYPH<21>71, 1997.\n- [22] J. Amores, ''Multiple instance classiGLYPH<28>cation: Review, taxonomy and comparative study,'' Artif. Intell. , vol. 201, pp. 81GLYPH<21>105, Aug. 2013.\n- [23] G. Liu, J. Wu, and Z.-H. Zhou, ''Key instance detection in multiinstance learning,'' in Proc. Asian Conf. Mach. Learn. , 2012, pp. 253GLYPH<21>268.\n- [24] D. Kotzias, M. Denil, N. de Freitas, and P. Smyth, ''From group to individual labels using deep features,'' in Proc. 21st ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD) , 2015, pp. 597GLYPH<21>606.\n- [25] J. Feng and Z.-H. Zhou, ''Deep MIML network,'' in Proc. 21st AAAI Conf. Artif. Intell. (AAAI) , 2017, pp. 1884GLYPH<21>1890.\n- [26] Y. Ning, S. Muthiah, H. Rangwala, and N. Ramakrishnan, ''Modeling precursors for event forecasting via nested multi-instance learning,'' in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD) , 2016, pp. 1095GLYPH<21>1104.\n- [27] G. E. Hinton and R. R. Salakhutdinov, ''Reducing the dimensionality of data with neural networks,'' Science , vol. 313, no. 5786, pp. 504GLYPH<21>507, 2006.\n- [28] X. Zhang et al. , ''IAD: Interaction-aware diffusion framework in social networks,'' IEEE Trans. Knowl. Data Eng. , to be published.\n- [29] W. X. Zhao, J. Jiang, H. Yan, and X. Li, ''Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid,'' in Proc. Conf. Empirical Methods Natural Lang. Process. , 2010, pp. 56GLYPH<21>65.",
    "context": "Introduces the core methodology: a multi-source multiple instance learning model that integrates news events, sentiments, and quantitative data to predict stock market movement, emphasizing the novel event extraction and representation techniques.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      8
    ],
    "id": "b8924c479fd4638dd76359ff91183307ed7f1d5947938673a5c3298a89152683"
  },
  {
    "text": "- [30] L.-W. Ku and H.-H. Chen, ''Mining opinions from the Web: Beyond relevance retrieval,'' J. Amer. Soc. Inf. Sci. Technol. , vol. 58, no. 12, pp. 1838GLYPH<21>1850, 2007.\n- [31] A. Fader, S. Soderland, and O. Etzioni, ''Identifying relations for open information extraction,'' in Proc. Conf. Empirical Methods Natural Lang. Process. , 2011, pp. 1535GLYPH<21>1545.\nXI ZHANG (M'17) received the Ph.D. degree in computer science from Tsinghua University. He was a Visiting Scholar at The University of Illinois at Chicago. He is currently an Associate Professor with the Beijing University of Posts and Telecommunications and is also the Vice Director of the Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, China. His research interests include data mining and computer architecture.\nSIYU QU received the bachelor's degree in computer science from Xidian University in 2012. She is currently pursuing the master's degree with the Key Laboratory of Trustworthy Distributed Computing and Service, Beijing University of Posts and Telecommunications, Ministry of Education, China. Her research interests include data mining and machine learning.\nJIEYUN HUANG received the bachelor's degree in information security from the Beijing University of Posts and Telecommunications in 2017, where she is currently pursuing the master's degree with the Key Laboratory of Trustworthy Distributed Computing and Service. Her research interests are in data mining and machine learning.\nBINXING FANG received the Ph.D. degree from the Harbin Institute of Technology, China, in 1989. He was the Chief Scientist of the State Key Development Program of Basic Research of China. He is currently a member of the Chinese Academy of Engineering and is also a Professor with the School of Cyberspace Security, Beijing University of Posts and Telecommunications. His current research interests include big data and cybersecurity.\nPHILIP YU (F'93) received the Ph.D. degree in electrical engineering from Stanford University. He is currently a Distinguished Professor in computer science at The University of Illinois at Chicago and is also the Wexler Chair in information technology. His research interests include big data, data mining, data stream, database, and privacy. He is a fellow of ACM. He received the Research Contributions Award from the IEEE International Conference on Data Mining in 2003, the Technical Achievement Award from the IEEE Computer Society in 2013, and the ACM SIGKDD 2016 Innovation Award. He was the Editor-in-Chief of the IEEE TRANSACTIONSON KNOWLEDGEAND DATA ENGINEERING and the ACM Transactions on Knowledge Discovery from Data .\n\nProvides context on the authors and their affiliations, establishing the research team involved in the study.",
    "original_text": "- [30] L.-W. Ku and H.-H. Chen, ''Mining opinions from the Web: Beyond relevance retrieval,'' J. Amer. Soc. Inf. Sci. Technol. , vol. 58, no. 12, pp. 1838GLYPH<21>1850, 2007.\n- [31] A. Fader, S. Soderland, and O. Etzioni, ''Identifying relations for open information extraction,'' in Proc. Conf. Empirical Methods Natural Lang. Process. , 2011, pp. 1535GLYPH<21>1545.\nXI ZHANG (M'17) received the Ph.D. degree in computer science from Tsinghua University. He was a Visiting Scholar at The University of Illinois at Chicago. He is currently an Associate Professor with the Beijing University of Posts and Telecommunications and is also the Vice Director of the Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, China. His research interests include data mining and computer architecture.\nSIYU QU received the bachelor's degree in computer science from Xidian University in 2012. She is currently pursuing the master's degree with the Key Laboratory of Trustworthy Distributed Computing and Service, Beijing University of Posts and Telecommunications, Ministry of Education, China. Her research interests include data mining and machine learning.\nJIEYUN HUANG received the bachelor's degree in information security from the Beijing University of Posts and Telecommunications in 2017, where she is currently pursuing the master's degree with the Key Laboratory of Trustworthy Distributed Computing and Service. Her research interests are in data mining and machine learning.\nBINXING FANG received the Ph.D. degree from the Harbin Institute of Technology, China, in 1989. He was the Chief Scientist of the State Key Development Program of Basic Research of China. He is currently a member of the Chinese Academy of Engineering and is also a Professor with the School of Cyberspace Security, Beijing University of Posts and Telecommunications. His current research interests include big data and cybersecurity.\nPHILIP YU (F'93) received the Ph.D. degree in electrical engineering from Stanford University. He is currently a Distinguished Professor in computer science at The University of Illinois at Chicago and is also the Wexler Chair in information technology. His research interests include big data, data mining, data stream, database, and privacy. He is a fellow of ACM. He received the Research Contributions Award from the IEEE International Conference on Data Mining in 2003, the Technical Achievement Award from the IEEE Computer Society in 2013, and the ACM SIGKDD 2016 Innovation Award. He was the Editor-in-Chief of the IEEE TRANSACTIONSON KNOWLEDGEAND DATA ENGINEERING and the ACM Transactions on Knowledge Discovery from Data .",
    "context": "Provides context on the authors and their affiliations, establishing the research team involved in the study.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      8,
      9
    ],
    "id": "eef4cdb3ae7db3a8433e04cb2678050d9bc1d66ad48ee7d7d1387f909c3127ee"
  },
  {
    "text": "Per Runeson per.runeson@cs.lth.se Lund University Lund, Sweden\nEmma Söderberg emma.soderberg@cs.lth.se Lund University Lund, Sweden\nMartin Höst martin.host@mau.se Malmö University Malmö, Sweden\n\nLists the authors and their contact information.",
    "original_text": "Per Runeson per.runeson@cs.lth.se Lund University Lund, Sweden\nEmma Söderberg emma.soderberg@cs.lth.se Lund University Lund, Sweden\nMartin Höst martin.host@mau.se Malmö University Malmö, Sweden",
    "context": "Lists the authors and their contact information.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      1
    ],
    "id": "43b625cdc355da9e2b6bdab7a35dadfb97b8627dfe54afb97110db5b01b7de8d"
  },
  {
    "text": "Background. Open science aims to improve research accessibility, replicability, and consequently its quality. Empirical software engineering entails both data and artifacts, which may be shared more or less openly, to support transparency. However, the trade-offs involved in balancing the openness against integrity and secrecy concerns need methodological guidance. Aim. We aim to derive such advice, based on our own experiences from a research project, in the field of gaze-assisted code reviews - the Gander case. Method. We draw on literature about open data and artifacts in socio-technical research. Next, we describe our case project and derive a conceptual framework of steps in research data analysis and artifact development, using our data and artifacts as illustrating examples. Results. The conceptual framework contains 1) a categorization of humans involved as participants and their concerns, 2) four steps for data analysis, each resulting in corresponding data and meta-data, and 3) three steps of artifact distribution, matching different levels of openness. We derive a preliminary set of recommendations for open science practices for data and artifacts. Conclusion. The conceptual framework has proven useful in summarizing and discussing data and artifacts in the studied case project. We envision that the framework and recommendations will provide a foundation for further advancement of open science research practices in empirical, socio-technical software engineering.\n\nOutlines a conceptual framework for open science practices in data and artifact development, based on a case study in gaze-assisted code reviews.",
    "original_text": "Background. Open science aims to improve research accessibility, replicability, and consequently its quality. Empirical software engineering entails both data and artifacts, which may be shared more or less openly, to support transparency. However, the trade-offs involved in balancing the openness against integrity and secrecy concerns need methodological guidance. Aim. We aim to derive such advice, based on our own experiences from a research project, in the field of gaze-assisted code reviews - the Gander case. Method. We draw on literature about open data and artifacts in socio-technical research. Next, we describe our case project and derive a conceptual framework of steps in research data analysis and artifact development, using our data and artifacts as illustrating examples. Results. The conceptual framework contains 1) a categorization of humans involved as participants and their concerns, 2) four steps for data analysis, each resulting in corresponding data and meta-data, and 3) three steps of artifact distribution, matching different levels of openness. We derive a preliminary set of recommendations for open science practices for data and artifacts. Conclusion. The conceptual framework has proven useful in summarizing and discussing data and artifacts in the studied case project. We envision that the framework and recommendations will provide a foundation for further advancement of open science research practices in empirical, socio-technical software engineering.",
    "context": "Outlines a conceptual framework for open science practices in data and artifact development, based on a case study in gaze-assisted code reviews.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      1
    ],
    "id": "18d60b82daee2894a31097cbcee6654ca0973b4449e75e73305e314c9953b373"
  },
  {
    "text": "- Software and its engineering → Integrated and visual development environments ;\n\nDetails the tools and methodologies for software development.",
    "original_text": "- Software and its engineering → Integrated and visual development environments ;",
    "context": "Details the tools and methodologies for software development.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      1
    ],
    "id": "4a1cc0d3184c87ff15b84d60b5228bc49a37db3117f584e5707369bd2bc915a0"
  },
  {
    "text": "Open science, Open research, Open data, FAIR data, Open artifacts, Socio-technical software engineering\n\nDefines key principles and practices underpinning the document’s focus on reproducible research.",
    "original_text": "Open science, Open research, Open data, FAIR data, Open artifacts, Socio-technical software engineering",
    "context": "Defines key principles and practices underpinning the document’s focus on reproducible research.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      1
    ],
    "id": "5f452aa54b19fc9c92f1fc32916612cf02eabcc69215d57af43f66942d8ccaa7"
  },
  {
    "text": "Per Runeson, Emma Söderberg, and Martin Höst. 2024. A Conceptual Framework and Recommendations for Open Data and Artifacts in Empirical Software Engineering. In International Workshop on Methodological Issues with Empirical Studies in Software Engineering (WSESE '24 ), April 16, 2024, Lisbon, Portugal. ACM, New York, NY, USA, 8 pages. https: //doi.org/10.1145/3643664.3648206\nThis work licensed under Creative Commons Attribution International 4.0 License.\nWSESE '24 , April 16, 2024, Lisbon, Portugal © 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0567-0/24/04 https://doi.org/10.1145/3643664.3648206\nOpen science is brought forward as a means to increase research quality and efficiency, e.g., by making it easier to reproduce and replicate studies, and to democratize research. Open science or open scientific knowledge , as defined by UNESCO 1 , includes open scientific publications, open research data, open educational resources, open source software, and open hardware. In open science, the transparency is expected to increase reviewability, reproducibility and replicability, and thereby the quality of research. The open access and data is expected to contribute to democratization and to increase efficiency by avoiding double work. This development is, for example, manifested in the Empirical Software Engineering journal open science initiative [21] and the ACM artifact evaluation policy 2 . In empirical software engineering, experimental material have been made available as replication or laboratory packages [30] to some extent.\nHowever, open does not mean out of control. According to the European Horison 2020 Program Guidelines on FAIR Data 3 , data should be 'as open as possible and as closed as necessary' - open in order to foster the reusability and to accelerate research, but at the same time they should be as closed as needed to safeguard the privacy of the subjects as well as legitimate secrecy concerns for commercial entities. Recently sharpened legislation on personal data protection - e.g. the GDPR in Europe 4 - and ethical approval - e.g. changed interpretation of the ethical approval act in Sweden - clearly illustrate these conflicting interests.\nSoftware engineering (SE) is a research and practice domain, which is fundamentally socio-technical [31]. This implies that research and practice involves humans, and data generated by and about humans. As many SE research challenges come with scaling, it is ideally being conducted in real-world industrial contexts [2], involving real-world products and business. These characteristics lead to a series of ethical and legal concerns for SE research, which conflict with the aims of open science, at first sight. Not only has research to protect personal data and integrity , company data and secrets have also to be safeguarded. Further, since researchers and companies may have commercial interests in software tools, they may be reluctant to sharing their artifacts.\nMendez et al. discuss open science in SE as a means to improve repeatability, replicability, and reproducibility of research [19]. They\n1 https://doi.org/10.54677/UTCD9302\n3 FAIR - Findable, Accessible, Interoperable, Reusable Data https://ec.europa.eu/ research/participants/data/ref/h2020/grants_manual/hi/oa_pilot/h2020-hi-oa-data-mgt_en.pdf\n2 https://www.acm.org/publications/policies/artifact-review-and-badging-current\n4 The General Data Protection Regulation (EU 2016/679, GDPR) is a European Union regulation on information privacy in the European Union (EU) and the European Economic Area (EEA). https://en.wikipedia.org/wiki/General_Data_Protection_Regulation\nacknowledge the above mentioned concerns, namely personal data and company data protection, and 'the conflict between anonymity and confidentiality on one side, and openness on the other' [19, p.493]. However, they offer little practical guidance for research projects in this balancing act. Further, based on observation of the citation of method guidelines for empirical software engineering (e.g., [14, 24, 33]), authors seem to prefer guidelines tailored to SE, rather than using their general counterparts.\nWefirst discuss relevant literature on open data in socio-technical research in Section 2. Then, we introduce the case project in Section 3 and analyze open science aspects that appeared in the project, rendering our conceptual framework in Section 4. In Section 5 we map our project to the conceptual framework and present recommendations for SE researchers. Section 6 concludes the paper.\nWe therefore share our experiences and considerations on open science in socio-technical SE from a research project on gaze-driven assistance in code review as a case study - the Gander case. The project covers interview and survey data from industry practitioners, eye-tracking data from human subjects, and open source tools for experimentation. From our experiences, as well as literature on data sharing and software reuse, we derive a conceptual framework, which aims to structure analysis and communication about data and artifact openness and thereby guide future open science in SE. We also provide our own project data and artifacts as an illustrating example and derive preliminary recommendations.\n\nIntroduces a conceptual framework and recommendations for open data and artifacts in empirical software engineering, highlighting the balance between open science ideals and ethical/legal concerns regarding data privacy and commercial interests.",
    "original_text": "Per Runeson, Emma Söderberg, and Martin Höst. 2024. A Conceptual Framework and Recommendations for Open Data and Artifacts in Empirical Software Engineering. In International Workshop on Methodological Issues with Empirical Studies in Software Engineering (WSESE '24 ), April 16, 2024, Lisbon, Portugal. ACM, New York, NY, USA, 8 pages. https: //doi.org/10.1145/3643664.3648206\nThis work licensed under Creative Commons Attribution International 4.0 License.\nWSESE '24 , April 16, 2024, Lisbon, Portugal © 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0567-0/24/04 https://doi.org/10.1145/3643664.3648206\nOpen science is brought forward as a means to increase research quality and efficiency, e.g., by making it easier to reproduce and replicate studies, and to democratize research. Open science or open scientific knowledge , as defined by UNESCO 1 , includes open scientific publications, open research data, open educational resources, open source software, and open hardware. In open science, the transparency is expected to increase reviewability, reproducibility and replicability, and thereby the quality of research. The open access and data is expected to contribute to democratization and to increase efficiency by avoiding double work. This development is, for example, manifested in the Empirical Software Engineering journal open science initiative [21] and the ACM artifact evaluation policy 2 . In empirical software engineering, experimental material have been made available as replication or laboratory packages [30] to some extent.\nHowever, open does not mean out of control. According to the European Horison 2020 Program Guidelines on FAIR Data 3 , data should be 'as open as possible and as closed as necessary' - open in order to foster the reusability and to accelerate research, but at the same time they should be as closed as needed to safeguard the privacy of the subjects as well as legitimate secrecy concerns for commercial entities. Recently sharpened legislation on personal data protection - e.g. the GDPR in Europe 4 - and ethical approval - e.g. changed interpretation of the ethical approval act in Sweden - clearly illustrate these conflicting interests.\nSoftware engineering (SE) is a research and practice domain, which is fundamentally socio-technical [31]. This implies that research and practice involves humans, and data generated by and about humans. As many SE research challenges come with scaling, it is ideally being conducted in real-world industrial contexts [2], involving real-world products and business. These characteristics lead to a series of ethical and legal concerns for SE research, which conflict with the aims of open science, at first sight. Not only has research to protect personal data and integrity , company data and secrets have also to be safeguarded. Further, since researchers and companies may have commercial interests in software tools, they may be reluctant to sharing their artifacts.\nMendez et al. discuss open science in SE as a means to improve repeatability, replicability, and reproducibility of research [19]. They\n1 https://doi.org/10.54677/UTCD9302\n3 FAIR - Findable, Accessible, Interoperable, Reusable Data https://ec.europa.eu/ research/participants/data/ref/h2020/grants_manual/hi/oa_pilot/h2020-hi-oa-data-mgt_en.pdf\n2 https://www.acm.org/publications/policies/artifact-review-and-badging-current\n4 The General Data Protection Regulation (EU 2016/679, GDPR) is a European Union regulation on information privacy in the European Union (EU) and the European Economic Area (EEA). https://en.wikipedia.org/wiki/General_Data_Protection_Regulation\nacknowledge the above mentioned concerns, namely personal data and company data protection, and 'the conflict between anonymity and confidentiality on one side, and openness on the other' [19, p.493]. However, they offer little practical guidance for research projects in this balancing act. Further, based on observation of the citation of method guidelines for empirical software engineering (e.g., [14, 24, 33]), authors seem to prefer guidelines tailored to SE, rather than using their general counterparts.\nWefirst discuss relevant literature on open data in socio-technical research in Section 2. Then, we introduce the case project in Section 3 and analyze open science aspects that appeared in the project, rendering our conceptual framework in Section 4. In Section 5 we map our project to the conceptual framework and present recommendations for SE researchers. Section 6 concludes the paper.\nWe therefore share our experiences and considerations on open science in socio-technical SE from a research project on gaze-driven assistance in code review as a case study - the Gander case. The project covers interview and survey data from industry practitioners, eye-tracking data from human subjects, and open source tools for experimentation. From our experiences, as well as literature on data sharing and software reuse, we derive a conceptual framework, which aims to structure analysis and communication about data and artifact openness and thereby guide future open science in SE. We also provide our own project data and artifacts as an illustrating example and derive preliminary recommendations.",
    "context": "Introduces a conceptual framework and recommendations for open data and artifacts in empirical software engineering, highlighting the balance between open science ideals and ethical/legal concerns regarding data privacy and commercial interests.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      1,
      2
    ],
    "id": "a7349d306ec369a5ff86cad4a20ef2d7ee6fe2318a481857c9a8a28b30c532df"
  },
  {
    "text": "Empirical data collection techniques in software engineering field studies were categorized by Lethbridge et al. into three degrees: (1) direct involvement of software engineers - human-enacted inquisitive and observational techniques (e.g. interviews), (2) indirect involvement of software engineers - technically enabled observations (e.g. eye-tracking), and (3) study of work artifacts only (e.g. code) [17]. All the three categories have similar concerns with respect to both the personal and company data protection needs. For example, eye-tracking and the code may reveal inefficient work practices by the individual, and interviews and commercial code may reveal company secrets. Generally, companies involved in empirical studies are concerned with protection of their data, preventing researchers from opening the data to the research community.\nA special branch of empirical software engineering is the mining of software repositories (MSR) studies, where data is collected from open development repositories. González-Barahona and Robles proposed a method for assessing the reproducibility of MSR studies [10], which they validated a decade later [11]. Since the raw data in MSR studies come from open sources, their method\nA special type of SE contexts is open source software (OSS) development. Then, software artifacts are open by default, as well as personal data in the form of contributors' names or pseudonyms and contact information. As a consequence of the easy access, SE research on OSS projects is popular, although only some aspects are comparable for corporate SE [23]. Also, there is a risk of revealing personal identities and data if interviewees are selected via open source software, for example, from an OSS community [20].\nprimarily focuses on the transparency of analysis methods and procedures.\nSince the data collected in SE research varies across these degrees and factors, a conceptual framework and recommendations for open data must take them into account.\nIn the research field of Open data ecosystems [25], the concern about sharing data between commercial actors is addressed, as well as open data from governmental and other public sources. Based on a survey of the literature, Enders et al. explored factors to take into account when deciding the degree of openness for data, i.e. selective revealing of data [6]. These factors are (1) Coreness (closeness to the core business), (2) Currentness (how recent is the data), (3) Extent (volume of data), (4) Granularity (level of detail), (5) Interoperability (e.g, standardized formats), and (6) Quality (fit for purpose).\n\nHighlights the diverse data protection concerns across different empirical software engineering research methods (direct, indirect, and artifact-based) and emphasizes the need for a framework considering factors like data coreness, currentness, and quality in open data ecosystems.",
    "original_text": "Empirical data collection techniques in software engineering field studies were categorized by Lethbridge et al. into three degrees: (1) direct involvement of software engineers - human-enacted inquisitive and observational techniques (e.g. interviews), (2) indirect involvement of software engineers - technically enabled observations (e.g. eye-tracking), and (3) study of work artifacts only (e.g. code) [17]. All the three categories have similar concerns with respect to both the personal and company data protection needs. For example, eye-tracking and the code may reveal inefficient work practices by the individual, and interviews and commercial code may reveal company secrets. Generally, companies involved in empirical studies are concerned with protection of their data, preventing researchers from opening the data to the research community.\nA special branch of empirical software engineering is the mining of software repositories (MSR) studies, where data is collected from open development repositories. González-Barahona and Robles proposed a method for assessing the reproducibility of MSR studies [10], which they validated a decade later [11]. Since the raw data in MSR studies come from open sources, their method\nA special type of SE contexts is open source software (OSS) development. Then, software artifacts are open by default, as well as personal data in the form of contributors' names or pseudonyms and contact information. As a consequence of the easy access, SE research on OSS projects is popular, although only some aspects are comparable for corporate SE [23]. Also, there is a risk of revealing personal identities and data if interviewees are selected via open source software, for example, from an OSS community [20].\nprimarily focuses on the transparency of analysis methods and procedures.\nSince the data collected in SE research varies across these degrees and factors, a conceptual framework and recommendations for open data must take them into account.\nIn the research field of Open data ecosystems [25], the concern about sharing data between commercial actors is addressed, as well as open data from governmental and other public sources. Based on a survey of the literature, Enders et al. explored factors to take into account when deciding the degree of openness for data, i.e. selective revealing of data [6]. These factors are (1) Coreness (closeness to the core business), (2) Currentness (how recent is the data), (3) Extent (volume of data), (4) Granularity (level of detail), (5) Interoperability (e.g, standardized formats), and (6) Quality (fit for purpose).",
    "context": "Highlights the diverse data protection concerns across different empirical software engineering research methods (direct, indirect, and artifact-based) and emphasizes the need for a framework considering factors like data coreness, currentness, and quality in open data ecosystems.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      2
    ],
    "id": "d9f30ae387268f0d4de08e612fb6b4aaff46b1fd9cde63e6723267e5f9eafd58"
  },
  {
    "text": "Socio-technical research in SE embraces both quantitative and qualitative data. 'Quantitative data is more exact, while qualitative data is 'richer' in what it may express.' [24, p.15]. Quantitative data are easier to summarize, e.g. through means and dispersion measures, or statistical distributions. They may also be easier anonymized, since the identities are mostly connected to the meta data and context descriptions, rather than the data itself. In some cases, removing or changing the scale of data may help addressing secrecy issues. Also quantitative data is more often encoded and does not reveal opinions, work tasks, etc. that make it possible to identify the data source.\nFor example, Chauvette et al. [3] are critical to open data for epistemological, methodological and ethical reasons. Epistemologically , qualitative data are tightly linked with the context, so changing context make them meaningless, they claim. Methodologically , the reflexivity in the qualitative analysis requires the researcher to be part of the data collection to be close enough to the studied phenomenon. Ethically , issues related to confidentiality and anonymity prevent open data.\nThere is, as far as we have found, no discussion on open qualitative research data in the literature specifically for SE, beyond open science policies for journals and conferences. However, the topic is discussed in social science and psychology, including a multitude of perspectives on the feasibility of open qualitative research data.\nOther researchers, e.g. Field et al. [7], are more neutral regarding open, qualitative data. They acknowledge the above mentioned problems, but weigh them against potential benefits. Among this, participants may want to share their data for the greater good, and improved research efficiency may speak for open data. They continue nuancing the issue, by proposing to share codebooks with 'a list of codes with associated definitions, examples and descriptions' that may add to the transparency and replicability of research.\nFinally, Joyce et al. [13], are proponents for openness and argue that 'there are several notable benefits to sharing qualitative research data'. They claim that most concerns can be addressed by good policies and practices of data repositories. This position is supported by DuBois et al. [5] who report that data sharing has been an established practice in the research field of Conversation Analysis for over three decades. They also offer practical guidelines for sharing qualitative data [4].\nThere are conceptual differences, related to open data, between qualitative and quantitative data. However, there are also differences between research domains in their principles and practices for open data. SE consequently has to find a position as a community.\n\nHighlights the contrasting approaches to open data between qualitative and quantitative research, particularly in socio-technical research, and explores existing debates and proposed solutions within the field.",
    "original_text": "Socio-technical research in SE embraces both quantitative and qualitative data. 'Quantitative data is more exact, while qualitative data is 'richer' in what it may express.' [24, p.15]. Quantitative data are easier to summarize, e.g. through means and dispersion measures, or statistical distributions. They may also be easier anonymized, since the identities are mostly connected to the meta data and context descriptions, rather than the data itself. In some cases, removing or changing the scale of data may help addressing secrecy issues. Also quantitative data is more often encoded and does not reveal opinions, work tasks, etc. that make it possible to identify the data source.\nFor example, Chauvette et al. [3] are critical to open data for epistemological, methodological and ethical reasons. Epistemologically , qualitative data are tightly linked with the context, so changing context make them meaningless, they claim. Methodologically , the reflexivity in the qualitative analysis requires the researcher to be part of the data collection to be close enough to the studied phenomenon. Ethically , issues related to confidentiality and anonymity prevent open data.\nThere is, as far as we have found, no discussion on open qualitative research data in the literature specifically for SE, beyond open science policies for journals and conferences. However, the topic is discussed in social science and psychology, including a multitude of perspectives on the feasibility of open qualitative research data.\nOther researchers, e.g. Field et al. [7], are more neutral regarding open, qualitative data. They acknowledge the above mentioned problems, but weigh them against potential benefits. Among this, participants may want to share their data for the greater good, and improved research efficiency may speak for open data. They continue nuancing the issue, by proposing to share codebooks with 'a list of codes with associated definitions, examples and descriptions' that may add to the transparency and replicability of research.\nFinally, Joyce et al. [13], are proponents for openness and argue that 'there are several notable benefits to sharing qualitative research data'. They claim that most concerns can be addressed by good policies and practices of data repositories. This position is supported by DuBois et al. [5] who report that data sharing has been an established practice in the research field of Conversation Analysis for over three decades. They also offer practical guidelines for sharing qualitative data [4].\nThere are conceptual differences, related to open data, between qualitative and quantitative data. However, there are also differences between research domains in their principles and practices for open data. SE consequently has to find a position as a community.",
    "context": "Highlights the contrasting approaches to open data between qualitative and quantitative research, particularly in socio-technical research, and explores existing debates and proposed solutions within the field.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      2,
      3
    ],
    "id": "84d4d0a0c7dd3ca4e7f45795b8196e3d2026a44ec337dc23a6f60c4027d18137"
  },
  {
    "text": "Wehypothesize that the ethical and legal conditions for open data in socio-technical SE research differs between data collection methods.\nInterview data are also first degree data, but primarily qualitative . Thus, researchers are in direct contact with respondents and can get their consent. However, the richness and strong connection to context in qualitative data makes it harder to anonymize data to enable openness. Similar conditions hold for focus group data .\nSurvey data are first degree data, primarily quantitative or semiquantitative (e.g. Likert scales). This implies that respondents may give consent to openness and that the opportunities for anonymization are good, particularly for large populations and samples.\nHuman-enacted observational data are also first degree data, mostly constituted of qualitative data. In contrast to interviews and focus group data, observed participants are (by design) less aware of the researcher's presence. Consequently, the participants have less control over the data they provide. Such data must be more actively cleared from sensitive and irrelevant information.\nFinally, archival data or work artifacts are third degree data. Since they are derived for other primary purposes, the openness must be considered in relation to the original contributors. Company internal artifacts are rarely possible to open, while OSS artifacts are open by definition. However, open data may still be personal, e.g. defect reports and commits, and ethical and legal concerns in relation to individuals and their data must be properly handled.\nTechnology-based observational data are second degree data and mostly of quantitative character. Thus, the data may be more easily separated from the context compared to qualitative data. However, the interpretation of the quantitative data is highly related to the context, and thus the value is reduced by anonymization. A special case is instrumented data for learning , where the data is collected by technical instruments, used to train machine learning algorithms. There are several ethical and legal concerns raised regarding these technologies, e.g. in relation to Microsoft's CoPilot 5 .\nDepending on the legal and ethical conditions for each data collection method, these must be reflected in how data is handled with respect to openness.\n\nHighlights the differing ethical and legal challenges of open data collection across various methods (interviews, surveys, observations, archival data, and technology-based data), emphasizing the complexities of anonymization and consent.",
    "original_text": "Wehypothesize that the ethical and legal conditions for open data in socio-technical SE research differs between data collection methods.\nInterview data are also first degree data, but primarily qualitative . Thus, researchers are in direct contact with respondents and can get their consent. However, the richness and strong connection to context in qualitative data makes it harder to anonymize data to enable openness. Similar conditions hold for focus group data .\nSurvey data are first degree data, primarily quantitative or semiquantitative (e.g. Likert scales). This implies that respondents may give consent to openness and that the opportunities for anonymization are good, particularly for large populations and samples.\nHuman-enacted observational data are also first degree data, mostly constituted of qualitative data. In contrast to interviews and focus group data, observed participants are (by design) less aware of the researcher's presence. Consequently, the participants have less control over the data they provide. Such data must be more actively cleared from sensitive and irrelevant information.\nFinally, archival data or work artifacts are third degree data. Since they are derived for other primary purposes, the openness must be considered in relation to the original contributors. Company internal artifacts are rarely possible to open, while OSS artifacts are open by definition. However, open data may still be personal, e.g. defect reports and commits, and ethical and legal concerns in relation to individuals and their data must be properly handled.\nTechnology-based observational data are second degree data and mostly of quantitative character. Thus, the data may be more easily separated from the context compared to qualitative data. However, the interpretation of the quantitative data is highly related to the context, and thus the value is reduced by anonymization. A special case is instrumented data for learning , where the data is collected by technical instruments, used to train machine learning algorithms. There are several ethical and legal concerns raised regarding these technologies, e.g. in relation to Microsoft's CoPilot 5 .\nDepending on the legal and ethical conditions for each data collection method, these must be reflected in how data is handled with respect to openness.",
    "context": "Highlights the differing ethical and legal challenges of open data collection across various methods (interviews, surveys, observations, archival data, and technology-based data), emphasizing the complexities of anonymization and consent.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      3
    ],
    "id": "2a0175332c45a13cfa875336eb4996ed01bc6d94292616ac512e4b69e41b2427"
  },
  {
    "text": "Artifact evaluation has emerged as an open research practice in computer science during the last decade 6 . It aims towards improved quality and transparency, by supporting reproducible research. ACM has established a policy with three separate badge categories for papers published with ACM, related to artifact review, namely Artifacts Evaluated (Functional, Reusable), Artifacts Available, and Results Validated (Reproduced, Replicated) 7 .\n5 https://www.sdxcentral.com/articles/news/github-copilot-raises-ownership-\n6 https://artifact-eval.org\nethical-concerns/2022/07/\n7 https://www.acm.org/publications/policies/artifact-review-and-badging-current\nFor software reuse in a broader sense, it is acknowledged that software reuse without any strategy or support is difficult and software must be prepared to make it reusable. For example, Belfadel et al. [1] propose an Enterprise Architecture Capability Framework, with the aim to increase reuse of software by upgrading technical components to match end-user's requirements. However, software can be made available for future reuse in several ways. Publishing software as Open Source is another way of using software available to a broader audience and therefore used in more systems.\nWhile open data includes aspects related to human subjects from which empirical data is collected, artifact openness is primarily an issue on the researcher side. Researchers have to decide what degree of openness they apply with respect to their intellectual property and with respect to the effort it takes to make the artifacts openly available, support their usage, etc.\n\nContextualizes the need for artifact evaluation and open research practices within computer science, highlighting the ACM's badge system and the researcher's role in ensuring artifact openness.",
    "original_text": "Artifact evaluation has emerged as an open research practice in computer science during the last decade 6 . It aims towards improved quality and transparency, by supporting reproducible research. ACM has established a policy with three separate badge categories for papers published with ACM, related to artifact review, namely Artifacts Evaluated (Functional, Reusable), Artifacts Available, and Results Validated (Reproduced, Replicated) 7 .\n5 https://www.sdxcentral.com/articles/news/github-copilot-raises-ownership-\n6 https://artifact-eval.org\nethical-concerns/2022/07/\n7 https://www.acm.org/publications/policies/artifact-review-and-badging-current\nFor software reuse in a broader sense, it is acknowledged that software reuse without any strategy or support is difficult and software must be prepared to make it reusable. For example, Belfadel et al. [1] propose an Enterprise Architecture Capability Framework, with the aim to increase reuse of software by upgrading technical components to match end-user's requirements. However, software can be made available for future reuse in several ways. Publishing software as Open Source is another way of using software available to a broader audience and therefore used in more systems.\nWhile open data includes aspects related to human subjects from which empirical data is collected, artifact openness is primarily an issue on the researcher side. Researchers have to decide what degree of openness they apply with respect to their intellectual property and with respect to the effort it takes to make the artifacts openly available, support their usage, etc.",
    "context": "Contextualizes the need for artifact evaluation and open research practices within computer science, highlighting the ACM's badge system and the researcher's role in ensuring artifact openness.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      3
    ],
    "id": "566f556221195b672e1d4c529739f9463e6f05d03ecbef0524b387f0ba5e7a83"
  },
  {
    "text": "We present the Gander research project as a case to identify a multitude of research data and related open data concerns. The objective of the Gander project was to gain an increased understanding of code review in practice and to use this understanding to inform the design of new code review tooling. The project had two main components; 1) an empirical component with focus on problem conceptualization, providing input to the tool design - conducted as a mixed-method study with practitioners in industry [29, 32], and 2) a development component focused on development of an experimental code review platform incorporating eye-tracking [9, 28]. A key aspect in the project was to explore how gaze data from eyetracking can be used to trigger assistance in code review tools. The Gander project was a continuation on a line of research started by the second author in two earlier studies connecting to code review, carried out in an industrial setting [26, 27].\nOne aim with the platform development in the Gander project was to build an experimental code review setup that would allow for more realistic code review experiments closer to practitioners [16], i.e., outside of the lab environment and with realistic data. With this goal in mind, the platform strives to provide a similar look-and-feel as well-used code review environments like GitLab or GitHub (with regard to the textual diff view) and it should be easy to populate the platform with realistic samples from open-source. The latter resulted in a connection to GitHub to facilitate the experimental setup process. Finally, the platform should be easy to run outside the lab, which means it has been developed using portable eye-trackers used with a laptop to increase the mobility of the setup. Figure 1\nThe empirical code review study [29, 32] in the Gander project included a series of 12 semi-structured interviews with practitioners at two companies, about the experience of code review (below referred to as Data and artifact set 1 ). The interviews were recorded after informed consent and transcribed for thematic analysis. They were followed by a survey, based on the interview results, that rendered replies from 78 practitioners ( Data and artifact set 2 ). The survey results were gathered, coded, and summarized for reporting. Neither the qualitative nor the quantitative data gathered in this study have been shared as supplementary material to any publication, but the protocols were shared during the review process.\nFigure 1: A screenshot of the Gander platform showing a textual diff view in replay mode, where the interaction of a session is replayed from logged interaction and eye-tracking data.\nshows a screenshot of the Gander platform where the textual diff is populated with data from the FlappyBird project on GitHub.\nAs a proof-of-concept, the Gander platform was used to develop a simple gaze assistant that triggers visualisation of use-declaration relationships in the code based on gaze fixation point on, for instance, variable names. This assistant was tested in a user study with eight participants. During the study, participants were given a number of tasks to solve on the Gander platform with the gaze assistant enabled and then they were interviewed about their experience ( Data and artifact sets 3a and b ). The study included both quantitative data gathering, in the form of interaction logs and eye-tracking data ( Data and artifacts sets 3c and d ), and qualitative data in the form of interviews recorded after informed consent. For this study, the quantitative data has been shared as a data set, both serving as supplementary material to the publication about the platform [28] and as a test set for how to use the replay function in the platform which has been released as open-source [9].\nThe connection to eye-tracking and processing of gaze data is central to the design of the platform. The architecture is structured around the needs of processing eye-tracking data, for replay of sessions with participants and for exploration of real-time gazebased assistance during a code review session. Gaze data is analyzed in real-time to detect fixation points which can be connected to programming language elements which may correspond to areas of interest. This data processing can be used to trigger assistance in response to certain gaze behavior in relation to the content of the code being reviewed.\nIn releasing the platform as open-source, licenses of any system used in the project, e.g., the JastAdd 8 project and the ExtendJ 9 project, had to be considered. During this review, it became clear that one of the used projects (for gaze data analysis) was available online but did not have a licence. However, after reaching out to the author of that project a licence file was added (the MIT license) and the use of the project remained unchanged. After considering the interaction of licenses in used projects and after discussion within the contributor team, a BSD license was selected for the open-source project.\n8 https://jastadd.org\n9 https://extendj.org\nThe project is conducted at Lund University, Sweden, which like many higher research institutes and funding agencies has an increasing focus on open science. Open science is one of the prioritised issues in the university's Research strategy 2023-26, although there is not yet any mandatory prescriptions. The Swedish Research Council, which is one of the funding agencies of this work, requires a data management plan (DMP) to be created and maintained for all projects funded 2019 or later, while the other funding agencies for the research do not yet have any requirements on open science. A data management plan was created in Lund university's DMP system, but it is not public. Creating a DMP is a first step towards fostering open data sciences, although there are no specified requirements on openess, neither from the university nor the funding agencies.\n\nDetails the Gander project’s development of a code review platform incorporating eye-tracking for gaze-based assistance, including empirical research through interviews and a user study, and its connection to open-source initiatives and licensing considerations.",
    "original_text": "We present the Gander research project as a case to identify a multitude of research data and related open data concerns. The objective of the Gander project was to gain an increased understanding of code review in practice and to use this understanding to inform the design of new code review tooling. The project had two main components; 1) an empirical component with focus on problem conceptualization, providing input to the tool design - conducted as a mixed-method study with practitioners in industry [29, 32], and 2) a development component focused on development of an experimental code review platform incorporating eye-tracking [9, 28]. A key aspect in the project was to explore how gaze data from eyetracking can be used to trigger assistance in code review tools. The Gander project was a continuation on a line of research started by the second author in two earlier studies connecting to code review, carried out in an industrial setting [26, 27].\nOne aim with the platform development in the Gander project was to build an experimental code review setup that would allow for more realistic code review experiments closer to practitioners [16], i.e., outside of the lab environment and with realistic data. With this goal in mind, the platform strives to provide a similar look-and-feel as well-used code review environments like GitLab or GitHub (with regard to the textual diff view) and it should be easy to populate the platform with realistic samples from open-source. The latter resulted in a connection to GitHub to facilitate the experimental setup process. Finally, the platform should be easy to run outside the lab, which means it has been developed using portable eye-trackers used with a laptop to increase the mobility of the setup. Figure 1\nThe empirical code review study [29, 32] in the Gander project included a series of 12 semi-structured interviews with practitioners at two companies, about the experience of code review (below referred to as Data and artifact set 1 ). The interviews were recorded after informed consent and transcribed for thematic analysis. They were followed by a survey, based on the interview results, that rendered replies from 78 practitioners ( Data and artifact set 2 ). The survey results were gathered, coded, and summarized for reporting. Neither the qualitative nor the quantitative data gathered in this study have been shared as supplementary material to any publication, but the protocols were shared during the review process.\nFigure 1: A screenshot of the Gander platform showing a textual diff view in replay mode, where the interaction of a session is replayed from logged interaction and eye-tracking data.\nshows a screenshot of the Gander platform where the textual diff is populated with data from the FlappyBird project on GitHub.\nAs a proof-of-concept, the Gander platform was used to develop a simple gaze assistant that triggers visualisation of use-declaration relationships in the code based on gaze fixation point on, for instance, variable names. This assistant was tested in a user study with eight participants. During the study, participants were given a number of tasks to solve on the Gander platform with the gaze assistant enabled and then they were interviewed about their experience ( Data and artifact sets 3a and b ). The study included both quantitative data gathering, in the form of interaction logs and eye-tracking data ( Data and artifacts sets 3c and d ), and qualitative data in the form of interviews recorded after informed consent. For this study, the quantitative data has been shared as a data set, both serving as supplementary material to the publication about the platform [28] and as a test set for how to use the replay function in the platform which has been released as open-source [9].\nThe connection to eye-tracking and processing of gaze data is central to the design of the platform. The architecture is structured around the needs of processing eye-tracking data, for replay of sessions with participants and for exploration of real-time gazebased assistance during a code review session. Gaze data is analyzed in real-time to detect fixation points which can be connected to programming language elements which may correspond to areas of interest. This data processing can be used to trigger assistance in response to certain gaze behavior in relation to the content of the code being reviewed.\nIn releasing the platform as open-source, licenses of any system used in the project, e.g., the JastAdd 8 project and the ExtendJ 9 project, had to be considered. During this review, it became clear that one of the used projects (for gaze data analysis) was available online but did not have a licence. However, after reaching out to the author of that project a licence file was added (the MIT license) and the use of the project remained unchanged. After considering the interaction of licenses in used projects and after discussion within the contributor team, a BSD license was selected for the open-source project.\n8 https://jastadd.org\n9 https://extendj.org\nThe project is conducted at Lund University, Sweden, which like many higher research institutes and funding agencies has an increasing focus on open science. Open science is one of the prioritised issues in the university's Research strategy 2023-26, although there is not yet any mandatory prescriptions. The Swedish Research Council, which is one of the funding agencies of this work, requires a data management plan (DMP) to be created and maintained for all projects funded 2019 or later, while the other funding agencies for the research do not yet have any requirements on open science. A data management plan was created in Lund university's DMP system, but it is not public. Creating a DMP is a first step towards fostering open data sciences, although there are no specified requirements on openess, neither from the university nor the funding agencies.",
    "context": "Details the Gander project’s development of a code review platform incorporating eye-tracking for gaze-based assistance, including empirical research through interviews and a user study, and its connection to open-source initiatives and licensing considerations.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      3,
      4
    ],
    "id": "1782f51bf8d5829dae2db01fdacaa942735963b7b91c953611d8bf2d247ba1db"
  },
  {
    "text": "To guide the analysis and discussion on open science in sociotechnical software engineering, we define a conceptual framework of the execution of a research study, and the analysis and generalization of research artifacts. The framework emerged during our post-mortem analysis of the research projects, its actors, data, and artifacts, by iterating over the following principal steps.\n- (1) Identify participants and other stakeholders\n- (3) Identify artifacts developed for and in the project\n- (2) Identify data collected, and analyze types of data and corresponding analysis processes\nWe identified specific instances of the Gander projects and then abstracted the framework elements towards more general concepts.\nThe framework has three main concepts: participants, data and artifacts, as shown in Figure 2. The participants have different roles and relations to the research endeavour, while the data and artifacts are refined and evolved in separate pipelines.\n\nDefines a conceptual framework for analyzing open science in sociotechnical software engineering, outlining key elements: participants, data, and artifacts.",
    "original_text": "To guide the analysis and discussion on open science in sociotechnical software engineering, we define a conceptual framework of the execution of a research study, and the analysis and generalization of research artifacts. The framework emerged during our post-mortem analysis of the research projects, its actors, data, and artifacts, by iterating over the following principal steps.\n- (1) Identify participants and other stakeholders\n- (3) Identify artifacts developed for and in the project\n- (2) Identify data collected, and analyze types of data and corresponding analysis processes\nWe identified specific instances of the Gander projects and then abstracted the framework elements towards more general concepts.\nThe framework has three main concepts: participants, data and artifacts, as shown in Figure 2. The participants have different roles and relations to the research endeavour, while the data and artifacts are refined and evolved in separate pipelines.",
    "context": "Defines a conceptual framework for analyzing open science in sociotechnical software engineering, outlining key elements: participants, data, and artifacts.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      4
    ],
    "id": "e45de937396e2485140d34ced084312e75d0ede0fafb2d0fc2cf5d1abff07577"
  },
  {
    "text": "Weidentify three typical categories of participants in socio-technical software engineering research, namely (1) employees or other stakeholders of software development organizations (marked with round cap in Figure 2), (2) students or other beneficiaries of the university involved (marked with square cap), and (3) independent participants (without cap). The categorization is conducted based on legal and ethical concerns in the relation between researchers and participants, and consequently the openness of data and artifacts emerging from the research. For example, employees or students may feel pressure to participate in a study, even if participation should be fully voluntary. Further, employees may participate under certain secrecy conditions bound by their employment contract.\n\nDefines participant categories based on their relationship to the research and highlights ethical considerations regarding voluntary participation and data openness.",
    "original_text": "Weidentify three typical categories of participants in socio-technical software engineering research, namely (1) employees or other stakeholders of software development organizations (marked with round cap in Figure 2), (2) students or other beneficiaries of the university involved (marked with square cap), and (3) independent participants (without cap). The categorization is conducted based on legal and ethical concerns in the relation between researchers and participants, and consequently the openness of data and artifacts emerging from the research. For example, employees or students may feel pressure to participate in a study, even if participation should be fully voluntary. Further, employees may participate under certain secrecy conditions bound by their employment contract.",
    "context": "Defines participant categories based on their relationship to the research and highlights ethical considerations regarding voluntary participation and data openness.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      4
    ],
    "id": "9b11cfbcb656a5a45ac5870a26301315dde05db2a397690abbdb12c289f21edb"
  },
  {
    "text": "Regarding our research project data we observe that the analysis process of research data at a general level resembles a data pipeline . In a typical research project the researcher starts with detailed raw data, analyse that data in a sequence of steps and ends up with a set of findings. The data from the last step, i.e., the findings, typically consist of data that is 'open' in the sense of being published, while the results from the steps prior to that are increasingly more difficult to make publicly available, due to privacy and secrecy concerns.\nFigure 2: Graphical representation of the conceptual framework with steps of data analysis and material generalization. Data sources to the left, the data pipeline top right and artifact pipeline bottom right.\nFor example, in a qualitative research project with interview data, it is common to publish the general findings of an interview, but the audio recordings from the interview are rarely openly available nor are the full transcripts. In terms of openness factors, discussed by Enders et al. [6] (see Section 2.1), audio recordings and transcripts are of finer granularity than the abstracted findings, and thus harder to make open than the course grained findings. Consequently, not only the raw data, but the full analysis pipeline contains relevant concepts for open science.\n- In a quantitative study this would mean statistical analysis, e.g., building prediction models, hypothesis testing, etc. In a study this would involve defining a set of\nThe conceptual data pipeline, illustrated in the top right part of Figure 2, is divided into four steps which are inspired by Majeed and Hwang [18] from the field of data science, and illustrated by typical study examples below.\n- (1) Data cleaning:\n2. (2)\nIn a quantitative study this involves transforming the data into a readable form for statistical tools, which may, for example, involve coding of the data based on a pre-defined scheme. It may also include anonymizing the data. In a qualitative study, this typically includes transcribing the data, as well as anonymization.\nIn a qualitative study this would include getting a first understanding of the data and probably a first idea of procedures for coding.\n- Data exploration and visualization: In a quantitative study this includes investigating descriptive statistics and visualising data. This is also a natural step for identifying outliers.\n- (3) Model building and analysis:\n- qualitative codes, coding, obtaining findings, and potentially theory building, in an iterative manner.\n- (4) Findings presentation: Findings are commonly presented in journal/conference publications, technical reports, or similarly, including data and analyses to support the findings.\nGenerally, outcomes from the later stages of the data pipeline are less sensitive to share openly, both with respect to participants' privacy and potential company secrets.\n\nDescribes a data pipeline framework for research, highlighting the decreasing accessibility of data stages beyond the final findings due to privacy and confidentiality concerns.",
    "original_text": "Regarding our research project data we observe that the analysis process of research data at a general level resembles a data pipeline . In a typical research project the researcher starts with detailed raw data, analyse that data in a sequence of steps and ends up with a set of findings. The data from the last step, i.e., the findings, typically consist of data that is 'open' in the sense of being published, while the results from the steps prior to that are increasingly more difficult to make publicly available, due to privacy and secrecy concerns.\nFigure 2: Graphical representation of the conceptual framework with steps of data analysis and material generalization. Data sources to the left, the data pipeline top right and artifact pipeline bottom right.\nFor example, in a qualitative research project with interview data, it is common to publish the general findings of an interview, but the audio recordings from the interview are rarely openly available nor are the full transcripts. In terms of openness factors, discussed by Enders et al. [6] (see Section 2.1), audio recordings and transcripts are of finer granularity than the abstracted findings, and thus harder to make open than the course grained findings. Consequently, not only the raw data, but the full analysis pipeline contains relevant concepts for open science.\n- In a quantitative study this would mean statistical analysis, e.g., building prediction models, hypothesis testing, etc. In a study this would involve defining a set of\nThe conceptual data pipeline, illustrated in the top right part of Figure 2, is divided into four steps which are inspired by Majeed and Hwang [18] from the field of data science, and illustrated by typical study examples below.\n- (1) Data cleaning:\n2. (2)\nIn a quantitative study this involves transforming the data into a readable form for statistical tools, which may, for example, involve coding of the data based on a pre-defined scheme. It may also include anonymizing the data. In a qualitative study, this typically includes transcribing the data, as well as anonymization.\nIn a qualitative study this would include getting a first understanding of the data and probably a first idea of procedures for coding.\n- Data exploration and visualization: In a quantitative study this includes investigating descriptive statistics and visualising data. This is also a natural step for identifying outliers.\n- (3) Model building and analysis:\n- qualitative codes, coding, obtaining findings, and potentially theory building, in an iterative manner.\n- (4) Findings presentation: Findings are commonly presented in journal/conference publications, technical reports, or similarly, including data and analyses to support the findings.\nGenerally, outcomes from the later stages of the data pipeline are less sensitive to share openly, both with respect to participants' privacy and potential company secrets.",
    "context": "Describes a data pipeline framework for research, highlighting the decreasing accessibility of data stages beyond the final findings due to privacy and confidentiality concerns.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      4,
      5
    ],
    "id": "5fe973cad82c579c4c022c445d1548343ebf66d9d279c73e7f43c56ed1bed0ff"
  },
  {
    "text": "Research artifacts may emerge from a study, like questionnaires and other tools for data collection, software script for analysing qualitative data, software tools for illustration of the research conducted (e.g., a tool for managing code reviews in a code review experiment) or the studied code itself.\nThis type of research artifacts can be used in studies where results are validated by other research groups repeating the studies. In ACM terminology a study may be reproduced (different team, same\nexperiment setup) or replicated (different team, different experiment setup) 10\nInspired by Belfadel et al. [1], we identify three steps of artifact generalization towards increasing reuse:\nIn these situations artifacts can either be input, or serve as background information. For example, in one case exactly the same research is conducted by another group, and in another case studies build on the research, but develop or adapt artifacts and introduce them in a new context. Notice that these terms are used in different ways by different researchers. In Empirical Software Engineering, the term replication can mean conducting new studies similar to previously conducted studies (see, e.g., Gómez et al. [12]).\n- i) Publication for reproduction, resulting in artifacts in original state (non-editable documents, executable code, etc).\n2. iii) Generalization for continued development, resulting in artifacts released with guidance how to adapt it, e.g., by inviting to a community.\n3. ii) Generalization for general use, resulting in artifacts in editable state (editable documents, editable source code, etc).\nEach of these steps require additional investments in making the artifacts openly available. The first step (i) focuses on transparency through accessibility, connecting to the goals of the ACM artifact badges. To advance the research, access to editable artifacts are needed (step ii). To build a community (step iii) around tools or other research artifacts, requires governance effort, like for any open source software.\nThese three dimensions constitute our conceptual framework, and is used next to analyze data and artifacts in the Gander case.\n\nDefines three stages of artifact reuse: publication for reproduction, generalization for continued development, and generalization for general use, emphasizing the need for open access and community governance.",
    "original_text": "Research artifacts may emerge from a study, like questionnaires and other tools for data collection, software script for analysing qualitative data, software tools for illustration of the research conducted (e.g., a tool for managing code reviews in a code review experiment) or the studied code itself.\nThis type of research artifacts can be used in studies where results are validated by other research groups repeating the studies. In ACM terminology a study may be reproduced (different team, same\nexperiment setup) or replicated (different team, different experiment setup) 10\nInspired by Belfadel et al. [1], we identify three steps of artifact generalization towards increasing reuse:\nIn these situations artifacts can either be input, or serve as background information. For example, in one case exactly the same research is conducted by another group, and in another case studies build on the research, but develop or adapt artifacts and introduce them in a new context. Notice that these terms are used in different ways by different researchers. In Empirical Software Engineering, the term replication can mean conducting new studies similar to previously conducted studies (see, e.g., Gómez et al. [12]).\n- i) Publication for reproduction, resulting in artifacts in original state (non-editable documents, executable code, etc).\n2. iii) Generalization for continued development, resulting in artifacts released with guidance how to adapt it, e.g., by inviting to a community.\n3. ii) Generalization for general use, resulting in artifacts in editable state (editable documents, editable source code, etc).\nEach of these steps require additional investments in making the artifacts openly available. The first step (i) focuses on transparency through accessibility, connecting to the goals of the ACM artifact badges. To advance the research, access to editable artifacts are needed (step ii). To build a community (step iii) around tools or other research artifacts, requires governance effort, like for any open source software.\nThese three dimensions constitute our conceptual framework, and is used next to analyze data and artifacts in the Gander case.",
    "context": "Defines three stages of artifact reuse: publication for reproduction, generalization for continued development, and generalization for general use, emphasizing the need for open access and community governance.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      5,
      6
    ],
    "id": "f9f243e04366dd30eb008bd9bc18d3ff03fb2269165827731c0cd9ca9ede5ddc"
  },
  {
    "text": "To demonstrate potential use of the conceptual framework from Figure 2, we extract data sets and artifacts shared in the Gander case and list them in Table 1. Further, based on trade-offs in our case with respect to different data sets, we propose recommendations for open data practices, summarized in framed boxes below. When possible, we have also indicated which ACM badge level we believe that the recommendation supports.\nThe data gathered in the empirical part of the Gander project, data set (1a) and (2a), went through the pipeline of Figure 2, i.e., data cleaning, data exploration and visualisation, model building and analysis, and were then shared in anonymized and summarized form in the presentation of the results [29, 32]. The protocols from the empirical part were shared as metadata (Step i) for inspection during the review process.\nThe data sets and artifacts are split into three sections; the semistructured interviews of the empirical part of the project (1), the survey part of the same empirical part (2), and the user study connected to the development part of the project (3).\nSimilarly, for the development part of the Gander project, the interview data gathered during the user study, data set (3a), were shared in anonymized and summarized form in the presentation of the results [28]. The protocols for the interviews were shared\n10 https://www.acm.org/publications/policies/artifact-review-badging Notice that ACM has recently swapped the meaning of the two terms after discussion with the National Information Standards Organization.\nas metadata (Step i) both during the review process and also later. In addition, the session data gathered during the user study, in the form of interaction data and gaze data, data set (3c), was shared as anonymized data (Data step 1) along side the experimental setup used, the Gander platform (Artifact step iii). The purpose of sharing the Gander platform is to contribute to the research community and to enable and facilitate further research into gaze-assisted code review tooling. With this goal in mind, care was taken to select an appropriate license for the project and to review dependencies with regard to licenses. There was one project among the dependencies that was shared without a license (matching artifact step ii) but after discussion with the Gander team the project added a license (matching artifact step iii).\nGiving open access to research artifacts , like interview and survey protocols (artifacts 1b, 2b, and 3b) is non-controversial and mostly a matter of practical procedures for their publication and sustained accessibility. In the Gander case they were provided for peer review in the first two cases, but then not published with the papers, while published with the platform artifact in the third case. Given the space constraints of conference papers, authors are reluctant to use the space by adding such protocols as appendices. However, conferences and journals may offer online publication of supplementary material with the main publication. Alternatively, artifacts may be given persistent digital object identifiers (DOI) on their own right, although this adds to the bureaucracy burden for researchers. Providing access through a university's persistent storage, like in our third case [8], is convenient for the researchers, although not optimal from a traceability point of view.\nR1 . We therefore advice open research artifacts be given persistent DOI to enable traceability, independently of storage solution - as long as it is persistent enough. [ACM Available]\nProviding research platform artifacts as open source software is a highly recommended practice. The Gander platform builds on other open source projects, which helped speed up the development. However, the licensing issues reported in Section 3 demonstrates clearly, that the artifact step ii is not sufficient to build further research on. This is both due to the unclear licensing situation, and lack of community that might respond to questions and improvement proposals. In our case, the issue was sorted out in dialogue with the originating author, but that is not a scalable solution.\nThere might be conflicting interests with opening research artifacts, if the originator aims to commercialise the material or some services or products build thereon. However, we still advocate for open source solutions, which actually may be compatible with business models, such as freemium [22] or servitization [15].\nR2 . We advice research software be made open source with an appropriate license. We further advice research institutes and funding agencies to cover costs related to governing OSS communities for such software. [ACM Artifacts Evaluated - Functional]\nAccess to research data is more sensitive and is in the Gander case published only in synthesized form in the publications. Both data sets (1a) and (2a), i.e. qualitative interview data and quantitative survey data, were collected within the same two multi-national companies.\nTable 1: Data and artifacts made open from the Gander case. * Shared as part of the peer review process but not after.\n\n1a) Semi-structured interview data (12 participants) 1b) Semi-structured interview protocol, Class = First -. 1a) Semi-structured interview data (12 participants) 1b) Semi-structured interview protocol, Kind = Qual Artifact. 1a) Semi-structured interview data (12 participants) 1b) Semi-structured interview protocol, Participants = Industry -. 1a) Semi-structured interview data (12 participants) 1b) Semi-structured interview protocol, Figure 2 step = 4 [29, 32] i*. 1a) Semi-structured interview data (12 participants) 1b) Semi-structured interview protocol, Purpose = Conceptualization. 2a) Survey data (78 practitioners) 2b) Survey protocol, Class = First -. 2a) Survey data (78 practitioners) 2b) Survey protocol, Kind = Quant Artifact. 2a) Survey data (78 practitioners) 2b) Survey protocol, Participants = Industry -. 2a) Survey data (78 practitioners) 2b) Survey protocol, Figure 2 step = 2, 3, 4 [29] i*. 2a) Survey data (78 practitioners) 2b) Survey protocol, Purpose = Conceptualization. 3a) User study interview data (8 participants) 3b) User study protocol 3c) User study gaze and interaction data 3d) User study software (the Gander platform), Class = First - Second -. 3a) User study interview data (8 participants) 3b) User study protocol 3c) User study gaze and interaction data 3d) User study software (the Gander platform), Kind = Qual Artifact Quant Artifact. 3a) User study interview data (8 participants) 3b) User study protocol 3c) User study gaze and interaction data 3d) User study software (the Gander platform), Participants = Students - Students -. 3a) User study interview data (8 participants) 3b) User study protocol 3c) User study gaze and interaction data 3d) User study software (the Gander platform), Figure 2 step = 4 [28] i 1 iii. 3a) User study interview data (8 participants) 3b) User study protocol 3c) User study gaze and interaction data 3d) User study software (the Gander platform), Purpose = Validation\nThe risks related to sharing the qualitative interview data are multifold: firstly, information related to the company that is not relevant for the focus of the study might be mentioned in the interview, e.g. information about the physical design of an embedded product to come. Secondly, the information about the company is relevant, but has to be filtered before publication, e.g. a critical event for the company in relation to security that both interviewer and interviewee knew about, but still was not in the public communication. Thirdly, the interviewee could mention facts or opinions that are sensitive with respect to their own future in the company, i.e. criticising a manager for certain actions, or lack thereof. Any of these factors on their own prevents from publishing the raw interview data, both with respect to the information as such, and that it is impossible to anonymize individuals among a such small set of interviewees. On top of that, fourthly, the general criticism with respect to epistemological concerns, raised by e.g. Chauvette et al. [3], that the lack of connection to the context makes data useless. We share these concerns partially in our case, since we have a long research collaboration track record with the companies in the study, which means that the shared understanding of the software engineering practice is significant. Transcripts of the interviews might be hard to understand without the knowledge of the context, e.g. code review practices of the company.\nthe questions are asked to shame the company. Finally, the statistical analysis methods and tools enable more standardized analyses, which reduce the need for transparency in terms of open data, unless there are suspicions of fake data which should be checked.\nR4 . We recommend quantitative data be shared openly, if and only if, the data is anonymized sufficiently to protect the identity of the individual or company (if requested).\n\nThe document outlines recommendations for open research artifact sharing, specifically detailing the data and protocols shared from the Gander project, including considerations for qualitative and quantitative data, and potential risks associated with sharing interview transcripts.",
    "original_text": "To demonstrate potential use of the conceptual framework from Figure 2, we extract data sets and artifacts shared in the Gander case and list them in Table 1. Further, based on trade-offs in our case with respect to different data sets, we propose recommendations for open data practices, summarized in framed boxes below. When possible, we have also indicated which ACM badge level we believe that the recommendation supports.\nThe data gathered in the empirical part of the Gander project, data set (1a) and (2a), went through the pipeline of Figure 2, i.e., data cleaning, data exploration and visualisation, model building and analysis, and were then shared in anonymized and summarized form in the presentation of the results [29, 32]. The protocols from the empirical part were shared as metadata (Step i) for inspection during the review process.\nThe data sets and artifacts are split into three sections; the semistructured interviews of the empirical part of the project (1), the survey part of the same empirical part (2), and the user study connected to the development part of the project (3).\nSimilarly, for the development part of the Gander project, the interview data gathered during the user study, data set (3a), were shared in anonymized and summarized form in the presentation of the results [28]. The protocols for the interviews were shared\n10 https://www.acm.org/publications/policies/artifact-review-badging Notice that ACM has recently swapped the meaning of the two terms after discussion with the National Information Standards Organization.\nas metadata (Step i) both during the review process and also later. In addition, the session data gathered during the user study, in the form of interaction data and gaze data, data set (3c), was shared as anonymized data (Data step 1) along side the experimental setup used, the Gander platform (Artifact step iii). The purpose of sharing the Gander platform is to contribute to the research community and to enable and facilitate further research into gaze-assisted code review tooling. With this goal in mind, care was taken to select an appropriate license for the project and to review dependencies with regard to licenses. There was one project among the dependencies that was shared without a license (matching artifact step ii) but after discussion with the Gander team the project added a license (matching artifact step iii).\nGiving open access to research artifacts , like interview and survey protocols (artifacts 1b, 2b, and 3b) is non-controversial and mostly a matter of practical procedures for their publication and sustained accessibility. In the Gander case they were provided for peer review in the first two cases, but then not published with the papers, while published with the platform artifact in the third case. Given the space constraints of conference papers, authors are reluctant to use the space by adding such protocols as appendices. However, conferences and journals may offer online publication of supplementary material with the main publication. Alternatively, artifacts may be given persistent digital object identifiers (DOI) on their own right, although this adds to the bureaucracy burden for researchers. Providing access through a university's persistent storage, like in our third case [8], is convenient for the researchers, although not optimal from a traceability point of view.\nR1 . We therefore advice open research artifacts be given persistent DOI to enable traceability, independently of storage solution - as long as it is persistent enough. [ACM Available]\nProviding research platform artifacts as open source software is a highly recommended practice. The Gander platform builds on other open source projects, which helped speed up the development. However, the licensing issues reported in Section 3 demonstrates clearly, that the artifact step ii is not sufficient to build further research on. This is both due to the unclear licensing situation, and lack of community that might respond to questions and improvement proposals. In our case, the issue was sorted out in dialogue with the originating author, but that is not a scalable solution.\nThere might be conflicting interests with opening research artifacts, if the originator aims to commercialise the material or some services or products build thereon. However, we still advocate for open source solutions, which actually may be compatible with business models, such as freemium [22] or servitization [15].\nR2 . We advice research software be made open source with an appropriate license. We further advice research institutes and funding agencies to cover costs related to governing OSS communities for such software. [ACM Artifacts Evaluated - Functional]\nAccess to research data is more sensitive and is in the Gander case published only in synthesized form in the publications. Both data sets (1a) and (2a), i.e. qualitative interview data and quantitative survey data, were collected within the same two multi-national companies.\nTable 1: Data and artifacts made open from the Gander case. * Shared as part of the peer review process but not after.\n\n1a) Semi-structured interview data (12 participants) 1b) Semi-structured interview protocol, Class = First -. 1a) Semi-structured interview data (12 participants) 1b) Semi-structured interview protocol, Kind = Qual Artifact. 1a) Semi-structured interview data (12 participants) 1b) Semi-structured interview protocol, Participants = Industry -. 1a) Semi-structured interview data (12 participants) 1b) Semi-structured interview protocol, Figure 2 step = 4 [29, 32] i*. 1a) Semi-structured interview data (12 participants) 1b) Semi-structured interview protocol, Purpose = Conceptualization. 2a) Survey data (78 practitioners) 2b) Survey protocol, Class = First -. 2a) Survey data (78 practitioners) 2b) Survey protocol, Kind = Quant Artifact. 2a) Survey data (78 practitioners) 2b) Survey protocol, Participants = Industry -. 2a) Survey data (78 practitioners) 2b) Survey protocol, Figure 2 step = 2, 3, 4 [29] i*. 2a) Survey data (78 practitioners) 2b) Survey protocol, Purpose = Conceptualization. 3a) User study interview data (8 participants) 3b) User study protocol 3c) User study gaze and interaction data 3d) User study software (the Gander platform), Class = First - Second -. 3a) User study interview data (8 participants) 3b) User study protocol 3c) User study gaze and interaction data 3d) User study software (the Gander platform), Kind = Qual Artifact Quant Artifact. 3a) User study interview data (8 participants) 3b) User study protocol 3c) User study gaze and interaction data 3d) User study software (the Gander platform), Participants = Students - Students -. 3a) User study interview data (8 participants) 3b) User study protocol 3c) User study gaze and interaction data 3d) User study software (the Gander platform), Figure 2 step = 4 [28] i 1 iii. 3a) User study interview data (8 participants) 3b) User study protocol 3c) User study gaze and interaction data 3d) User study software (the Gander platform), Purpose = Validation\nThe risks related to sharing the qualitative interview data are multifold: firstly, information related to the company that is not relevant for the focus of the study might be mentioned in the interview, e.g. information about the physical design of an embedded product to come. Secondly, the information about the company is relevant, but has to be filtered before publication, e.g. a critical event for the company in relation to security that both interviewer and interviewee knew about, but still was not in the public communication. Thirdly, the interviewee could mention facts or opinions that are sensitive with respect to their own future in the company, i.e. criticising a manager for certain actions, or lack thereof. Any of these factors on their own prevents from publishing the raw interview data, both with respect to the information as such, and that it is impossible to anonymize individuals among a such small set of interviewees. On top of that, fourthly, the general criticism with respect to epistemological concerns, raised by e.g. Chauvette et al. [3], that the lack of connection to the context makes data useless. We share these concerns partially in our case, since we have a long research collaboration track record with the companies in the study, which means that the shared understanding of the software engineering practice is significant. Transcripts of the interviews might be hard to understand without the knowledge of the context, e.g. code review practices of the company.\nthe questions are asked to shame the company. Finally, the statistical analysis methods and tools enable more standardized analyses, which reduce the need for transparency in terms of open data, unless there are suspicions of fake data which should be checked.\nR4 . We recommend quantitative data be shared openly, if and only if, the data is anonymized sufficiently to protect the identity of the individual or company (if requested).",
    "context": "The document outlines recommendations for open research artifact sharing, specifically detailing the data and protocols shared from the Gander project, including considerations for qualitative and quantitative data, and potential risks associated with sharing interview transcripts.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      6,
      7
    ],
    "id": "4afc9d300975fc1ec80fce9fe6b1847bd4a82ab86b8b60608b70ff8447564c48"
  },
  {
    "text": "Finally, we have a set of data (3c) which is collected through 'human-enacted inquisitive and observational techniques', i.e. eyetracking data. In the case where raw image data is collected, the data is by definition personal and non-anonymizable, since the eye iris is possible to uniquely identifiable to persons. Thus, in such cases anonymization of eye-tracking data must take place to an abstracted level, e.g. eye movement positions. In the case of the Gander project, the eye-tracker used does not collect iris images, but rather details such as left and right gaze positions and pupil diameter.\nR5 . In case of data that can be directly or indirectly traceable to individuals, it cannot be open. Transforming such data into anonymized forms may enable publication.\nWe derived this conceptual framework from one line of research in software engineering, in the context of our experience of many years of empirical software engineering research. The project contains a multitude of data and artifacts, and our collective experience is extensive. Still we do not claim this conceptual framework is generally applicable nor in a final state. We therefore invite the research community to validate and further extend the framework and its recommendations for practice.\n\nDetails the specific data collection methods (eyetracking) and the necessity for anonymization due to the potential for individual identification.",
    "original_text": "Finally, we have a set of data (3c) which is collected through 'human-enacted inquisitive and observational techniques', i.e. eyetracking data. In the case where raw image data is collected, the data is by definition personal and non-anonymizable, since the eye iris is possible to uniquely identifiable to persons. Thus, in such cases anonymization of eye-tracking data must take place to an abstracted level, e.g. eye movement positions. In the case of the Gander project, the eye-tracker used does not collect iris images, but rather details such as left and right gaze positions and pupil diameter.\nR5 . In case of data that can be directly or indirectly traceable to individuals, it cannot be open. Transforming such data into anonymized forms may enable publication.\nWe derived this conceptual framework from one line of research in software engineering, in the context of our experience of many years of empirical software engineering research. The project contains a multitude of data and artifacts, and our collective experience is extensive. Still we do not claim this conceptual framework is generally applicable nor in a final state. We therefore invite the research community to validate and further extend the framework and its recommendations for practice.",
    "context": "Details the specific data collection methods (eyetracking) and the necessity for anonymization due to the potential for individual identification.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      7
    ],
    "id": "24819910ba8712ebd113856f6b29de125368d8ea40d3b4935fbad891e5ef788e"
  },
  {
    "text": "To support the transition of SE research towards open science, we have derived a conceptual framework, based on our experiences with a multitude of data and artifacts in a socio-technical software engineering project, that entails participants, data and artifacts. We unfold the variation in these concepts across our project, and discuss openness practices in relation to those.\nBased on the guiding principles for FAIR data - 'as open as possible and as closed as nescessary' - we recommend that research artifacts, such as survey and interview instruments are always made open access. Research platforms should also be made open, but need governance, e.g. licence and community, to reach its full potential. Quantitative data may be more open, due to its standardization and\nThis discussion leads us not to recommend sharing qualitative data from companies openly. However, researchers could consider publishing code books from the qualitative analysis (data step 3), as proposed by Field et al. [7] and DuBois et al. [4], as well as transparently reporting evolution of codes, conceptual model and theory, for example, as done by Runeson et al. [25]. Regarding qualitative data from students, the participants' integrity must be protected although there are no company secrets to protect, which leads to a similar recommendation as for company participants.\nR3 . We advice not to openly publish qualitative research data, but to publish study and analysis artifacts, such as study protocols, interview guides, interviewee descriptions, and code books from thematic analysis. [4, 7]\nThe quantitative survey data is somewhat easier to share more openly. Firstly, there are more participants - finding a person in a large pool is harder than in a small one, although modern data analyses are very powerful in finding a 'nail in a haystack'. Secondly, the opinions shared in Likert scale responses are not as rich and detailed as qualitative survey or interview responses, unless\npure volume, while opening qualitative data comes with several risks and challenges. At the end of the day, participants' integrity and companies secrecy concerns are essential to respect, also while advocating the benefits of open science.\nWe advise that our conceptual framework be used to guide tradeoffs between openness and closeness. We hope that the preliminary recommendations become a starting point for the research community on open science practices for empirical software engineering. We further invite the community to validate and evolve the guidelines to be more comprehensive.\nThe framework and recommendations align with open science and FAIR data principles, as well as artifact evaluation policies. Our contribution is to interweave these with our experiences from a concrete research project and to generalize for a broader range of software engineering projects.\n\nOutlines a conceptual framework for balancing openness and data privacy in software engineering research, advocating for open access to artifacts like codebooks and study protocols while cautioning against open publication of qualitative data due to participant integrity concerns.",
    "original_text": "To support the transition of SE research towards open science, we have derived a conceptual framework, based on our experiences with a multitude of data and artifacts in a socio-technical software engineering project, that entails participants, data and artifacts. We unfold the variation in these concepts across our project, and discuss openness practices in relation to those.\nBased on the guiding principles for FAIR data - 'as open as possible and as closed as nescessary' - we recommend that research artifacts, such as survey and interview instruments are always made open access. Research platforms should also be made open, but need governance, e.g. licence and community, to reach its full potential. Quantitative data may be more open, due to its standardization and\nThis discussion leads us not to recommend sharing qualitative data from companies openly. However, researchers could consider publishing code books from the qualitative analysis (data step 3), as proposed by Field et al. [7] and DuBois et al. [4], as well as transparently reporting evolution of codes, conceptual model and theory, for example, as done by Runeson et al. [25]. Regarding qualitative data from students, the participants' integrity must be protected although there are no company secrets to protect, which leads to a similar recommendation as for company participants.\nR3 . We advice not to openly publish qualitative research data, but to publish study and analysis artifacts, such as study protocols, interview guides, interviewee descriptions, and code books from thematic analysis. [4, 7]\nThe quantitative survey data is somewhat easier to share more openly. Firstly, there are more participants - finding a person in a large pool is harder than in a small one, although modern data analyses are very powerful in finding a 'nail in a haystack'. Secondly, the opinions shared in Likert scale responses are not as rich and detailed as qualitative survey or interview responses, unless\npure volume, while opening qualitative data comes with several risks and challenges. At the end of the day, participants' integrity and companies secrecy concerns are essential to respect, also while advocating the benefits of open science.\nWe advise that our conceptual framework be used to guide tradeoffs between openness and closeness. We hope that the preliminary recommendations become a starting point for the research community on open science practices for empirical software engineering. We further invite the community to validate and evolve the guidelines to be more comprehensive.\nThe framework and recommendations align with open science and FAIR data principles, as well as artifact evaluation policies. Our contribution is to interweave these with our experiences from a concrete research project and to generalize for a broader range of software engineering projects.",
    "context": "Outlines a conceptual framework for balancing openness and data privacy in software engineering research, advocating for open access to artifacts like codebooks and study protocols while cautioning against open publication of qualitative data due to participant integrity concerns.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      8,
      7
    ],
    "id": "6eeabf194b8c4a2893b27cb725541e05c7f5d165723d351d7feebab586e0bfe0"
  },
  {
    "text": "The authors would like to thanks the co-workers in the Gander project. This work has been partially supported by the Swedish Foundation for Strategic Research (grant no. FFL18-0231), the Swedish Research Council (grant no. 2019-05658), ELLIIT - the Swedish Strategic Research Area in IT and Mobile Communications, and the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.\n\nAcknowledges funding sources and collaborators for the Gander project.",
    "original_text": "The authors would like to thanks the co-workers in the Gander project. This work has been partially supported by the Swedish Foundation for Strategic Research (grant no. FFL18-0231), the Swedish Research Council (grant no. 2019-05658), ELLIIT - the Swedish Strategic Research Area in IT and Mobile Communications, and the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.",
    "context": "Acknowledges funding sources and collaborators for the Gander project.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      8
    ],
    "id": "84a0ccb912e41aed931c63ac561570f8f3a7fe5063b6ecc0e5ac837cc4826cd0"
  },
  {
    "text": "The following data and artifacts are openly available for use.\n- The Gander platform [9]\n- Supplementary data to the Gander user study and platform (protocols and session data)[8]\n\nProvides access to research materials for replication and further study.",
    "original_text": "The following data and artifacts are openly available for use.\n- The Gander platform [9]\n- Supplementary data to the Gander user study and platform (protocols and session data)[8]",
    "context": "Provides access to research materials for replication and further study.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      8
    ],
    "id": "68761c7ba0a30e9e55ccfc5f8609cf29c0990b3c31d37c855a593cd5f21e8ad9"
  },
  {
    "text": "- [1] Belfadel, A., Amdouni, E., Laval, J., Cherifi, C.B., Moalla, N., 2022. Towards software reuse through an enterprise architecture-based software capability profile. Enterprise Information Systems 16, 29 - 70. doi: 10.1080/17517575. 2020.1843076 .\n- [3] Chauvette, A., Schick-Makaroff, K., Molzahn, A.E., 2019. Open data in qualitative research. International Journal of Qualitative Methods 18, 160940691882386. doi: 10.1177/1609406918823863 .\n- [2] Briand, L.C., Bianculli, D., Nejati, S., Pastore, F., Sabetzadeh, M., 2017. The case for context-driven software engineering research: Generalizability is overrated. IEEE Software 34, 72-75. doi: 10.1109/MS.2017.3571562 .\n- [4] DuBois, J.M., Mozersky, J., Parsons, M., Walsh, H.A., Friedrich, A., Pienta, A., 2023. Exchanging words: Engaging the challenges of sharing qualitative research data. Proceedings of the National Academy of Sciences 120. doi: 10.1073/pnas. 2206981120 .\n- [6] Enders, T., Wolff, C., Satzger, G., 2020. Knowing what to share: Selective revealing in open data, in: European Conference on Information Systems (ECIS) Researchin-Progress Papers, p. 11. URL: https://aisel.aisnet.org/ecis2020_rip/11.\n- [5] DuBois, J.M., Strait, M., Walsh, H., 2018. Is it time to share qualitative research data? Qualitative Psychology 5, 380-393. doi: 10.1037/qup0000076 .\n- [7] Field, S.M., van Ravenzwaaij, D., Pittelkow, M.M., Hoek, J.M., Derksen, M., 2021. Qualitative open science - pain points and perspectives, in: OSF preprints, Center for Open Science. doi: 10.31219/osf.io/e3cq4 .\n- [9] Gander Contributors, 2023b. The Gander open source platform. https://gitlab. com/lund-university/gander.\n- [8] Gander Contributors, 2023a. Gander: a platform for exploration of gaze-driven assistance in code review - supplementary material. https://doi.org/10.5281/ zenodo.10527122.\n- [10] González-Barahona, J.M., Robles, G., 2012. On the reproducibility of empirical software engineering studies based on data retrieved from development repositories. Empirirical Software Engineering 17, 75-89. doi: 10.1007/S10664-011-9181-9 .\n- [11] González-Barahona, J.M., Robles, G., 2023. Revisiting the reproducibility of empirical software engineering studies based on data retrieved from development\n12. repositories. Information and Software Technology 164, 107318. doi: 10.1016/J. INFSOF.2023.107318 .\n- [13] Joyce, J.B., Douglass, T., Benwell, B., Rhys, C.S., Parry, R., Simmons, R., Kerrison, A., 2022. Should we share qualitative data? Epistemological and practical insights from conversation analysis. International Journal of Social Research Methodology 0, 1-15. doi: 10.1080/13645579.2022.2087851 .\n- [12] Gómez, O.S., Juristo, N., Vegas, S., 2014. Understanding replication of experiments in software engineering: A classification. Information and Software Technology 56, 1033-1048. doi: https://doi.org/10.1016/j.infsof.2014.04.004 .\n- [14] Kitchenham, B.A., Budgen, D., Brereton, P., 2015. Evidence-Based Software Engineering and Systematic Reviews. Routledge.\n- [16] Kuang, P., Söderberg, E., Niehorster, D., Höst, M., 2023. Toward gaze-assisted developer tools, in: Proceedings of the 45th IEEE/ACM International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER). doi: 10. 1109/ICSE-NIER58687.2023.00015 .\n- [15] Kowalkowski, C., Gebauer, H., Kamp, B., Parry, G., 2017. Servitization and deservitization: Overview, concepts, and definitions. Industrial Marketing Management 60, 4-10. doi: 10.1016/j.indmarman.2016.12.007 .\n- [17] Lethbridge, T.C., Sim, S.E., Singer, J., 2005. Studying software engineers: Data collection techniques for software field studies. Empirical Software Engineering 10, 311-341. doi: 10.1007/s10664-005-1290-x .\n- [19] Mendez, D., Graziotin, D., Wagner, S., Seibold, H., 2020. Open science in software engineering, in: Felderer, M., Travassos, G.H. (Eds.), Contemporary Empirical Methods in Software Engineering. Springer International Publishing, Cham, pp. 477-501. doi: 10.1007/978-3-030-32489-6_17 .\n- [18] Majeed, A., Hwang, S.O., 2023. Data-centric artificial intelligence, preprocessing, and the quest for transformative artificial intelligence systems development. Computer 56, 109-115. doi: 10.1109/MC.2023.3240450 .\n- [20] Munir, H., Linåker, J., Wnuk, K., Runeson, P., Regnell, B., 2017. Open innovation using open source tools: A case study at Sony Mobile. Empirical Software Engineering 23, 186-223. doi: 10.1007/s10664-017-9511-7 .\n- [22] Niculescu, M.F., Wu, D.J., 2014. Economics of free under perpetual licensing: Implications for the software industry. Information Systems Research 25, 173-199. doi: 10.2139/ssrn.1853603 .\n- [21] Méndez Fernández, D., Monperrus, M., Feldt, R., Zimmermann, T., 2019. The open science initiative of the empirical software engineering journal. Empirical Software Engineering 24, 1057-1060. doi: 10.1007/s10664-019-09712-x .\n- [23] Robinson, B., Francis, P., 2010. Improving industrial adoption of software engineering research: a comparison of open and closed source software, in: Succi, G., Morisio, M., Nagappan, N. (Eds.), Proceedings of the International Symposium on Empirical Software Engineering and Measurement (ESEM), ACM. doi: 10.1145/1852786.1852814 .\n- [25] Runeson, P., Olsson, T., Linåker, J., 2021. Open data ecosystems - an empirical investigation into an emerging industry collaboration concept. Journal of Systems and Software 182, 111088. doi: 10.1016/j.jss.2021.111088 .\n- [24] Runeson, P., Höst, M., Rainer, A., Regnell, B., 2012. Case Study Research in Software Engineering - Guidelines and Examples. Wiley. doi: 10.1002/ 9781118181034 .\n- [26] Sadowski, C., Söderberg, E., Church, L., Sipko, M., Bacchelli, A., 2018. Modern code review: A case study at google, in: Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice, ACM. pp. 181-190. doi: 10.1145/3183519.3183525 .\n\nThe documents collectively explore the increasing emphasis on data sharing and open science practices within software engineering research, examining challenges and opportunities related to reproducibility, data accessibility, and the adoption of open source tools.",
    "original_text": "- [1] Belfadel, A., Amdouni, E., Laval, J., Cherifi, C.B., Moalla, N., 2022. Towards software reuse through an enterprise architecture-based software capability profile. Enterprise Information Systems 16, 29 - 70. doi: 10.1080/17517575. 2020.1843076 .\n- [3] Chauvette, A., Schick-Makaroff, K., Molzahn, A.E., 2019. Open data in qualitative research. International Journal of Qualitative Methods 18, 160940691882386. doi: 10.1177/1609406918823863 .\n- [2] Briand, L.C., Bianculli, D., Nejati, S., Pastore, F., Sabetzadeh, M., 2017. The case for context-driven software engineering research: Generalizability is overrated. IEEE Software 34, 72-75. doi: 10.1109/MS.2017.3571562 .\n- [4] DuBois, J.M., Mozersky, J., Parsons, M., Walsh, H.A., Friedrich, A., Pienta, A., 2023. Exchanging words: Engaging the challenges of sharing qualitative research data. Proceedings of the National Academy of Sciences 120. doi: 10.1073/pnas. 2206981120 .\n- [6] Enders, T., Wolff, C., Satzger, G., 2020. Knowing what to share: Selective revealing in open data, in: European Conference on Information Systems (ECIS) Researchin-Progress Papers, p. 11. URL: https://aisel.aisnet.org/ecis2020_rip/11.\n- [5] DuBois, J.M., Strait, M., Walsh, H., 2018. Is it time to share qualitative research data? Qualitative Psychology 5, 380-393. doi: 10.1037/qup0000076 .\n- [7] Field, S.M., van Ravenzwaaij, D., Pittelkow, M.M., Hoek, J.M., Derksen, M., 2021. Qualitative open science - pain points and perspectives, in: OSF preprints, Center for Open Science. doi: 10.31219/osf.io/e3cq4 .\n- [9] Gander Contributors, 2023b. The Gander open source platform. https://gitlab. com/lund-university/gander.\n- [8] Gander Contributors, 2023a. Gander: a platform for exploration of gaze-driven assistance in code review - supplementary material. https://doi.org/10.5281/ zenodo.10527122.\n- [10] González-Barahona, J.M., Robles, G., 2012. On the reproducibility of empirical software engineering studies based on data retrieved from development repositories. Empirirical Software Engineering 17, 75-89. doi: 10.1007/S10664-011-9181-9 .\n- [11] González-Barahona, J.M., Robles, G., 2023. Revisiting the reproducibility of empirical software engineering studies based on data retrieved from development\n12. repositories. Information and Software Technology 164, 107318. doi: 10.1016/J. INFSOF.2023.107318 .\n- [13] Joyce, J.B., Douglass, T., Benwell, B., Rhys, C.S., Parry, R., Simmons, R., Kerrison, A., 2022. Should we share qualitative data? Epistemological and practical insights from conversation analysis. International Journal of Social Research Methodology 0, 1-15. doi: 10.1080/13645579.2022.2087851 .\n- [12] Gómez, O.S., Juristo, N., Vegas, S., 2014. Understanding replication of experiments in software engineering: A classification. Information and Software Technology 56, 1033-1048. doi: https://doi.org/10.1016/j.infsof.2014.04.004 .\n- [14] Kitchenham, B.A., Budgen, D., Brereton, P., 2015. Evidence-Based Software Engineering and Systematic Reviews. Routledge.\n- [16] Kuang, P., Söderberg, E., Niehorster, D., Höst, M., 2023. Toward gaze-assisted developer tools, in: Proceedings of the 45th IEEE/ACM International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER). doi: 10. 1109/ICSE-NIER58687.2023.00015 .\n- [15] Kowalkowski, C., Gebauer, H., Kamp, B., Parry, G., 2017. Servitization and deservitization: Overview, concepts, and definitions. Industrial Marketing Management 60, 4-10. doi: 10.1016/j.indmarman.2016.12.007 .\n- [17] Lethbridge, T.C., Sim, S.E., Singer, J., 2005. Studying software engineers: Data collection techniques for software field studies. Empirical Software Engineering 10, 311-341. doi: 10.1007/s10664-005-1290-x .\n- [19] Mendez, D., Graziotin, D., Wagner, S., Seibold, H., 2020. Open science in software engineering, in: Felderer, M., Travassos, G.H. (Eds.), Contemporary Empirical Methods in Software Engineering. Springer International Publishing, Cham, pp. 477-501. doi: 10.1007/978-3-030-32489-6_17 .\n- [18] Majeed, A., Hwang, S.O., 2023. Data-centric artificial intelligence, preprocessing, and the quest for transformative artificial intelligence systems development. Computer 56, 109-115. doi: 10.1109/MC.2023.3240450 .\n- [20] Munir, H., Linåker, J., Wnuk, K., Runeson, P., Regnell, B., 2017. Open innovation using open source tools: A case study at Sony Mobile. Empirical Software Engineering 23, 186-223. doi: 10.1007/s10664-017-9511-7 .\n- [22] Niculescu, M.F., Wu, D.J., 2014. Economics of free under perpetual licensing: Implications for the software industry. Information Systems Research 25, 173-199. doi: 10.2139/ssrn.1853603 .\n- [21] Méndez Fernández, D., Monperrus, M., Feldt, R., Zimmermann, T., 2019. The open science initiative of the empirical software engineering journal. Empirical Software Engineering 24, 1057-1060. doi: 10.1007/s10664-019-09712-x .\n- [23] Robinson, B., Francis, P., 2010. Improving industrial adoption of software engineering research: a comparison of open and closed source software, in: Succi, G., Morisio, M., Nagappan, N. (Eds.), Proceedings of the International Symposium on Empirical Software Engineering and Measurement (ESEM), ACM. doi: 10.1145/1852786.1852814 .\n- [25] Runeson, P., Olsson, T., Linåker, J., 2021. Open data ecosystems - an empirical investigation into an emerging industry collaboration concept. Journal of Systems and Software 182, 111088. doi: 10.1016/j.jss.2021.111088 .\n- [24] Runeson, P., Höst, M., Rainer, A., Regnell, B., 2012. Case Study Research in Software Engineering - Guidelines and Examples. Wiley. doi: 10.1002/ 9781118181034 .\n- [26] Sadowski, C., Söderberg, E., Church, L., Sipko, M., Bacchelli, A., 2018. Modern code review: A case study at google, in: Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice, ACM. pp. 181-190. doi: 10.1145/3183519.3183525 .",
    "context": "The documents collectively explore the increasing emphasis on data sharing and open science practices within software engineering research, examining challenges and opportunities related to reproducibility, data accessibility, and the adoption of open source tools.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      8
    ],
    "id": "4e914b4d0caa8c8003e13998109c56cb47cbeab7db342ddf764d4371a7b15013"
  },
  {
    "text": "- [28] Saranpää, W., Apell Skjutar, F., Heander, J., Söderberg, E., Niehorster, D.C., Mattsson, O., Klintskog, H., Church, L., 2023. Gander: A platform for exploration of gaze-driven assistance in code review, in: Proceedings of the 2023 Symposium on Eye Tracking Research and Applications, ACM. doi: 10.1145/3588015.3589191 .\n- [27] Sadowski, C., Van Gogh, J., Jaspan, C., Soderberg, E., Winter, C., 2015. Tricorder: Building a program analysis ecosystem, in: 37th IEEE International Conference on Software Engineering, IEEE. pp. 598-608. doi: 10.1109/ICSE.2015.76 .\n- [29] Söderberg, E., Church, L., Börstler, J., Niehorster, D., Rydenfält, C., 2022. Understanding the experience of code review: Misalignments, attention, and units of analysis, in: Proceedings of the International Conference on Evaluation and Assessment in Software Engineering (EASE), ACM. doi: 10.1145/3530019. 3530037 .\n- [31] Storey, M.A., Ernst, N.A., Williams, C., Kalliamvakou, E., 2020. The who, what, how of software engineering research: a socio-technical framework. Empirical Software Engineering 25, 4097-4129. doi: 10.1007/s10664-020-09858-z .\n- [30] Solari, M., Vegas, S., Juristo, N., 2018. Content and structure of laboratory packages for software engineering experiments. Information and Software Technology 97, 64-79. doi: 10.1016/j.infsof.2017.12.016 .\n- [32] Söderberg, E., Church, L., Börstler, J., Niehorster, D.C., Rydenfält, C., 2022. What's bothering developers in code review?, in: IEEE/ACM 44th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), pp. 341-342. doi: 10.1145/3510457.3513083 .\n- [33] Wohlin, C., Runeson, P., Höst, M., Ohlsson, M.C., Regnell, B., Wesslén, A., 2012. Experimentation in Software Engineering. Springer. doi: 10.1007/978-3-64229044-2 .\n\nProvides historical context and research on experimentation and code review methodologies.",
    "original_text": "- [28] Saranpää, W., Apell Skjutar, F., Heander, J., Söderberg, E., Niehorster, D.C., Mattsson, O., Klintskog, H., Church, L., 2023. Gander: A platform for exploration of gaze-driven assistance in code review, in: Proceedings of the 2023 Symposium on Eye Tracking Research and Applications, ACM. doi: 10.1145/3588015.3589191 .\n- [27] Sadowski, C., Van Gogh, J., Jaspan, C., Soderberg, E., Winter, C., 2015. Tricorder: Building a program analysis ecosystem, in: 37th IEEE International Conference on Software Engineering, IEEE. pp. 598-608. doi: 10.1109/ICSE.2015.76 .\n- [29] Söderberg, E., Church, L., Börstler, J., Niehorster, D., Rydenfält, C., 2022. Understanding the experience of code review: Misalignments, attention, and units of analysis, in: Proceedings of the International Conference on Evaluation and Assessment in Software Engineering (EASE), ACM. doi: 10.1145/3530019. 3530037 .\n- [31] Storey, M.A., Ernst, N.A., Williams, C., Kalliamvakou, E., 2020. The who, what, how of software engineering research: a socio-technical framework. Empirical Software Engineering 25, 4097-4129. doi: 10.1007/s10664-020-09858-z .\n- [30] Solari, M., Vegas, S., Juristo, N., 2018. Content and structure of laboratory packages for software engineering experiments. Information and Software Technology 97, 64-79. doi: 10.1016/j.infsof.2017.12.016 .\n- [32] Söderberg, E., Church, L., Börstler, J., Niehorster, D.C., Rydenfält, C., 2022. What's bothering developers in code review?, in: IEEE/ACM 44th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), pp. 341-342. doi: 10.1145/3510457.3513083 .\n- [33] Wohlin, C., Runeson, P., Höst, M., Ohlsson, M.C., Regnell, B., Wesslén, A., 2012. Experimentation in Software Engineering. Springer. doi: 10.1007/978-3-64229044-2 .",
    "context": "Provides historical context and research on experimentation and code review methodologies.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      8
    ],
    "id": "b01310556ab2cc4367d6bb3cb538a4ab63e6aa14fc3b88556fca1155d7c62d54"
  },
  {
    "text": "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000\n\nConfirms publication acceptance and provides publication details.",
    "original_text": "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000",
    "context": "Confirms publication acceptance and provides publication details.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      1
    ],
    "id": "95c9c5cc4a08bf7cbfb06ae644547a5ee4d953c0729eb4b4bcfc79a2e96622c0"
  },
  {
    "text": "1 Korea Institute of Science and Technology, Seoul, South Korea\n2 KHU-KIST Department of Converging Science and Technology, Kyung Hee University, Seoul, South Korea\nCorresponding author: Min-Koo Kang (e-mail: minkoo@kist.re.kr).\nThis work was financially supported by the Institute of Civil-Military Technology Cooperation Program funded by the Defense Acquisition Program Administration and Ministry of Trade, Industry and Energy of Korean government (grant No. 17-CM-DP-29).\nABSTRACT To deliver an optimal Mixed Reality (MR) experience, wherein virtual elements and real-world objects are seamlessly merged, it is vital to ensure a consistent vergence-accommodation distance. This necessitates the advancement of technology to precisely estimate the user's gaze distance. Presently, various MR devices employ small eye-tracking cameras to capture both eyes and infer the gaze distance based on vergence angle data. However, this technique faces significant challenges, as it is highly sensitive to several human errors, such as strabismus, blinking, and fatigue of the eyes due to prolonged use. To address these issues, this paper introduces an innovative hybrid algorithm for estimating gaze distances. The proposed approach concurrently utilizes an eye camera and a depth camera to conduct parallel estimations: one based on the conventional vergence angle and the other on gaze-mapped depth information. The confidence of each method is then assessed and cross-referenced, and an adaptive weighted average is computed to derive a more precise and stable gaze distance estimation. In the experiment, three challenging test scenarios designed to induce human and environmental errors were administered to 12 subjects under uniform conditions to evaluate the accuracy and stability of the proposed method. The experimental results were validated through both qualitative and quantitative analysis. The findings showed that the proposed method significantly outperformed current methods with a visual angle error of 0.132 degrees under ideal conditions. Furthermore, it consistently maintained robustness against human and environmental errors, achieving an error range of 0.14 to 0.21 degrees even in demanding environments.\nINDEX TERMS Augmented reality (AR), extended reality (XR), eye tracking, gaze distance estimation, mixed reality (MR), varifocal, vergence-accommodation conflict, virtual reality (VR)\n\nDetails the development and validation of a hybrid algorithm for more accurate and robust gaze distance estimation in mixed reality, addressing limitations of existing eye-tracking methods.",
    "original_text": "1 Korea Institute of Science and Technology, Seoul, South Korea\n2 KHU-KIST Department of Converging Science and Technology, Kyung Hee University, Seoul, South Korea\nCorresponding author: Min-Koo Kang (e-mail: minkoo@kist.re.kr).\nThis work was financially supported by the Institute of Civil-Military Technology Cooperation Program funded by the Defense Acquisition Program Administration and Ministry of Trade, Industry and Energy of Korean government (grant No. 17-CM-DP-29).\nABSTRACT To deliver an optimal Mixed Reality (MR) experience, wherein virtual elements and real-world objects are seamlessly merged, it is vital to ensure a consistent vergence-accommodation distance. This necessitates the advancement of technology to precisely estimate the user's gaze distance. Presently, various MR devices employ small eye-tracking cameras to capture both eyes and infer the gaze distance based on vergence angle data. However, this technique faces significant challenges, as it is highly sensitive to several human errors, such as strabismus, blinking, and fatigue of the eyes due to prolonged use. To address these issues, this paper introduces an innovative hybrid algorithm for estimating gaze distances. The proposed approach concurrently utilizes an eye camera and a depth camera to conduct parallel estimations: one based on the conventional vergence angle and the other on gaze-mapped depth information. The confidence of each method is then assessed and cross-referenced, and an adaptive weighted average is computed to derive a more precise and stable gaze distance estimation. In the experiment, three challenging test scenarios designed to induce human and environmental errors were administered to 12 subjects under uniform conditions to evaluate the accuracy and stability of the proposed method. The experimental results were validated through both qualitative and quantitative analysis. The findings showed that the proposed method significantly outperformed current methods with a visual angle error of 0.132 degrees under ideal conditions. Furthermore, it consistently maintained robustness against human and environmental errors, achieving an error range of 0.14 to 0.21 degrees even in demanding environments.\nINDEX TERMS Augmented reality (AR), extended reality (XR), eye tracking, gaze distance estimation, mixed reality (MR), varifocal, vergence-accommodation conflict, virtual reality (VR)",
    "context": "Details the development and validation of a hybrid algorithm for more accurate and robust gaze distance estimation in mixed reality, addressing limitations of existing eye-tracking methods.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      1
    ],
    "id": "53d9884230df76f031443d1e69929c1ae3a7108e786e985d56c3a63304fd19ad"
  },
  {
    "text": "Extended Reality (XR) technologies, including Augmented Reality (AR), Virtual Reality (VR), and Mixed Reality (MR), are currently utilized across various industries such as the integrated visual augmentation system for individual soldiers in the defense field and image-guided surgery in the medical field. Additionally, in the everyday lives of general consumers, XR enables convenient, time and space-transcending experiences such as virtual reality gaming, augmented realitybased shopping, virtual classrooms, remote health care and exercise programs, and virtual travel. Despite the rapid market growth driven by strong industry demand and active investments from global companies, XR technologies still faces many technical obstacles that must be swiftly addressed to ensure sustainable growth and commercialization.\nTo achieve an ideal XR where virtual images seamlessly blend with real environments, both AR and VR require common features: lightweight and comfortable wearability, wide field of view with high resolution, high-brightness displays ensuring visibility even outdoors, and long operational hours without battery or heat issues. Above all, overcoming the vergence-accommodation conflict (VAC) is crucial [1]. VAC can cause severe visual fatigue when viewing virtual images for extended periods and prevents users from seeing real objects at various distances and virtual images at a fixed focal distance simultaneously. Therefore, many global companies and research institutions are currently focusing on developing new technologies to resolve VAC issues, such as as varifocal, multi-focal, and all-in-focus technologies [2]-[5]. However, to control the focal distance of virtual images so that vergence and accommodation distances match, the development of gaze distance estimation technology must be preceded.\nIEEE Access\nCurrently, most commercial AR and VR devices incorporate small near-infrared eye cameras to perform eye-tracking, but this pertains to 2D gaze point tracking for human-friendly interaction with XR applications. Although research on accurately performing gaze distance estimation is till lacking, typically, in wearable XR devices, gaze distance is estimated by capturing both eyes with an eye camera, as depicted in Fig.1(a) and Fig.1(b), and using the convergence angle ( θ ) and inter pupillary distance ( β ) parameters to derive the gaze distance [6], [7]. However, this method is highly susceptible to human errors such as blinking, squinting, and eye relaxation [8], limiting the accuracy of estimating the gaze distance that matches the optical system's focal distance.\nAs an alternative to the above method, depth estimation commonly used in the computer vision field can be considered. Depth estimation generally involves analyzing the disparity using infrared (IR) pattern light and stereo vision, and depth cameras that provide distance information between the camera and surrounding environmental elements in the form of 2D images (i.e., depth maps) are already widely used. Thus, as depicted in Fig. 1(c), by estimating the user's 2D gaze point coordinates with an eye-camera and mapping it to the depth map acquired by a depth-camera, the corresponding depth value can be retrieved to estimate the user's gaze distance [9][11]. This method can solidly eliminate human errors arising from the convergence angle-based method. However, it can easily malfunction due to external environmental factors such as disocclusion regions or reflective media.\nThis paper proposes a novel hybrid method that mimics the human visual perception mechanism [12] to complement the limitations of the two conventional gaze distance estimation approaches. Human vergence-accommodation response is generally modeled as two dual-parallel feedback control systems, where these responses interact to enhance gaze distance perception and ensure better visual stability. Similarly, the proposed method combines the vergence angle-based approach with the gaze-mapped depth approach, allowing these two methods to interact and achieve synergy. This enhances the accuracy of each method and applies adaptive weighted averaging to assess the confidence of each method, thereby ensuring the stability of the final derived gaze distance.\n\nDetails the challenges and a proposed solution for accurate gaze distance estimation in XR, combining existing methods with a novel hybrid approach to overcome limitations and improve stability.",
    "original_text": "Extended Reality (XR) technologies, including Augmented Reality (AR), Virtual Reality (VR), and Mixed Reality (MR), are currently utilized across various industries such as the integrated visual augmentation system for individual soldiers in the defense field and image-guided surgery in the medical field. Additionally, in the everyday lives of general consumers, XR enables convenient, time and space-transcending experiences such as virtual reality gaming, augmented realitybased shopping, virtual classrooms, remote health care and exercise programs, and virtual travel. Despite the rapid market growth driven by strong industry demand and active investments from global companies, XR technologies still faces many technical obstacles that must be swiftly addressed to ensure sustainable growth and commercialization.\nTo achieve an ideal XR where virtual images seamlessly blend with real environments, both AR and VR require common features: lightweight and comfortable wearability, wide field of view with high resolution, high-brightness displays ensuring visibility even outdoors, and long operational hours without battery or heat issues. Above all, overcoming the vergence-accommodation conflict (VAC) is crucial [1]. VAC can cause severe visual fatigue when viewing virtual images for extended periods and prevents users from seeing real objects at various distances and virtual images at a fixed focal distance simultaneously. Therefore, many global companies and research institutions are currently focusing on developing new technologies to resolve VAC issues, such as as varifocal, multi-focal, and all-in-focus technologies [2]-[5]. However, to control the focal distance of virtual images so that vergence and accommodation distances match, the development of gaze distance estimation technology must be preceded.\nIEEE Access\nCurrently, most commercial AR and VR devices incorporate small near-infrared eye cameras to perform eye-tracking, but this pertains to 2D gaze point tracking for human-friendly interaction with XR applications. Although research on accurately performing gaze distance estimation is till lacking, typically, in wearable XR devices, gaze distance is estimated by capturing both eyes with an eye camera, as depicted in Fig.1(a) and Fig.1(b), and using the convergence angle ( θ ) and inter pupillary distance ( β ) parameters to derive the gaze distance [6], [7]. However, this method is highly susceptible to human errors such as blinking, squinting, and eye relaxation [8], limiting the accuracy of estimating the gaze distance that matches the optical system's focal distance.\nAs an alternative to the above method, depth estimation commonly used in the computer vision field can be considered. Depth estimation generally involves analyzing the disparity using infrared (IR) pattern light and stereo vision, and depth cameras that provide distance information between the camera and surrounding environmental elements in the form of 2D images (i.e., depth maps) are already widely used. Thus, as depicted in Fig. 1(c), by estimating the user's 2D gaze point coordinates with an eye-camera and mapping it to the depth map acquired by a depth-camera, the corresponding depth value can be retrieved to estimate the user's gaze distance [9][11]. This method can solidly eliminate human errors arising from the convergence angle-based method. However, it can easily malfunction due to external environmental factors such as disocclusion regions or reflective media.\nThis paper proposes a novel hybrid method that mimics the human visual perception mechanism [12] to complement the limitations of the two conventional gaze distance estimation approaches. Human vergence-accommodation response is generally modeled as two dual-parallel feedback control systems, where these responses interact to enhance gaze distance perception and ensure better visual stability. Similarly, the proposed method combines the vergence angle-based approach with the gaze-mapped depth approach, allowing these two methods to interact and achieve synergy. This enhances the accuracy of each method and applies adaptive weighted averaging to assess the confidence of each method, thereby ensuring the stability of the final derived gaze distance.",
    "context": "Details the challenges and a proposed solution for accurate gaze distance estimation in XR, combining existing methods with a novel hybrid approach to overcome limitations and improve stability.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      1,
      2
    ],
    "id": "c41d64ca686313077df261af3b8668dd771aa3b0392fbe521708d9a6c36e3c78"
  },
  {
    "text": "The vergence has been regarded as one of the most important distance cues of the human visual system for several reasons: simple principles of geometry, natural solution in computer vision field, and effectiveness, etc [13].\nKwon et al. [6] proposed geometry-based gaze depth estimation method. They utilized Prukinje images from two IR light sources with pupil center, then triangulated them to estimate gaze direction. With the idea that the pupil center distance (PCD) changes according to the target depth, of which PCD increases and decreases when the target object appears far and near from the eyes, respectively, they estimated gaze depth using PCD variations. Lee et al. [14] also\nFIGURE 1. (a) Hardware configuration for data acquisition; RGB scene image and corresponding depth image with two eye images. Conventional methods for gaze distance estimation using (b) vergence angle and (c) 2D gaze-mapped depth images.\nproposed 3D gaze estimation method with Prukinje images. Different with previous studies, they applied a multi-layer perceptron (MLP) to estimate gaze distance and designed three features used to train of which are the relative position of first and fourth Prukinje images to the pupil center, interdistance between these two Prukinje images, and pupil size. Note that they utilized the pupil size based on the fact that pupil accommodation happens according to the gaze distance changes. They finally estimated 2D gaze position with the pupil center based on a geometric transform considering the gaze distance obtained by MLP.\nMlot et al. [7] proposed fast and robust method to estimate the 3D gaze position based on the eye vergence. They utilized the ExCuSe algorithm [15] to find a pupil position, then applied a 2 nd order polynomial mapping function that describes the relationship between the pupil position and the target depth to estimate 3D gaze.\nLee et al. [16] also utilized MLP model to estimate the gaze depth. They trained the network using the gaze normal vectors obtained by a commercial eye tracker, Pupil Labs [8], instead Prukinje images, which differs from previous studies.\nThe gaze normal vector represented the three-dimensional direction of the eye's line of sight. They collected the training data by recording gaze normal vectors along with corresponding gaze depths, instructing participants to focus on a target positioned at distances ranging from 1 meter to 5 meters.\n\nDetails the evolution of gaze estimation methods, contrasting techniques using Prukinje images with those employing pupil size variations and MLP models, and highlighting a shift towards utilizing commercial eye tracker data.",
    "original_text": "The vergence has been regarded as one of the most important distance cues of the human visual system for several reasons: simple principles of geometry, natural solution in computer vision field, and effectiveness, etc [13].\nKwon et al. [6] proposed geometry-based gaze depth estimation method. They utilized Prukinje images from two IR light sources with pupil center, then triangulated them to estimate gaze direction. With the idea that the pupil center distance (PCD) changes according to the target depth, of which PCD increases and decreases when the target object appears far and near from the eyes, respectively, they estimated gaze depth using PCD variations. Lee et al. [14] also\nFIGURE 1. (a) Hardware configuration for data acquisition; RGB scene image and corresponding depth image with two eye images. Conventional methods for gaze distance estimation using (b) vergence angle and (c) 2D gaze-mapped depth images.\nproposed 3D gaze estimation method with Prukinje images. Different with previous studies, they applied a multi-layer perceptron (MLP) to estimate gaze distance and designed three features used to train of which are the relative position of first and fourth Prukinje images to the pupil center, interdistance between these two Prukinje images, and pupil size. Note that they utilized the pupil size based on the fact that pupil accommodation happens according to the gaze distance changes. They finally estimated 2D gaze position with the pupil center based on a geometric transform considering the gaze distance obtained by MLP.\nMlot et al. [7] proposed fast and robust method to estimate the 3D gaze position based on the eye vergence. They utilized the ExCuSe algorithm [15] to find a pupil position, then applied a 2 nd order polynomial mapping function that describes the relationship between the pupil position and the target depth to estimate 3D gaze.\nLee et al. [16] also utilized MLP model to estimate the gaze depth. They trained the network using the gaze normal vectors obtained by a commercial eye tracker, Pupil Labs [8], instead Prukinje images, which differs from previous studies.\nThe gaze normal vector represented the three-dimensional direction of the eye's line of sight. They collected the training data by recording gaze normal vectors along with corresponding gaze depths, instructing participants to focus on a target positioned at distances ranging from 1 meter to 5 meters.",
    "context": "Details the evolution of gaze estimation methods, contrasting techniques using Prukinje images with those employing pupil size variations and MLP models, and highlighting a shift towards utilizing commercial eye tracker data.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      2
    ],
    "id": "70cd40d43461b6c3628100b26257f9807642a5134874c9779abae1eac409dad3"
  },
  {
    "text": "Though the depth cue from an active sensor has an advantage that is invariant to human factors, there are a few studies exploiting the RGB-D camera for gaze distance estimation.\nElmadjian et al. [9] proposed a calibration procedure to estimate 3D gaze information using an uncalibrated headmounted binocular eye tracker coupled with a RGB-D camera. They compared the accuracy of 3D points of regard (PoR) using both geometric and regressor approaches. Based on\nIEEE Access\nFIGURE 2. (a) Human visual perception mechanism introduced in [12] and (b) overall framework of the proposed method by mimicking the visual perception mechanism.\nthe geometric model, they calculated intersection points of each eye gaze ray to determine the PoRs. For the regressor approach, they employed a Gaussian regressor to estimate both gaze direction and gaze depth. Because the hardware was uncalibrated, they tackled the issue that the inter pupillary distance (IPD) used to estimate a vergence angle did not align with the coordinate system of the scene camera. To address this issue, they designed a regressor pipeline consisting of two separate Gaussian regressors, one for estimating gaze direction and the other for computing the corresponding gaze depth. Despite demonstrating performance improvements, they also noted that there is still room for improvement in the accuracy of gaze distance estimation.\nLiu et al. [10] proposed an automatic calibration method for 3D gaze estimation by integrating gaze vectors with saliency maps. This approach addresses the limitations of traditional calibration methods, which typically require predefined targets and can be time-consuming. To mitigate these issues, they used saliency maps [17] to generate gaze targets based on the premise that human are more likely to fixate on salient features in scene images. To avoid redundant scene images in the calibration data, they applied bag-of-word algorithm [18] to measure image similarity. They then computed the 2D gaze points corresponding to the scene images by calibrating the relationship between the scene images and the gaze vectors from both eyes. Finally, they reconstructed a dense 3D environmental point cloud using an RGB-D camera to accuratelyt determine the 3D points of regards (PoRs).\nThe aforementioned studies utilized RGB-D cameras to estimate 3D gaze, particularly for gaze depths, whereas conventional 3D gaze estimation methods focused on minimizing angular errors in vergence using learning-based approaches. Although the depth information from RGB-D cameras was used to estimate gaze depth, it was not employed to enhance gaze distance accuracy through guided information.\n\nDetails the use of RGB-D cameras for 3D gaze estimation, outlining calibration methods and comparing them to traditional angular error minimization techniques.",
    "original_text": "Though the depth cue from an active sensor has an advantage that is invariant to human factors, there are a few studies exploiting the RGB-D camera for gaze distance estimation.\nElmadjian et al. [9] proposed a calibration procedure to estimate 3D gaze information using an uncalibrated headmounted binocular eye tracker coupled with a RGB-D camera. They compared the accuracy of 3D points of regard (PoR) using both geometric and regressor approaches. Based on\nIEEE Access\nFIGURE 2. (a) Human visual perception mechanism introduced in [12] and (b) overall framework of the proposed method by mimicking the visual perception mechanism.\nthe geometric model, they calculated intersection points of each eye gaze ray to determine the PoRs. For the regressor approach, they employed a Gaussian regressor to estimate both gaze direction and gaze depth. Because the hardware was uncalibrated, they tackled the issue that the inter pupillary distance (IPD) used to estimate a vergence angle did not align with the coordinate system of the scene camera. To address this issue, they designed a regressor pipeline consisting of two separate Gaussian regressors, one for estimating gaze direction and the other for computing the corresponding gaze depth. Despite demonstrating performance improvements, they also noted that there is still room for improvement in the accuracy of gaze distance estimation.\nLiu et al. [10] proposed an automatic calibration method for 3D gaze estimation by integrating gaze vectors with saliency maps. This approach addresses the limitations of traditional calibration methods, which typically require predefined targets and can be time-consuming. To mitigate these issues, they used saliency maps [17] to generate gaze targets based on the premise that human are more likely to fixate on salient features in scene images. To avoid redundant scene images in the calibration data, they applied bag-of-word algorithm [18] to measure image similarity. They then computed the 2D gaze points corresponding to the scene images by calibrating the relationship between the scene images and the gaze vectors from both eyes. Finally, they reconstructed a dense 3D environmental point cloud using an RGB-D camera to accuratelyt determine the 3D points of regards (PoRs).\nThe aforementioned studies utilized RGB-D cameras to estimate 3D gaze, particularly for gaze depths, whereas conventional 3D gaze estimation methods focused on minimizing angular errors in vergence using learning-based approaches. Although the depth information from RGB-D cameras was used to estimate gaze depth, it was not employed to enhance gaze distance accuracy through guided information.",
    "context": "Details the use of RGB-D cameras for 3D gaze estimation, outlining calibration methods and comparing them to traditional angular error minimization techniques.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      2,
      3
    ],
    "id": "5eae6f653ef942e6d8350fa7684d1ab047f0ecb16a16ad6f59bf55e097a304cc"
  },
  {
    "text": "In this section, we explain a hybrid method for eye-gaze distance estimation. Note that we do not explain specifics of 2D gaze and a gaze distance from vergence because we only applied one of the commercial eye trackers, Pupil-Labs [8] of which provides 2D gaze with an accuracy of 0 . 60 ◦ and a precision of 0 . 02 ◦ . It is compatible with Intel RealSense RGB-D camera [19] (Fig. 1(a)) as the scene camera, and consists of two near-infrared (NIR) eye cameras, where the 2D gaze is calculated by the intersection of the gaze vectors from each eye in the scene camera. We exploit the human visual perception mechanism [12] (Fig. 2(a)) within the eyegaze distance estimation framework by cross-referencing the gaze distance derived from vergence with that obtained from\nIEEE Access gaze-mapped depth as shown in Fig. 2(b). We utilize the gaze distance from vergence, obtained with Pupil-Labs [8], as the initial estimate for vergence and the depth from the depth image pointed by 2D gaze as the initial estimate for gazemapped depth.\nFIGURE 3. Visualization of gaze distances from vergence captured with Pupil Eye Tracker [8] by gazing at targets placed at different distances, 0.5m, 1.0m, and 1.3m. The gaze distance from vergence has a linear correlation with target depths.\n\nDetails the hybrid method for eye-gaze distance estimation, utilizing Pupil-Labs eye tracker data and depth information from a 2D gaze-mapped depth image.",
    "original_text": "In this section, we explain a hybrid method for eye-gaze distance estimation. Note that we do not explain specifics of 2D gaze and a gaze distance from vergence because we only applied one of the commercial eye trackers, Pupil-Labs [8] of which provides 2D gaze with an accuracy of 0 . 60 ◦ and a precision of 0 . 02 ◦ . It is compatible with Intel RealSense RGB-D camera [19] (Fig. 1(a)) as the scene camera, and consists of two near-infrared (NIR) eye cameras, where the 2D gaze is calculated by the intersection of the gaze vectors from each eye in the scene camera. We exploit the human visual perception mechanism [12] (Fig. 2(a)) within the eyegaze distance estimation framework by cross-referencing the gaze distance derived from vergence with that obtained from\nIEEE Access gaze-mapped depth as shown in Fig. 2(b). We utilize the gaze distance from vergence, obtained with Pupil-Labs [8], as the initial estimate for vergence and the depth from the depth image pointed by 2D gaze as the initial estimate for gazemapped depth.\nFIGURE 3. Visualization of gaze distances from vergence captured with Pupil Eye Tracker [8] by gazing at targets placed at different distances, 0.5m, 1.0m, and 1.3m. The gaze distance from vergence has a linear correlation with target depths.",
    "context": "Details the hybrid method for eye-gaze distance estimation, utilizing Pupil-Labs eye tracker data and depth information from a 2D gaze-mapped depth image.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      3,
      4
    ],
    "id": "f5bc3efc084edd5e7f70cc2f2e5481cb7190ca7d42cacd156c2bc269c75c520b"
  },
  {
    "text": "Different face shapes of users (i.e. eye position, eye ball size, inter pupillary distance, etc.) can cause scale errors of gaze distance from vergence even though the 2D gaze point is well estimated. We exploit the gaze distance from gazemapped depth as a guided information to refine the scale of the initial gaze distance from vergence because the IR active sensor is relatively robust to human factors. Based on the characteristics of which the gaze distance from vergence changes roughly linearly to that of gaze-mapped depth as shown in Fig. 3, we exploit 1 st order polynomial function to refine the initial gaze distance from vergence.\n<!-- formula-not-decoded -->\nwhere ˆ V is the refined gaze distance from vergence for given initial gaze distance from vergence V and α 0 , α 1 are the polynomial coefficients. The optimal coefficients can be estimated by minimizing an objective function defined as\n<!-- formula-not-decoded -->\nwhere N is the number of samples, V n , and D n are n th initial gaze distance from vergence and corresponding that of gazemapped depth, respectively.\nAfter gaze fixation, the relaxation of ocular muscles can lead to drifts in the gaze distance derived from vergence, causing inaccuracies in estimation. Since pupil diameter tends to increase when such drift occurs (Fig. 4), we define a basis gaze distance from vergence to minimize the drift error. This defined basis is continuously updated only when the pupil\nFIGURE 4. Comparison of gaze distance from vergence and normalized pupil diameter. The pupil diameter increases when the gaze distance from vergence drifts (red arrows).\ndiameter constricts, effectively helping to suppress the drift error, maintain estimation accuracy, and stabilize the overall gaze distance measurements.\n<!-- formula-not-decoded -->\nwhere V t is a refined gaze distance from vergence at time t , V basis is the basis gaze distance from vergence, and ▽ P size is the normalized differentiation of the pupil diameter.\nBecause the pupil diameter changes smoothly over time, we apply temporal consistency to it.\n<!-- formula-not-decoded -->\nwhere t pupil is a temporal weight for pupil diameter and P T size is the pupil diameter at time T .\n\nRefines initial gaze distance from vergence using pupil diameter changes to correct for drift and maintain estimation accuracy.",
    "original_text": "Different face shapes of users (i.e. eye position, eye ball size, inter pupillary distance, etc.) can cause scale errors of gaze distance from vergence even though the 2D gaze point is well estimated. We exploit the gaze distance from gazemapped depth as a guided information to refine the scale of the initial gaze distance from vergence because the IR active sensor is relatively robust to human factors. Based on the characteristics of which the gaze distance from vergence changes roughly linearly to that of gaze-mapped depth as shown in Fig. 3, we exploit 1 st order polynomial function to refine the initial gaze distance from vergence.\n<!-- formula-not-decoded -->\nwhere ˆ V is the refined gaze distance from vergence for given initial gaze distance from vergence V and α 0 , α 1 are the polynomial coefficients. The optimal coefficients can be estimated by minimizing an objective function defined as\n<!-- formula-not-decoded -->\nwhere N is the number of samples, V n , and D n are n th initial gaze distance from vergence and corresponding that of gazemapped depth, respectively.\nAfter gaze fixation, the relaxation of ocular muscles can lead to drifts in the gaze distance derived from vergence, causing inaccuracies in estimation. Since pupil diameter tends to increase when such drift occurs (Fig. 4), we define a basis gaze distance from vergence to minimize the drift error. This defined basis is continuously updated only when the pupil\nFIGURE 4. Comparison of gaze distance from vergence and normalized pupil diameter. The pupil diameter increases when the gaze distance from vergence drifts (red arrows).\ndiameter constricts, effectively helping to suppress the drift error, maintain estimation accuracy, and stabilize the overall gaze distance measurements.\n<!-- formula-not-decoded -->\nwhere V t is a refined gaze distance from vergence at time t , V basis is the basis gaze distance from vergence, and ▽ P size is the normalized differentiation of the pupil diameter.\nBecause the pupil diameter changes smoothly over time, we apply temporal consistency to it.\n<!-- formula-not-decoded -->\nwhere t pupil is a temporal weight for pupil diameter and P T size is the pupil diameter at time T .",
    "context": "Refines initial gaze distance from vergence using pupil diameter changes to correct for drift and maintain estimation accuracy.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      4
    ],
    "id": "2342b78717dbd5003ea60dd2263f9650d17a6a95a6d628f468d6839ea0d2a516"
  },
  {
    "text": "The gaze distances from gaze-mapped depth in disocclusion regions and reflective material are unreliable for eye-gazedistance estimation. To avoid using unreliable gaze distances from gaze-mapped depth in eye-gaze distance estimation, we define activation functions for each environment as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere ▽ D is the normalized differentiation of the gaze distance from gaze-mapped depth, threshdis and errref are the minimum depth value considered as reliable and the maximum error of depth sensor in the reflection, respectively. Because the active depth sensor [19] has a sensing range of 0.3m to 10.0m and returns a near-zero depth value in disoccluded areas, we set the threshold, threshdis , to 0.25. In addition, because the depth sensor typically returns temporally stable depth values but becomes unstable in reflective areas, we empirically set the error threshold, errref , to 0.9925.\n\nDefines activation functions to mitigate inaccuracies in gaze distance estimation due to unreliable depth sensor readings in disocclusion and reflective regions.",
    "original_text": "The gaze distances from gaze-mapped depth in disocclusion regions and reflective material are unreliable for eye-gazedistance estimation. To avoid using unreliable gaze distances from gaze-mapped depth in eye-gaze distance estimation, we define activation functions for each environment as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere ▽ D is the normalized differentiation of the gaze distance from gaze-mapped depth, threshdis and errref are the minimum depth value considered as reliable and the maximum error of depth sensor in the reflection, respectively. Because the active depth sensor [19] has a sensing range of 0.3m to 10.0m and returns a near-zero depth value in disoccluded areas, we set the threshold, threshdis , to 0.25. In addition, because the depth sensor typically returns temporally stable depth values but becomes unstable in reflective areas, we empirically set the error threshold, errref , to 0.9925.",
    "context": "Defines activation functions to mitigate inaccuracies in gaze distance estimation due to unreliable depth sensor readings in disocclusion and reflective regions.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      4
    ],
    "id": "a6317ec53c6bcb169e68ed93a4b4b1336beb398a0a242b3a3b371a93dc564cdb"
  },
  {
    "text": "Human errors such as blinking can be transferred to the gaze distance from vergence in the form of peak noises. To avoid utilizing such noisy gaze distances from vergence for eyegaze distance estimation, we design a confidence measure of gaze distance from vergence as\n<!-- formula-not-decoded -->\nwhere ▽ ˆ V is a normalized differentiation of the gaze distance from vergence. In addition, the gaze distance from gazemapped depth may have oscillation according to the environments. To avoid relying on noisy gaze distance from gazemapped depth, we design a confidence measure for the gaze distance from gaze-mapped depth as\n<!-- formula-not-decoded -->\nwhere ▽ D is a normalized differentiation of the gaze distance from gaze-mapped depth. Because the gaze distance from gaze-mapped depth remains temporally consistent when fixating on the same object, we apply temporal consistency to the confidence of gaze distance from gaze-mapped depth as\n<!-- formula-not-decoded -->\nwhere d T conf and t depth denote the gaze-mapped depth confidence at time T and its temporal weight, respectively.\nBased on the gaze distance from vergence confidence and that from the gaze-mapped depth confidence, we estimate the eye-gaze distance by weighting each cue as follows:\n<!-- formula-not-decoded -->\nNote that because temporal consistency is applied to both the pupil diameter used for refining the gaze distance from vergence and the gaze-mapped depth confidence, the estimated eye-gaze distance also remains temporally consistent.\n\nDetails the development of confidence measures to mitigate noise in gaze distance estimation, incorporating temporal consistency for both vergence and gaze-mapped depth cues.",
    "original_text": "Human errors such as blinking can be transferred to the gaze distance from vergence in the form of peak noises. To avoid utilizing such noisy gaze distances from vergence for eyegaze distance estimation, we design a confidence measure of gaze distance from vergence as\n<!-- formula-not-decoded -->\nwhere ▽ ˆ V is a normalized differentiation of the gaze distance from vergence. In addition, the gaze distance from gazemapped depth may have oscillation according to the environments. To avoid relying on noisy gaze distance from gazemapped depth, we design a confidence measure for the gaze distance from gaze-mapped depth as\n<!-- formula-not-decoded -->\nwhere ▽ D is a normalized differentiation of the gaze distance from gaze-mapped depth. Because the gaze distance from gaze-mapped depth remains temporally consistent when fixating on the same object, we apply temporal consistency to the confidence of gaze distance from gaze-mapped depth as\n<!-- formula-not-decoded -->\nwhere d T conf and t depth denote the gaze-mapped depth confidence at time T and its temporal weight, respectively.\nBased on the gaze distance from vergence confidence and that from the gaze-mapped depth confidence, we estimate the eye-gaze distance by weighting each cue as follows:\n<!-- formula-not-decoded -->\nNote that because temporal consistency is applied to both the pupil diameter used for refining the gaze distance from vergence and the gaze-mapped depth confidence, the estimated eye-gaze distance also remains temporally consistent.",
    "context": "Details the development of confidence measures to mitigate noise in gaze distance estimation, incorporating temporal consistency for both vergence and gaze-mapped depth cues.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      5
    ],
    "id": "0b5ad2fedd5a2228d299860b6f31034ea3d09ab985b6c157f8b88af79611ae72"
  },
  {
    "text": "In this research, we did not solely focus on comparing the optimal gaze distance estimation accuracy achievable under perfect experimental conditions. Rather, we performed experiments to quantitatively and qualitatively evaluate the accuracy and stability of the proposed hybrid method in more realistic settings, taking into account various potential errors, in comparison to traditional single-method approaches. For this purpose, we arranged the experimental setup as shown in Figure 5. We set up three gaze targets at different distances (0.5m, 1.0m, 1.3m), labeled as near, middle, and far targets, respectively. Specifically, to examine the human error aspect of the existing vergence angle-based gaze distance estimation technique, participants were asked to gaze at each target for several seconds to encourage natural eye blinking and relaxation. To assess errors from external environmental factors\nIEEE Access\nFIGURE 5. Experimental environments setup for the evaluation. Chin holder is used to fixate head. Three targets with different markers are placed in front of the chin holder at 0.5m, 1.0m, and 1.3m. Additional marker is displayed on the laptop with reflective screen.\nin the gaze-mapped depth-based gaze distance estimation approach, a laptop with a reflective screen was placed next to the middle target, and the near and far targets were arranged to overlap, creating non-occluded areas. Furthermore, all experiments were carried out with twelve participants under identical conditions, setting the temporal consistency weights for pupil and depth, t pupil and t depth as 0.95. A chin rest was also utilized to prevent unintended gaze distance variations due to user posture.\nFIGURE 6. Three experimental scenarios; S1: gazing at the near target only, S2: gazing at the middle target and the reflective screen located at the same distance alternately, and S3: gazing at the far target and disocclusion alternately.\n\nDetails the experimental setup and conditions for evaluating gaze distance estimation accuracy in realistic settings, considering potential errors and participant variability.",
    "original_text": "In this research, we did not solely focus on comparing the optimal gaze distance estimation accuracy achievable under perfect experimental conditions. Rather, we performed experiments to quantitatively and qualitatively evaluate the accuracy and stability of the proposed hybrid method in more realistic settings, taking into account various potential errors, in comparison to traditional single-method approaches. For this purpose, we arranged the experimental setup as shown in Figure 5. We set up three gaze targets at different distances (0.5m, 1.0m, 1.3m), labeled as near, middle, and far targets, respectively. Specifically, to examine the human error aspect of the existing vergence angle-based gaze distance estimation technique, participants were asked to gaze at each target for several seconds to encourage natural eye blinking and relaxation. To assess errors from external environmental factors\nIEEE Access\nFIGURE 5. Experimental environments setup for the evaluation. Chin holder is used to fixate head. Three targets with different markers are placed in front of the chin holder at 0.5m, 1.0m, and 1.3m. Additional marker is displayed on the laptop with reflective screen.\nin the gaze-mapped depth-based gaze distance estimation approach, a laptop with a reflective screen was placed next to the middle target, and the near and far targets were arranged to overlap, creating non-occluded areas. Furthermore, all experiments were carried out with twelve participants under identical conditions, setting the temporal consistency weights for pupil and depth, t pupil and t depth as 0.95. A chin rest was also utilized to prevent unintended gaze distance variations due to user posture.\nFIGURE 6. Three experimental scenarios; S1: gazing at the near target only, S2: gazing at the middle target and the reflective screen located at the same distance alternately, and S3: gazing at the far target and disocclusion alternately.",
    "context": "Details the experimental setup and conditions for evaluating gaze distance estimation accuracy in realistic settings, considering potential errors and participant variability.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      5
    ],
    "id": "7d87c624c373aac3ae7ec885243685b8017fc8ca21faa6f84e5d22498cf404f4"
  },
  {
    "text": "To verify the stability and accuracy of the proposed method against the aforementioned human and environmental factors, we designed three scenarios of gaze fixation change (S1, S2,\nIEEE Access\nTABLE 1. Average accuracy of the proposed method for each scenario with Euclidean distance and angular difference metrics.\n\nA, Section/Scenario. = S1. A, Vergence [8].mm = 33.678. A, Vergence [8].deg = 0.248. A, Gazed-Depth [19].mm = 26.658. A, Gazed-Depth [19].deg = 0.185. A, Proposed.mm = 23.861. A, Proposed.deg = 0.1777. B, Section/Scenario. = S2. B, Vergence [8].mm = 65.410. B, Vergence [8].deg = 0.205. B, Gazed-Depth [19].mm = 417.046. B, Gazed-Depth [19].deg = 0.987. B, Proposed.mm = 64.807. B, Proposed.deg = 0.199. C, Section/Scenario. = S3. C, Vergence [8].mm = 77.587. C, Vergence [8].deg = 0.108. C, Gazed-Depth [19].mm = 484.946. C, Gazed-Depth [19].deg = 0.280. C, Proposed.mm = 111.133. C, Proposed.deg = 0.141. D, Section/Scenario. = S1. D, Vergence [8].mm = 109.184. D, Vergence [8].deg = 0.628. D, Gazed-Depth [19].mm = 16.583. D, Gazed-Depth [19].deg = 0.114. D, Proposed.mm = 56.801. D, Proposed.deg = 0.210. E, Section/Scenario. = S2. E, Vergence [8].mm = 152.996. E, Vergence [8].deg = 0.531. E, Gazed-Depth [19].mm = 481.143. E, Gazed-Depth [19].deg = 1.267. E, Proposed.mm = 37.421. E, Proposed.deg = 0.132. Overall, Section/Scenario. = Overall. Overall, Vergence [8].mm = 90.867. Overall, Vergence [8].deg = 0.312. Overall, Gazed-Depth [19].mm = 352.98. Overall, Gazed-Depth [19].deg = 0.635. Overall, Proposed.mm = 66.868. Overall, Proposed.deg = 0.167\nS3) as shown in Fig. 6. S1 is a test scenario designed to evaluate the impact of gaze distance estimation errors that may occur due to human factors, such as peak noise from blinking or drift caused by eye relaxation. In this scenario, the user is instructed to continuously gaze at the near target for 8 seconds. S2 and S3 are test scenarios intended to assess the impact of environmental factors on gaze distance estimation errors. In S2, the user is instructed to gaze at the middle target, then shift their gaze to the laptop located at the same distance for 7 seconds, and finally return to the middle target. This scenario is designed to examine the influence of the laptop's reflective surface on gaze estimation. In S3, the user is instructed to gaze at the far target, initially focusing on the center of the target and then shifting their gaze to the lower right corner of the same target for 8 seconds. Most commercial depth cameras use infrared (IR) pattern light, and when multiple objects overlap in the captured scene, the object closer to the camera may block the pattern light, creating disoccluded areas on the more distant object, making depth estimation challenging. The S3 scenario is designed to evaluate the impact of these disoccluded regions. The three test scenarios mentioned were applied in a continuous experimental sequence in the order of S1-S2-S3-S1-S2.\n\nPresents experimental scenarios (S1-S3) designed to test the method's stability and accuracy under varying human and environmental factors, including gaze distance estimation errors and disocclusion challenges.",
    "original_text": "To verify the stability and accuracy of the proposed method against the aforementioned human and environmental factors, we designed three scenarios of gaze fixation change (S1, S2,\nIEEE Access\nTABLE 1. Average accuracy of the proposed method for each scenario with Euclidean distance and angular difference metrics.\n\nA, Section/Scenario. = S1. A, Vergence [8].mm = 33.678. A, Vergence [8].deg = 0.248. A, Gazed-Depth [19].mm = 26.658. A, Gazed-Depth [19].deg = 0.185. A, Proposed.mm = 23.861. A, Proposed.deg = 0.1777. B, Section/Scenario. = S2. B, Vergence [8].mm = 65.410. B, Vergence [8].deg = 0.205. B, Gazed-Depth [19].mm = 417.046. B, Gazed-Depth [19].deg = 0.987. B, Proposed.mm = 64.807. B, Proposed.deg = 0.199. C, Section/Scenario. = S3. C, Vergence [8].mm = 77.587. C, Vergence [8].deg = 0.108. C, Gazed-Depth [19].mm = 484.946. C, Gazed-Depth [19].deg = 0.280. C, Proposed.mm = 111.133. C, Proposed.deg = 0.141. D, Section/Scenario. = S1. D, Vergence [8].mm = 109.184. D, Vergence [8].deg = 0.628. D, Gazed-Depth [19].mm = 16.583. D, Gazed-Depth [19].deg = 0.114. D, Proposed.mm = 56.801. D, Proposed.deg = 0.210. E, Section/Scenario. = S2. E, Vergence [8].mm = 152.996. E, Vergence [8].deg = 0.531. E, Gazed-Depth [19].mm = 481.143. E, Gazed-Depth [19].deg = 1.267. E, Proposed.mm = 37.421. E, Proposed.deg = 0.132. Overall, Section/Scenario. = Overall. Overall, Vergence [8].mm = 90.867. Overall, Vergence [8].deg = 0.312. Overall, Gazed-Depth [19].mm = 352.98. Overall, Gazed-Depth [19].deg = 0.635. Overall, Proposed.mm = 66.868. Overall, Proposed.deg = 0.167\nS3) as shown in Fig. 6. S1 is a test scenario designed to evaluate the impact of gaze distance estimation errors that may occur due to human factors, such as peak noise from blinking or drift caused by eye relaxation. In this scenario, the user is instructed to continuously gaze at the near target for 8 seconds. S2 and S3 are test scenarios intended to assess the impact of environmental factors on gaze distance estimation errors. In S2, the user is instructed to gaze at the middle target, then shift their gaze to the laptop located at the same distance for 7 seconds, and finally return to the middle target. This scenario is designed to examine the influence of the laptop's reflective surface on gaze estimation. In S3, the user is instructed to gaze at the far target, initially focusing on the center of the target and then shifting their gaze to the lower right corner of the same target for 8 seconds. Most commercial depth cameras use infrared (IR) pattern light, and when multiple objects overlap in the captured scene, the object closer to the camera may block the pattern light, creating disoccluded areas on the more distant object, making depth estimation challenging. The S3 scenario is designed to evaluate the impact of these disoccluded regions. The three test scenarios mentioned were applied in a continuous experimental sequence in the order of S1-S2-S3-S1-S2.",
    "context": "Presents experimental scenarios (S1-S3) designed to test the method's stability and accuracy under varying human and environmental factors, including gaze distance estimation errors and disocclusion challenges.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      5,
      6
    ],
    "id": "5ec191def5ee5f64478896bc1a91b93aa2c0b5156889553208df4c174155d80c"
  },
  {
    "text": "Initially, the experimental results comparing the impact of imposing a pupil diameter constraint on refining gaze distance estimations due to human error using the vergence are depicted in Fig. 7. In the plot, the method without the pupil diameter constraint, detailed in Eq. 1, is shown by a blue line; the method including the pupil diameter constraint, detailed in Eq. 3, by a red line; and the ground truth by a green line. The findings indicate that enforcing the pupil diameter constraint effectively mitigates peak noise caused by eye blinks across all segments and substantially reduces drift noise due to eye relaxation in segments A and D, where a gaze fixation is sustained. However, some deviations between the refined and the true values are noticeable in segments B, C, and E. These discrepancies could be due to variations in initial gaze distance estimates following gaze shifts, indicating inherent limitations in the precision of this method.\nSimilarly, the experimental findings comparing the ef-\ngaze distance from vergence gaze distance from vergence W pupil constraint)\nground truth\nFIGURE 7. Comparison of the gaze distance from vergence with/without the pupil diameter constraint. Gaze distance from vergence with pupil diameter constraint successively suppresses both drift noise caused by eye relaxation and peak noise caused by eye blinks.\nfectiveness of the proposed noise filter in mitigating gaze distance estimation errors due to environmental factors using gaze-mapped depth are depicted in Fig. 8. Particularly, Fig. 8(a) illustrates the efficiency of the proposed noise filter in detecting noise when gazing at disocclusion regions, while Fig. 8 shows its performance in identifying noise when gazing at reflective surfaces. Across the graphs, the gaze distance estimates based on gaze-mapped depth are denoted by red lines, and the estimation errors due to disoccluded regions and reflective surfaces are overlaid with green dotted lines and blue dotted lines, respectively. The findings indicate that the proposed gaze-mapped depth noise filter effectively identifies depth estimation errors caused by environmental factors. This implies that selectively employing gaze-mapped depth-based gaze distance estimation methods based on their confidence can result in highly accurate gaze distance measurements.\nFigure 9 presents a qualitative comparison of the perfor-\nIEEE Access\nFIGURE 8. Results of the proposed gaze-mapped depth noise filters described in Sec. III-B. Shading describes the frames of which the filters detected noise; (a) disocclusion and (b) reflection.\nmance of the proposed hybrid method. To aid understanding, representative data are overlaid in Fig. 9(a) for each of the tests S1, S2, and S3, including the user's gaze coordinates, estimated gaze distance, and the positions of both pupils and the depth map acquired by the depth camera. Figure 9(b) indicates the confidence of vergence-based and gaze-mapped depth-based gaze distance estimates at each timestamp, along with the weights assigned to these estimates in computing the final gaze distance estimation from Eq. 10. Figure 9(c) shows the final ground truth gaze distance values (green solid line), vergence-based gaze distance estimates (blue solid line), gaze-mapped depth-based gaze distance estimates (orange solid line), and the proposed hybrid method's estimates (red solid line). The graphs reveal that the proposed hybrid method remains robust against drift and peak noise in segments corresponding to S1, which commonly suffer from human error, and it also performs reliably in segments corresponding to S2 and S3, which are susceptible to depth estimation errors due to environmental factors like reflective surfaces and occlusion areas. Ultimately, the proposed hybrid method effectively mitigates each type of noise in challenging conditions, yielding estimates close to the ground truth values.\nTo quantitatively evaluate the proposed hybrid approach, we utilized the Euclidean distance and angular difference metrics [7], which are defined as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere Gest , Ggt , and O are the estimated gaze point, ground truth, and origin, represented as ( xest , yest , zest ) , ( xgt , ygt , zgt ) , and (0 , 0 , 0) , respectively.\nCommercial eye trackers typically perform with a visual angle error ranging from 0.5 to 1 degree. Despite the difficulty of fair evaluation due to differing experimental setups, considering the challenging test conditions discussed in this paper, Table 1 illustrates that the proposed method demonstrates superior quantitative performance comparable to existing methods and commercial products. Nevertheless, the primary aim of this study is to validate the adequate performance and stability of the proposed method under practically challenging circumstances. Within this context, the results of the quantitative performance assessment reveal several important insights. First, as observed in Segments A, B, and E, the proposed hybrid method does not solely choose the more reliable value from the two existing methods running in parallel. Instead, it cross-references the values estimated by each method to produce a more precise final estimation. This feature closely mimics the human visual perception mechanism previously discussed. Secondly, as seen in Segments C and D, unless one of the existing methods experiences a severe error, the proposed method remains resilient against errors even in various demanding environments, achieving an accuracy of up to 0.132 degrees. Lastly, even when severe errors occur in one of the two existing methods, as seen in Segments C and D, the proposed method effectively manages these errors, maintaining a moderate accuracy of approximately 0.14 to 0.21 degrees.\n\nEvaluates the hybrid method's performance, demonstrating its ability to mitigate noise and errors in challenging conditions, particularly by cross-referencing estimates from multiple gaze estimation methods.",
    "original_text": "Initially, the experimental results comparing the impact of imposing a pupil diameter constraint on refining gaze distance estimations due to human error using the vergence are depicted in Fig. 7. In the plot, the method without the pupil diameter constraint, detailed in Eq. 1, is shown by a blue line; the method including the pupil diameter constraint, detailed in Eq. 3, by a red line; and the ground truth by a green line. The findings indicate that enforcing the pupil diameter constraint effectively mitigates peak noise caused by eye blinks across all segments and substantially reduces drift noise due to eye relaxation in segments A and D, where a gaze fixation is sustained. However, some deviations between the refined and the true values are noticeable in segments B, C, and E. These discrepancies could be due to variations in initial gaze distance estimates following gaze shifts, indicating inherent limitations in the precision of this method.\nSimilarly, the experimental findings comparing the ef-\ngaze distance from vergence gaze distance from vergence W pupil constraint)\nground truth\nFIGURE 7. Comparison of the gaze distance from vergence with/without the pupil diameter constraint. Gaze distance from vergence with pupil diameter constraint successively suppresses both drift noise caused by eye relaxation and peak noise caused by eye blinks.\nfectiveness of the proposed noise filter in mitigating gaze distance estimation errors due to environmental factors using gaze-mapped depth are depicted in Fig. 8. Particularly, Fig. 8(a) illustrates the efficiency of the proposed noise filter in detecting noise when gazing at disocclusion regions, while Fig. 8 shows its performance in identifying noise when gazing at reflective surfaces. Across the graphs, the gaze distance estimates based on gaze-mapped depth are denoted by red lines, and the estimation errors due to disoccluded regions and reflective surfaces are overlaid with green dotted lines and blue dotted lines, respectively. The findings indicate that the proposed gaze-mapped depth noise filter effectively identifies depth estimation errors caused by environmental factors. This implies that selectively employing gaze-mapped depth-based gaze distance estimation methods based on their confidence can result in highly accurate gaze distance measurements.\nFigure 9 presents a qualitative comparison of the perfor-\nIEEE Access\nFIGURE 8. Results of the proposed gaze-mapped depth noise filters described in Sec. III-B. Shading describes the frames of which the filters detected noise; (a) disocclusion and (b) reflection.\nmance of the proposed hybrid method. To aid understanding, representative data are overlaid in Fig. 9(a) for each of the tests S1, S2, and S3, including the user's gaze coordinates, estimated gaze distance, and the positions of both pupils and the depth map acquired by the depth camera. Figure 9(b) indicates the confidence of vergence-based and gaze-mapped depth-based gaze distance estimates at each timestamp, along with the weights assigned to these estimates in computing the final gaze distance estimation from Eq. 10. Figure 9(c) shows the final ground truth gaze distance values (green solid line), vergence-based gaze distance estimates (blue solid line), gaze-mapped depth-based gaze distance estimates (orange solid line), and the proposed hybrid method's estimates (red solid line). The graphs reveal that the proposed hybrid method remains robust against drift and peak noise in segments corresponding to S1, which commonly suffer from human error, and it also performs reliably in segments corresponding to S2 and S3, which are susceptible to depth estimation errors due to environmental factors like reflective surfaces and occlusion areas. Ultimately, the proposed hybrid method effectively mitigates each type of noise in challenging conditions, yielding estimates close to the ground truth values.\nTo quantitatively evaluate the proposed hybrid approach, we utilized the Euclidean distance and angular difference metrics [7], which are defined as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere Gest , Ggt , and O are the estimated gaze point, ground truth, and origin, represented as ( xest , yest , zest ) , ( xgt , ygt , zgt ) , and (0 , 0 , 0) , respectively.\nCommercial eye trackers typically perform with a visual angle error ranging from 0.5 to 1 degree. Despite the difficulty of fair evaluation due to differing experimental setups, considering the challenging test conditions discussed in this paper, Table 1 illustrates that the proposed method demonstrates superior quantitative performance comparable to existing methods and commercial products. Nevertheless, the primary aim of this study is to validate the adequate performance and stability of the proposed method under practically challenging circumstances. Within this context, the results of the quantitative performance assessment reveal several important insights. First, as observed in Segments A, B, and E, the proposed hybrid method does not solely choose the more reliable value from the two existing methods running in parallel. Instead, it cross-references the values estimated by each method to produce a more precise final estimation. This feature closely mimics the human visual perception mechanism previously discussed. Secondly, as seen in Segments C and D, unless one of the existing methods experiences a severe error, the proposed method remains resilient against errors even in various demanding environments, achieving an accuracy of up to 0.132 degrees. Lastly, even when severe errors occur in one of the two existing methods, as seen in Segments C and D, the proposed method effectively manages these errors, maintaining a moderate accuracy of approximately 0.14 to 0.21 degrees.",
    "context": "Evaluates the hybrid method's performance, demonstrating its ability to mitigate noise and errors in challenging conditions, particularly by cross-referencing estimates from multiple gaze estimation methods.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      6,
      7
    ],
    "id": "d5948d204d107a412b6ec54bab23b45770f82d12d2d8473c92c3510d3582fd18"
  },
  {
    "text": "This paper proposed a novel hybrid gaze distance estimation method to overcome the limitations of existing approaches. Typically, gaze distance estimation in wearable XR devices is mainly based on binocular vergence angles. However, this approach is highly prone to errors induced by human factors (e.g., eye blinks, pupil dilation). As an alternative, a\nIEEE Access\nFIGURE 9. Qualitative comparison of the gaze distances estimated by vergence, gaze-mapped depth, and the proposed hybrid method with ground truth. (a) describes gaze transitions for each scenario with pupil detection results. (b) and (c) depict the results of the proposed confidence measures and the estimated gaze distance values, respectively.\nmethod integrating depth estimation techniques from computer vision with 2D gaze point tracking to utilize gazemapped depth information can be considered. However, this method often encounters difficulties in accurately measuring depth due to external environmental factors (e.g. disocclusion regions, reflective surfaces). To tackle these challenges, the introduced method conducts parallel gaze distance estimations: one based on binocular vergence and the other on gaze-mapped depth. It subsequently assesses the confidence of each approach and calculates an adaptive weighted sum through cross-referencing to derive an optimal gaze distance estimate. Qualitative and quantitative evaluations indicate that the proposed method is robust against errors arising from human factors and environmental conditions, which are the inherent drawbacks of existing methods. Additionally, it is evidenced that the proposed technique does not simply select the most reliable value from the two existing methods but crossreferences the estimated values to yield a more precise final estimate. The introduced method can be directly applied to developing user gaze distance estimation modules for varifocal AR/VR devices, aiming for an ideal mixed reality experience without VAC. In addition, it has potential applications in diverse fields, including digital healthcare, human-computer interaction, and analyzing user behavior and preferences.\n\nIntroduces a hybrid gaze distance estimation method combining vergence and gaze-mapped depth to improve accuracy and robustness compared to existing approaches.",
    "original_text": "This paper proposed a novel hybrid gaze distance estimation method to overcome the limitations of existing approaches. Typically, gaze distance estimation in wearable XR devices is mainly based on binocular vergence angles. However, this approach is highly prone to errors induced by human factors (e.g., eye blinks, pupil dilation). As an alternative, a\nIEEE Access\nFIGURE 9. Qualitative comparison of the gaze distances estimated by vergence, gaze-mapped depth, and the proposed hybrid method with ground truth. (a) describes gaze transitions for each scenario with pupil detection results. (b) and (c) depict the results of the proposed confidence measures and the estimated gaze distance values, respectively.\nmethod integrating depth estimation techniques from computer vision with 2D gaze point tracking to utilize gazemapped depth information can be considered. However, this method often encounters difficulties in accurately measuring depth due to external environmental factors (e.g. disocclusion regions, reflective surfaces). To tackle these challenges, the introduced method conducts parallel gaze distance estimations: one based on binocular vergence and the other on gaze-mapped depth. It subsequently assesses the confidence of each approach and calculates an adaptive weighted sum through cross-referencing to derive an optimal gaze distance estimate. Qualitative and quantitative evaluations indicate that the proposed method is robust against errors arising from human factors and environmental conditions, which are the inherent drawbacks of existing methods. Additionally, it is evidenced that the proposed technique does not simply select the most reliable value from the two existing methods but crossreferences the estimated values to yield a more precise final estimate. The introduced method can be directly applied to developing user gaze distance estimation modules for varifocal AR/VR devices, aiming for an ideal mixed reality experience without VAC. In addition, it has potential applications in diverse fields, including digital healthcare, human-computer interaction, and analyzing user behavior and preferences.",
    "context": "Introduces a hybrid gaze distance estimation method combining vergence and gaze-mapped depth to improve accuracy and robustness compared to existing approaches.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      8,
      7
    ],
    "id": "861372246dc73e55716b93740dff088eed9b6d9349f16e2d7e692ff988f24ef5"
  },
  {
    "text": "- [1] D. M Hoffman, A. R Girshick, K. Akeley, and M. S Banks. ''Vergenceaccommdation conflicts hinder visual performance and cause visual fatigue,'' Journal of vision , vol.8, no.3, pp. 33-33, 2008.\n- [2] A. K. Bhowmick, A. Jamali, D. Bryant, S. Pintz, and P. J. Bos. ''Liquid Crystal Based 5 cm Adaptive Focus Lens to Solve AccommodationConvergence (AC) Mismatch Issue of AR/VR/3D Displays.'' Society of Information Display (SID) , June 28, 2021.\n- [3] A. K. Bhowmick, A. Jamali, D. Bryant, S. Pintz, S. and P. J. Bos. ''Optical performance characterization of 5 cm aperture size continuous focus tunable liquid crystal lens for resolving Accommodation-Convergence mismatch conflict of AR/VR/3D HMDs.'' In Dig. Tech. Pap.-Soc. Inf. Disp. Int. Symp . Vol. 53, no. 1, pp. 166-169, 2022.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\nIEEE Access\n- [4] D. Dunn, C. Tippets, K. Torell, H. Fuchs, P. Kellnhofer, K. Myszkowski, ... and D. Luebke. ''Membrane AR: varifocal, wide field of view augmented reality display from deformable membranes.'' In ACM SIGGRAPH 2017 Emerging Technologies , pp. 1-2, 2017.\n- [5] S. G. Park, K. Kim, and J. Ha, ''T-Glasses: Light-Weight Hight Efficient Augmented Reality Smart Glasses.'' Proceedings of the International Display Workshops , 2022.\n- [6] Y. M. Kwon, K. W. Jeon, J. Ki, Q. M. Shahab, S. Jo, and S. K. Kim. ''3d gaze estimation and interaction to stereo display,'' International Journal of Virtual Reality , vol.5, no.3, pp. 41-45, 2006.\n- [7] E. G. Mlot, H. Bahmani, S. Wahl, and E. Kasneci. ''3d gaze estimation using eye vergence,'' International Conference on Health Informatics , vol.6, pp. 125-131, Scitepress, 2016.\n- [8] M. Kassner, W. Patera, and A. Bulling. ''Pupil: an open source platform for pervasive eye tracking and mobile gaze-based interaction.\", In Proceedings of the 2014 ACM international joint conference on pervasive and ubiquitous computing: Adjunct publication , pp. 1151-1160, 2014.\n- [9] C. Elmadjian, P. Shukla, A. D. Tula, and C. H. Morimoto. ''3D gaze estimation in the scene volume with a head-mounted eye tracker,'' In Proceedings of the Workshop on Communcation by Gaze Interaction , pp. 1-9, 2018.\n- [10] M. Liu, Y. Li, and H. Liu. ''Robust 3-D gaze estimation via data optimization and saliency aggregation for mobile eye-tracking systems.'' IEEE Transactions on Instrumentation and Measurement , 70, pp. 1-10, 2021.\n- [11] M. Liu, Y. Li, and H. Liu. ''3d gaze estimation for head-mounted devices based on visual saliency.'' In IEEE RSJ International Conference on Intelligent Robots and Systems(IROS) , pp. 10611-10616, 2020.\n- [12] M. Lambooij, W. IJsselsteijn, M. Fortuin, and I. Heynderickx. ''Visual discomfort and visual fatigue of stereoscopic displays: a review.'' Journal of imaging science and technology , vol.53, no.3, 30201-1, 2009.\n- [13] P. Linton. ''Does vision extract absolute distance from vergence?'' Attention, Perception, & Psychophysics , vol.82, no.6, pp. 3176-3195, 2020.\n- [14] J. W. Lee, C. W. Cho, K. Y. Shin, E. C. Lee, and K. R. Park. ''3D gaze tracking method using Purkinje images on eye optimal and pupil.'' Optics and Lasers in Engineering , vol.50, no.5, pp. 736-751, 2021.\n- [15] W. Fuhl, T. Kübler, K. Sippel, W. Rosenstiel, and E. Rosenstiel. ''Excuse: Robust pupil detection in real-world scenarios.'' In Computer Analysis of Images and Patterns: 16th International Conference, CAIP 2015, Valletta, Malta, September 2-4, Proceedings, Part I , pp. 39-51, Springer International Publishing, 2015.\n- [16] Y. Lee, C. Shin, A. Plopski, Y. Itoh, T. Piumsomboon, A. Dey, ... , and M. Billinghurst, ''Estimating gaze depth using multi-layer perceptron.'' In IEEE 2017 International Symposium on Ubiquitous Virtual Reality (ISUVR) , pp. 26-29, 2017.\n- [17] J. Harel, C. Koch, and P. Perona. ''Graph-based visual saliency.'' Advances in neural information processing systems , 2006.\n- [18] D. Gálvez-López, and J. D. Tardos. ''Bags of binary words for fast place recognition in image sequences.\" IEEE Transactions on robotics , vol.28, no.5, pp. 1188-1197, 2012.\n- [19] L. Keselman, W. J., Iselin, A. G.- Jepsen, and A. Bhowmik. ''Intel realsense stereoscopic depth cameras.'' In Proceedings of the IEEE conference on computer vision and pattern recognition workshops , pp. 1-10, 2017.\nDAE-YONG CHO received his B.S. degree in Intelligent Systems from the Department of Robotics at Kwangwoon University, South Korea, in 2014. In 2016, he earned a M.S. degree from the School of Information and Communications Engineering at the Gwangju Institute of Science and Technology (GIST). He worked as a Assistant Researcher at ADAS ONE, Inc., Seoul, South Korea, for approximately two years. From 2019, he served as a researcher at the Korea Institute of Science and\nTechnology (KIST) for about thee years. Currently, he is pursuing a Ph.D. in the KHU-KIST Department of Converging Science and Technology at Kyung Hee University, South Korea. His current research interests include smart glasses, focusing on the integration of augmented reality and artificial intelligence technologies for enhanced user interaction and accessibility in various applications, such as real-time information display, health monitoring, and assistive technology for visually impaired users.\nMIN-KOO KANG received his B.S. degree in Electronic Engineering from Inha University, South Korea, in 2008, and his M.S. degree in Electrical Engineering and Ph.D. degree in Information and Communications Engineering from the Gwangju Institute of Science and Technology (GIST) in 2010 and 2015, respectively. He worked as a Postdoctoral Researcher at the Center for Imaging Media Research, Korea Institute of Science and Technology (KIST), Seoul, South\nKorea, for three years, and has been a Principal Researcher at the Intelligent · Interaction Research Center since 2018. In addition, he has been an Adjunct Professor at Korea University since 2020 and at Kyung Hee University since 2023. His current research interests include telepresence, holoporation, remote monitoring and pilot technologies that transcend space and time; integrated visual/perceptual augmentation glass technology to enhance human capabilities; and other areas such as user behavior and intent analysis, digital healthcare, and human-computer interaction technologies.\n\nThe document focuses on research related to augmented and virtual reality displays, specifically addressing issues of visual discomfort, accommodation-convergence mismatch, and gaze tracking. It details various approaches to improving display technology, including liquid crystal lenses, smart glasses, and pupil detection, alongside research into the underlying mechanisms of visual perception and depth estimation.",
    "original_text": "- [1] D. M Hoffman, A. R Girshick, K. Akeley, and M. S Banks. ''Vergenceaccommdation conflicts hinder visual performance and cause visual fatigue,'' Journal of vision , vol.8, no.3, pp. 33-33, 2008.\n- [2] A. K. Bhowmick, A. Jamali, D. Bryant, S. Pintz, and P. J. Bos. ''Liquid Crystal Based 5 cm Adaptive Focus Lens to Solve AccommodationConvergence (AC) Mismatch Issue of AR/VR/3D Displays.'' Society of Information Display (SID) , June 28, 2021.\n- [3] A. K. Bhowmick, A. Jamali, D. Bryant, S. Pintz, S. and P. J. Bos. ''Optical performance characterization of 5 cm aperture size continuous focus tunable liquid crystal lens for resolving Accommodation-Convergence mismatch conflict of AR/VR/3D HMDs.'' In Dig. Tech. Pap.-Soc. Inf. Disp. Int. Symp . Vol. 53, no. 1, pp. 166-169, 2022.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\nIEEE Access\n- [4] D. Dunn, C. Tippets, K. Torell, H. Fuchs, P. Kellnhofer, K. Myszkowski, ... and D. Luebke. ''Membrane AR: varifocal, wide field of view augmented reality display from deformable membranes.'' In ACM SIGGRAPH 2017 Emerging Technologies , pp. 1-2, 2017.\n- [5] S. G. Park, K. Kim, and J. Ha, ''T-Glasses: Light-Weight Hight Efficient Augmented Reality Smart Glasses.'' Proceedings of the International Display Workshops , 2022.\n- [6] Y. M. Kwon, K. W. Jeon, J. Ki, Q. M. Shahab, S. Jo, and S. K. Kim. ''3d gaze estimation and interaction to stereo display,'' International Journal of Virtual Reality , vol.5, no.3, pp. 41-45, 2006.\n- [7] E. G. Mlot, H. Bahmani, S. Wahl, and E. Kasneci. ''3d gaze estimation using eye vergence,'' International Conference on Health Informatics , vol.6, pp. 125-131, Scitepress, 2016.\n- [8] M. Kassner, W. Patera, and A. Bulling. ''Pupil: an open source platform for pervasive eye tracking and mobile gaze-based interaction.\", In Proceedings of the 2014 ACM international joint conference on pervasive and ubiquitous computing: Adjunct publication , pp. 1151-1160, 2014.\n- [9] C. Elmadjian, P. Shukla, A. D. Tula, and C. H. Morimoto. ''3D gaze estimation in the scene volume with a head-mounted eye tracker,'' In Proceedings of the Workshop on Communcation by Gaze Interaction , pp. 1-9, 2018.\n- [10] M. Liu, Y. Li, and H. Liu. ''Robust 3-D gaze estimation via data optimization and saliency aggregation for mobile eye-tracking systems.'' IEEE Transactions on Instrumentation and Measurement , 70, pp. 1-10, 2021.\n- [11] M. Liu, Y. Li, and H. Liu. ''3d gaze estimation for head-mounted devices based on visual saliency.'' In IEEE RSJ International Conference on Intelligent Robots and Systems(IROS) , pp. 10611-10616, 2020.\n- [12] M. Lambooij, W. IJsselsteijn, M. Fortuin, and I. Heynderickx. ''Visual discomfort and visual fatigue of stereoscopic displays: a review.'' Journal of imaging science and technology , vol.53, no.3, 30201-1, 2009.\n- [13] P. Linton. ''Does vision extract absolute distance from vergence?'' Attention, Perception, & Psychophysics , vol.82, no.6, pp. 3176-3195, 2020.\n- [14] J. W. Lee, C. W. Cho, K. Y. Shin, E. C. Lee, and K. R. Park. ''3D gaze tracking method using Purkinje images on eye optimal and pupil.'' Optics and Lasers in Engineering , vol.50, no.5, pp. 736-751, 2021.\n- [15] W. Fuhl, T. Kübler, K. Sippel, W. Rosenstiel, and E. Rosenstiel. ''Excuse: Robust pupil detection in real-world scenarios.'' In Computer Analysis of Images and Patterns: 16th International Conference, CAIP 2015, Valletta, Malta, September 2-4, Proceedings, Part I , pp. 39-51, Springer International Publishing, 2015.\n- [16] Y. Lee, C. Shin, A. Plopski, Y. Itoh, T. Piumsomboon, A. Dey, ... , and M. Billinghurst, ''Estimating gaze depth using multi-layer perceptron.'' In IEEE 2017 International Symposium on Ubiquitous Virtual Reality (ISUVR) , pp. 26-29, 2017.\n- [17] J. Harel, C. Koch, and P. Perona. ''Graph-based visual saliency.'' Advances in neural information processing systems , 2006.\n- [18] D. Gálvez-López, and J. D. Tardos. ''Bags of binary words for fast place recognition in image sequences.\" IEEE Transactions on robotics , vol.28, no.5, pp. 1188-1197, 2012.\n- [19] L. Keselman, W. J., Iselin, A. G.- Jepsen, and A. Bhowmik. ''Intel realsense stereoscopic depth cameras.'' In Proceedings of the IEEE conference on computer vision and pattern recognition workshops , pp. 1-10, 2017.\nDAE-YONG CHO received his B.S. degree in Intelligent Systems from the Department of Robotics at Kwangwoon University, South Korea, in 2014. In 2016, he earned a M.S. degree from the School of Information and Communications Engineering at the Gwangju Institute of Science and Technology (GIST). He worked as a Assistant Researcher at ADAS ONE, Inc., Seoul, South Korea, for approximately two years. From 2019, he served as a researcher at the Korea Institute of Science and\nTechnology (KIST) for about thee years. Currently, he is pursuing a Ph.D. in the KHU-KIST Department of Converging Science and Technology at Kyung Hee University, South Korea. His current research interests include smart glasses, focusing on the integration of augmented reality and artificial intelligence technologies for enhanced user interaction and accessibility in various applications, such as real-time information display, health monitoring, and assistive technology for visually impaired users.\nMIN-KOO KANG received his B.S. degree in Electronic Engineering from Inha University, South Korea, in 2008, and his M.S. degree in Electrical Engineering and Ph.D. degree in Information and Communications Engineering from the Gwangju Institute of Science and Technology (GIST) in 2010 and 2015, respectively. He worked as a Postdoctoral Researcher at the Center for Imaging Media Research, Korea Institute of Science and Technology (KIST), Seoul, South\nKorea, for three years, and has been a Principal Researcher at the Intelligent · Interaction Research Center since 2018. In addition, he has been an Adjunct Professor at Korea University since 2020 and at Kyung Hee University since 2023. His current research interests include telepresence, holoporation, remote monitoring and pilot technologies that transcend space and time; integrated visual/perceptual augmentation glass technology to enhance human capabilities; and other areas such as user behavior and intent analysis, digital healthcare, and human-computer interaction technologies.",
    "context": "The document focuses on research related to augmented and virtual reality displays, specifically addressing issues of visual discomfort, accommodation-convergence mismatch, and gaze tracking. It details various approaches to improving display technology, including liquid crystal lenses, smart glasses, and pupil detection, alongside research into the underlying mechanisms of visual perception and depth estimation.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      9
    ],
    "id": "6106058ab963a48f5653068fdc95d69b172640dd522caefcf15f7799fb8d8483"
  },
  {
    "text": "Received August 2, 2021, accepted September 16, 2021, date of publication September 27, 2021, date of current version October 6, 2021.\nDigital Object Identifier 10.1 109/ACCESS.2021.31 1591 1\n\nDetails the publication timeline and DOI for the article.",
    "original_text": "Received August 2, 2021, accepted September 16, 2021, date of publication September 27, 2021, date of current version October 6, 2021.\nDigital Object Identifier 10.1 109/ACCESS.2021.31 1591 1",
    "context": "Details the publication timeline and DOI for the article.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      1
    ],
    "id": "eef640352621390927f15e0344fc031b0d8eaf10d5d13cafa32dac406b49703c"
  },
  {
    "text": "Department of Computer Science and Engineering, Kyung Hee University, Yongin 17104, Republic of Korea\nCorresponding author: Sung-Ho Bae (shbae@khu.ac.kr)\nThis work was supported by the Technology Innovation Program or the Industrial Strategic Technology Development Program funded by the Ministry of Trade, Industry and Energy (MOTIE), South Korea (Memristor Fault-Aware Neuromorphic System for 3D Memristor Array) under Grant 10085646.\nABSTRACT Neural Architecture Search Without Training (NASWOT) has been proposed recently to replace the conventional Neural Architecture Search (NAS). Pioneer works only deploy one or two indicator(s) to search. Nevertheless, the quantitative assessment for indicators is not fully studied and evaluated. In this paper, we GLYPH<28>rst review several indicators, which are used to evaluate the network in a training-free manner, including the correlation of Jacobian, the output sensitivity, the number of linear regions, and the condition number of the neural tangent kernel. Our observation is that each indicator is responsible for characterizing a network in a speciGLYPH<28>c aspect and there is no single indicator that achieves good performance in all cases, e.g. highly correlated with the test accuracy. This motivated us to develop a novel indicator where all properties of a network are taken into account. To obtain better indicator that can consider various characteristics of networks in a harmonized form, we propose a Fusion Indicator (FI). SpeciGLYPH<28>cally, the proposed FI is formed by combining multiple indicators in a weighted sum manner. We minimize the mean squared error loss between the predicted and actual accuracy of networks to acquire the weights. Moreover, as the conventional training-free NAS researches used limited metrics to evaluate the quality of indicators, we introduce more desirable metrics that can evaluate the quality of training-free NAS indicator in terms of GLYPH<28>delity, correlation and rank-order similarity between the predicted quality value and actual accuracy of networks. That is, we introduce the Pearson Linear CoefGLYPH<28>cient Correlation (PLCC), the Root Mean Square Error (RMSE), the Spearman Rank-Order Correlation CoefGLYPH<28>cient (SROCC), and Kendall Rank-Order Correlation CoefGLYPH<28>cient (KROCC). Extensive experiments on NAS-Bench-101 and NAS-Bench201 demonstrate the effectiveness of our FI, outperforming existing methods by a large margin.\nINDEX TERMS Neural architecture search, training-free neural architecture search, fusion indicator, evaluation metrics.\n\nIntroduces a novel fusion indicator (FI) for evaluating training-free neural architecture search, incorporating multiple metrics and demonstrating improved performance compared to existing methods.",
    "original_text": "Department of Computer Science and Engineering, Kyung Hee University, Yongin 17104, Republic of Korea\nCorresponding author: Sung-Ho Bae (shbae@khu.ac.kr)\nThis work was supported by the Technology Innovation Program or the Industrial Strategic Technology Development Program funded by the Ministry of Trade, Industry and Energy (MOTIE), South Korea (Memristor Fault-Aware Neuromorphic System for 3D Memristor Array) under Grant 10085646.\nABSTRACT Neural Architecture Search Without Training (NASWOT) has been proposed recently to replace the conventional Neural Architecture Search (NAS). Pioneer works only deploy one or two indicator(s) to search. Nevertheless, the quantitative assessment for indicators is not fully studied and evaluated. In this paper, we GLYPH<28>rst review several indicators, which are used to evaluate the network in a training-free manner, including the correlation of Jacobian, the output sensitivity, the number of linear regions, and the condition number of the neural tangent kernel. Our observation is that each indicator is responsible for characterizing a network in a speciGLYPH<28>c aspect and there is no single indicator that achieves good performance in all cases, e.g. highly correlated with the test accuracy. This motivated us to develop a novel indicator where all properties of a network are taken into account. To obtain better indicator that can consider various characteristics of networks in a harmonized form, we propose a Fusion Indicator (FI). SpeciGLYPH<28>cally, the proposed FI is formed by combining multiple indicators in a weighted sum manner. We minimize the mean squared error loss between the predicted and actual accuracy of networks to acquire the weights. Moreover, as the conventional training-free NAS researches used limited metrics to evaluate the quality of indicators, we introduce more desirable metrics that can evaluate the quality of training-free NAS indicator in terms of GLYPH<28>delity, correlation and rank-order similarity between the predicted quality value and actual accuracy of networks. That is, we introduce the Pearson Linear CoefGLYPH<28>cient Correlation (PLCC), the Root Mean Square Error (RMSE), the Spearman Rank-Order Correlation CoefGLYPH<28>cient (SROCC), and Kendall Rank-Order Correlation CoefGLYPH<28>cient (KROCC). Extensive experiments on NAS-Bench-101 and NAS-Bench201 demonstrate the effectiveness of our FI, outperforming existing methods by a large margin.\nINDEX TERMS Neural architecture search, training-free neural architecture search, fusion indicator, evaluation metrics.",
    "context": "Introduces a novel fusion indicator (FI) for evaluating training-free neural architecture search, incorporating multiple metrics and demonstrating improved performance compared to existing methods.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      1
    ],
    "id": "3a20bec0f0c0a43137a4edcdf0312d663f5f24b9040f95b11d5c4a76f39dac71"
  },
  {
    "text": "Deep neural networks (DNNs) have shown remarkable performance on various computer vision tasks. Since the success of AlexNet on ImageNet [1] classiGLYPH<28>cation task in 2012 [2], many high-performance networks have been introduced [3]GLYPH<21>[5] where these networks have been designed by experts. However, manual design is not an optimal choice especially when the network goes deeper. Moreover, the process for designing these networks requires immense time and effort. To reduce the cost of designing the network, researchers have studied to automate the process, leading to Neural Architecture Search (NAS). Instead of designing the architecture, experts design the search algorithms that GLYPH<28>nd good candidates (e.g., the number of layers, GLYPH<28>lters and types of activation, etc.) on a given search space.\nThe associate editor coordinating the review of this manuscript and approving it for publication was Danilo Pelusi .\nNAS is able to discover superior architecture on various computer vision tasks such as image classiGLYPH<28>cation [6]GLYPH<21>[8], object detection [9], [10]. However, it suffers from several limitations. Firstly, the search cost is extremely high. It takes years of reinforcement learning (RL) [6] to search for networks which achieve state-of-the-art accuracy on largescale dataset such as ImageNet. The most expensive part in RL approach is the training from scratch of child networks. To alleviate this limitation, subsequent works have suggested to search on smaller conGLYPH<28>guration (e.g., cell) [11], performed with shared weights [12], search on a continuous search space for NAS [7], or incorporate Bi-layer parallel training [13].\nSecondly, there has a barrier to apply NAS to practical applications. That is, we need to search for a new network when the environment (e.g target task or hardware) is changed. For example, the architecture found on this dataset may not work well on others [14], or the latency for a same network is different when being executed on different hardware devices [15]. Moreover, the budget for searching is still high (e.g hours of GPU).\nIn order to search for best architecture in a short period time (e.g., a few minutes), NAS without training (NASWOT) [16] is introduced. SpeciGLYPH<28>cally, NASWOT proposed a trainingfree indicator that predicts the score which is highly correlated with the actual accuracy of a network, guiding the search process. Because the step to calculate this indicator does not require any training of networks, the total cost for NAS reduces signiGLYPH<28>cantly. With its simplicity, they have GLYPH<28>rst demonstrated the possibility of performing NAS without involving any training.\nMeanwhile, there have been growing works on deep learning theory that enables us to understand the behavior of DNNs [17]GLYPH<21>[23]. TE-NAS [24] has made the GLYPH<28>rst attempt to apply the indicators derived from theoretical works for revealing network characteristics, namely trainability and the expressivity, for training-free NAS. Different from [16], the search algorithm of TE-NAS is inspired by pruning-fromscratch.\nTraining-free NAS can replace the conventional NAS algorithms. However, prior works suffer from several limitations. [16] only uses one indicator, namely correlation Jacobian of a batch of images augmented with Cutout [25], by taking one characteristic (robustness to the input perturbation) of the network into account. As [16] uses only one indicator, it cannot represent other aspects of the network such as trainability or expressivity. So, it turns out to have limited performance in correlation between predicted score and actual accuracy. Reference [24] deals with this problem by using two indicators. However, the score (criterion for selecting the best network) is calculated with the sum of two ranks from each indicator, meaning that the two indicators have the same importance. However, this does not guarantee the optimal solution because different indicators may have different behaviors and contributions to the GLYPH<28>nal scores in different importance.\nTo solve this problem, this paper investigate several training-free indicators and harmonizes them in a fusion framework. The FI is capable of measuring the performance of networks under various aspects. Instead of treating all indicators equally, we propose a simple training approach to GLYPH<28>nd the appropriate weights for each indicator. The main contributions of the paper can be summarized as follows V\n- GLYPH<15> We GLYPH<28>rst collect and analyze several indicators that can be used to estimate the network's performance without training. The collected four indicators are the correlation of Jacobian (CJ), output sensitivity (OS), condition number of Neural Tangent Kernel (CNNTK), and the number of Linear Regions (NLR).\n- GLYPH<15> We propose a FI which is a combination of output from multiple indicators with learned weights. The proposed indicator beneGLYPH<28>ts from various properties of a network such as trainability, expressivity, generalibility and robustness against perturbation.\n- GLYPH<15> We introduce new quantitative metrics to measure goodness of indicators for training-free NAS.\nThe rest of the paper is organized as follows. Section II introduces several related works. Section III presents the FI. Section IV demonstrates the experimental results. Section V is the conclusion of the paper.\n\nIntroduces Neural Architecture Search (NAS) and its limitations, specifically high search costs and challenges in adapting to changing environments, leading to the development of training-free NAS approaches like NASWOT and TE-NAS.",
    "original_text": "Deep neural networks (DNNs) have shown remarkable performance on various computer vision tasks. Since the success of AlexNet on ImageNet [1] classiGLYPH<28>cation task in 2012 [2], many high-performance networks have been introduced [3]GLYPH<21>[5] where these networks have been designed by experts. However, manual design is not an optimal choice especially when the network goes deeper. Moreover, the process for designing these networks requires immense time and effort. To reduce the cost of designing the network, researchers have studied to automate the process, leading to Neural Architecture Search (NAS). Instead of designing the architecture, experts design the search algorithms that GLYPH<28>nd good candidates (e.g., the number of layers, GLYPH<28>lters and types of activation, etc.) on a given search space.\nThe associate editor coordinating the review of this manuscript and approving it for publication was Danilo Pelusi .\nNAS is able to discover superior architecture on various computer vision tasks such as image classiGLYPH<28>cation [6]GLYPH<21>[8], object detection [9], [10]. However, it suffers from several limitations. Firstly, the search cost is extremely high. It takes years of reinforcement learning (RL) [6] to search for networks which achieve state-of-the-art accuracy on largescale dataset such as ImageNet. The most expensive part in RL approach is the training from scratch of child networks. To alleviate this limitation, subsequent works have suggested to search on smaller conGLYPH<28>guration (e.g., cell) [11], performed with shared weights [12], search on a continuous search space for NAS [7], or incorporate Bi-layer parallel training [13].\nSecondly, there has a barrier to apply NAS to practical applications. That is, we need to search for a new network when the environment (e.g target task or hardware) is changed. For example, the architecture found on this dataset may not work well on others [14], or the latency for a same network is different when being executed on different hardware devices [15]. Moreover, the budget for searching is still high (e.g hours of GPU).\nIn order to search for best architecture in a short period time (e.g., a few minutes), NAS without training (NASWOT) [16] is introduced. SpeciGLYPH<28>cally, NASWOT proposed a trainingfree indicator that predicts the score which is highly correlated with the actual accuracy of a network, guiding the search process. Because the step to calculate this indicator does not require any training of networks, the total cost for NAS reduces signiGLYPH<28>cantly. With its simplicity, they have GLYPH<28>rst demonstrated the possibility of performing NAS without involving any training.\nMeanwhile, there have been growing works on deep learning theory that enables us to understand the behavior of DNNs [17]GLYPH<21>[23]. TE-NAS [24] has made the GLYPH<28>rst attempt to apply the indicators derived from theoretical works for revealing network characteristics, namely trainability and the expressivity, for training-free NAS. Different from [16], the search algorithm of TE-NAS is inspired by pruning-fromscratch.\nTraining-free NAS can replace the conventional NAS algorithms. However, prior works suffer from several limitations. [16] only uses one indicator, namely correlation Jacobian of a batch of images augmented with Cutout [25], by taking one characteristic (robustness to the input perturbation) of the network into account. As [16] uses only one indicator, it cannot represent other aspects of the network such as trainability or expressivity. So, it turns out to have limited performance in correlation between predicted score and actual accuracy. Reference [24] deals with this problem by using two indicators. However, the score (criterion for selecting the best network) is calculated with the sum of two ranks from each indicator, meaning that the two indicators have the same importance. However, this does not guarantee the optimal solution because different indicators may have different behaviors and contributions to the GLYPH<28>nal scores in different importance.\nTo solve this problem, this paper investigate several training-free indicators and harmonizes them in a fusion framework. The FI is capable of measuring the performance of networks under various aspects. Instead of treating all indicators equally, we propose a simple training approach to GLYPH<28>nd the appropriate weights for each indicator. The main contributions of the paper can be summarized as follows V\n- GLYPH<15> We GLYPH<28>rst collect and analyze several indicators that can be used to estimate the network's performance without training. The collected four indicators are the correlation of Jacobian (CJ), output sensitivity (OS), condition number of Neural Tangent Kernel (CNNTK), and the number of Linear Regions (NLR).\n- GLYPH<15> We propose a FI which is a combination of output from multiple indicators with learned weights. The proposed indicator beneGLYPH<28>ts from various properties of a network such as trainability, expressivity, generalibility and robustness against perturbation.\n- GLYPH<15> We introduce new quantitative metrics to measure goodness of indicators for training-free NAS.\nThe rest of the paper is organized as follows. Section II introduces several related works. Section III presents the FI. Section IV demonstrates the experimental results. Section V is the conclusion of the paper.",
    "context": "Introduces Neural Architecture Search (NAS) and its limitations, specifically high search costs and challenges in adapting to changing environments, leading to the development of training-free NAS approaches like NASWOT and TE-NAS.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      1,
      2
    ],
    "id": "57d2720cfd1b80e36eb5ede1aa5f2016b951d05a4c3423cd9ea426e7ed1e6cac"
  },
  {
    "text": "Neural architecture search (NAS) has attracted much attentions nowadays because of the ability to discover superior architectures automatically. However, most NAS algorithms require a huge amount of resources.\nThe earliest works on NAS were based on reinforcement learning [6], [11], [26], [27]. In [6], a controller was trained to generate the network's conGLYPH<28>guration which were used to construct the network. The exhaustive training and evaluation of child-network and the macro search (e.g searching the entire network) made this method unaffordable for practical applications. Particularly, the method in [6] used 800 GPUs and GLYPH<28>nished the searching phase in 28 days. To deal with this limitation, [11] searched for cells (i.e., normal cell and reduction cell). These were stacked to build the complete network. The method achieved 11.2x less search cost than [6]. Another type of NAS algorithm was based on evolutionary algorithms [28]GLYPH<21>[30], reduced the search cost to weeks of GPUs.\nMany attempts have been made to perform NAS just in a few hours [7], [12], [14], [31]. Especially ENAS [12] allowed sharing the weights among candidates. Thus, the most expensive part in the searching phase was eliminated. DARTS [7] proposed to search in a differentiable manner. During searching, a supernet was trained and the network was obtained by removing operators which has a low weight.\n\nDetails the historical development of NAS, highlighting the increasing efficiency of algorithms like ENAS and DARTS in reducing computational costs.",
    "original_text": "Neural architecture search (NAS) has attracted much attentions nowadays because of the ability to discover superior architectures automatically. However, most NAS algorithms require a huge amount of resources.\nThe earliest works on NAS were based on reinforcement learning [6], [11], [26], [27]. In [6], a controller was trained to generate the network's conGLYPH<28>guration which were used to construct the network. The exhaustive training and evaluation of child-network and the macro search (e.g searching the entire network) made this method unaffordable for practical applications. Particularly, the method in [6] used 800 GPUs and GLYPH<28>nished the searching phase in 28 days. To deal with this limitation, [11] searched for cells (i.e., normal cell and reduction cell). These were stacked to build the complete network. The method achieved 11.2x less search cost than [6]. Another type of NAS algorithm was based on evolutionary algorithms [28]GLYPH<21>[30], reduced the search cost to weeks of GPUs.\nMany attempts have been made to perform NAS just in a few hours [7], [12], [14], [31]. Especially ENAS [12] allowed sharing the weights among candidates. Thus, the most expensive part in the searching phase was eliminated. DARTS [7] proposed to search in a differentiable manner. During searching, a supernet was trained and the network was obtained by removing operators which has a low weight.",
    "context": "Details the historical development of NAS, highlighting the increasing efficiency of algorithms like ENAS and DARTS in reducing computational costs.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      2
    ],
    "id": "fd6a3fa010405c0ef17f8e884eeaa0a7d2225b407673fe0605d22dfbe91e4401"
  },
  {
    "text": "Conventional NAS algorithms required a heavy search cost which is unaffordable to most applications especially for training the candidate networks. Therefore, if the performance of architectures is predicted without any training, the budget for NAS can be reduced signiGLYPH<28>cantly. Reference [16] was the GLYPH<28>rst to demonstrate the feasibility of performing NAS without any training. The authors in [16] empirically found the positive correlation between the correlation of Jacobian matrix among augmented input images and the network's performance. Thus, they suggested using this indicator to score the quality of the networks. Finally, NAS in [16] was performed with a simple search strategy based on Random Search where the indicator is used to replace the training process with simple prediction of the quality in a network. This work opened a new direction\nfor NAS where one can utilize some indicators to estimate the network's performance. However, the major drawback of this method is that the whole framework relies on a single indicator which only captures one characteristic of a network.\nTE-NAS [24] leveraged two training-free indicators, that is, NLR and CNNTK. The NLR is used to measure the expressivity of DNNs [21], [22]. The CNNTK measures the trainability of DNNs [23]. Based on these two indicators, they proposed a pruning-based algorithm to perform the search. Particularly, the criterion for pruning was based on the sum of two ranks measured by NLR and of CNNTK. The process was repeated until a certain stopping criteria was met.\nIn summary, the above works [16], [24] focus on performing NAS without involving any training by leveraging several training-free indicators. The search algorithm is based on random or pruning approach. Although training-free NAS has demonstrated a potential alternative for conventional NAS algorithms, naively summing the ranks in [24] does not reGLYPH<29>ect the importance of each indicator. In order to solve the aforementioned limitation, GLYPH<28>rst, we consider multiple indicators where each indicator can express different characteristic of a network. Overall, there are four indicators in our Fusion method. Second, instead of assigning an equal weight for each indicator in a weighted sum manner, we adopt a training approach to GLYPH<28>nd appropriate weights. This way we can highlight the important of each indicator. Furthermore, we perform extensive evaluation for the indicators in a systematic manner.\n\nIntroduces training-free NAS methods using indicators to predict network performance and proposes a fusion method with multiple indicators and learned weights.",
    "original_text": "Conventional NAS algorithms required a heavy search cost which is unaffordable to most applications especially for training the candidate networks. Therefore, if the performance of architectures is predicted without any training, the budget for NAS can be reduced signiGLYPH<28>cantly. Reference [16] was the GLYPH<28>rst to demonstrate the feasibility of performing NAS without any training. The authors in [16] empirically found the positive correlation between the correlation of Jacobian matrix among augmented input images and the network's performance. Thus, they suggested using this indicator to score the quality of the networks. Finally, NAS in [16] was performed with a simple search strategy based on Random Search where the indicator is used to replace the training process with simple prediction of the quality in a network. This work opened a new direction\nfor NAS where one can utilize some indicators to estimate the network's performance. However, the major drawback of this method is that the whole framework relies on a single indicator which only captures one characteristic of a network.\nTE-NAS [24] leveraged two training-free indicators, that is, NLR and CNNTK. The NLR is used to measure the expressivity of DNNs [21], [22]. The CNNTK measures the trainability of DNNs [23]. Based on these two indicators, they proposed a pruning-based algorithm to perform the search. Particularly, the criterion for pruning was based on the sum of two ranks measured by NLR and of CNNTK. The process was repeated until a certain stopping criteria was met.\nIn summary, the above works [16], [24] focus on performing NAS without involving any training by leveraging several training-free indicators. The search algorithm is based on random or pruning approach. Although training-free NAS has demonstrated a potential alternative for conventional NAS algorithms, naively summing the ranks in [24] does not reGLYPH<29>ect the importance of each indicator. In order to solve the aforementioned limitation, GLYPH<28>rst, we consider multiple indicators where each indicator can express different characteristic of a network. Overall, there are four indicators in our Fusion method. Second, instead of assigning an equal weight for each indicator in a weighted sum manner, we adopt a training approach to GLYPH<28>nd appropriate weights. This way we can highlight the important of each indicator. Furthermore, we perform extensive evaluation for the indicators in a systematic manner.",
    "context": "Introduces training-free NAS methods using indicators to predict network performance and proposes a fusion method with multiple indicators and learned weights.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      2,
      3
    ],
    "id": "17c8cdee8474906adcc1fc36d75362f1bf58e89c46de53e5233e1cbe7306af8c"
  },
  {
    "text": "Reproducible and benchmarking NAS datasets are the most important factors for comparing the algorithms. There have been a lot of efforts to develop a benchmark dataset [32], [33]. Particularly, NAS-Bench-101 [32] has made the GLYPH<28>rst effort to build a dataset for benchmarking NAS. There are 423k architectures in this dataset. Each architecture is trained on CIFAR-10 under the same hyper-parameters. NAS-Bench101 provides the test accuracy of all architectures on this search space.\nNAS-Bench-201 [33] extends NAS-Bench-101 [32] by adding more operators (i.e., None and skip-connect), supporting more NAS algorithms (i.e., differentiable NAS), and evaluating on more datasets (i.e., CIFAR-100 and ImageNet16-120 [34]). The network in NAS-Bench-201 is a cell-based structure where the cell is deGLYPH<28>ned as a densely-connected directed acyclic graph with 4 nodes. With this conGLYPH<28>guration, the total architecture in NAS-Bench-201 is 15625. All networks are trained under the same settings. SpeciGLYPH<28>cally, they are trained from scratch with Nesterov momentum SGD for 200 epochs. The initial learning rate is 0.1 and is decayed with cosine annealing. The weight decay is set to 0.0005 and the batch size is 256.\nIn our work, we utilise NAS-Bench-101 and NAS-Bench201 to demonstrate the effectiveness of the proposed FI on all classiGLYPH<28>cation tasks in the dataset.\n\nEstablishes the importance of NAS benchmarks (NAS-Bench-101 and NAS-Bench-201) and details their structure, datasets, and training procedures, setting the stage for demonstrating the effectiveness of the proposed method.",
    "original_text": "Reproducible and benchmarking NAS datasets are the most important factors for comparing the algorithms. There have been a lot of efforts to develop a benchmark dataset [32], [33]. Particularly, NAS-Bench-101 [32] has made the GLYPH<28>rst effort to build a dataset for benchmarking NAS. There are 423k architectures in this dataset. Each architecture is trained on CIFAR-10 under the same hyper-parameters. NAS-Bench101 provides the test accuracy of all architectures on this search space.\nNAS-Bench-201 [33] extends NAS-Bench-101 [32] by adding more operators (i.e., None and skip-connect), supporting more NAS algorithms (i.e., differentiable NAS), and evaluating on more datasets (i.e., CIFAR-100 and ImageNet16-120 [34]). The network in NAS-Bench-201 is a cell-based structure where the cell is deGLYPH<28>ned as a densely-connected directed acyclic graph with 4 nodes. With this conGLYPH<28>guration, the total architecture in NAS-Bench-201 is 15625. All networks are trained under the same settings. SpeciGLYPH<28>cally, they are trained from scratch with Nesterov momentum SGD for 200 epochs. The initial learning rate is 0.1 and is decayed with cosine annealing. The weight decay is set to 0.0005 and the batch size is 256.\nIn our work, we utilise NAS-Bench-101 and NAS-Bench201 to demonstrate the effectiveness of the proposed FI on all classiGLYPH<28>cation tasks in the dataset.",
    "context": "Establishes the importance of NAS benchmarks (NAS-Bench-101 and NAS-Bench-201) and details their structure, datasets, and training procedures, setting the stage for demonstrating the effectiveness of the proposed method.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      3
    ],
    "id": "e0ccb61d92da0c93645e597bea34c68f02b954cbfff1675c4e336d05db1294ee"
  },
  {
    "text": "The motivation of training-free NAS is to select highperformance potential networks from a search space without any training. In order to design a better indicator, we study several methods that can evaluate some characteristics of the network before training, i.e., CJ, OS, NLR, and CNNTK. In this section, we will GLYPH<28>rst summarize these methods and then explain our proposed FI.\n\nSummarizes existing network evaluation methods (CJ, OS, NLR, and CNNTK) before introducing the proposed method.",
    "original_text": "The motivation of training-free NAS is to select highperformance potential networks from a search space without any training. In order to design a better indicator, we study several methods that can evaluate some characteristics of the network before training, i.e., CJ, OS, NLR, and CNNTK. In this section, we will GLYPH<28>rst summarize these methods and then explain our proposed FI.",
    "context": "Summarizes existing network evaluation methods (CJ, OS, NLR, and CNNTK) before introducing the proposed method.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      3
    ],
    "id": "86055ab056b47a33e8badcdb3e3f17a11a774aab744f64316c4277828d244376"
  },
  {
    "text": "To understand the neural network (NN) behavior, we use CJ, OS, NLR, and CNNTK. These methods provide essential knowledge for characterizing NNs.\n\nDefines the methods used to analyze neural network behavior.",
    "original_text": "To understand the neural network (NN) behavior, we use CJ, OS, NLR, and CNNTK. These methods provide essential knowledge for characterizing NNs.",
    "context": "Defines the methods used to analyze neural network behavior.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      3
    ],
    "id": "4504d1b07c3b94f06bd5f783370d2f239b2bd8ad7fe98d20fd55297badebb9a8"
  },
  {
    "text": "In order to score a network at an initial state, [16] designs CJ that computes the correlation of activations of a network with a mini-batch of n augmented images. SpeciGLYPH<28>cally, to compute the score, CJ GLYPH<28>rst calculates the derivative of output y with respect to input x of this batch V\n<!-- formula-not-decoded -->\nThen we calculate the correlation for this Jacobian matrix P J and count the number of entries that are smaller than a predeGLYPH<28>ned threshold ( GLYPH<12> ). Thus, the score for Jacobian is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->\n\nDefines a scoring method for network initialization based on Jacobian correlation with augmented images.",
    "original_text": "In order to score a network at an initial state, [16] designs CJ that computes the correlation of activations of a network with a mini-batch of n augmented images. SpeciGLYPH<28>cally, to compute the score, CJ GLYPH<28>rst calculates the derivative of output y with respect to input x of this batch V\n<!-- formula-not-decoded -->\nThen we calculate the correlation for this Jacobian matrix P J and count the number of entries that are smaller than a predeGLYPH<28>ned threshold ( GLYPH<12> ). Thus, the score for Jacobian is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->",
    "context": "Defines a scoring method for network initialization based on Jacobian correlation with augmented images.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      3
    ],
    "id": "44a1097de93f96d31fbb349e8974f7662d854f3f43a2189b477c4adf329aebb3"
  },
  {
    "text": "For improving the network's generalization ability, authors in [35] proposed an ensemble approach called OS that can estimate the degree of generalization power of a network. Forouzesh et al. [17] further extended the results of [35] where the goal is to study the relation between the sensitivity and generalization in NNs. For determining the sensitivity of the network, external noise is added to the input. Let x be the input vector, GLYPH<18> be the parameters of a NN f GLYPH<18> , and \" be the noise which is sampled from a uniform distribution, then the error err is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->\nThe averaged error is calculated as below V\n<!-- formula-not-decoded -->\nwhere N is the number of classes and err n y indicates the n -th value of erry . The sensitivity of an NN can be measured by computing the variance of the output error. To this end, the score of sensitivity is formulated as V\n<!-- formula-not-decoded -->\nwhere X is the vector of averaged error for M samples and Var is the variance.\n\nDefines a method for measuring network sensitivity by calculating the variance of the output error with added noise.",
    "original_text": "For improving the network's generalization ability, authors in [35] proposed an ensemble approach called OS that can estimate the degree of generalization power of a network. Forouzesh et al. [17] further extended the results of [35] where the goal is to study the relation between the sensitivity and generalization in NNs. For determining the sensitivity of the network, external noise is added to the input. Let x be the input vector, GLYPH<18> be the parameters of a NN f GLYPH<18> , and \" be the noise which is sampled from a uniform distribution, then the error err is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->\nThe averaged error is calculated as below V\n<!-- formula-not-decoded -->\nwhere N is the number of classes and err n y indicates the n -th value of erry . The sensitivity of an NN can be measured by computing the variance of the output error. To this end, the score of sensitivity is formulated as V\n<!-- formula-not-decoded -->\nwhere X is the vector of averaged error for M samples and Var is the variance.",
    "context": "Defines a method for measuring network sensitivity by calculating the variance of the output error with added noise.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      3,
      4
    ],
    "id": "33f6e16f0dd01f603c4c35fadab13cb7d34047e8eecba40fab852577b77d9de4"
  },
  {
    "text": "To answer why deep networks outperform a shallow network, [36] analyzes the deep ReLU networks with respect to their complexity. They have shown that, given the same computational resources, a deep network can divide the input space into many regions than a shallow one. Motivated by [21], [36] provided an in-depth analysis on NLR for convolution neural networks (CNNs). Following [24], NLR can be used to measure the expressivity of NNs. Let N be a ReLU CNN and GLYPH<18> be the parameters of N sampled from some distributions. The score for NLR can then be calculated as V\n<!-- formula-not-decoded -->\nwhere R ( GLYPH<1> ) is the region corresponding to P and GLYPH<18> , and is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->\nwhere z ( x 0 I GLYPH<18> ) is the pre-activation of a neuron z and P is an activation pattern such that P ( z ) 2 fGLYPH<0> 1 ; 1 g for each neuron z in N .\n\nDetails the calculation of Neural Layer Region (NLR) score for ReLU CNNs, relating network complexity to activation patterns.",
    "original_text": "To answer why deep networks outperform a shallow network, [36] analyzes the deep ReLU networks with respect to their complexity. They have shown that, given the same computational resources, a deep network can divide the input space into many regions than a shallow one. Motivated by [21], [36] provided an in-depth analysis on NLR for convolution neural networks (CNNs). Following [24], NLR can be used to measure the expressivity of NNs. Let N be a ReLU CNN and GLYPH<18> be the parameters of N sampled from some distributions. The score for NLR can then be calculated as V\n<!-- formula-not-decoded -->\nwhere R ( GLYPH<1> ) is the region corresponding to P and GLYPH<18> , and is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->\nwhere z ( x 0 I GLYPH<18> ) is the pre-activation of a neuron z and P is an activation pattern such that P ( z ) 2 fGLYPH<0> 1 ; 1 g for each neuron z in N .",
    "context": "Details the calculation of Neural Layer Region (NLR) score for ReLU CNNs, relating network complexity to activation patterns.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      4
    ],
    "id": "46cc18d0fddfa8d1888e76d0442c88e7ce51d14a44d8ceaab34b61c6029a1cc6"
  },
  {
    "text": "In [20] a new tool was proposed to help understand the behavior of DNNs during training, which is called NTK. It has been proved that using NTK, we can obtain the time evolution of linearized NNs at time t without running gradient descent V\n<!-- formula-not-decoded -->\nwhere GLYPH<22> t ( x ) D E [ z L i ( x )] is the expected outputs of inGLYPH<28>nitely wide network, z L i is the output of i -th neuron in the last layer L , GLYPH<17> is the learning rate, and O 2 train,train is the NTK between two training inputs. X train and Y train are the input and target which are drawn from the training set. Id is a constant. The trainability of NNs is studied in [23]. Let GLYPH<21> i be the i -th eigenvalue in the D diagonal matrix and U be the unitary matrix of O 2 train,train , i.e., O 2 train,train D UDU GLYPH<0> 1 . Then Eq. (8) can be rewritten as V\n<!-- formula-not-decoded -->\nwhere Q GLYPH<22> t ( X train ) i D U GLYPH<22> t ( X train) and Q Y train ; i D UY train .\nLet GLYPH<21> 0 and GLYPH<21> m be the minimum and maximum eigenvalues of O 2 train,train. In Eq. (9), the maximum learning rate scales as GLYPH<17> GLYPH<24> 2 =GLYPH<21> 0 in [37]. Thus, the smallest eigenvalue will converge exponentially at a rate given by 1 = k , where k D GLYPH<21> 0 =GLYPH<21> m and is the condition number. If the condition number of the NTK diverges, the network is untrainable. In our work, we inverse the condition number of NTK such that with a higher value,\nFIGURE 1. The illustration of the proposed fusion indicator. CJ, NLR, CNNTK, OS are the four training-free indicators. Each indicator represents a single characteristic of a network. In the fusion indicator, each standalone indicator contributes a certain property for ranking the neural network.\nwe can get better trainability. Thus, the score for trainability can be written as V\n<!-- formula-not-decoded -->\n\nIntroduces NTK, a tool for analyzing DNN training behavior, and presents a method for inverting its condition number to improve trainability.",
    "original_text": "In [20] a new tool was proposed to help understand the behavior of DNNs during training, which is called NTK. It has been proved that using NTK, we can obtain the time evolution of linearized NNs at time t without running gradient descent V\n<!-- formula-not-decoded -->\nwhere GLYPH<22> t ( x ) D E [ z L i ( x )] is the expected outputs of inGLYPH<28>nitely wide network, z L i is the output of i -th neuron in the last layer L , GLYPH<17> is the learning rate, and O 2 train,train is the NTK between two training inputs. X train and Y train are the input and target which are drawn from the training set. Id is a constant. The trainability of NNs is studied in [23]. Let GLYPH<21> i be the i -th eigenvalue in the D diagonal matrix and U be the unitary matrix of O 2 train,train , i.e., O 2 train,train D UDU GLYPH<0> 1 . Then Eq. (8) can be rewritten as V\n<!-- formula-not-decoded -->\nwhere Q GLYPH<22> t ( X train ) i D U GLYPH<22> t ( X train) and Q Y train ; i D UY train .\nLet GLYPH<21> 0 and GLYPH<21> m be the minimum and maximum eigenvalues of O 2 train,train. In Eq. (9), the maximum learning rate scales as GLYPH<17> GLYPH<24> 2 =GLYPH<21> 0 in [37]. Thus, the smallest eigenvalue will converge exponentially at a rate given by 1 = k , where k D GLYPH<21> 0 =GLYPH<21> m and is the condition number. If the condition number of the NTK diverges, the network is untrainable. In our work, we inverse the condition number of NTK such that with a higher value,\nFIGURE 1. The illustration of the proposed fusion indicator. CJ, NLR, CNNTK, OS are the four training-free indicators. Each indicator represents a single characteristic of a network. In the fusion indicator, each standalone indicator contributes a certain property for ranking the neural network.\nwe can get better trainability. Thus, the score for trainability can be written as V\n<!-- formula-not-decoded -->",
    "context": "Introduces NTK, a tool for analyzing DNN training behavior, and presents a method for inverting its condition number to improve trainability.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      4
    ],
    "id": "4e1c60ae351aea95d72b48cffe5614b403536c02f2eaff323385a9495bf5e211"
  },
  {
    "text": "In order to evaluate NNs without any training, the process requires a powerful indicator for correctly characterizing the network. Each indicator has its purpose and signiGLYPH<28>cance in specifying our model. Using these indicators separately makes them ineffective and weak since they only characterize the network with a speciGLYPH<28>c factor hence inculcating a certain bias in the network. Thus, this motivates us to design the FI where all indicators are combined. Let S be the set of training free indicators (e.g., S CJ ; S NLR ; S CNNTK ; S OS), our score for FI is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->\nwhere n is the total indicators, b is the bias, wi and Si are the weight and the score using i -th indicator in S . The overview of the proposed Fusion Indicator is demonstrated in Figure 1. We hypothesize that our proposed FI can take all indicators' advantage and make our training-free framework more reliable. We want our FI to reGLYPH<29>ect our network's accuracy closely. We are interested in minimizing the difference between the score and the accuracy. SpeciGLYPH<28>cally, we minimize the following loss function V\n<!-- formula-not-decoded -->\nwhere MSE is the mean squared error. S FI and Acc in Eq. (12) denote the score and the accuracy of a network. Thus, our S FI is the predicted accuracy of a network. The model parameters in Eq. (11) are trained with the stochastic gradient descent (SGD) method. Note that it is prohibitive to use support vector regression or linear regression in this case\nsince there are a huge numbers of samples in NAS-Bench101/202 datasets.\n\nIntroduces a fusion indicator (FI) designed to combine multiple training-free network indicators to improve reliability and accuracy prediction.",
    "original_text": "In order to evaluate NNs without any training, the process requires a powerful indicator for correctly characterizing the network. Each indicator has its purpose and signiGLYPH<28>cance in specifying our model. Using these indicators separately makes them ineffective and weak since they only characterize the network with a speciGLYPH<28>c factor hence inculcating a certain bias in the network. Thus, this motivates us to design the FI where all indicators are combined. Let S be the set of training free indicators (e.g., S CJ ; S NLR ; S CNNTK ; S OS), our score for FI is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->\nwhere n is the total indicators, b is the bias, wi and Si are the weight and the score using i -th indicator in S . The overview of the proposed Fusion Indicator is demonstrated in Figure 1. We hypothesize that our proposed FI can take all indicators' advantage and make our training-free framework more reliable. We want our FI to reGLYPH<29>ect our network's accuracy closely. We are interested in minimizing the difference between the score and the accuracy. SpeciGLYPH<28>cally, we minimize the following loss function V\n<!-- formula-not-decoded -->\nwhere MSE is the mean squared error. S FI and Acc in Eq. (12) denote the score and the accuracy of a network. Thus, our S FI is the predicted accuracy of a network. The model parameters in Eq. (11) are trained with the stochastic gradient descent (SGD) method. Note that it is prohibitive to use support vector regression or linear regression in this case\nsince there are a huge numbers of samples in NAS-Bench101/202 datasets.",
    "context": "Introduces a fusion indicator (FI) designed to combine multiple training-free network indicators to improve reliability and accuracy prediction.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      4,
      5
    ],
    "id": "c692d2409119c8cfa90f190b170bf4e62cc901ce2edc02432df5992a5aca3ae7"
  },
  {
    "text": "We use four metrics to evaluate the performance of the indicator. The GLYPH<28>rst two metrics are the Spearman Rank-Order Correlation CoefGLYPH<28>cient (SROCC) and the Kendall RankOrder Correlation CoefGLYPH<28>cient (KROCC). The third metric and the fourth metric are the Pearson Linear Correlation CoefGLYPH<28>cient (PLCC) and the Root Mean Square Error (RMSE) between the score and the accuracy after nonlinear regression. The GLYPH<28>rst two metrics measure the prediction monotonicity of the indicator while the third one measures the linear correlation between the actual accuracy and the predicted one. These quality assessment metrics are made for evaluating such characteristics (GLYPH<28>delity and correlation) and are widely used in practice [38]GLYPH<21>[40]. To compute PLCC and RMSE, we apply the following logistic function as suggested in [41]:\n<!-- formula-not-decoded -->\nwhere x is the score (predicted accuracy) and GLYPH<12> i ( i D 1 ; 2 ; 3 ; 4 ; 5) are the set of parameters that minimize the least squares error between the output from indicator and the network's accuracy.\nSince our FI is a learning-based method, the optimal w may not generalize well on other test data. To prevent overGLYPH<28>tting, we use n-fold cross-validation. SpeciGLYPH<28>cally, we use 5-fold cross-validation and average the results obtained from 5 folds to get the overall performance. In all experiments, we run the 5-fold cross-validation 10 times and average the results to obtain the GLYPH<28>nal performance.\n\nDetails the performance evaluation metrics (SROCC, KROCC, PLCC, RMSE) and cross-validation procedure used to assess the indicator's accuracy and correlation.",
    "original_text": "We use four metrics to evaluate the performance of the indicator. The GLYPH<28>rst two metrics are the Spearman Rank-Order Correlation CoefGLYPH<28>cient (SROCC) and the Kendall RankOrder Correlation CoefGLYPH<28>cient (KROCC). The third metric and the fourth metric are the Pearson Linear Correlation CoefGLYPH<28>cient (PLCC) and the Root Mean Square Error (RMSE) between the score and the accuracy after nonlinear regression. The GLYPH<28>rst two metrics measure the prediction monotonicity of the indicator while the third one measures the linear correlation between the actual accuracy and the predicted one. These quality assessment metrics are made for evaluating such characteristics (GLYPH<28>delity and correlation) and are widely used in practice [38]GLYPH<21>[40]. To compute PLCC and RMSE, we apply the following logistic function as suggested in [41]:\n<!-- formula-not-decoded -->\nwhere x is the score (predicted accuracy) and GLYPH<12> i ( i D 1 ; 2 ; 3 ; 4 ; 5) are the set of parameters that minimize the least squares error between the output from indicator and the network's accuracy.\nSince our FI is a learning-based method, the optimal w may not generalize well on other test data. To prevent overGLYPH<28>tting, we use n-fold cross-validation. SpeciGLYPH<28>cally, we use 5-fold cross-validation and average the results obtained from 5 folds to get the overall performance. In all experiments, we run the 5-fold cross-validation 10 times and average the results to obtain the GLYPH<28>nal performance.",
    "context": "Details the performance evaluation metrics (SROCC, KROCC, PLCC, RMSE) and cross-validation procedure used to assess the indicator's accuracy and correlation.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      5
    ],
    "id": "43cb228980e28ab7fa936d3ed19b5cef8541a9763e26f7c113e20dbfcc14f369"
  },
  {
    "text": "We compare the performance of our FI and standalone indicators, i.e. CJ, NLR, CNNTK, OS on CIFAR-10 of NASBench-101 search space. Because we have four indicators, there are 11 combinations for our FI. FI2 means two indicators are used and so on. We use f CJ, OS g for FI2, f CJ, CNNTK, OS g for FI3, and f CJ, NLR, CNNTK, OS g for FI4. We refer the reader to the Ablation section for the performance of other combinations. The results are shown in Table 1.\nAs shown in Table 1, the proposed FI outperforms other indicators signiGLYPH<28>cantly in all performance assessment methods. Notably, our FI achieves 5% higher KROCC and SROCCthan the CJ-based single indicator [16]. Compared to NLR, CNNTK and OS, the proposed FI has around 24% and 30% higher KROCC and SROCC, respectively. Additionally, it is worth noting that increasing the number of indicator does not improve the performance on NAS-Bench-101. One possible reason for this is that there are only three operations (i.e., 3 GLYPH<2> 3 convolution, 1 GLYPH<2> 1 convolution, 3 GLYPH<2> 3 max pool) in the search space. This reduces the diversity of the network architectures even though NAS-Bench-101 contains\nTABLE 1. Performance of several training-free indicators measured on CIFAR-10 of NAS-Bench-101. The first, second, and third ranked performances are highlighted in blue, red, and black bold, respectively.\nmore architectures than NAS-Bench-201. As a result, several indicators do not contribute to the overall performance.\n\nHighlights the FI’s superior performance compared to other indicators on NAS-Bench-101, demonstrating a 5% improvement in KROCC and SROCC over the CJ-based indicator, and explains why increasing the number of indicators doesn't further improve performance due to limited architectural diversity.",
    "original_text": "We compare the performance of our FI and standalone indicators, i.e. CJ, NLR, CNNTK, OS on CIFAR-10 of NASBench-101 search space. Because we have four indicators, there are 11 combinations for our FI. FI2 means two indicators are used and so on. We use f CJ, OS g for FI2, f CJ, CNNTK, OS g for FI3, and f CJ, NLR, CNNTK, OS g for FI4. We refer the reader to the Ablation section for the performance of other combinations. The results are shown in Table 1.\nAs shown in Table 1, the proposed FI outperforms other indicators signiGLYPH<28>cantly in all performance assessment methods. Notably, our FI achieves 5% higher KROCC and SROCCthan the CJ-based single indicator [16]. Compared to NLR, CNNTK and OS, the proposed FI has around 24% and 30% higher KROCC and SROCC, respectively. Additionally, it is worth noting that increasing the number of indicator does not improve the performance on NAS-Bench-101. One possible reason for this is that there are only three operations (i.e., 3 GLYPH<2> 3 convolution, 1 GLYPH<2> 1 convolution, 3 GLYPH<2> 3 max pool) in the search space. This reduces the diversity of the network architectures even though NAS-Bench-101 contains\nTABLE 1. Performance of several training-free indicators measured on CIFAR-10 of NAS-Bench-101. The first, second, and third ranked performances are highlighted in blue, red, and black bold, respectively.\nmore architectures than NAS-Bench-201. As a result, several indicators do not contribute to the overall performance.",
    "context": "Highlights the FI’s superior performance compared to other indicators on NAS-Bench-101, demonstrating a 5% improvement in KROCC and SROCC over the CJ-based indicator, and explains why increasing the number of indicators doesn't further improve performance due to limited architectural diversity.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      5
    ],
    "id": "28d263abe193d001cdcc1bb3c45c1ba0c213c48199be31515d797bda4f2be284"
  },
  {
    "text": "We further evaluate the performance of training-free indicators on NAS-Bench-201. We use f CNNTK, OS g for FI2, f NLR, CNNTK, OS g for FI3, and all indicators for FI4. The performance measured by PLCC, RMSE, SROCC, and KROCC for all competitors on this dataset are shown in Table 2.\nFrom Table 2, it can be seen that the proposed FI outperforms standalone indicators in all performance assessment methods for all datasets on NAS-Bench-201. It is worth noting that OS performs the best compared to CJ, NLR, and CNNTK. On CIFAR-10, FI2 has 7.29% and 8.58% higher SROCC and KROCC than OS. Adding more indicators to our FI further increases the performance. SpeciGLYPH<28>cally, FI3 has 7.43% and 8.85% SROCC and KROCC improvement over OS. When using all indicators, FI4 reaches its peak with 0.88 and 0.7017 SROCC and KROCC, respectively.\nOn CIFAR-100, our FI performs consistently well. Standalone indicator achieves less than 0.8 SROCC and 0.6 KROCC. By contrast, the proposed FI obtains more than 0.86 SROCC and 0.68 KROCC. On ImageNet-16-120, we can see that FI4 has the best performance for all assessment metrics. Compared to OS, FI4 has 9% and 11% higher SROCC and KROCC.\nAdditionally, we illustrate the scatter plots of the predicted scores versus the accuracy measured by CJ, NLR, CNNTK, and OS in Figure 2. We show the test accuracy with respect to the score of CJ, NLR, CNNTK, and OS in the GLYPH<28>rst, second, third, and fourth column of Figure 2, respectively. We also show the scatter plots of FI4 for 5-fold cross-validation (CV) in Figure 3. Each column in Figure 3 represents the test accuracy with respect to the score of each fold, namely Fold-1, Fold-2, Fold-3, Fold-4, and Fold-5 in CV. In both GLYPH<28>gures, the green line is the best GLYPH<28>tting curve. Figure 2 demonstrates that the OS indicator has a higher correlation than others. From Figure 3, we can see that there is a very strong correlation between the predicted score and the accuracy when using the proposed FI.\nIn summary, the performance of our FI improves consistently when adding more indicators. This may come from the diversity of the search space. Besides the three operations\nTABLE 2. Performance of several training-free indicators measured on CIFAR-10, CIFAR-100, and ImageNet-16-120 of NAS-Bench-201 search space. The first, second, and third ranked performances are highlighted in blue, red, and black bold, respectively.\nFIGURE 2. The plots of the score for all architectures in NAS-Bench-201 against the test accuracies on CIFAR-10, CIFAR-100, and ImageNet-16-120. For better visualization, the score is scaled to the range of 0 and 1. The best fitting curve is shown in green line. The Kendall Tau values show a strong correlation for all indicators.\nused in [32], NAS-Bench-201 uses two more operators, zero and skip-connect, which makes the search space richer. Thus, the proposed FI can fully utilize all characteristics of a network to evaluate the network correctly.\n\nDemonstrates the superior performance of the proposed training-free indicator (FI) across multiple datasets and benchmarks, highlighting its consistent improvement with added indicators and its ability to effectively utilize a richer search space.",
    "original_text": "We further evaluate the performance of training-free indicators on NAS-Bench-201. We use f CNNTK, OS g for FI2, f NLR, CNNTK, OS g for FI3, and all indicators for FI4. The performance measured by PLCC, RMSE, SROCC, and KROCC for all competitors on this dataset are shown in Table 2.\nFrom Table 2, it can be seen that the proposed FI outperforms standalone indicators in all performance assessment methods for all datasets on NAS-Bench-201. It is worth noting that OS performs the best compared to CJ, NLR, and CNNTK. On CIFAR-10, FI2 has 7.29% and 8.58% higher SROCC and KROCC than OS. Adding more indicators to our FI further increases the performance. SpeciGLYPH<28>cally, FI3 has 7.43% and 8.85% SROCC and KROCC improvement over OS. When using all indicators, FI4 reaches its peak with 0.88 and 0.7017 SROCC and KROCC, respectively.\nOn CIFAR-100, our FI performs consistently well. Standalone indicator achieves less than 0.8 SROCC and 0.6 KROCC. By contrast, the proposed FI obtains more than 0.86 SROCC and 0.68 KROCC. On ImageNet-16-120, we can see that FI4 has the best performance for all assessment metrics. Compared to OS, FI4 has 9% and 11% higher SROCC and KROCC.\nAdditionally, we illustrate the scatter plots of the predicted scores versus the accuracy measured by CJ, NLR, CNNTK, and OS in Figure 2. We show the test accuracy with respect to the score of CJ, NLR, CNNTK, and OS in the GLYPH<28>rst, second, third, and fourth column of Figure 2, respectively. We also show the scatter plots of FI4 for 5-fold cross-validation (CV) in Figure 3. Each column in Figure 3 represents the test accuracy with respect to the score of each fold, namely Fold-1, Fold-2, Fold-3, Fold-4, and Fold-5 in CV. In both GLYPH<28>gures, the green line is the best GLYPH<28>tting curve. Figure 2 demonstrates that the OS indicator has a higher correlation than others. From Figure 3, we can see that there is a very strong correlation between the predicted score and the accuracy when using the proposed FI.\nIn summary, the performance of our FI improves consistently when adding more indicators. This may come from the diversity of the search space. Besides the three operations\nTABLE 2. Performance of several training-free indicators measured on CIFAR-10, CIFAR-100, and ImageNet-16-120 of NAS-Bench-201 search space. The first, second, and third ranked performances are highlighted in blue, red, and black bold, respectively.\nFIGURE 2. The plots of the score for all architectures in NAS-Bench-201 against the test accuracies on CIFAR-10, CIFAR-100, and ImageNet-16-120. For better visualization, the score is scaled to the range of 0 and 1. The best fitting curve is shown in green line. The Kendall Tau values show a strong correlation for all indicators.\nused in [32], NAS-Bench-201 uses two more operators, zero and skip-connect, which makes the search space richer. Thus, the proposed FI can fully utilize all characteristics of a network to evaluate the network correctly.",
    "context": "Demonstrates the superior performance of the proposed training-free indicator (FI) across multiple datasets and benchmarks, highlighting its consistent improvement with added indicators and its ability to effectively utilize a richer search space.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      5,
      6
    ],
    "id": "2a0165a2a3aaa6d62e3fd315101d8e2eea31bb5747d36937586887d56f3d194e"
  },
  {
    "text": "We perform cross dataset tests to verify the generalizability of our FI for different datasets. To do this, we obtain the weights for the FI trained on one dataset (e.g., CIFAR-10) and evaluate it on other datasets. For example, we use the score evaluated on CIFAR-10 to train the weight for the FI and use the same weights to perform testing on CIFAR-100 and ImageNet-16-120. We denote the weight obtained from CIFAR-10, CIFAR-100, and ImageNet-16-120 as w CIFAR-10, w CIFAR-100, and w ImageNet-16-120, respectively. The performance results are listed in Table 3.\nTABLE 3. Performance of FI 4 on NAS-Bench-201, measured by KROCC.\nAs shown in Table 3, we can see that the proposed FI achieves a high KROCC. For all datasets, the value of KROCC is greater than 0.67. The weights obtained on one dataset are highly compatible with other datasets. This shows the generality and robustness of our approach.\n\nDemonstrates the generalizability and robustness of the FI across multiple datasets.",
    "original_text": "We perform cross dataset tests to verify the generalizability of our FI for different datasets. To do this, we obtain the weights for the FI trained on one dataset (e.g., CIFAR-10) and evaluate it on other datasets. For example, we use the score evaluated on CIFAR-10 to train the weight for the FI and use the same weights to perform testing on CIFAR-100 and ImageNet-16-120. We denote the weight obtained from CIFAR-10, CIFAR-100, and ImageNet-16-120 as w CIFAR-10, w CIFAR-100, and w ImageNet-16-120, respectively. The performance results are listed in Table 3.\nTABLE 3. Performance of FI 4 on NAS-Bench-201, measured by KROCC.\nAs shown in Table 3, we can see that the proposed FI achieves a high KROCC. For all datasets, the value of KROCC is greater than 0.67. The weights obtained on one dataset are highly compatible with other datasets. This shows the generality and robustness of our approach.",
    "context": "Demonstrates the generalizability and robustness of the FI across multiple datasets.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      6
    ],
    "id": "fceabdefb5548ecb215a2e03d2581a9689472226f748e9e3ab1770589dcfad9f"
  },
  {
    "text": "The ultimate goal of NAS without Training is to replace the heavy cost of training candidate networks with inexpensive\nFIGURE 3. Scatter plots of predicted score, FI 4 , against the accuracy for each fold on CIFAR-10, CIFAR-100, and ImageNet-16-120 of NAS-Bench-201 search space. The best fitting curve is shown in green line.\nones (e.g., CJ, NLR...). In order to demonstrate the effectiveness of training-free indicators, we incorporate the score from training-free indicator into Aging Evolution (AE), an evolutionary algorithm for NAS [42]. We replace the network accuracy obtained from the network training step ( train and evaluation in [42]) with the predicted score from the indicator. For the mutation phase, the network which has the highest predicted score is selected as a parent. After mutation, the child network is scored using the indicator. Finally, the output is the network which has the highest predicted score. We choose the AE algorithm due to its simplicity.\nWe compare the performance of AE using CJ, NLR, CNNTK,OS,andtheproposedFI4 on CIFAR-100 with NASBench-201 search space. We evaluate around 300 networks. Werunthe experiment 100 times and show the average results in Figure 4. As demonstrated in Figure 4, it is clear that the proposed FI achieves higher test accuracy for the network with the highest predicted score than others. SpeciGLYPH<28>cally, NLR has the lowest test accuracy for the network with the highest predicted score. The three indicators CJ, CNNTK, and OS achieve comparable test accuracy. It is noticeable that the proposed FI achieves much higher accuracy than others, where we perform experiments 100 times for each case and take an average of the 100 test accuracies.\n\nIntroduces the use of FI4 as a training-free indicator within the Aging Evolution algorithm for NAS, demonstrating its superior performance compared to other indicators on CIFAR-100.",
    "original_text": "The ultimate goal of NAS without Training is to replace the heavy cost of training candidate networks with inexpensive\nFIGURE 3. Scatter plots of predicted score, FI 4 , against the accuracy for each fold on CIFAR-10, CIFAR-100, and ImageNet-16-120 of NAS-Bench-201 search space. The best fitting curve is shown in green line.\nones (e.g., CJ, NLR...). In order to demonstrate the effectiveness of training-free indicators, we incorporate the score from training-free indicator into Aging Evolution (AE), an evolutionary algorithm for NAS [42]. We replace the network accuracy obtained from the network training step ( train and evaluation in [42]) with the predicted score from the indicator. For the mutation phase, the network which has the highest predicted score is selected as a parent. After mutation, the child network is scored using the indicator. Finally, the output is the network which has the highest predicted score. We choose the AE algorithm due to its simplicity.\nWe compare the performance of AE using CJ, NLR, CNNTK,OS,andtheproposedFI4 on CIFAR-100 with NASBench-201 search space. We evaluate around 300 networks. Werunthe experiment 100 times and show the average results in Figure 4. As demonstrated in Figure 4, it is clear that the proposed FI achieves higher test accuracy for the network with the highest predicted score than others. SpeciGLYPH<28>cally, NLR has the lowest test accuracy for the network with the highest predicted score. The three indicators CJ, CNNTK, and OS achieve comparable test accuracy. It is noticeable that the proposed FI achieves much higher accuracy than others, where we perform experiments 100 times for each case and take an average of the 100 test accuracies.",
    "context": "Introduces the use of FI4 as a training-free indicator within the Aging Evolution algorithm for NAS, demonstrating its superior performance compared to other indicators on CIFAR-100.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      6,
      7
    ],
    "id": "42727955d1ac6f37c331024abe82ee090bc504b0a27ab55cb80aff99c55662f0"
  },
  {
    "text": "In this section, we study the behavior of the training-free indicator. We investigate how weight initialization affects performance. We also study the behavior of the proposed FI when more training-free indicators are added.\nFIGURE 4. Performance of Aging Evolution using different training-free indicators on CIFAR-100 with NAS-Bench-201 search space. The black dot horizontal line is the best test accuracy on this benchmark.\n\nEvaluates the impact of weight initialization and multiple training-free indicators on CIFAR-100 performance.",
    "original_text": "In this section, we study the behavior of the training-free indicator. We investigate how weight initialization affects performance. We also study the behavior of the proposed FI when more training-free indicators are added.\nFIGURE 4. Performance of Aging Evolution using different training-free indicators on CIFAR-100 with NAS-Bench-201 search space. The black dot horizontal line is the best test accuracy on this benchmark.",
    "context": "Evaluates the impact of weight initialization and multiple training-free indicators on CIFAR-100 performance.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      7
    ],
    "id": "9ddc8b72473ad24147642d6495d392b254a079c69b557a3861590ab537bfb63c"
  },
  {
    "text": "When we develop an indicator for training-free NAS framework, the most critical factor is how robust our indicator is. At the initial state, our network is unstable. There are several initialization methodologies for a network. However, in this study, we assess the performance of training-free indicators using the following initialization methods V\n- GLYPH<15> Uniform distribution: For uniform distribution, the weights are initialized from U ( GLYPH<0> p k ; p k ), where k D groups = (C in GLYPH<3> Q 1 i D 0 k [ i ]).\n- GLYPH<15> Normal distribution: The weights are sampled from N (0 ; std 2 ). For OS indicator, we only use the standard\nTABLE 4. KROCC of indicators with different weight sampling methods on CIFAR-10 of NAS-Bench-201.\nnormal distribution without any batch normalization as mentioned in [17].\n- GLYPH<15> Kaiming He: Following [43], the weights are sampled from N (0 ; std 2 ), where std D gain = fanmode 1 = 2 .\nWe compute KROCC for each indicator with different weight sampling methods. Table 4 demonstrates the quantitative performance of each indicator for different weight sampling methods. As shown in Table 4, the performance of training-free indicators depends on how the weights are sampled, especially for CJ. One possible explanation for this is that the hypothesis of the Jacobian indicator requires the same local linear operators, where the linear maps are the Jacobian of augmented input x , should have low correlation. This means that if any weight initialization method violates the hypothesis, the Jacobian indicator will not work (e.g., strong correlation with different augmented input x ).\nTo avoid exhaustively computing all possible weight sampling combinations for FI, we use a simple technique that selects the best sampling method for each indicator. Thus, uniform distribution is used for computing CJ, NLR, CNNTK and normal distribution is used for OS.\n\nDetails the impact of different weight initialization methods on the performance of training-free NAS indicators, specifically highlighting the dependence of the Jacobian indicator on weight sampling and the selection of optimal methods for different indicators.",
    "original_text": "When we develop an indicator for training-free NAS framework, the most critical factor is how robust our indicator is. At the initial state, our network is unstable. There are several initialization methodologies for a network. However, in this study, we assess the performance of training-free indicators using the following initialization methods V\n- GLYPH<15> Uniform distribution: For uniform distribution, the weights are initialized from U ( GLYPH<0> p k ; p k ), where k D groups = (C in GLYPH<3> Q 1 i D 0 k [ i ]).\n- GLYPH<15> Normal distribution: The weights are sampled from N (0 ; std 2 ). For OS indicator, we only use the standard\nTABLE 4. KROCC of indicators with different weight sampling methods on CIFAR-10 of NAS-Bench-201.\nnormal distribution without any batch normalization as mentioned in [17].\n- GLYPH<15> Kaiming He: Following [43], the weights are sampled from N (0 ; std 2 ), where std D gain = fanmode 1 = 2 .\nWe compute KROCC for each indicator with different weight sampling methods. Table 4 demonstrates the quantitative performance of each indicator for different weight sampling methods. As shown in Table 4, the performance of training-free indicators depends on how the weights are sampled, especially for CJ. One possible explanation for this is that the hypothesis of the Jacobian indicator requires the same local linear operators, where the linear maps are the Jacobian of augmented input x , should have low correlation. This means that if any weight initialization method violates the hypothesis, the Jacobian indicator will not work (e.g., strong correlation with different augmented input x ).\nTo avoid exhaustively computing all possible weight sampling combinations for FI, we use a simple technique that selects the best sampling method for each indicator. Thus, uniform distribution is used for computing CJ, NLR, CNNTK and normal distribution is used for OS.",
    "context": "Details the impact of different weight initialization methods on the performance of training-free NAS indicators, specifically highlighting the dependence of the Jacobian indicator on weight sampling and the selection of optimal methods for different indicators.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      8,
      7
    ],
    "id": "8c2ab019548833124e6b86c4671090fd547cf4c239561027ab13a9918f55cc11"
  },
  {
    "text": "We also investigate the performance of our FI by combining different indicators on CIFAR-10. There are 11 combinations in total. The weights are sampled following the above analysis. KROCC is used to measure performance. We list the results in Table 5.\nFrom Table 5, it can be seen that on NAS-Bench-101 combining indicators further improves the performance. For example, NLR, CNNTK, OS achieves around 0.27 KROCC but combining them enhances the performance to 0.30 GLYPH<24> 0.34 KROCC. The best combination is f CJ, OS g .\nOn NAS-Bench-201, we observe that when two indicators are used, the performance measured by KROCC has a high standard deviation (e.g., the value of KROCC is ranging from 0.5610 to 0.6965). On a search space that has higher diversity (e.g., more operators), fusing more indicators helps improve the performance of the network. SpeciGLYPH<28>cally, our best KROCC increases from 0.6965 to 0.6993 and 0.7016 when three, and four indicators are used respectively. We GLYPH<28>nd that when using two indicators, the combination of CNNTK and OS outperforms other combinations. It is natural since CNNTK and OS are the top-two standalone indicator. The overall performance increases when more indicators are used. The behaviour is consistent for all quality assessment methods. We can see that the best correlation is obtained when all indicators are used. Figure 5 shows the trend of\nTABLE 5. Comparison with different combination of indicators for FI on CIFAR-10, measured by KROCC.\nFIGURE 5. Overall comparison for different combination of indicators measured on CIFAR-10, NAS-Bench-201.\nincreasing the number of indicators for FI on NAS-Bench201 improves the performance.\nFor comparing the difference in performance, we conduct the statistical tests between the proposed Fusion Indicator and the standalone one. We GLYPH<28>rst deGLYPH<28>ne the absolute difference values between the predicted score O y (after nonlinear regression) and the actual accuracy y as 1 D j y GLYPH<0> O y j . The GLYPH<28>rst test veriGLYPH<28>es the normality of 1 and the second test determines whether the 1 from one indicator are statistically indistinguishable from another indicator. The signiGLYPH<28>cance level is 5% for both tests. We use Shapiro-Wilk test for normality and the results ( p -value) are shown in Table 6. For the second test, Wilcoxon rank-sum is performed because there is no normal distribution case as in Table 6. The results for the second test are shown in Table 7. In most cases, the indicators are statistically different from each other, except for CNNTK versus NLR on NAS-Bench-101, which is not different. In general, the proposed FI statistically improves the performance.\n\nDetails the performance improvements achieved by combining indicators on CIFAR-10 and NAS-Bench-201, highlighting statistically significant differences between indicator combinations.",
    "original_text": "We also investigate the performance of our FI by combining different indicators on CIFAR-10. There are 11 combinations in total. The weights are sampled following the above analysis. KROCC is used to measure performance. We list the results in Table 5.\nFrom Table 5, it can be seen that on NAS-Bench-101 combining indicators further improves the performance. For example, NLR, CNNTK, OS achieves around 0.27 KROCC but combining them enhances the performance to 0.30 GLYPH<24> 0.34 KROCC. The best combination is f CJ, OS g .\nOn NAS-Bench-201, we observe that when two indicators are used, the performance measured by KROCC has a high standard deviation (e.g., the value of KROCC is ranging from 0.5610 to 0.6965). On a search space that has higher diversity (e.g., more operators), fusing more indicators helps improve the performance of the network. SpeciGLYPH<28>cally, our best KROCC increases from 0.6965 to 0.6993 and 0.7016 when three, and four indicators are used respectively. We GLYPH<28>nd that when using two indicators, the combination of CNNTK and OS outperforms other combinations. It is natural since CNNTK and OS are the top-two standalone indicator. The overall performance increases when more indicators are used. The behaviour is consistent for all quality assessment methods. We can see that the best correlation is obtained when all indicators are used. Figure 5 shows the trend of\nTABLE 5. Comparison with different combination of indicators for FI on CIFAR-10, measured by KROCC.\nFIGURE 5. Overall comparison for different combination of indicators measured on CIFAR-10, NAS-Bench-201.\nincreasing the number of indicators for FI on NAS-Bench201 improves the performance.\nFor comparing the difference in performance, we conduct the statistical tests between the proposed Fusion Indicator and the standalone one. We GLYPH<28>rst deGLYPH<28>ne the absolute difference values between the predicted score O y (after nonlinear regression) and the actual accuracy y as 1 D j y GLYPH<0> O y j . The GLYPH<28>rst test veriGLYPH<28>es the normality of 1 and the second test determines whether the 1 from one indicator are statistically indistinguishable from another indicator. The signiGLYPH<28>cance level is 5% for both tests. We use Shapiro-Wilk test for normality and the results ( p -value) are shown in Table 6. For the second test, Wilcoxon rank-sum is performed because there is no normal distribution case as in Table 6. The results for the second test are shown in Table 7. In most cases, the indicators are statistically different from each other, except for CNNTK versus NLR on NAS-Bench-101, which is not different. In general, the proposed FI statistically improves the performance.",
    "context": "Details the performance improvements achieved by combining indicators on CIFAR-10 and NAS-Bench-201, highlighting statistically significant differences between indicator combinations.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      8
    ],
    "id": "d42f8336a49eaf8d7d0634bf686ffb2a26214feb4a708e6c3af60943fcbc5534"
  },
  {
    "text": "There can be a few factors that threaten the validity of this research. We brieGLYPH<29>y list several potential threats V\n- GLYPH<15> Search space: There are several search spaces which are used in other NAS algorithms such as AmoebaNet [42],\nTABLE 6. The results from Shapiro-Wilk tests on CIFAR-10.\nTABLE 7. The statistical significance matrix on CIFAR-10 with 95% confidence. Each element in the table is a codeword for 2 symbols. The first and second position in the symbol indicate the result of the hypothesis test on NAS-Bench-201 and NAS-Bench-101.\nFBNet [44]. The search space of NAS-Bench-101/201 is much smaller than the aforementioned search space.\n- GLYPH<15> Dataset: Current NAS benchmark only uses small dataset such as CIFAR-10, CIFAR-100, and reduced ImageNet-16-120. Thus, it is common to ask whether the training-free indicator works well on large-scale datasets (e.g., ImageNet [1]).\n- GLYPH<15> Bias in the experiment: Since the training-free indicator evaluates the network before training, the score is affected by the value of the network's parameters.\nRegarding the GLYPH<28>rst and the second threats, there is no benchmark dataset for these search spaces at the current state. It is because training on a large-scale dataset requires a lot of computational costs, as mentioned in [33]. Due to this reason, most NAS benchmark datasets use smaller search spaces and train the networks on a small dataset such as CIFAR-10. However, we will validate the performance of training-free indicators as well as the Fusion indicator once they are available. In order to counter the last threat, we conduct the experiments multiple times (i.e., 10 times) with different random seeds and take the average as the GLYPH<28>nal results. This mitigates the inGLYPH<29>uence of random weights on the results. Extensive experiments on two recently released NAS-Bench-101/201 conGLYPH<28>rm that the Fusion Indicator brings many beneGLYPH<28>ts in ranking the networks.\n\nHighlights limitations of current NAS benchmarks due to small datasets and potential bias, while outlining strategies to mitigate these issues through repeated experimentation.",
    "original_text": "There can be a few factors that threaten the validity of this research. We brieGLYPH<29>y list several potential threats V\n- GLYPH<15> Search space: There are several search spaces which are used in other NAS algorithms such as AmoebaNet [42],\nTABLE 6. The results from Shapiro-Wilk tests on CIFAR-10.\nTABLE 7. The statistical significance matrix on CIFAR-10 with 95% confidence. Each element in the table is a codeword for 2 symbols. The first and second position in the symbol indicate the result of the hypothesis test on NAS-Bench-201 and NAS-Bench-101.\nFBNet [44]. The search space of NAS-Bench-101/201 is much smaller than the aforementioned search space.\n- GLYPH<15> Dataset: Current NAS benchmark only uses small dataset such as CIFAR-10, CIFAR-100, and reduced ImageNet-16-120. Thus, it is common to ask whether the training-free indicator works well on large-scale datasets (e.g., ImageNet [1]).\n- GLYPH<15> Bias in the experiment: Since the training-free indicator evaluates the network before training, the score is affected by the value of the network's parameters.\nRegarding the GLYPH<28>rst and the second threats, there is no benchmark dataset for these search spaces at the current state. It is because training on a large-scale dataset requires a lot of computational costs, as mentioned in [33]. Due to this reason, most NAS benchmark datasets use smaller search spaces and train the networks on a small dataset such as CIFAR-10. However, we will validate the performance of training-free indicators as well as the Fusion indicator once they are available. In order to counter the last threat, we conduct the experiments multiple times (i.e., 10 times) with different random seeds and take the average as the GLYPH<28>nal results. This mitigates the inGLYPH<29>uence of random weights on the results. Extensive experiments on two recently released NAS-Bench-101/201 conGLYPH<28>rm that the Fusion Indicator brings many beneGLYPH<28>ts in ranking the networks.",
    "context": "Highlights limitations of current NAS benchmarks due to small datasets and potential bias, while outlining strategies to mitigate these issues through repeated experimentation.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      8,
      9
    ],
    "id": "a4de586ab7e0ebefe179cfc9aa7fd2d8cd4432e25be12b8cc4aff1a170cde144"
  },
  {
    "text": "In this paper, we proposed a simple yet effective method for training-free NAS, called FI. The core motivation of our work is to harmonize several characteristics, including correlation of Jacobian, the number of linear regions, the condition number of the neural tangent kernel, and the output sensitivity of a network in a weighted sum manner. Through extensive experiments, our work demonstrated that the proposed FI achieves a higher correlation between the predicted score and the network's accuracy than standalone indicator in all quality assessment methods. The best combination on NAS-Bench-101 and NAS-Bench-201 is f CJ, OS g and f CJ, NLR, CNNTK, OS g , respectively. The large search space such as NAS-Bench-201 can fully use the characteristics effectively because of the diversity in their search space. Interestingly, we GLYPH<28>nd that fusing only two indicators (e.g., f CNNTK, OS g on NAS-Bench-201) achieved comparable performance to fuse four indicators.\nWe encourage the researchers in this GLYPH<28>eld to discover more characteristics of a network so that we can develop a powerful training-free indicator. The proposed FI can be applied to query-based NAS algorithms such as random search or evolutionary search easily. We plan to develop an efGLYPH<28>cient search method that uses the proposed FI in our future work.\n\nIntroduces a training-free NAS method (FI) that correlates multiple network characteristics to improve accuracy in search algorithms.",
    "original_text": "In this paper, we proposed a simple yet effective method for training-free NAS, called FI. The core motivation of our work is to harmonize several characteristics, including correlation of Jacobian, the number of linear regions, the condition number of the neural tangent kernel, and the output sensitivity of a network in a weighted sum manner. Through extensive experiments, our work demonstrated that the proposed FI achieves a higher correlation between the predicted score and the network's accuracy than standalone indicator in all quality assessment methods. The best combination on NAS-Bench-101 and NAS-Bench-201 is f CJ, OS g and f CJ, NLR, CNNTK, OS g , respectively. The large search space such as NAS-Bench-201 can fully use the characteristics effectively because of the diversity in their search space. Interestingly, we GLYPH<28>nd that fusing only two indicators (e.g., f CNNTK, OS g on NAS-Bench-201) achieved comparable performance to fuse four indicators.\nWe encourage the researchers in this GLYPH<28>eld to discover more characteristics of a network so that we can develop a powerful training-free indicator. The proposed FI can be applied to query-based NAS algorithms such as random search or evolutionary search easily. We plan to develop an efGLYPH<28>cient search method that uses the proposed FI in our future work.",
    "context": "Introduces a training-free NAS method (FI) that correlates multiple network characteristics to improve accuracy in search algorithms.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      9
    ],
    "id": "fe5207211383b3333adb2a41e8f489d3fc080a9e8a46cf53449feff449f26bb6"
  },
  {
    "text": "- [1] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ''ImageNet: A large-scale hierarchical image database,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , Jun. 2009, pp. 248GLYPH<21>255.\n- [2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ''ImageNet classiGLYPH<28>cation with deep convolutional neural networks,'' in Proc. Adv. Neural Inf. Process. Syst. , vol. 25, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, Eds. Red Hook, NY, USA: Curran Associates, 2012, pp. 1097GLYPH<21>1105.\n- [3] K. He, X. Zhang, S. Ren, and J. Sun, ''Deep residual learning for image recognition,'' 2015, arXiv:1512.03385 . [Online]. Available: https://arxiv.org/abs/1512.03385\n- [4] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, ''Densely connected convolutional networks,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jul. 2017, pp. 4700GLYPH<21>4708.\n- [5] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, ''MobileNets: EfGLYPH<28>cient convolutional neural networks for mobile vision applications,'' 2017, arXiv:1704.04861 . [Online]. Available: https://arxiv.org/abs/1704.04861\n- [6] B. Zoph and Q. V. Le, ''Neural architecture search with reinforcement learning,'' 2016, arXiv:1611.01578 . [Online]. Available: https://arxiv. org/abs/1611.01578\n- [7] H. Liu, K. Simonyan, and Y. Yang, ''DARTS: Differentiable architecture search,'' in Proc. Int. Conf. Learn. Represent. , 2019, pp. 1GLYPH<21>12.\n- [8] M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard, and Q. V. Le, ''MnasNet: Platform-aware neural architecture search for mobile,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2019, pp. 2820GLYPH<21>2828.\n- [9] B. Chen, G. Ghiasi, H. Liu, T.-Y. Lin, D. Kalenichenko, H. Adam, and Q. V. Le, ''MnasFPN: Learning latency-aware pyramid architecture for object detection on mobile devices,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2020, pp. 2820GLYPH<21>2828.\n- [10] N. Wang, Y. Gao, H. Chen, P. Wang, Z. Tian, C. Shen, and Y. Zhang, ''NAS-FCOS: Fast neural architecture search for object detection,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2020, pp. 11943GLYPH<21>11951.\n- [11] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, ''Learning transferable architectures for scalable image recognition,'' 2017, arXiv:1707.07012 . [Online]. Available: https://arxiv.org/abs/1707.07012\n- [12] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean, ''EfGLYPH<28>cient neural architecture search via parameter sharing,'' 2018, arXiv:1802.03268 . [Online]. Available: https://arxiv.org/abs/1802.03268\n\nSummarizes key publications on neural network architectures for efficient image recognition and object detection, particularly focusing on advancements in architecture search and mobile deployment.",
    "original_text": "- [1] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ''ImageNet: A large-scale hierarchical image database,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , Jun. 2009, pp. 248GLYPH<21>255.\n- [2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ''ImageNet classiGLYPH<28>cation with deep convolutional neural networks,'' in Proc. Adv. Neural Inf. Process. Syst. , vol. 25, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, Eds. Red Hook, NY, USA: Curran Associates, 2012, pp. 1097GLYPH<21>1105.\n- [3] K. He, X. Zhang, S. Ren, and J. Sun, ''Deep residual learning for image recognition,'' 2015, arXiv:1512.03385 . [Online]. Available: https://arxiv.org/abs/1512.03385\n- [4] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, ''Densely connected convolutional networks,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jul. 2017, pp. 4700GLYPH<21>4708.\n- [5] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, ''MobileNets: EfGLYPH<28>cient convolutional neural networks for mobile vision applications,'' 2017, arXiv:1704.04861 . [Online]. Available: https://arxiv.org/abs/1704.04861\n- [6] B. Zoph and Q. V. Le, ''Neural architecture search with reinforcement learning,'' 2016, arXiv:1611.01578 . [Online]. Available: https://arxiv. org/abs/1611.01578\n- [7] H. Liu, K. Simonyan, and Y. Yang, ''DARTS: Differentiable architecture search,'' in Proc. Int. Conf. Learn. Represent. , 2019, pp. 1GLYPH<21>12.\n- [8] M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard, and Q. V. Le, ''MnasNet: Platform-aware neural architecture search for mobile,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2019, pp. 2820GLYPH<21>2828.\n- [9] B. Chen, G. Ghiasi, H. Liu, T.-Y. Lin, D. Kalenichenko, H. Adam, and Q. V. Le, ''MnasFPN: Learning latency-aware pyramid architecture for object detection on mobile devices,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2020, pp. 2820GLYPH<21>2828.\n- [10] N. Wang, Y. Gao, H. Chen, P. Wang, Z. Tian, C. Shen, and Y. Zhang, ''NAS-FCOS: Fast neural architecture search for object detection,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2020, pp. 11943GLYPH<21>11951.\n- [11] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, ''Learning transferable architectures for scalable image recognition,'' 2017, arXiv:1707.07012 . [Online]. Available: https://arxiv.org/abs/1707.07012\n- [12] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean, ''EfGLYPH<28>cient neural architecture search via parameter sharing,'' 2018, arXiv:1802.03268 . [Online]. Available: https://arxiv.org/abs/1802.03268",
    "context": "Summarizes key publications on neural network architectures for efficient image recognition and object detection, particularly focusing on advancements in architecture search and mobile deployment.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      9
    ],
    "id": "84e627c0c4d3f2f721013b401fa44b5cdc8a4ca8f4be88476e6f1bf67764d03c"
  },
  {
    "text": "- [13] J. Chen, K. Li, K. Bilal, X. Zhou, K. Li, and P. S. Yu, ''A bi-layered parallel training architecture for large-scale convolutional neural networks,'' IEEE Trans. Parallel Distrib. Syst. , vol. 30, no. 5, pp. 965GLYPH<21>976, May 2019.\n- [14] Y. Xu, L. Xie, X. Zhang, X. Chen, G. Qi, Q. Tian, and H. Xiong, ''PC-DARTS: Partial channel connections for memory-efGLYPH<28>cient differentiable architecture search,'' 2019, arXiv:1907.05737 . [Online]. Available: https://arxiv.org/abs/1907.05737\n- [15] C. Li, Z. Yu, Y. Fu, Y. Zhang, Y. Zhao, H. You, Q. Yu, Y. Wang, C. Hao, and Y. Lin, ''HW-NAS-Bench: Hardware-aware neural architecture search benchmark,'' in Proc. Int. Conf. Learn. Represent. , 2021. [Online]. Available: https://openreview.net/forum?id=_0kaDkv3dVf\n- [16] J. Mellor, J. Turner, A. Storkey, and E. J. Crowley, ''Neural architecture search without training,'' 2020, arXiv:2006.04647 . [Online]. Available: https://arxiv.org/abs/2006.04647\n- [17] M. Forouzesh, F. Salehi, and P. Thiran, ''Generalization comparison of deep neural networks via output sensitivity,'' in Proc. 25th Int. Conf. Pattern Recognit. (ICPR) , Jan. 2021, pp. 7411GLYPH<21>7418.\n- [18] R. Novak, Y. Bahri, D. A. AbolaGLYPH<28>a, J. Pennington, and J. Sohl-Dickstein, ''Sensitivity and generalization in neural networks: An empirical study,'' 2018, arXiv:1802.08760 . [Online]. Available: https://arxiv.org/abs/1802.08760\n- [19] K. Kawaguchi, L. P. Kaelbling, and Y. Bengio, ''Generalization in deep learning,'' 2017, arXiv:1710.05468 . [Online]. Available: https://arxiv.org/abs/1710.05468\n- [20] A. Jacot, F. Gabriel, and C. Hongler, ''Neural tangent kernel: Convergence and generalization in neural networks,'' 2018, arXiv:1806.07572 . [Online]. Available: https://arxiv.org/abs/1806.07572\n- [21] H. Xiong, L. Huang, M. Yu, L. Liu, F. Zhu, and L. Shao, ''On the number of linear regions of convolutional neural networks,'' in Proc. 37th Int. Conf. Mach. Learn. , vol. 119, H. D. III and A. Singh, Eds. Jul. 2020, pp. 10514GLYPH<21>10523.\n- [22] M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, and J. Sohl-Dickstein, ''On the expressive power of deep neural networks,'' 2017, arXiv:1606.05336 . [Online]. Available: https://arxiv.org/abs/1606.05336\n- [23] L. Xiao, J. Pennington, and S. S. Schoenholz, ''Disentangling trainability and generalization in deep learning,'' 2019, arXiv:1912.13053 . [Online]. Available: https://arxiv.org/abs/1912.13053\n- [24] W. Chen, X. Gong, and Z. Wang, ''Neural architecture search on ImageNet in four GPU hours: A theoretically inspired perspective,'' in Proc. Int. Conf. Learn. Represent. , 2021. [Online]. Available: https://openreview.net/forum?id=Cnon5ezMHtu\n- [25] T. Devries and G. W. Taylor, ''Improved regularization of convolutional neural networks with cutout,'' 2017, arXiv:1708.04552 . [Online]. Available: https://arxiv.org/abs/1708.04552\n- [26] B. Baker, O. Gupta, N. Naik, and R. Raskar, ''Designing neural network architectures using reinforcement learning,'' 2016, arXiv:1611.02167 . [Online]. Available: https://arxiv.org/abs/1611.02167\n- [27] Z. Zhong, J. Yan, W. Wu, J. Shao, and C.-L. Liu, ''Practical blockwise neural network architecture generation,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , Jun. 2018, pp. 2423GLYPH<21>2432.\n- [28] L. Xie and A. Yuille, ''Genetic CNN,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , Oct. 2017, pp. 1379GLYPH<21>1388.\n- [29] E. Real, S. Moore, A. Selle, S. Saxena, Y . L. Suematsu, J. Tan, Q. V . Le, and A. Kurakin, ''Large-scale evolution of image classiGLYPH<28>ers,'' in Proc. 34th Int. Conf. Mach. Learn. (ICML) , Sydney, NSW, Australia, vol. 70, Aug. 2017, pp. 2902GLYPH<21>2911.\n- [30] T. Elsken, J. H. Metzen, and F. Hutter, ''EfGLYPH<28>cient multi-objective neural architecture search via Lamarckian evolution,'' in Proc. Int. Conf. Learn. Represent. , Sep. 2019. [Online]. Available: https:// openreview.net/forum?id=ByME42AqK7\n- [31] X. Chen, L. Xie, J. Wu, and Q. Tian, ''Progressive differentiable architecture search: Bridging the depth gap between search and evaluation,'' 2019, arXiv:1904.12760 . [Online]. Available: https://arxiv.org/abs/1904.12760\n- [32] C. Ying, A. Klein, E. Christiansen, E. Real, K. Murphy, and F. Hutter, ''Nas-bench-101: Towards reproducible neural architecture search,'' in Proc. ICML , 2019, pp. 7105GLYPH<21>7114.\n- [33] X. Dong and Y. Yang, ''NAS-Bench-201: Extending the scope of reproducible neural architecture search,'' in Proc. Int. Conf. Learn. Represent. , 2020, pp. 2GLYPH<21>17.\n- [34] P. Chrabaszcz, I. Loshchilov, and F. Hutter, ''A downsampled variant of ImageNet as an alternative to the CIFAR datasets,'' 2017, arXiv:1707.08819 . [Online]. Available: https://arxiv.org/abs/1707.08819\n- [35] J. Yang, X. Zeng, S. Zhong, and S. Wu, ''Effective neural network ensemble approach for improving generalization performance,'' IEEE Trans. Neural Netw. Learn. Syst. , vol. 24, no. 6, pp. 878GLYPH<21>887, Jun. 2013.\n- [36] R. Pascanu, G. Montufar, and Y. Bengio, ''On the number of response regions of deep feed forward networks with piece-wise linear activations,'' 2014, arXiv:1312.6098 . [Online]. Available: https://arxiv.org/abs/1312.6098\n\nProvides a collection of research papers exploring neural architecture search (NAS) techniques and their impact on generalization and efficiency.",
    "original_text": "- [13] J. Chen, K. Li, K. Bilal, X. Zhou, K. Li, and P. S. Yu, ''A bi-layered parallel training architecture for large-scale convolutional neural networks,'' IEEE Trans. Parallel Distrib. Syst. , vol. 30, no. 5, pp. 965GLYPH<21>976, May 2019.\n- [14] Y. Xu, L. Xie, X. Zhang, X. Chen, G. Qi, Q. Tian, and H. Xiong, ''PC-DARTS: Partial channel connections for memory-efGLYPH<28>cient differentiable architecture search,'' 2019, arXiv:1907.05737 . [Online]. Available: https://arxiv.org/abs/1907.05737\n- [15] C. Li, Z. Yu, Y. Fu, Y. Zhang, Y. Zhao, H. You, Q. Yu, Y. Wang, C. Hao, and Y. Lin, ''HW-NAS-Bench: Hardware-aware neural architecture search benchmark,'' in Proc. Int. Conf. Learn. Represent. , 2021. [Online]. Available: https://openreview.net/forum?id=_0kaDkv3dVf\n- [16] J. Mellor, J. Turner, A. Storkey, and E. J. Crowley, ''Neural architecture search without training,'' 2020, arXiv:2006.04647 . [Online]. Available: https://arxiv.org/abs/2006.04647\n- [17] M. Forouzesh, F. Salehi, and P. Thiran, ''Generalization comparison of deep neural networks via output sensitivity,'' in Proc. 25th Int. Conf. Pattern Recognit. (ICPR) , Jan. 2021, pp. 7411GLYPH<21>7418.\n- [18] R. Novak, Y. Bahri, D. A. AbolaGLYPH<28>a, J. Pennington, and J. Sohl-Dickstein, ''Sensitivity and generalization in neural networks: An empirical study,'' 2018, arXiv:1802.08760 . [Online]. Available: https://arxiv.org/abs/1802.08760\n- [19] K. Kawaguchi, L. P. Kaelbling, and Y. Bengio, ''Generalization in deep learning,'' 2017, arXiv:1710.05468 . [Online]. Available: https://arxiv.org/abs/1710.05468\n- [20] A. Jacot, F. Gabriel, and C. Hongler, ''Neural tangent kernel: Convergence and generalization in neural networks,'' 2018, arXiv:1806.07572 . [Online]. Available: https://arxiv.org/abs/1806.07572\n- [21] H. Xiong, L. Huang, M. Yu, L. Liu, F. Zhu, and L. Shao, ''On the number of linear regions of convolutional neural networks,'' in Proc. 37th Int. Conf. Mach. Learn. , vol. 119, H. D. III and A. Singh, Eds. Jul. 2020, pp. 10514GLYPH<21>10523.\n- [22] M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, and J. Sohl-Dickstein, ''On the expressive power of deep neural networks,'' 2017, arXiv:1606.05336 . [Online]. Available: https://arxiv.org/abs/1606.05336\n- [23] L. Xiao, J. Pennington, and S. S. Schoenholz, ''Disentangling trainability and generalization in deep learning,'' 2019, arXiv:1912.13053 . [Online]. Available: https://arxiv.org/abs/1912.13053\n- [24] W. Chen, X. Gong, and Z. Wang, ''Neural architecture search on ImageNet in four GPU hours: A theoretically inspired perspective,'' in Proc. Int. Conf. Learn. Represent. , 2021. [Online]. Available: https://openreview.net/forum?id=Cnon5ezMHtu\n- [25] T. Devries and G. W. Taylor, ''Improved regularization of convolutional neural networks with cutout,'' 2017, arXiv:1708.04552 . [Online]. Available: https://arxiv.org/abs/1708.04552\n- [26] B. Baker, O. Gupta, N. Naik, and R. Raskar, ''Designing neural network architectures using reinforcement learning,'' 2016, arXiv:1611.02167 . [Online]. Available: https://arxiv.org/abs/1611.02167\n- [27] Z. Zhong, J. Yan, W. Wu, J. Shao, and C.-L. Liu, ''Practical blockwise neural network architecture generation,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , Jun. 2018, pp. 2423GLYPH<21>2432.\n- [28] L. Xie and A. Yuille, ''Genetic CNN,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , Oct. 2017, pp. 1379GLYPH<21>1388.\n- [29] E. Real, S. Moore, A. Selle, S. Saxena, Y . L. Suematsu, J. Tan, Q. V . Le, and A. Kurakin, ''Large-scale evolution of image classiGLYPH<28>ers,'' in Proc. 34th Int. Conf. Mach. Learn. (ICML) , Sydney, NSW, Australia, vol. 70, Aug. 2017, pp. 2902GLYPH<21>2911.\n- [30] T. Elsken, J. H. Metzen, and F. Hutter, ''EfGLYPH<28>cient multi-objective neural architecture search via Lamarckian evolution,'' in Proc. Int. Conf. Learn. Represent. , Sep. 2019. [Online]. Available: https:// openreview.net/forum?id=ByME42AqK7\n- [31] X. Chen, L. Xie, J. Wu, and Q. Tian, ''Progressive differentiable architecture search: Bridging the depth gap between search and evaluation,'' 2019, arXiv:1904.12760 . [Online]. Available: https://arxiv.org/abs/1904.12760\n- [32] C. Ying, A. Klein, E. Christiansen, E. Real, K. Murphy, and F. Hutter, ''Nas-bench-101: Towards reproducible neural architecture search,'' in Proc. ICML , 2019, pp. 7105GLYPH<21>7114.\n- [33] X. Dong and Y. Yang, ''NAS-Bench-201: Extending the scope of reproducible neural architecture search,'' in Proc. Int. Conf. Learn. Represent. , 2020, pp. 2GLYPH<21>17.\n- [34] P. Chrabaszcz, I. Loshchilov, and F. Hutter, ''A downsampled variant of ImageNet as an alternative to the CIFAR datasets,'' 2017, arXiv:1707.08819 . [Online]. Available: https://arxiv.org/abs/1707.08819\n- [35] J. Yang, X. Zeng, S. Zhong, and S. Wu, ''Effective neural network ensemble approach for improving generalization performance,'' IEEE Trans. Neural Netw. Learn. Syst. , vol. 24, no. 6, pp. 878GLYPH<21>887, Jun. 2013.\n- [36] R. Pascanu, G. Montufar, and Y. Bengio, ''On the number of response regions of deep feed forward networks with piece-wise linear activations,'' 2014, arXiv:1312.6098 . [Online]. Available: https://arxiv.org/abs/1312.6098",
    "context": "Provides a collection of research papers exploring neural architecture search (NAS) techniques and their impact on generalization and efficiency.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      10
    ],
    "id": "c5d60e8a2b8e555927e995f66f4402b07d4a183e4821aa3e5cf09bf4bf8605e5"
  },
  {
    "text": "- [37] J. Lee, L. Xiao, S. S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, and J. Pennington, ''Wide neural networks of any depth evolve as linear models under gradient descent,'' J. Stat. Mech. Theory Exp. , vol. 2020, no. 12, Dec. 2020, Art. no. 124002.\n- [38] L. Zhang, Y. Shen, and H. Li, ''VSI: A visual saliency-induced index for perceptual image quality assessment,'' IEEE Trans. Image Process. , vol. 23, no. 10, pp. 4270GLYPH<21>4281, Aug. 2014.\n- [39] S.-H. Bae and M. Kim, ''DCT-QM: A DCT-based quality degradation metric for image quality optimization problems,'' IEEE Trans. Image Process. , vol. 25, no. 10, pp. 4916GLYPH<21>4930, Oct. 2016.\n- [40] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, ''Image quality assessment: From error visibility to structural similarity,'' IEEE Trans. Image Process. , vol. 13, no. 4, pp. 600GLYPH<21>612, Apr. 2004.\n- [41] H. R. Sheikh, M. F. Sabir, and A. C. Bovik, ''A statistical evaluation of recent full reference image quality assessment algorithms,'' IEEE Trans. Image Process. , vol. 15, no. 11, pp. 3440GLYPH<21>3451, Nov. 2006.\n- [42] E. Real, A. Aggarwal, Y. Huang, and Q. V. Le, ''Regularized evolution for image classiGLYPH<28>er architecture search,'' in Proc. AAAI Conf. Artif. Intell. , vol. 33, Jul. 2019, pp. 4780GLYPH<21>4789.\n- [43] K. He, X. Zhang, S. Ren, and J. Sun, ''Delving deep into rectiGLYPH<28>ers: Surpassing human-level performance on imagenet classiGLYPH<28>cation,'' 2015, arXiv:1502.01852 . [Online]. Available: https://arxiv.org/abs/1502.01852\n- [44] B. Wu, K. Keutzer, X. Dai, P. Zhang, Y. Wang, F. Sun, Y. Wu, Y. Tian, P. Vajda, and Y. Jia, ''FBNet: Hardware-aware efGLYPH<28>cient ConvNet design via differentiable neural architecture search,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2019, pp. 10734GLYPH<21>10742.\nLINH-TAM TRAN received the bachelor's degree from the Department of Computer Science and Engineering, Ho Chi Minh City University of Technology, Vietnam, in 2014, and the M.S. degree from Hongik University, Seoul, South Korea, in 2018. He is currently pursuing the Ph.D. degree with the Department of Computer Science and Engineering, Kyung Hee University, Suwon, South Korea. His research interest includes neural architecture search for practical applications.\nMUHAMMAD SALMAN ALI received the bachelor's degree in computer science from the National University of Sciences and Technology (NUST), Islamabad, Pakistan, in 2018. He is currently pursuing the M.S. degree leading to the Ph.D. degree with Kyung Hee University, South Korea. His research interests include deep learning interpretation and the effect of soft errors on deep neural networks. He was a recipient of the gold medal for being a high-achiever during his UG studies.\nSUNG-HO BAE (Member, IEEE) received the B.S. degree from Kyung Hee University, South Korea, in 2011, and the M.S. and Ph.D. degrees from Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea, in 2012 and 2016, respectively. From 2016 to 2017, he was a Postdoctoral Associate with the Computer Science and ArtiGLYPH<28>cial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT), MA, USA. Since 2017, he has been an Assistant Professor with the Department of Computer Science and Engineering, Kyung Hee University. He has been involved in model compression/interpretation for deep neural networks and inverse problems in image processing and computer vision.\n\nProvides a survey of image quality assessment algorithms and techniques, including both traditional and neural network-based approaches.",
    "original_text": "- [37] J. Lee, L. Xiao, S. S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, and J. Pennington, ''Wide neural networks of any depth evolve as linear models under gradient descent,'' J. Stat. Mech. Theory Exp. , vol. 2020, no. 12, Dec. 2020, Art. no. 124002.\n- [38] L. Zhang, Y. Shen, and H. Li, ''VSI: A visual saliency-induced index for perceptual image quality assessment,'' IEEE Trans. Image Process. , vol. 23, no. 10, pp. 4270GLYPH<21>4281, Aug. 2014.\n- [39] S.-H. Bae and M. Kim, ''DCT-QM: A DCT-based quality degradation metric for image quality optimization problems,'' IEEE Trans. Image Process. , vol. 25, no. 10, pp. 4916GLYPH<21>4930, Oct. 2016.\n- [40] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, ''Image quality assessment: From error visibility to structural similarity,'' IEEE Trans. Image Process. , vol. 13, no. 4, pp. 600GLYPH<21>612, Apr. 2004.\n- [41] H. R. Sheikh, M. F. Sabir, and A. C. Bovik, ''A statistical evaluation of recent full reference image quality assessment algorithms,'' IEEE Trans. Image Process. , vol. 15, no. 11, pp. 3440GLYPH<21>3451, Nov. 2006.\n- [42] E. Real, A. Aggarwal, Y. Huang, and Q. V. Le, ''Regularized evolution for image classiGLYPH<28>er architecture search,'' in Proc. AAAI Conf. Artif. Intell. , vol. 33, Jul. 2019, pp. 4780GLYPH<21>4789.\n- [43] K. He, X. Zhang, S. Ren, and J. Sun, ''Delving deep into rectiGLYPH<28>ers: Surpassing human-level performance on imagenet classiGLYPH<28>cation,'' 2015, arXiv:1502.01852 . [Online]. Available: https://arxiv.org/abs/1502.01852\n- [44] B. Wu, K. Keutzer, X. Dai, P. Zhang, Y. Wang, F. Sun, Y. Wu, Y. Tian, P. Vajda, and Y. Jia, ''FBNet: Hardware-aware efGLYPH<28>cient ConvNet design via differentiable neural architecture search,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2019, pp. 10734GLYPH<21>10742.\nLINH-TAM TRAN received the bachelor's degree from the Department of Computer Science and Engineering, Ho Chi Minh City University of Technology, Vietnam, in 2014, and the M.S. degree from Hongik University, Seoul, South Korea, in 2018. He is currently pursuing the Ph.D. degree with the Department of Computer Science and Engineering, Kyung Hee University, Suwon, South Korea. His research interest includes neural architecture search for practical applications.\nMUHAMMAD SALMAN ALI received the bachelor's degree in computer science from the National University of Sciences and Technology (NUST), Islamabad, Pakistan, in 2018. He is currently pursuing the M.S. degree leading to the Ph.D. degree with Kyung Hee University, South Korea. His research interests include deep learning interpretation and the effect of soft errors on deep neural networks. He was a recipient of the gold medal for being a high-achiever during his UG studies.\nSUNG-HO BAE (Member, IEEE) received the B.S. degree from Kyung Hee University, South Korea, in 2011, and the M.S. and Ph.D. degrees from Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea, in 2012 and 2016, respectively. From 2016 to 2017, he was a Postdoctoral Associate with the Computer Science and ArtiGLYPH<28>cial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT), MA, USA. Since 2017, he has been an Assistant Professor with the Department of Computer Science and Engineering, Kyung Hee University. He has been involved in model compression/interpretation for deep neural networks and inverse problems in image processing and computer vision.",
    "context": "Provides a survey of image quality assessment algorithms and techniques, including both traditional and neural network-based approaches.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      10
    ],
    "id": "a8ce875bbcef1cf37e2779e3b776eb75a4b37921d7d8eb3ef6e1bd90bd4e921d"
  },
  {
    "text": "Received July 9, 2021, accepted July 18, 2021, date of publication July 26, 2021, date of current version August 3, 2021.\nDigital Object Identifier 10.1 109/ACCESS.2021.3100316\n\nDocuments publication details and version history.",
    "original_text": "Received July 9, 2021, accepted July 18, 2021, date of publication July 26, 2021, date of current version August 3, 2021.\nDigital Object Identifier 10.1 109/ACCESS.2021.3100316",
    "context": "Documents publication details and version history.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      1
    ],
    "id": "06660b4f59094e9bccd86f11ac4654aadb9b137697af5c4c5abffdbf9e7a2659"
  },
  {
    "text": "A. B. M. BODRUL ALAM 1 , (Member, IEEE), ZUBAIR MD. FADLULLAH 1,2 , (Senior Member, IEEE), AND SALIMUR CHOUDHURY 2 , (Senior Member, IEEE)\n1 Thunder Bay Regional Health Research Institute (TBRHRI), Thunder Bay, ON P7B 7A5, Canada 2 Department of Computer Science, Lakehead University, Thunder Bay, ON P7B 5E1, Canada\nCorresponding author: A. B. M. Bodrul Alam (abalam@lakeheadu.ca)\nABSTRACT Allocating and managing resources while considering the quality of service is considered a fundamental and complex research problem in a cloud environment. An optimal resource allocation optimizes several parameters such as optimizing cost and resource utilization or maximizing any quality parameters. However, to ensure better customer service, Cloud Service Providers (CSPs) should consider most of the quality attributes while allocating resources to the cloud infrastructures. Existing research does not evaluate trust as a quantitative attribute, thus a trade-off between trust and performance in resource allocation is also absent in the research area. We propose a model to consider both trust and delay in this paper. The trust of a CSP is quantitatively estimated through some attributes and metrics. Availability, reliability, data integrity, and efGLYPH<28>ciency are considered to estimate the trust. The objective is to maximize the trust of the allocation while minimizing the communication delay. The proposed joint optimization model combines the previous credentials of the CSPs and the present resource constraints. To solve the problem heuristically, a genetic algorithm is applied. The model uses a number of parameters that provide the GLYPH<29>exibility to adapt several service requirements. The effectiveness and applicability of the proposed approach are demonstrated through experiments. The results ensure that the effectiveness in the estimation of trust evaluation for different CSPs with the proposed attributes. Moreover, integrating trust in the resource allocation model allocates appropriate resources while enhancing the trust and reducing the communication delay in the overall allocation.\nINDEX TERMS Cloud computing, trust evaluation, resource allocation, availability, reliability, optimization, genetic algorithm.\n\nIntroduces a model for jointly optimizing trust and delay in cloud resource allocation, incorporating CSP credentials and present resource constraints.",
    "original_text": "A. B. M. BODRUL ALAM 1 , (Member, IEEE), ZUBAIR MD. FADLULLAH 1,2 , (Senior Member, IEEE), AND SALIMUR CHOUDHURY 2 , (Senior Member, IEEE)\n1 Thunder Bay Regional Health Research Institute (TBRHRI), Thunder Bay, ON P7B 7A5, Canada 2 Department of Computer Science, Lakehead University, Thunder Bay, ON P7B 5E1, Canada\nCorresponding author: A. B. M. Bodrul Alam (abalam@lakeheadu.ca)\nABSTRACT Allocating and managing resources while considering the quality of service is considered a fundamental and complex research problem in a cloud environment. An optimal resource allocation optimizes several parameters such as optimizing cost and resource utilization or maximizing any quality parameters. However, to ensure better customer service, Cloud Service Providers (CSPs) should consider most of the quality attributes while allocating resources to the cloud infrastructures. Existing research does not evaluate trust as a quantitative attribute, thus a trade-off between trust and performance in resource allocation is also absent in the research area. We propose a model to consider both trust and delay in this paper. The trust of a CSP is quantitatively estimated through some attributes and metrics. Availability, reliability, data integrity, and efGLYPH<28>ciency are considered to estimate the trust. The objective is to maximize the trust of the allocation while minimizing the communication delay. The proposed joint optimization model combines the previous credentials of the CSPs and the present resource constraints. To solve the problem heuristically, a genetic algorithm is applied. The model uses a number of parameters that provide the GLYPH<29>exibility to adapt several service requirements. The effectiveness and applicability of the proposed approach are demonstrated through experiments. The results ensure that the effectiveness in the estimation of trust evaluation for different CSPs with the proposed attributes. Moreover, integrating trust in the resource allocation model allocates appropriate resources while enhancing the trust and reducing the communication delay in the overall allocation.\nINDEX TERMS Cloud computing, trust evaluation, resource allocation, availability, reliability, optimization, genetic algorithm.",
    "context": "Introduces a model for jointly optimizing trust and delay in cloud resource allocation, incorporating CSP credentials and present resource constraints.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      1
    ],
    "id": "267eba1200173fbc0bf2a11dcf56d5b9425739b5cb16ca3ab7a63b2a0ea693b1"
  },
  {
    "text": "Cloud is a popular computing paradigm and addressed intensively by both academia and industry. Cloud has become popular due to its unique characteristics. Some of the cloud characteristics also negatively impact the quality of service. The unique GLYPH<28>ve characteristics of cloud which improve performance can also increase risks [1]. For instance, there are several applications for which missing any provisioning of services is considered as a failure [2]. Broad network access introduces new challenges. The networking service reliability, availability, and latency directly impact the Quality of Service (QoS) in a cloud environment. Another characteristic is related to rapid elasticity. If the service is not capable of adapting to the current load then elasticity failure may occur that will impact the reliability of the service. The last feature concerns the measured service. The measured service is the computation of resource usage. If any loss or unavailability of resources occurs, it can lead to unreliable or inaccurate estimation in data-related measurements.\nThe associate editor coordinating the review of this manuscript and approving it for publication was Ali Kashif Bashir .\nThe fundamental characteristic of the cloud is resource sharing by multiple users. Resources in the cloud can be dynamically re-allocated based on users' demands and preferences. Allocating resources appropriately is considered one of the primary concerns in cloud computing. The word appropriate can be any of the performance metrics: cost-effectiveness, accuracy, and reliability. Allocating resource appropriately refers to such an allocation that integrates the resource utilization of the service providers while meeting customer's requirements. Allocation of an\noptimal resource must avoid resource contention, fragmentation, over-provisioning, and under-provisioning. Resource contention occurs when several applications request the same resources at a speciGLYPH<28>c time. Due to the unavailability of resources, scarcity of resources may happen. Fragmentation of resources may take place for resource isolation. Over-provisioning happens if providers allocate more resources than the demand and under-provisioning may happen if providers allocate fewer resources than its demand. We try to avoid all these cases in our resource allocation framework.\nThe main obstacles of this paradigm are reliability and security. To address reliability and security both, we propose a model using four quality attributes: availability, reliability, data integrity, and efGLYPH<28>ciency to estimate the trust value of a CSP. At GLYPH<28>rst, the experiments on the evaluation for the trust model are conducted. After that, the results are integrated into the optimization of the resource allocation model. Our contributions in this paper are threefold V\n- GLYPH<15> Most of the research work in the literature used trust as a quality attribute. In this work, we evaluate trust as a quantitative attribute and estimate the values of trust of different service providers based on our proposed four quality attributes. The values of the quality attributes are estimated based on a set of metrics through a simulation environment. We combine the four quality attributes as a weighted sum to obtain the trust of a provider.\n- GLYPH<15> We integrate the estimated trust value into a resource allocation model in a multi-cloud environment. The resource allocation model also considers the trafGLYPH<28>c and capacity of the CSPs to evaluate the communication delay. We design the model as a joint optimization problem and aim to maximize trust and minimize communication delay to improve the QoS.\n- GLYPH<15> We solve the resource allocation problem optimally and heuristically. Intlinprog in MATLAB is used as an optimizer to solve the proposed resource allocation model. The optimizer requires several hours to obtain an optimized solution in some scenarios. To avoid such situations, we use a meta-heuristic, a Genetic Algorithm for generating computationally efGLYPH<28>cient approximated solutions. We compare the results of our heuristics with the optimal solution. The solution of the Genetic algorithm achieves 90% similarity to the exact solution. The execution time of the Genetic Algorithm increases linearly with an increase in the number of the CSPs and servers. These trends ensure the validity of the proposed trust model in a practical cloud environment.\nThe trust evaluation model considers most of the main elements of a cloud that impact the service performance. Results demonstrated that evaluation of trust and integrating that into the resource allocation model are effectively conducted. The simulation of the model conGLYPH<28>rms the trust maximization and delay minimization in the allocation. Thereby improves the quality of services. The model introduces GLYPH<29>exibility into the resource allocation process as adjusting some parameters can provide customized users' requirements. As trust and delay are conGLYPH<29>icting objectives of the model, the trade-off should be estimated with proper caution.\nThe organization of the rest of the paper is following. The existing research work regarding the evaluation of quality attributes of cloud, optimization of resource utilization, and application of the meta-heuristic algorithm in cloud resource allocation are discussed in Section II. The proposed trust evaluation model is described in Section III. The resource allocation and problem formulation of our optimization model are presented in Section IV. This section also presents the proposed algorithm to solve the problem. The experimental setup and results analysis are discussed in Section V. Finally, Section VI concludes the paper by mentioning some future research directions.\n\nDetails the challenges of cloud characteristics (elasticity, measured service) and proposes a trust model integrated into a resource allocation framework to improve QoS.",
    "original_text": "Cloud is a popular computing paradigm and addressed intensively by both academia and industry. Cloud has become popular due to its unique characteristics. Some of the cloud characteristics also negatively impact the quality of service. The unique GLYPH<28>ve characteristics of cloud which improve performance can also increase risks [1]. For instance, there are several applications for which missing any provisioning of services is considered as a failure [2]. Broad network access introduces new challenges. The networking service reliability, availability, and latency directly impact the Quality of Service (QoS) in a cloud environment. Another characteristic is related to rapid elasticity. If the service is not capable of adapting to the current load then elasticity failure may occur that will impact the reliability of the service. The last feature concerns the measured service. The measured service is the computation of resource usage. If any loss or unavailability of resources occurs, it can lead to unreliable or inaccurate estimation in data-related measurements.\nThe associate editor coordinating the review of this manuscript and approving it for publication was Ali Kashif Bashir .\nThe fundamental characteristic of the cloud is resource sharing by multiple users. Resources in the cloud can be dynamically re-allocated based on users' demands and preferences. Allocating resources appropriately is considered one of the primary concerns in cloud computing. The word appropriate can be any of the performance metrics: cost-effectiveness, accuracy, and reliability. Allocating resource appropriately refers to such an allocation that integrates the resource utilization of the service providers while meeting customer's requirements. Allocation of an\noptimal resource must avoid resource contention, fragmentation, over-provisioning, and under-provisioning. Resource contention occurs when several applications request the same resources at a speciGLYPH<28>c time. Due to the unavailability of resources, scarcity of resources may happen. Fragmentation of resources may take place for resource isolation. Over-provisioning happens if providers allocate more resources than the demand and under-provisioning may happen if providers allocate fewer resources than its demand. We try to avoid all these cases in our resource allocation framework.\nThe main obstacles of this paradigm are reliability and security. To address reliability and security both, we propose a model using four quality attributes: availability, reliability, data integrity, and efGLYPH<28>ciency to estimate the trust value of a CSP. At GLYPH<28>rst, the experiments on the evaluation for the trust model are conducted. After that, the results are integrated into the optimization of the resource allocation model. Our contributions in this paper are threefold V\n- GLYPH<15> Most of the research work in the literature used trust as a quality attribute. In this work, we evaluate trust as a quantitative attribute and estimate the values of trust of different service providers based on our proposed four quality attributes. The values of the quality attributes are estimated based on a set of metrics through a simulation environment. We combine the four quality attributes as a weighted sum to obtain the trust of a provider.\n- GLYPH<15> We integrate the estimated trust value into a resource allocation model in a multi-cloud environment. The resource allocation model also considers the trafGLYPH<28>c and capacity of the CSPs to evaluate the communication delay. We design the model as a joint optimization problem and aim to maximize trust and minimize communication delay to improve the QoS.\n- GLYPH<15> We solve the resource allocation problem optimally and heuristically. Intlinprog in MATLAB is used as an optimizer to solve the proposed resource allocation model. The optimizer requires several hours to obtain an optimized solution in some scenarios. To avoid such situations, we use a meta-heuristic, a Genetic Algorithm for generating computationally efGLYPH<28>cient approximated solutions. We compare the results of our heuristics with the optimal solution. The solution of the Genetic algorithm achieves 90% similarity to the exact solution. The execution time of the Genetic Algorithm increases linearly with an increase in the number of the CSPs and servers. These trends ensure the validity of the proposed trust model in a practical cloud environment.\nThe trust evaluation model considers most of the main elements of a cloud that impact the service performance. Results demonstrated that evaluation of trust and integrating that into the resource allocation model are effectively conducted. The simulation of the model conGLYPH<28>rms the trust maximization and delay minimization in the allocation. Thereby improves the quality of services. The model introduces GLYPH<29>exibility into the resource allocation process as adjusting some parameters can provide customized users' requirements. As trust and delay are conGLYPH<29>icting objectives of the model, the trade-off should be estimated with proper caution.\nThe organization of the rest of the paper is following. The existing research work regarding the evaluation of quality attributes of cloud, optimization of resource utilization, and application of the meta-heuristic algorithm in cloud resource allocation are discussed in Section II. The proposed trust evaluation model is described in Section III. The resource allocation and problem formulation of our optimization model are presented in Section IV. This section also presents the proposed algorithm to solve the problem. The experimental setup and results analysis are discussed in Section V. Finally, Section VI concludes the paper by mentioning some future research directions.",
    "context": "Details the challenges of cloud characteristics (elasticity, measured service) and proposes a trust model integrated into a resource allocation framework to improve QoS.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      1,
      2
    ],
    "id": "06655edbbfd91f61791b74897d6919a43f632b94d61b97ebac8a0bcfb18fd03a"
  },
  {
    "text": "The demand for cloud resources changes rapidly and impacts the availability of cloud resources over time. The occurrence of failures is unpredictable and directly impacts the reliability of the cloud. Optimal allocation of resources in a cloud is crucial and can be solved using heuristics. The general process for allocating resources requires three parties: customer, broker, and service provider. At GLYPH<28>rst, customers specify the amounts of resources along with the constraints related to service performance. Then, a broker is responsible for GLYPH<28>nding a proper CSP or multiple CSPs that can satisfy customers' requirements and transfers the requests to the chosen CSPs. After that, CSPs allocate resources to their data centers based on availability. In such allocation, resource utilization, and customers' requirements both constraints are fulGLYPH<28>lled. Managing and optimizing the allocation of resources appropriately in a cloud environment is crucial. There have been many recent efforts to optimize resource allocation and we acknowledge a few of them below.\nAnalysis of hardware reliability of cloud and the causes of the failures impacting the reliability are discussed in [3]. The authors investigate different parameters such as server age, conGLYPH<28>guration, location, load executing on the infrastructure and conclude that none of the parameters directly impact the failures signiGLYPH<28>cantly. Selecting an optimal channel for improving reliability and minimizing energy budget is the objective of this work [4]. A machine learning approach with reliability improvement and energy awareness is applied in the proposed method. The unique dimensions of resource management in IoT such as managing memory, energy, and process are discussed with pros and cons [5]. A modiGLYPH<28>ed Max-Min algorithm is applied for allocating resources in a cloud environment [6]. The modiGLYPH<28>cation improves the average waiting time and completion time. The method maximizes the overall resource utilization. A model to evaluate cloud reliability is presented in [7]. The authors use a state transition graph with three states for the evaluation. A Semi-Markov model is used for estimating the reliability using the state transition probability. Some of the software failures are considered in the evaluation. For the physical\nresource breakdown, the authors use the mean-time to recovery and failure for availability and downtime. Failures of all main elements of a cloud are not considered in the estimation of cloud reliability. In the model, the authors calculate the reliability of each physical resource there is no focus on performing a reliable allocation of resources for the requested services. A reliability evaluation model for cloud environment is presented in [8]. The model proposes some attributes and metrics for reliability evaluation. Four quality attributes are chosen for calculating the reliability of a cloud. For each of the attributes, the authors proposed some metrics for the estimation and GLYPH<28>nally combine them using a weighted sum technique. Based on response time, reliability and performance in a cloud are presented in [9]. The response time was calculated based on three parameters: waiting time, service time, and execution time. Queuing theory is applied in the estimation of waiting time and service time. Response time was estimated by combining the above attributes. Faragardi et al. [10] proposed an evaluation model for cloud reliability. To evaluate system reliability, they considered the reliability of servers and links.\nThe primary concern of most of the research is optimizing resource utilization. Resource allocation techniques are discussed with an objective to control various parameters: costs, execution time, and scheduling policy. For instance, Obtaining the minimum servers is the objective of this scheme [11]. The authors propose a heuristic for an allocation that minimizes the requirements of the servers through response time distribution. The effectiveness of the mechanism is evaluated with several performance measurements. The approach does not consider any other parameters that also directly impact service performance. Scheduling task in an efGLYPH<28>cient manner while minimizing the power consumption is the aim of this work [12]. A prediction mechanism is used for allocating resources and reducing power consumption. A resource allocation problem in a cloud radio access network is shown in the model for choosing the optimal technique to combine all the available spectral bands [13]. A modiGLYPH<28>ed version of the cloud radio access network is proposed for handling spectral resources efGLYPH<28>ciently. Another model for minimizing the server quantity of cloud is presented in [14]. A prediction algorithm that can model the load in advance is applied to predict future resource usages. The authors presented an evaluation to prove the validity of the proposed model. The evaluation shows that it can avoid the occurring of overload and achieve green computing. A resource allocation scheme for monitoring and predicting future requirements is proposed in [15]. The scheme adjusts VMs based on actual resource needs. There are two primary challenges to this task. The GLYPH<28>rst one is when to reallocate resources and the second one is how much resource needs adjustment. However, the performance shows the applicability of the method but adjusting and selecting the threshold is critical and it significantly impacts resource optimization. For a hybrid Cloud, Grewal and Pateria [16] propose a resource provisioning technique. A private cloud is capable of scaling up using\npublic cloud resources with the help of a resource manager. In allocating resources, most of the approaches are concerned only with resource utilization. To improve the performance, integration of several other performance metrics like reliability, efGLYPH<28>ciency should also be required into the allocation model.\nFor an appropriate VM allocation in a cloud, a trafGLYPH<28>c-aware algorithm is presented in [17]. A swarm optimization algorithm is applied for solving the VM allocation problem. The simulation results show that the algorithm can signiGLYPH<28>cantly improve energy consumption and control the loss of communication links. The authors propose a model to maximize resource utilization and minimize energy consumption [18]. A particle swarm optimization method is used to solve the problem. A cloud resource allocation model to improve security is proposed by Halabi et al. [19]. The authors designed the allocation problem as an optimization problem and propose a heuristic for the solution. For forecasting the utilization of resources in a data center, a Genetic Algorithm is proposed in [20]. The GA forecasts the requirements of the resources depending on the previous data. A resource allocation approach depending on resource prediction is also proposed in the work. The validity and applicability of the method are presented through simulation results. The results show that GA is superior for an accurate prediction. Moreover, it improves the utilization of CPU and memory and the consumption of energy. Finally, a fault-aware resource allocation for cloud is proposed by Gupta and Ghrera [21]. The authors aim for the maximization of the utilization of resources and consumption of power for the servers.\nMost of the research in this area proposes approaches to evaluate some quality attributes and optimize those attributes. The absence of standard quantitative attributes and metrics and the incompleteness in considering the major elements of cloud infrastructure are ensured through the existing cloud literature. No research work in the literature considers most of the major cloud elements in the evaluation to improve QoS. The objective of our approach is to maximize trust and minimize the communication delay to improve service performance. Trust will be calculated based on the availability, reliability, data integrity, and efGLYPH<28>ciency of a CSP while allocating resources efGLYPH<28>ciently based on users' requirements. Consideration of trust and communication delay together in the optimization problem makes our model demanding and appropriate.\n\nSummarizes recent research focusing on cloud resource allocation, primarily emphasizing optimization techniques and quality attribute evaluation, while noting a lack of standardized metrics and comprehensive infrastructure consideration.",
    "original_text": "The demand for cloud resources changes rapidly and impacts the availability of cloud resources over time. The occurrence of failures is unpredictable and directly impacts the reliability of the cloud. Optimal allocation of resources in a cloud is crucial and can be solved using heuristics. The general process for allocating resources requires three parties: customer, broker, and service provider. At GLYPH<28>rst, customers specify the amounts of resources along with the constraints related to service performance. Then, a broker is responsible for GLYPH<28>nding a proper CSP or multiple CSPs that can satisfy customers' requirements and transfers the requests to the chosen CSPs. After that, CSPs allocate resources to their data centers based on availability. In such allocation, resource utilization, and customers' requirements both constraints are fulGLYPH<28>lled. Managing and optimizing the allocation of resources appropriately in a cloud environment is crucial. There have been many recent efforts to optimize resource allocation and we acknowledge a few of them below.\nAnalysis of hardware reliability of cloud and the causes of the failures impacting the reliability are discussed in [3]. The authors investigate different parameters such as server age, conGLYPH<28>guration, location, load executing on the infrastructure and conclude that none of the parameters directly impact the failures signiGLYPH<28>cantly. Selecting an optimal channel for improving reliability and minimizing energy budget is the objective of this work [4]. A machine learning approach with reliability improvement and energy awareness is applied in the proposed method. The unique dimensions of resource management in IoT such as managing memory, energy, and process are discussed with pros and cons [5]. A modiGLYPH<28>ed Max-Min algorithm is applied for allocating resources in a cloud environment [6]. The modiGLYPH<28>cation improves the average waiting time and completion time. The method maximizes the overall resource utilization. A model to evaluate cloud reliability is presented in [7]. The authors use a state transition graph with three states for the evaluation. A Semi-Markov model is used for estimating the reliability using the state transition probability. Some of the software failures are considered in the evaluation. For the physical\nresource breakdown, the authors use the mean-time to recovery and failure for availability and downtime. Failures of all main elements of a cloud are not considered in the estimation of cloud reliability. In the model, the authors calculate the reliability of each physical resource there is no focus on performing a reliable allocation of resources for the requested services. A reliability evaluation model for cloud environment is presented in [8]. The model proposes some attributes and metrics for reliability evaluation. Four quality attributes are chosen for calculating the reliability of a cloud. For each of the attributes, the authors proposed some metrics for the estimation and GLYPH<28>nally combine them using a weighted sum technique. Based on response time, reliability and performance in a cloud are presented in [9]. The response time was calculated based on three parameters: waiting time, service time, and execution time. Queuing theory is applied in the estimation of waiting time and service time. Response time was estimated by combining the above attributes. Faragardi et al. [10] proposed an evaluation model for cloud reliability. To evaluate system reliability, they considered the reliability of servers and links.\nThe primary concern of most of the research is optimizing resource utilization. Resource allocation techniques are discussed with an objective to control various parameters: costs, execution time, and scheduling policy. For instance, Obtaining the minimum servers is the objective of this scheme [11]. The authors propose a heuristic for an allocation that minimizes the requirements of the servers through response time distribution. The effectiveness of the mechanism is evaluated with several performance measurements. The approach does not consider any other parameters that also directly impact service performance. Scheduling task in an efGLYPH<28>cient manner while minimizing the power consumption is the aim of this work [12]. A prediction mechanism is used for allocating resources and reducing power consumption. A resource allocation problem in a cloud radio access network is shown in the model for choosing the optimal technique to combine all the available spectral bands [13]. A modiGLYPH<28>ed version of the cloud radio access network is proposed for handling spectral resources efGLYPH<28>ciently. Another model for minimizing the server quantity of cloud is presented in [14]. A prediction algorithm that can model the load in advance is applied to predict future resource usages. The authors presented an evaluation to prove the validity of the proposed model. The evaluation shows that it can avoid the occurring of overload and achieve green computing. A resource allocation scheme for monitoring and predicting future requirements is proposed in [15]. The scheme adjusts VMs based on actual resource needs. There are two primary challenges to this task. The GLYPH<28>rst one is when to reallocate resources and the second one is how much resource needs adjustment. However, the performance shows the applicability of the method but adjusting and selecting the threshold is critical and it significantly impacts resource optimization. For a hybrid Cloud, Grewal and Pateria [16] propose a resource provisioning technique. A private cloud is capable of scaling up using\npublic cloud resources with the help of a resource manager. In allocating resources, most of the approaches are concerned only with resource utilization. To improve the performance, integration of several other performance metrics like reliability, efGLYPH<28>ciency should also be required into the allocation model.\nFor an appropriate VM allocation in a cloud, a trafGLYPH<28>c-aware algorithm is presented in [17]. A swarm optimization algorithm is applied for solving the VM allocation problem. The simulation results show that the algorithm can signiGLYPH<28>cantly improve energy consumption and control the loss of communication links. The authors propose a model to maximize resource utilization and minimize energy consumption [18]. A particle swarm optimization method is used to solve the problem. A cloud resource allocation model to improve security is proposed by Halabi et al. [19]. The authors designed the allocation problem as an optimization problem and propose a heuristic for the solution. For forecasting the utilization of resources in a data center, a Genetic Algorithm is proposed in [20]. The GA forecasts the requirements of the resources depending on the previous data. A resource allocation approach depending on resource prediction is also proposed in the work. The validity and applicability of the method are presented through simulation results. The results show that GA is superior for an accurate prediction. Moreover, it improves the utilization of CPU and memory and the consumption of energy. Finally, a fault-aware resource allocation for cloud is proposed by Gupta and Ghrera [21]. The authors aim for the maximization of the utilization of resources and consumption of power for the servers.\nMost of the research in this area proposes approaches to evaluate some quality attributes and optimize those attributes. The absence of standard quantitative attributes and metrics and the incompleteness in considering the major elements of cloud infrastructure are ensured through the existing cloud literature. No research work in the literature considers most of the major cloud elements in the evaluation to improve QoS. The objective of our approach is to maximize trust and minimize the communication delay to improve service performance. Trust will be calculated based on the availability, reliability, data integrity, and efGLYPH<28>ciency of a CSP while allocating resources efGLYPH<28>ciently based on users' requirements. Consideration of trust and communication delay together in the optimization problem makes our model demanding and appropriate.",
    "context": "Summarizes recent research focusing on cloud resource allocation, primarily emphasizing optimization techniques and quality attribute evaluation, while noting a lack of standardized metrics and comprehensive infrastructure consideration.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      2,
      3
    ],
    "id": "dc3b06a81174ed4365d3a79e01fa4a3863d0be4d0ff2d92bdbf526e36b63b0cc"
  },
  {
    "text": "Trust evaluation in a cloud requires data of several parameters obtained from a real cloud computing environment. The unavailability of such practical environment's data makes any research problem more complex [22]. The problem is to evaluate the trust value of different service providers and design a model for the selection of CSPs while improving the QoS. We assume that all resources of a CSP stay in one data center. The proposed evaluation metrics are used to estimate the trust of a CSP.\n\nEstablishes the need for practical cloud data to evaluate service provider trust and proposes metrics for CSP selection and QoS improvement.",
    "original_text": "Trust evaluation in a cloud requires data of several parameters obtained from a real cloud computing environment. The unavailability of such practical environment's data makes any research problem more complex [22]. The problem is to evaluate the trust value of different service providers and design a model for the selection of CSPs while improving the QoS. We assume that all resources of a CSP stay in one data center. The proposed evaluation metrics are used to estimate the trust of a CSP.",
    "context": "Establishes the need for practical cloud data to evaluate service provider trust and proposes metrics for CSP selection and QoS improvement.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      3
    ],
    "id": "fe7a4d413a2c6ba8fc641563154b231ac89e7f70bbe53fae1a550bf4c370a534"
  },
  {
    "text": "Trust is evaluated quantitatively in the proposed model. We use two steps to evaluate the trust of a CSP. One is workload modeling that involves the assessment of the arrival rates of requests for resources such as CPU, memory, bandwidth requirements placed by users. In the second step, system modeling aims at evaluating the performance of a cloud system at the allocation based on the performance of the service providers. The model estimates trust based on the value of the QoS metrics which are availability, reliability, data integrity, and time efGLYPH<28>ciency.\nFIGURE 1. Proposed trust evaluation approach.\nWe compute trust based on the previous credentials of the CSPs using the four quality attributes and our proposed evaluation metrics shown in Fig. 1. Availability is one of the major concerning parameters in a cloud environment. Availability is calculated based on the resource availability of a CSP. All cloud service providers emphasize the availability of their service in their Service Level Agreement (SLA). Amazon, Google, Microsoft, IBM, and Salesforce maintain the availability of 99.9% which is still lower than the expectation [23]GLYPH<21>[26]. The availability of a CSP is calculated based on the rate of the accepted and total requests. We consider a request accepted if the request queue can process the request within the prescribed time threshold. The availability of CSP n is calculated using the equation below where ACrq is the rate of accepted requests and TOrq is the rate of incoming total requests.\n<!-- formula-not-decoded -->\nReliability ensures the ability of a service to keep functioning with a speciGLYPH<28>c performance level for a speciGLYPH<28>ed time. Reliability of cloud is not addressed or appropriately considered by the previous cloud researchers [27], [28]. However, due to many outages in a cloud, reliability issues have become a major concern in a cloud environment. We consider reliability as a failure-free operation. In terms of failures in the cloud, we consider hardware and software failures. Reliability is calculated based on the rate of successful and accepted requests. If a request completes the execution without any failure occurrence, we consider that request a successful one. The reliability of CSP n is calculated as below where SUrq is the rate of successful requests and ACrq is the rate of accepted requests.\n<!-- formula-not-decoded -->\nData integrity considers data loss due to latency and network failures. If a service is unable to grow fast then elasticity failure occurs that will impact the quality of service. We consider the elasticity failure as a network failure as it occurs due to an error in measuring load. Data integrity is checked by estimating the data loss. We use the equation below to estimate the data integrity of CSP n where SUrq is the rate of successful requests and NFrq is the rate of requests where a data failure occurs and generates a data loss.\n<!-- formula-not-decoded -->\nIn terms of efGLYPH<28>ciency, time efGLYPH<28>ciency is evaluated that considers the execution time and waiting time of each CSP. EfGLYPH<28>ciency is calculated based on the rate of promised and actual execution time. In an ideal case, the promised time ( PRt ) should be equal to the execution time ( EXt ) as the waiting time ( WAt ) is supposed to be 0.\n<!-- formula-not-decoded -->\nFinally, we combine all the individual rates multiplying with their corresponding weighting factors to compute the trust of a CSP as follows V\n<!-- formula-not-decoded -->\nWe repeat the same process for each of the CSP and evaluate the trust for all of them. The following architecture is used to evaluate the trust.\n\nDetails the quantitative evaluation of trust in cloud service providers based on QoS metrics (availability, reliability, data integrity, and efficiency) and their calculation methods.",
    "original_text": "Trust is evaluated quantitatively in the proposed model. We use two steps to evaluate the trust of a CSP. One is workload modeling that involves the assessment of the arrival rates of requests for resources such as CPU, memory, bandwidth requirements placed by users. In the second step, system modeling aims at evaluating the performance of a cloud system at the allocation based on the performance of the service providers. The model estimates trust based on the value of the QoS metrics which are availability, reliability, data integrity, and time efGLYPH<28>ciency.\nFIGURE 1. Proposed trust evaluation approach.\nWe compute trust based on the previous credentials of the CSPs using the four quality attributes and our proposed evaluation metrics shown in Fig. 1. Availability is one of the major concerning parameters in a cloud environment. Availability is calculated based on the resource availability of a CSP. All cloud service providers emphasize the availability of their service in their Service Level Agreement (SLA). Amazon, Google, Microsoft, IBM, and Salesforce maintain the availability of 99.9% which is still lower than the expectation [23]GLYPH<21>[26]. The availability of a CSP is calculated based on the rate of the accepted and total requests. We consider a request accepted if the request queue can process the request within the prescribed time threshold. The availability of CSP n is calculated using the equation below where ACrq is the rate of accepted requests and TOrq is the rate of incoming total requests.\n<!-- formula-not-decoded -->\nReliability ensures the ability of a service to keep functioning with a speciGLYPH<28>c performance level for a speciGLYPH<28>ed time. Reliability of cloud is not addressed or appropriately considered by the previous cloud researchers [27], [28]. However, due to many outages in a cloud, reliability issues have become a major concern in a cloud environment. We consider reliability as a failure-free operation. In terms of failures in the cloud, we consider hardware and software failures. Reliability is calculated based on the rate of successful and accepted requests. If a request completes the execution without any failure occurrence, we consider that request a successful one. The reliability of CSP n is calculated as below where SUrq is the rate of successful requests and ACrq is the rate of accepted requests.\n<!-- formula-not-decoded -->\nData integrity considers data loss due to latency and network failures. If a service is unable to grow fast then elasticity failure occurs that will impact the quality of service. We consider the elasticity failure as a network failure as it occurs due to an error in measuring load. Data integrity is checked by estimating the data loss. We use the equation below to estimate the data integrity of CSP n where SUrq is the rate of successful requests and NFrq is the rate of requests where a data failure occurs and generates a data loss.\n<!-- formula-not-decoded -->\nIn terms of efGLYPH<28>ciency, time efGLYPH<28>ciency is evaluated that considers the execution time and waiting time of each CSP. EfGLYPH<28>ciency is calculated based on the rate of promised and actual execution time. In an ideal case, the promised time ( PRt ) should be equal to the execution time ( EXt ) as the waiting time ( WAt ) is supposed to be 0.\n<!-- formula-not-decoded -->\nFinally, we combine all the individual rates multiplying with their corresponding weighting factors to compute the trust of a CSP as follows V\n<!-- formula-not-decoded -->\nWe repeat the same process for each of the CSP and evaluate the trust for all of them. The following architecture is used to evaluate the trust.",
    "context": "Details the quantitative evaluation of trust in cloud service providers based on QoS metrics (availability, reliability, data integrity, and efficiency) and their calculation methods.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      4
    ],
    "id": "39c9d8b65658d620c6eef25bebe364bdb99a3735461e6daff8fed21a14c0a12a"
  },
  {
    "text": "Cloud providers can have multiple data centers in different geographic locations [29]. The failure rates and reliability of servers have different values that exist on different racks. The primary cause of a rack failure is the interruption on the power distribution or the rack switches [30]. Each of the servers is a combination of resources such as CPU, storage, RAM, and bandwidth and each of these resources has different failure rates and reliability levels. An entire data center may fail due to a natural disaster or due to a prolonged power outage that can cause a network interruption, and as a consequence service outage can occur. In our model, for simulating a cloud architecture, we consider a layered software framework and\ncloud components through CloudSim. The core functionalities are implemented at the lowest layer such as arriving and processing requests, resource creation (data centers, servers, VMs), communication among resources, and management of the load execution. We implement our customized VM allocation policy extending the functionality. We develop user requests, and resource availability, and different scenarios at this layer. We model a data center for handling users' requests. This request requires the allocation of VM to a data center's servers. In our considered architecture, the data center of a CSP is composed of many servers and is responsible for managing resources. Each server has a GLYPH<28>xed amount of pre-conGLYPH<28>gured resources for allocating to VMs [31]. The primary concerns in the cloud such as allocation of VMs, managing load execution, and monitoring are controlled at this layer. We used the aforementioned architecture for emulating a cloud environment and collected the necessary data to estimate the value of different attributes.\n\nDetails the layered architecture and core functionalities of the cloud simulation model, including VM allocation and resource management within data centers.",
    "original_text": "Cloud providers can have multiple data centers in different geographic locations [29]. The failure rates and reliability of servers have different values that exist on different racks. The primary cause of a rack failure is the interruption on the power distribution or the rack switches [30]. Each of the servers is a combination of resources such as CPU, storage, RAM, and bandwidth and each of these resources has different failure rates and reliability levels. An entire data center may fail due to a natural disaster or due to a prolonged power outage that can cause a network interruption, and as a consequence service outage can occur. In our model, for simulating a cloud architecture, we consider a layered software framework and\ncloud components through CloudSim. The core functionalities are implemented at the lowest layer such as arriving and processing requests, resource creation (data centers, servers, VMs), communication among resources, and management of the load execution. We implement our customized VM allocation policy extending the functionality. We develop user requests, and resource availability, and different scenarios at this layer. We model a data center for handling users' requests. This request requires the allocation of VM to a data center's servers. In our considered architecture, the data center of a CSP is composed of many servers and is responsible for managing resources. Each server has a GLYPH<28>xed amount of pre-conGLYPH<28>gured resources for allocating to VMs [31]. The primary concerns in the cloud such as allocation of VMs, managing load execution, and monitoring are controlled at this layer. We used the aforementioned architecture for emulating a cloud environment and collected the necessary data to estimate the value of different attributes.",
    "context": "Details the layered architecture and core functionalities of the cloud simulation model, including VM allocation and resource management within data centers.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      4,
      5
    ],
    "id": "5b373cbd590f59e2ce8dee118bbdce5941cdd1b5605d42f785e7df131bfd844d"
  },
  {
    "text": "CloudSim simulator is used in the simulation due to its acceptance in the research community for representing real cloud environments [32]GLYPH<21>[34]. To validate the proposed evaluation and integration models, it is essential to create a cloud using a simulator that can imitate practical cloud instances and failures by executing different loads in different scenarios. Failure injection and failure data collection in VM granularity levels are restricted to real cloud provider's conGLYPH<28>gurations and infrastructures. To verify the effectiveness of the proposed models, modeling and simulations are used as an alternative. Our proposed approach creates an option to improve the performance of quality attributes in a cloud scenario. The problem modeling and simulation environments ensure the applicability and validity of the proposed approach in a real cloud environment. Moreover, CSPs can achieve beneGLYPH<28>ts from the results by integrating new features and extensions to their infrastructures.\nAll the above-mentioned metrics are used in the trust evaluation model using the simulator. In the simulation, Poisson distribution is used for generating requests and occurring failures. Due to the popularity and applicability of Poisson distribution in modeling request and failure generation in cloud, it is the most appropriate distribution [35], [36], [37]. At GLYPH<28>rst, the initialization of all required parameters for simulation is set to obtain the necessary data. Multiple environments with different problem sizes are being created in the simulator. Weexecute the model in distinct environments and workloads for validation. We run each of the experiments ten times and obtain the average value to evaluate all the required attributes.\n\nValidates the proposed models through simulation using CloudSim, focusing on performance improvements and replicable results across diverse environments and workloads.",
    "original_text": "CloudSim simulator is used in the simulation due to its acceptance in the research community for representing real cloud environments [32]GLYPH<21>[34]. To validate the proposed evaluation and integration models, it is essential to create a cloud using a simulator that can imitate practical cloud instances and failures by executing different loads in different scenarios. Failure injection and failure data collection in VM granularity levels are restricted to real cloud provider's conGLYPH<28>gurations and infrastructures. To verify the effectiveness of the proposed models, modeling and simulations are used as an alternative. Our proposed approach creates an option to improve the performance of quality attributes in a cloud scenario. The problem modeling and simulation environments ensure the applicability and validity of the proposed approach in a real cloud environment. Moreover, CSPs can achieve beneGLYPH<28>ts from the results by integrating new features and extensions to their infrastructures.\nAll the above-mentioned metrics are used in the trust evaluation model using the simulator. In the simulation, Poisson distribution is used for generating requests and occurring failures. Due to the popularity and applicability of Poisson distribution in modeling request and failure generation in cloud, it is the most appropriate distribution [35], [36], [37]. At GLYPH<28>rst, the initialization of all required parameters for simulation is set to obtain the necessary data. Multiple environments with different problem sizes are being created in the simulator. Weexecute the model in distinct environments and workloads for validation. We run each of the experiments ten times and obtain the average value to evaluate all the required attributes.",
    "context": "Validates the proposed models through simulation using CloudSim, focusing on performance improvements and replicable results across diverse environments and workloads.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      5
    ],
    "id": "58551f7f79449db2d25ace9cb8f312722132e4b3b10cb4616032c461994a1e5d"
  },
  {
    "text": "The objective of this model is to allocate resources in such a way that increases trust and decreases delay. Availability, reliability, data integrity, and efGLYPH<28>ciency are considered to obtain the trust of a CSP. Allocating resources in the cloud while maximizing the service quality is treated as one of the\ncomplex problems. The resource allocation model integrates trust obtained from our trust evaluation model. The optimization model is designed for joint optimization criteria as a Mixed Integer Linear Programming (MILP) problem. The joint optimization aims at maximizing trust and minimizing delay to improve the service performance.\nFIGURE 2. Maximizing trust in the resource allocation model.\nAn example of the proposed resource allocation process is shown in Fig. 2 with N CSPs. At GLYPH<28>rst, users provide requests for resources to a broker. Broker has access to the value of trust and delay for all the data centers of all CSPs. The broker executes the proposed resource allocation model and allocates resources while maintaining the objectives and required constraints. The model aims for an optimized allocation of resources that maximizes trust to improve the quality of service.\n\nDetails the resource allocation model’s objective of maximizing trust and minimizing delay within a cloud service provider (CSP) environment.",
    "original_text": "The objective of this model is to allocate resources in such a way that increases trust and decreases delay. Availability, reliability, data integrity, and efGLYPH<28>ciency are considered to obtain the trust of a CSP. Allocating resources in the cloud while maximizing the service quality is treated as one of the\ncomplex problems. The resource allocation model integrates trust obtained from our trust evaluation model. The optimization model is designed for joint optimization criteria as a Mixed Integer Linear Programming (MILP) problem. The joint optimization aims at maximizing trust and minimizing delay to improve the service performance.\nFIGURE 2. Maximizing trust in the resource allocation model.\nAn example of the proposed resource allocation process is shown in Fig. 2 with N CSPs. At GLYPH<28>rst, users provide requests for resources to a broker. Broker has access to the value of trust and delay for all the data centers of all CSPs. The broker executes the proposed resource allocation model and allocates resources while maintaining the objectives and required constraints. The model aims for an optimized allocation of resources that maximizes trust to improve the quality of service.",
    "context": "Details the resource allocation model’s objective of maximizing trust and minimizing delay within a cloud service provider (CSP) environment.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      5
    ],
    "id": "39a11cf74cd9616e8e1db01bbf6291c57e672b0f53b7112bfa5486786b7a0c72"
  },
  {
    "text": "The research problem that we want to formulate is allocating resources in multiple CSPs while increasing trust and decreasing communication delay. The capacity of the servers located in the data center and total trafGLYPH<28>c are used in estimating communication delay. The description of all the notations of our model is presented in Table 1.\nWe allocate a set of servers for each of the CSP. A number of VMs can be allocated inside each of the servers based on the required amount of resources. A VM is created by applying the users' requirements for the combination of processing unit, storage, memory, and bandwidth. We use R to determine the amount of resources of a VM.\n\nDefines the research problem and outlines the resource allocation strategy within the CSPs.",
    "original_text": "The research problem that we want to formulate is allocating resources in multiple CSPs while increasing trust and decreasing communication delay. The capacity of the servers located in the data center and total trafGLYPH<28>c are used in estimating communication delay. The description of all the notations of our model is presented in Table 1.\nWe allocate a set of servers for each of the CSP. A number of VMs can be allocated inside each of the servers based on the required amount of resources. A VM is created by applying the users' requirements for the combination of processing unit, storage, memory, and bandwidth. We use R to determine the amount of resources of a VM.",
    "context": "Defines the research problem and outlines the resource allocation strategy within the CSPs.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      5
    ],
    "id": "0f223b7d8be580f4f7c417d073dde63c1160e84e64d3371f6c457bfc55068d84"
  },
  {
    "text": "We want to maximize fTR , the overall trust value of the resource allocation. TRisn is the trust of VM i in server s in CSP n , and Aisn is used as a GLYPH<29>ag to determine the allocation of VM i in server s in CSP n .\n<!-- formula-not-decoded -->\nThe communication delay of a server is evaluated using the values of the total trafGLYPH<28>c between VMs and the capacity of the server where the VMs will be placed. In DLisn , we estimate the delay to allocate VM i of server s in CSP n . For measuring\nTABLE 1. Description of the symbols.\ntotal trafGLYPH<28>c, we combine the communication of VM i with all other VMs of the corresponding server s . The delay of a VM i is evaluated by combining all the above trafGLYPH<28>c divided by the capacity of server s in CSP n using the equation below V\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nWe set all the constraints in the above. With constraint 10, we conGLYPH<28>rm the allocation of one VM to one server of a CSP only. The limitations and availability of resources are maintained by constraint 11. Integrity constraint is fulGLYPH<28>lled using constraint 12. In the literature, it is shown that such type of problems are NP-hard problems [38].\n\nDefines trust metrics and constraints for VM allocation, including communication delay evaluation and NP-hard problem acknowledgment.",
    "original_text": "We want to maximize fTR , the overall trust value of the resource allocation. TRisn is the trust of VM i in server s in CSP n , and Aisn is used as a GLYPH<29>ag to determine the allocation of VM i in server s in CSP n .\n<!-- formula-not-decoded -->\nThe communication delay of a server is evaluated using the values of the total trafGLYPH<28>c between VMs and the capacity of the server where the VMs will be placed. In DLisn , we estimate the delay to allocate VM i of server s in CSP n . For measuring\nTABLE 1. Description of the symbols.\ntotal trafGLYPH<28>c, we combine the communication of VM i with all other VMs of the corresponding server s . The delay of a VM i is evaluated by combining all the above trafGLYPH<28>c divided by the capacity of server s in CSP n using the equation below V\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nWe set all the constraints in the above. With constraint 10, we conGLYPH<28>rm the allocation of one VM to one server of a CSP only. The limitations and availability of resources are maintained by constraint 11. Integrity constraint is fulGLYPH<28>lled using constraint 12. In the literature, it is shown that such type of problems are NP-hard problems [38].",
    "context": "Defines trust metrics and constraints for VM allocation, including communication delay evaluation and NP-hard problem acknowledgment.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      5,
      6
    ],
    "id": "b008d65f2eb868624488ddf9f540d838a0c5fa1feda23ce4600808c3ff2e172c"
  },
  {
    "text": "We solve the joint optimization model using the optimizer varying several weight combinations to the objective functions. The variation of weights is required due to multiple reasons such as the requirements of the type of the application or QoS parameters. Due to the NP-hard nature of the problem, formal methods are suitable only for small-scale instances. To overcome this situation, a heuristic is proposed for generating an approximated near-optimal solution.\nFor solving the resource allocation problem, we propose a Genetic Algorithm that is usually applied in several machine learning approaches [39]. This type of meta-heuristic algorithm is appropriate to GLYPH<28>nd a near-optimal solution in large-scale problem instances. The complexity of the three major operations of the genetic algorithm determines the total\nAlgorithm 1 Optimizing Trust and Delay in Resource Allocation\n\nDetails the use of a Genetic Algorithm for near-optimal resource allocation solutions due to the problem's complexity.",
    "original_text": "We solve the joint optimization model using the optimizer varying several weight combinations to the objective functions. The variation of weights is required due to multiple reasons such as the requirements of the type of the application or QoS parameters. Due to the NP-hard nature of the problem, formal methods are suitable only for small-scale instances. To overcome this situation, a heuristic is proposed for generating an approximated near-optimal solution.\nFor solving the resource allocation problem, we propose a Genetic Algorithm that is usually applied in several machine learning approaches [39]. This type of meta-heuristic algorithm is appropriate to GLYPH<28>nd a near-optimal solution in large-scale problem instances. The complexity of the three major operations of the genetic algorithm determines the total\nAlgorithm 1 Optimizing Trust and Delay in Resource Allocation",
    "context": "Details the use of a Genetic Algorithm for near-optimal resource allocation solutions due to the problem's complexity.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      6
    ],
    "id": "fdf2541ec87513ee893d24921e900ad98ba8b2052ef10a87ad1d1b629dd74b83"
  },
  {
    "text": "- Resource capacities for all servers of all CSPs\n- Availability, reliability, data integrity, and time efGLYPH<28>ciency of the servers of all CSPs\n- Matrix for communication trafGLYPH<28>c of all VMs\n\nDetails the technical specifications and performance metrics of cloud service provider infrastructure.",
    "original_text": "- Resource capacities for all servers of all CSPs\n- Availability, reliability, data integrity, and time efGLYPH<28>ciency of the servers of all CSPs\n- Matrix for communication trafGLYPH<28>c of all VMs",
    "context": "Details the technical specifications and performance metrics of cloud service provider infrastructure.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      6
    ],
    "id": "0699892e5c75d7c048a3b9564d03ef29c89311ac222398817c338ab61c7ef0ce"
  },
  {
    "text": "- The best VM allocation set with maximum GLYPH<28>tness value CRb\n- 1: for all incoming requests rq 2 R Q\n- 2: for all existing CSPs n 2 N\n- 3: for all available servers s 2 S n do\n- 4: Estimate trust\n5:\nEstimate delay\n- 6: end for\n- 7: end for\n- 8: end for\n- 9: CRb Initialize this set empty\n- 10: Randomly generate a set of C chromosomes\n- 11: Evaluate GLYPH<28>tness (trust and delay) for the above C chromosomes\n- 12: generation D 0\n- 13: while generation < D MAXgeneration\n14:\nwhile new chromosomes set\n6D\nC\n- 15: Select two chromosomes applying roulette wheel operator\n16:\nBased on the crossover probability, crossover occurs\n- 17: According to the mutation probability, mutation occurs\n- 18: end while\n- 19: Estimate GLYPH<28>tness of the new set of chromosomes\n- 20: Select the GLYPH<28>ttest chromosomes CRn using selection operation\n- 21: if f ( CRn ) > f ( CRb )\n- 22: CRb D CRn\n- 23: end if\n- 24: generation D generation C 1\n- 25: end while\n- 26: return the best allocation set ( CRb )\ntime complexity of such an algorithm. The time complexity for selection operation depends on the size of the request queue and it is O ( RQ ). On the other hand, crossover and mutation operations have a time complexity of O ( N GLYPH<3> V GLYPH<3> RQ ). We estimate the total complexity by combining the above operations and it is O ( N GLYPH<3> V GLYPH<3> RQ GLYPH<3> MAXgen ). The complexity of the algorithm provides the feasibility for implementing the model in a large-scale real cloud environment to ensure the allocation of VMs while jointly optimizing trust and delay.\nWe design the chromosome CR as a vector where the size of the vector depends on the requests. The size of the vector can be S GLYPH<3> N -dimensional while we keep the number of VMs the same in all requests. The value of the chromosome indicates the location of a VM to a server of a speciGLYPH<28>c CSP.\nWedeGLYPH<28>ne trust and delay as the GLYPH<28>tness of the chromosome. Weestimate the trust and delay and combine them to compute the GLYPH<28>tness. The GLYPH<28>ttest set of chromosomes represents the best solution for allocating VMs. We deGLYPH<28>ne the GLYPH<28>tness function by combining the objective functions using a weighted sum technique. Multiple combinations of the weights are examined for the objective functions to determine the proper tradeoff. We present the pseudo-code of the proposed algorithm in Algorithm 1. As input of the algorithm, it takes the following: available resource capacities, availability, reliability, data integrity, time efGLYPH<28>ciency, and trafGLYPH<28>c matrix of VMs for all servers and CSPs. At GLYPH<28>rst, a random chromosome population of size C is generated and the algorithm calculates the GLYPH<28>tness of each of them. After the GLYPH<28>tness being evaluated, based on the GLYPH<28>tness value selection, crossover, and mutation are applied and a new set of chromosomes are generated [40] (lines 12-20).\nThe roulette wheel method is used for selection operations. We use a two-point crossover process to have the better parts of parent chromosomes. In the process, the selected chromosomes exchange their parts and create two new child chromosomes. A single bit of the chromosome is changed in mutation operation, and thereby a new chromosome is generated. We assign the crossover and mutation rates and based on the values the operations occur in the algorithm. After obtaining a new generation of chromosomes, we estimate the GLYPH<28>tness of each of them, and the set of chromosomes with maximum GLYPH<28>tness value is selected as our new solution CRn . If the GLYPH<28>tness of this set is more than that of the previous set, the algorithm selects this set as the best solution CRb . The algorithm will return the best set of solutions when it reaches the termination conditions. In our algorithm, we set the maximum number of generations MAXgeneration to terminate the proposed algorithm.\nThe proposed optimization problem is modeled with several constraints and the constraints handling method depends on the application of selection operation. We enforce several rules while comparing two solutions such as for any solution in terms of feasibility, we discard the infeasible solution and choose the feasible one. The algorithm GLYPH<28>nally returns CRb , the best VM allocation set that maximizes trust and minimizes delay.\n\nDetails the algorithm's core process of generating and selecting VM allocation sets based on trust and delay, including selection, crossover, mutation, and the iterative improvement process.",
    "original_text": "- The best VM allocation set with maximum GLYPH<28>tness value CRb\n- 1: for all incoming requests rq 2 R Q\n- 2: for all existing CSPs n 2 N\n- 3: for all available servers s 2 S n do\n- 4: Estimate trust\n5:\nEstimate delay\n- 6: end for\n- 7: end for\n- 8: end for\n- 9: CRb Initialize this set empty\n- 10: Randomly generate a set of C chromosomes\n- 11: Evaluate GLYPH<28>tness (trust and delay) for the above C chromosomes\n- 12: generation D 0\n- 13: while generation < D MAXgeneration\n14:\nwhile new chromosomes set\n6D\nC\n- 15: Select two chromosomes applying roulette wheel operator\n16:\nBased on the crossover probability, crossover occurs\n- 17: According to the mutation probability, mutation occurs\n- 18: end while\n- 19: Estimate GLYPH<28>tness of the new set of chromosomes\n- 20: Select the GLYPH<28>ttest chromosomes CRn using selection operation\n- 21: if f ( CRn ) > f ( CRb )\n- 22: CRb D CRn\n- 23: end if\n- 24: generation D generation C 1\n- 25: end while\n- 26: return the best allocation set ( CRb )\ntime complexity of such an algorithm. The time complexity for selection operation depends on the size of the request queue and it is O ( RQ ). On the other hand, crossover and mutation operations have a time complexity of O ( N GLYPH<3> V GLYPH<3> RQ ). We estimate the total complexity by combining the above operations and it is O ( N GLYPH<3> V GLYPH<3> RQ GLYPH<3> MAXgen ). The complexity of the algorithm provides the feasibility for implementing the model in a large-scale real cloud environment to ensure the allocation of VMs while jointly optimizing trust and delay.\nWe design the chromosome CR as a vector where the size of the vector depends on the requests. The size of the vector can be S GLYPH<3> N -dimensional while we keep the number of VMs the same in all requests. The value of the chromosome indicates the location of a VM to a server of a speciGLYPH<28>c CSP.\nWedeGLYPH<28>ne trust and delay as the GLYPH<28>tness of the chromosome. Weestimate the trust and delay and combine them to compute the GLYPH<28>tness. The GLYPH<28>ttest set of chromosomes represents the best solution for allocating VMs. We deGLYPH<28>ne the GLYPH<28>tness function by combining the objective functions using a weighted sum technique. Multiple combinations of the weights are examined for the objective functions to determine the proper tradeoff. We present the pseudo-code of the proposed algorithm in Algorithm 1. As input of the algorithm, it takes the following: available resource capacities, availability, reliability, data integrity, time efGLYPH<28>ciency, and trafGLYPH<28>c matrix of VMs for all servers and CSPs. At GLYPH<28>rst, a random chromosome population of size C is generated and the algorithm calculates the GLYPH<28>tness of each of them. After the GLYPH<28>tness being evaluated, based on the GLYPH<28>tness value selection, crossover, and mutation are applied and a new set of chromosomes are generated [40] (lines 12-20).\nThe roulette wheel method is used for selection operations. We use a two-point crossover process to have the better parts of parent chromosomes. In the process, the selected chromosomes exchange their parts and create two new child chromosomes. A single bit of the chromosome is changed in mutation operation, and thereby a new chromosome is generated. We assign the crossover and mutation rates and based on the values the operations occur in the algorithm. After obtaining a new generation of chromosomes, we estimate the GLYPH<28>tness of each of them, and the set of chromosomes with maximum GLYPH<28>tness value is selected as our new solution CRn . If the GLYPH<28>tness of this set is more than that of the previous set, the algorithm selects this set as the best solution CRb . The algorithm will return the best set of solutions when it reaches the termination conditions. In our algorithm, we set the maximum number of generations MAXgeneration to terminate the proposed algorithm.\nThe proposed optimization problem is modeled with several constraints and the constraints handling method depends on the application of selection operation. We enforce several rules while comparing two solutions such as for any solution in terms of feasibility, we discard the infeasible solution and choose the feasible one. The algorithm GLYPH<28>nally returns CRb , the best VM allocation set that maximizes trust and minimizes delay.",
    "context": "Details the algorithm's core process of generating and selecting VM allocation sets based on trust and delay, including selection, crossover, mutation, and the iterative improvement process.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      6,
      7
    ],
    "id": "00e4b5853c2692886fd94325f13941f8f1b68d04e68422f78cac133c0b884273"
  },
  {
    "text": "The details of the experiments and results analysis are discussed in this section. CloudSim [31] is used for cloud data collection, and MATLAB is used for the optimized resource allocation in the experiments. We perform the experiments in multiple scenarios for testing the applicability of the approach. We implement the trust evaluation model using CloudSim and obtain the required data to calculate the trust of a CSP. Finally, the performance of the optimization model for resource allocation is compared with the exact optimized solution and the performance of the proposed heuristic is presented.\nWe obtain the necessary cloud data to compute trust from the experiments executed in the CloudSim environment. Based on the collected data, we calculate the trust of different CSPs. We use the trust value of our model and integrate this in the optimization model to execute the optimizer and GA in MATLAB environments to analyze the performance. To run experiments, we use an Intel Core i7 machine with a 4 GHz processor. In GA, we set the population size based on our requirements and generation size to 1000.\nWe execute the proposed model in four distinct scenarios. The number of servers and CSPs are varied in each of the scenarios. We modify the number of servers and CSPs from 30 to 60 and 6 to 12 respectively. In each scenario, the number of servers is increased by 10 and CSPs by 2. The above scenarios may represent real scenarios as the number of CSPs that can participate together cannot be more as several constraints need to be fulGLYPH<28>lled.\nThe trust of a CSP is estimated using the weighted sum of the value of availability, reliability, data integrity, and time efGLYPH<28>ciency. Fig. 3 depicts the trend of the availability metric with respect to different loads. The number of incoming requests and executing loads are varied, and three distinct cases are considered. A small load represents the number of requests and executing loads are minimum, on the other hand, there is one case with a maximum or heavy load. In between the minimum and maximum loads, we also consider one case which we name medium load. The trend in the GLYPH<28>gure shows that with a small load the availability is maximum and with a heavy load it is minimum. For calculating the availability value, we consider several such scenarios and take the average of 10 executions of those scenarios for the availability value.\nFIGURE 3. Resources availability with different loads.\nFor calculating reliability, we consider some of the hardware and software failures and combine them together to get the reliability attribute. In case of hardware failures, failures of virtual machines and processing elements are considered. The required software that is involved to process requests and execute workload is considered software failures. We execute several experiments with variations in the requests and loads\nFIGURE 4. Reliability of resources with different loads.\nFIGURE 5. Data integrity with different loads.\nand show that in Fig. 4. The reliability trend depicts that failures are more frequent in the case of a maximum number of requests and loads.\nFig. 5 shows the trend of the value of data integrity that was varied with the variations of the different loads. Fig. 6 conGLYPH<28>rms the efGLYPH<28>ciency in terms of the required time for all the different scenarios. In an ideal case, it is supposed to be 1 but practically we can try to get closer to the maximum possible value. After obtaining all the four attributes, we take the averages of all the attributes and combined them together as a weighted sum to get the trust value of one CSP.\nFig. 7 shows an important performance trend of the approach on the basis of trust value. The trust value is calculated using the weighted sum of availability, reliability, data integrity, and time efGLYPH<28>ciency. The results show the variation in the values of trust of our model with a variation in the weight. A 0.25 value in the weight of trust means trust is considered partially but the delay has more weight which is 0.75 in the objective function. Similarly, the other side is also considered in the objective function. A weight value of 0.5 indicates an equal weight on both of the objective functions. Fig. 7 shows that the trust of the model improves approximately 10% in all distinct scenarios. This performance gain ensures the validity of the approach.\nFIGURE 6. Time efficiency with different loads.\nFIGURE 7. Trust with different weight values.\nFIGURE 8. Delay with different weight values.\nFig. 8 depicts the trends of delay. The same weight variations are considered to show the variations in the values of communication delay. Applying the model delay is decreased by approximately 10%. The performance improvement is not massive but even a small reduction of delay improves the quality of service. The results can be more prominent in the real environment.\nFIGURE 9. Objective value comparison between optimal and genetic algorithm.\nFIGURE 10. Execution time comparison between optimal and genetic algorithm.\nFig. 9 shows one of the principal performance trends of the approach that is the objective value. The objectives of trust and delay are combined to calculate the objective value. The obtained value of the objective in the approach is nearly the same as an optimal solution. The aim is to maximize trust and minimize delay, and the proposed GA achieves almost the same objective values as an optimizer. The performance trend remains the same while varying the size of the problem in terms of CSPs and servers. This trend conGLYPH<28>rms that the proposed model is appropriate in a large cloud environment.\nFig. 10 also shows the performance of the approach on the basis of execution time. Results depict a linear increase in the execution time with an increase in the number of CSPs and servers. The above trend conGLYPH<28>rms the usability of the approach in real scenarios. The results exhibit that increase in the problem size does not require any noticeable execution time. This trend provides the assurance of the approach while increasing trust and decreasing delay. Moreover, the execution time of the approach is more signiGLYPH<28>cant compare to the optimizer in a large scenario.\nFig. 11 reveals the Pareto front obtained from the proposed heuristic. The trade-off between the objective functions is depicted using the Pareto front. Allocating resources in the\nFIGURE 11. Pareto font between trust and delay.\nmost trusted CSPs may increase trust but it will acquire a massive delay. Distributing resources will reduce delay as the load will be divided but trust will be decreased.\nThe performance results show that the trust of different CSPs is effectively estimated while varying the simulation environments. The performance results exhibit that the proposed approach can be an appropriate solution for optimizing resource allocation in a large cloud environment while fulGLYPH<28>lling the objectives. The improvement of the performance can be more prominent with an application in a real large cloud environment.\n\nDetails of experimental setup and results analysis, including trust value calculation and performance comparison with optimal solutions.",
    "original_text": "The details of the experiments and results analysis are discussed in this section. CloudSim [31] is used for cloud data collection, and MATLAB is used for the optimized resource allocation in the experiments. We perform the experiments in multiple scenarios for testing the applicability of the approach. We implement the trust evaluation model using CloudSim and obtain the required data to calculate the trust of a CSP. Finally, the performance of the optimization model for resource allocation is compared with the exact optimized solution and the performance of the proposed heuristic is presented.\nWe obtain the necessary cloud data to compute trust from the experiments executed in the CloudSim environment. Based on the collected data, we calculate the trust of different CSPs. We use the trust value of our model and integrate this in the optimization model to execute the optimizer and GA in MATLAB environments to analyze the performance. To run experiments, we use an Intel Core i7 machine with a 4 GHz processor. In GA, we set the population size based on our requirements and generation size to 1000.\nWe execute the proposed model in four distinct scenarios. The number of servers and CSPs are varied in each of the scenarios. We modify the number of servers and CSPs from 30 to 60 and 6 to 12 respectively. In each scenario, the number of servers is increased by 10 and CSPs by 2. The above scenarios may represent real scenarios as the number of CSPs that can participate together cannot be more as several constraints need to be fulGLYPH<28>lled.\nThe trust of a CSP is estimated using the weighted sum of the value of availability, reliability, data integrity, and time efGLYPH<28>ciency. Fig. 3 depicts the trend of the availability metric with respect to different loads. The number of incoming requests and executing loads are varied, and three distinct cases are considered. A small load represents the number of requests and executing loads are minimum, on the other hand, there is one case with a maximum or heavy load. In between the minimum and maximum loads, we also consider one case which we name medium load. The trend in the GLYPH<28>gure shows that with a small load the availability is maximum and with a heavy load it is minimum. For calculating the availability value, we consider several such scenarios and take the average of 10 executions of those scenarios for the availability value.\nFIGURE 3. Resources availability with different loads.\nFor calculating reliability, we consider some of the hardware and software failures and combine them together to get the reliability attribute. In case of hardware failures, failures of virtual machines and processing elements are considered. The required software that is involved to process requests and execute workload is considered software failures. We execute several experiments with variations in the requests and loads\nFIGURE 4. Reliability of resources with different loads.\nFIGURE 5. Data integrity with different loads.\nand show that in Fig. 4. The reliability trend depicts that failures are more frequent in the case of a maximum number of requests and loads.\nFig. 5 shows the trend of the value of data integrity that was varied with the variations of the different loads. Fig. 6 conGLYPH<28>rms the efGLYPH<28>ciency in terms of the required time for all the different scenarios. In an ideal case, it is supposed to be 1 but practically we can try to get closer to the maximum possible value. After obtaining all the four attributes, we take the averages of all the attributes and combined them together as a weighted sum to get the trust value of one CSP.\nFig. 7 shows an important performance trend of the approach on the basis of trust value. The trust value is calculated using the weighted sum of availability, reliability, data integrity, and time efGLYPH<28>ciency. The results show the variation in the values of trust of our model with a variation in the weight. A 0.25 value in the weight of trust means trust is considered partially but the delay has more weight which is 0.75 in the objective function. Similarly, the other side is also considered in the objective function. A weight value of 0.5 indicates an equal weight on both of the objective functions. Fig. 7 shows that the trust of the model improves approximately 10% in all distinct scenarios. This performance gain ensures the validity of the approach.\nFIGURE 6. Time efficiency with different loads.\nFIGURE 7. Trust with different weight values.\nFIGURE 8. Delay with different weight values.\nFig. 8 depicts the trends of delay. The same weight variations are considered to show the variations in the values of communication delay. Applying the model delay is decreased by approximately 10%. The performance improvement is not massive but even a small reduction of delay improves the quality of service. The results can be more prominent in the real environment.\nFIGURE 9. Objective value comparison between optimal and genetic algorithm.\nFIGURE 10. Execution time comparison between optimal and genetic algorithm.\nFig. 9 shows one of the principal performance trends of the approach that is the objective value. The objectives of trust and delay are combined to calculate the objective value. The obtained value of the objective in the approach is nearly the same as an optimal solution. The aim is to maximize trust and minimize delay, and the proposed GA achieves almost the same objective values as an optimizer. The performance trend remains the same while varying the size of the problem in terms of CSPs and servers. This trend conGLYPH<28>rms that the proposed model is appropriate in a large cloud environment.\nFig. 10 also shows the performance of the approach on the basis of execution time. Results depict a linear increase in the execution time with an increase in the number of CSPs and servers. The above trend conGLYPH<28>rms the usability of the approach in real scenarios. The results exhibit that increase in the problem size does not require any noticeable execution time. This trend provides the assurance of the approach while increasing trust and decreasing delay. Moreover, the execution time of the approach is more signiGLYPH<28>cant compare to the optimizer in a large scenario.\nFig. 11 reveals the Pareto front obtained from the proposed heuristic. The trade-off between the objective functions is depicted using the Pareto front. Allocating resources in the\nFIGURE 11. Pareto font between trust and delay.\nmost trusted CSPs may increase trust but it will acquire a massive delay. Distributing resources will reduce delay as the load will be divided but trust will be decreased.\nThe performance results show that the trust of different CSPs is effectively estimated while varying the simulation environments. The performance results exhibit that the proposed approach can be an appropriate solution for optimizing resource allocation in a large cloud environment while fulGLYPH<28>lling the objectives. The improvement of the performance can be more prominent with an application in a real large cloud environment.",
    "context": "Details of experimental setup and results analysis, including trust value calculation and performance comparison with optimal solutions.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      8,
      9,
      7
    ],
    "id": "370d559156b8e3e4ea7c5398b8a9872bb9682995b4c0b664da5ff774bb1e71b2"
  },
  {
    "text": "The contributions of this work can be outlined into three main parts: evaluation of trust of cloud computing, integration of trust in resource allocation model for improving QoS, and the application of Genetic algorithm to allocate resources effectively in a reasonable amount of time. The GLYPH<28>rst contribution is the trust evaluation in multi-cloud settings. These include consideration of proper attributes and metrics and the quantiGLYPH<28>cation of the metrics for estimating trust. The evaluation is shown in several environments for providing practical solutions. The trust evaluation is crucial in the adoption of a multi-cloud environment. The proposed evaluation model will attract more users as it estimates the trust of the cloud more accurately. This type of evaluation model will be of high importance to both service providers and customers.\nThe performance analysis of the resource allocation model depicts that the model effectively allocates resources while maintaining the objectives. Applying a Genetic algorithm to get an optimal solution in a reasonable amount of time is our third contribution. The execution time of the algorithm to GLYPH<28>nd a solution requires signiGLYPH<28>cantly less time compared to an optimizer and, this feature validates the applicability of the method. Achieving the same performance trend while varying numbers of resources conGLYPH<28>rms that the improvement of the performance will not be reduced even if a real cloud environment is used with a large number of resources. In the future, we plan to provide a reliable and secure model for trust management among different cloud service providers.\n\nDetails the trust evaluation model and its application in multi-cloud settings, highlighting its efficiency and potential for attracting users and future development in trust management.",
    "original_text": "The contributions of this work can be outlined into three main parts: evaluation of trust of cloud computing, integration of trust in resource allocation model for improving QoS, and the application of Genetic algorithm to allocate resources effectively in a reasonable amount of time. The GLYPH<28>rst contribution is the trust evaluation in multi-cloud settings. These include consideration of proper attributes and metrics and the quantiGLYPH<28>cation of the metrics for estimating trust. The evaluation is shown in several environments for providing practical solutions. The trust evaluation is crucial in the adoption of a multi-cloud environment. The proposed evaluation model will attract more users as it estimates the trust of the cloud more accurately. This type of evaluation model will be of high importance to both service providers and customers.\nThe performance analysis of the resource allocation model depicts that the model effectively allocates resources while maintaining the objectives. Applying a Genetic algorithm to get an optimal solution in a reasonable amount of time is our third contribution. The execution time of the algorithm to GLYPH<28>nd a solution requires signiGLYPH<28>cantly less time compared to an optimizer and, this feature validates the applicability of the method. Achieving the same performance trend while varying numbers of resources conGLYPH<28>rms that the improvement of the performance will not be reduced even if a real cloud environment is used with a large number of resources. In the future, we plan to provide a reliable and secure model for trust management among different cloud service providers.",
    "context": "Details the trust evaluation model and its application in multi-cloud settings, highlighting its efficiency and potential for attracting users and future development in trust management.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      9
    ],
    "id": "17d285ecaa73e7e507292ac1558b28c3ed6b702c991fb7405d9d4e6e896c2384"
  },
  {
    "text": "- [1] E. Buer and R. Adams, Reliability and Availability of Cloud Computing . Hoboken, NJ, USA: Wiley, 2012.\n- [2] A. B. M. B. Alam, T. Halabi, A. Haque, and M. Zulkernine, ''Multiobjective interdependent VM placement model based on cloud reliability evaluation,'' in Proc. IEEE Int. Conf. Commun. (ICC) , Dublin, Ireland, Jun. 2020, pp. 1GLYPH<21>7.\n- [3] K. V. Vishwanath and N. Nagappan, ''Characterizing cloud computing hardware reliability,'' in Proc. 1st ACM Symp. Cloud Comput. (SoCC) , 2010, pp. 193GLYPH<21>204.\n- [4] H. Liao, Z. Zhou, X. Zhao, L. Zhang, S. Mumtaz, A. Jolfaei, S. H. Ahmed, and A. K. Bashir, ''Learning-based context-aware resource allocation for edge-computing-empowered industrial IoT,'' IEEE Internet Things J. , vol. 7, no. 5, pp. 4260GLYPH<21>4277, May 2020.\n- [5] A. Musaddiq, Y. B. Zikria, O. Hahm, H. Yu, A. K. Bashir, and S. W. Kim, ''A survey on resource management in IoT operating systems,'' IEEE Access , vol. 6, pp. 8459GLYPH<21>8482, 2018.\n- [6] P. Pradhan, P. K. Behera, and B. N. B. Ray, ''Improved max-min algorithm for resource allocation in cloud computing,'' in Proc. 6th Int. Conf. Parallel, Distrib. Grid Comput. (PDGC) , Nov. 2020, pp. 22GLYPH<21>24.\n- [7] Z. Xuejie, W. Zhijian, and X. Feng, ''Reliability evaluation of cloud computing systems using hybrid methods,'' Intell. Autom. Soft Comput. , vol. 19, no. 2, pp. 165GLYPH<21>174, Apr. 2013.\n- [8] N. Padmapriya and R. Rajmohan, ''Reliability evaluation suite for cloud services,'' in Proc. 3rd Int. Conf. Comput., Commun. Netw. Technol. (ICCCNT) , Jul. 2012, pp. 1GLYPH<21>6.\n- [9] B. Yang, F. Tan, Y.-S. Dai, and S. Guo, ''Performance evaluation of cloud service considering fault recovery,'' in Proc. IEEE Int. Conf. Cloud Comput. , Dec. 2009, pp. 571GLYPH<21>576.\n- [10] H. R. Faragardi, R. Shojaee, H. Tabani, and A. Rajabi, ''An analytical model to evaluate reliability of cloud computing systems in the presence of QoS requirements,'' in Proc. IEEE/ACIS 12th Int. Conf. Comput. Inf. Sci. (ICIS) , Niigata, Japan, Jun. 2013, pp. 315GLYPH<21>321.\n- [11] Y. Hu, J. Wong, G. Iszlai, and M. Litoiu, ''Resource provisioning for cloud computing,'' in Proc. Conf. Center Adv. Stud. Collaborative Res. , 2009, pp. 101GLYPH<21>111.\n- [12] J. Praveenchandar and A. Tamilarasi, ''Dynamic resource allocation with optimized task scheduling and improved power management in cloud computing,'' J. Ambient Intell. Humanized Comput. , vol. 12, no. 3, pp. 4147GLYPH<21>4159, Mar. 2021.\n- [13] A. K. Bashir, R. Arul, S. Basheer, G. Raja, R. Jayaraman, and N. M. F. Qureshi, ''An optimal multitier resource allocation of cloud RAN in 5G using machine learning,'' Trans. Emerg. Telecommun. Technol. , vol. 30, no. 8, p. e3627, Aug. 2019.\n- [14] Z. Xiao, W. Song, and Q. Chen, ''Dynamic resource allocation using virtual machines for cloud computing environment,'' IEEETrans. Parallel Distrib. Syst. , vol. 24, no. 6, pp. 1107GLYPH<21>1117, Jun. 2013.\n- [15] W. Lin, J. Z. Wang, C. Liang, and D. Qi, ''A threshold-based dynamic resource allocation scheme for cloud computing,'' Procedia Eng. , vol. 23, pp. 695GLYPH<21>703, Jan. 2011.\n- [16] R. K. Grewal and P. K. Pateriya, ''A rule-based approach for effective resource provisioning in hybrid cloud environment,'' in New Paradigms in Internet Computing . Berlin, Germany: Springer, 2013, pp. 41GLYPH<21>57.\n- [17] J. Luo, W. Song, and L. Yin, ''Reliable virtual machine placement based on multi-objective optimization with trafGLYPH<28>c-aware algorithm in industrial cloud,'' IEEE Access , vol. 6, pp. 23043GLYPH<21>23052, 2018.\n- [18] X. Wang, H. Gu, and Y. Yue, ''The optimization of virtual resource allocation in cloud computing based on RBPSO,'' Concurrency Comput., Pract. Exper. , vol. 32, no. 16, p. e5113, Aug. 2020.\n- [19] T. Halabi, M. Bellaiche, and A. Abusitta, ''Online allocation of cloud resources based on security satisfaction,'' in Proc. 17th IEEE Int. Conf. Trust, Secur. Privacy Comput. Commun./12th IEEE Int. Conf. Big Data Sci. Eng. (TrustCom/BigDataSE) , Aug. 2018, pp. 379GLYPH<21>384.\n- [20] F.-H. Tseng, X. Wang, L.-D. Chou, H.-C. Chao, and V. C. M. Leung, ''Dynamic resource prediction and allocation for cloud data center using the multiobjective genetic algorithm,'' IEEE Syst. J. , vol. 12, no. 2, pp. 1688GLYPH<21>1699, Jun. 2018.\n- [21] P. Gupta and S. P. Ghrera, ''Power and fault aware reliable resource allocation for cloud infrastructure,'' Procedia Comput. Sci. , vol. 78, pp. 457GLYPH<21>463, Jan. 2016.\n- [22] A. B. M. B. Alam, T. Halabi, A. Haque, and M. Zulkernine, ''Optimizing virtual machine migration in multi-clouds,'' in Proc. Int. Symp. Netw., Comput. Commun. (ISNCC) , Montreal, QC, Canada, Oct. 2020, pp. 1GLYPH<21>7.\n- [23] (2013). Windows Azure . Accessed: Apr. 2019. [Online]. Available: http://www.windowsazure.com/en-us/support/legal/sla/\n- [24] (2020). Amazon EC2 . Accessed: May 2020. [Online]. Available: http://aws.amazon.com/ec2-sla/\n- [25] (2020). Google Gmail . Accessed: May 2020. [Online]. Available: https://developers.google.com/storage/docs/sla\n- [26] (2020). Salesforce . Accessed: May 2020. [Online]. Available: https://www.salesforce.com/editions-pricing/service-cloud/\n- [27] O. Beaumont, L. Eyraud-Dubois, and H. LarchevŒque, ''Reliable service allocation in clouds,'' in Proc. IEEE 27th Int. Symp. Parallel Distrib. Process. , May 2013, pp. 55GLYPH<21>66.\n- [28] A. B. M. B. Alam, M. Zulkernine, and A. Haque, ''A reliability-based resource allocation approach for cloud computing,'' in Proc. IEEE 7th Int. Symp. Cloud Service Comput. (SC) , Kanazawa, Japan, Nov. 2017, pp. 249GLYPH<21>252.\n\nThe documents primarily focus on evaluating and optimizing resource allocation, reliability, and service level agreements within cloud computing environments.",
    "original_text": "- [1] E. Buer and R. Adams, Reliability and Availability of Cloud Computing . Hoboken, NJ, USA: Wiley, 2012.\n- [2] A. B. M. B. Alam, T. Halabi, A. Haque, and M. Zulkernine, ''Multiobjective interdependent VM placement model based on cloud reliability evaluation,'' in Proc. IEEE Int. Conf. Commun. (ICC) , Dublin, Ireland, Jun. 2020, pp. 1GLYPH<21>7.\n- [3] K. V. Vishwanath and N. Nagappan, ''Characterizing cloud computing hardware reliability,'' in Proc. 1st ACM Symp. Cloud Comput. (SoCC) , 2010, pp. 193GLYPH<21>204.\n- [4] H. Liao, Z. Zhou, X. Zhao, L. Zhang, S. Mumtaz, A. Jolfaei, S. H. Ahmed, and A. K. Bashir, ''Learning-based context-aware resource allocation for edge-computing-empowered industrial IoT,'' IEEE Internet Things J. , vol. 7, no. 5, pp. 4260GLYPH<21>4277, May 2020.\n- [5] A. Musaddiq, Y. B. Zikria, O. Hahm, H. Yu, A. K. Bashir, and S. W. Kim, ''A survey on resource management in IoT operating systems,'' IEEE Access , vol. 6, pp. 8459GLYPH<21>8482, 2018.\n- [6] P. Pradhan, P. K. Behera, and B. N. B. Ray, ''Improved max-min algorithm for resource allocation in cloud computing,'' in Proc. 6th Int. Conf. Parallel, Distrib. Grid Comput. (PDGC) , Nov. 2020, pp. 22GLYPH<21>24.\n- [7] Z. Xuejie, W. Zhijian, and X. Feng, ''Reliability evaluation of cloud computing systems using hybrid methods,'' Intell. Autom. Soft Comput. , vol. 19, no. 2, pp. 165GLYPH<21>174, Apr. 2013.\n- [8] N. Padmapriya and R. Rajmohan, ''Reliability evaluation suite for cloud services,'' in Proc. 3rd Int. Conf. Comput., Commun. Netw. Technol. (ICCCNT) , Jul. 2012, pp. 1GLYPH<21>6.\n- [9] B. Yang, F. Tan, Y.-S. Dai, and S. Guo, ''Performance evaluation of cloud service considering fault recovery,'' in Proc. IEEE Int. Conf. Cloud Comput. , Dec. 2009, pp. 571GLYPH<21>576.\n- [10] H. R. Faragardi, R. Shojaee, H. Tabani, and A. Rajabi, ''An analytical model to evaluate reliability of cloud computing systems in the presence of QoS requirements,'' in Proc. IEEE/ACIS 12th Int. Conf. Comput. Inf. Sci. (ICIS) , Niigata, Japan, Jun. 2013, pp. 315GLYPH<21>321.\n- [11] Y. Hu, J. Wong, G. Iszlai, and M. Litoiu, ''Resource provisioning for cloud computing,'' in Proc. Conf. Center Adv. Stud. Collaborative Res. , 2009, pp. 101GLYPH<21>111.\n- [12] J. Praveenchandar and A. Tamilarasi, ''Dynamic resource allocation with optimized task scheduling and improved power management in cloud computing,'' J. Ambient Intell. Humanized Comput. , vol. 12, no. 3, pp. 4147GLYPH<21>4159, Mar. 2021.\n- [13] A. K. Bashir, R. Arul, S. Basheer, G. Raja, R. Jayaraman, and N. M. F. Qureshi, ''An optimal multitier resource allocation of cloud RAN in 5G using machine learning,'' Trans. Emerg. Telecommun. Technol. , vol. 30, no. 8, p. e3627, Aug. 2019.\n- [14] Z. Xiao, W. Song, and Q. Chen, ''Dynamic resource allocation using virtual machines for cloud computing environment,'' IEEETrans. Parallel Distrib. Syst. , vol. 24, no. 6, pp. 1107GLYPH<21>1117, Jun. 2013.\n- [15] W. Lin, J. Z. Wang, C. Liang, and D. Qi, ''A threshold-based dynamic resource allocation scheme for cloud computing,'' Procedia Eng. , vol. 23, pp. 695GLYPH<21>703, Jan. 2011.\n- [16] R. K. Grewal and P. K. Pateriya, ''A rule-based approach for effective resource provisioning in hybrid cloud environment,'' in New Paradigms in Internet Computing . Berlin, Germany: Springer, 2013, pp. 41GLYPH<21>57.\n- [17] J. Luo, W. Song, and L. Yin, ''Reliable virtual machine placement based on multi-objective optimization with trafGLYPH<28>c-aware algorithm in industrial cloud,'' IEEE Access , vol. 6, pp. 23043GLYPH<21>23052, 2018.\n- [18] X. Wang, H. Gu, and Y. Yue, ''The optimization of virtual resource allocation in cloud computing based on RBPSO,'' Concurrency Comput., Pract. Exper. , vol. 32, no. 16, p. e5113, Aug. 2020.\n- [19] T. Halabi, M. Bellaiche, and A. Abusitta, ''Online allocation of cloud resources based on security satisfaction,'' in Proc. 17th IEEE Int. Conf. Trust, Secur. Privacy Comput. Commun./12th IEEE Int. Conf. Big Data Sci. Eng. (TrustCom/BigDataSE) , Aug. 2018, pp. 379GLYPH<21>384.\n- [20] F.-H. Tseng, X. Wang, L.-D. Chou, H.-C. Chao, and V. C. M. Leung, ''Dynamic resource prediction and allocation for cloud data center using the multiobjective genetic algorithm,'' IEEE Syst. J. , vol. 12, no. 2, pp. 1688GLYPH<21>1699, Jun. 2018.\n- [21] P. Gupta and S. P. Ghrera, ''Power and fault aware reliable resource allocation for cloud infrastructure,'' Procedia Comput. Sci. , vol. 78, pp. 457GLYPH<21>463, Jan. 2016.\n- [22] A. B. M. B. Alam, T. Halabi, A. Haque, and M. Zulkernine, ''Optimizing virtual machine migration in multi-clouds,'' in Proc. Int. Symp. Netw., Comput. Commun. (ISNCC) , Montreal, QC, Canada, Oct. 2020, pp. 1GLYPH<21>7.\n- [23] (2013). Windows Azure . Accessed: Apr. 2019. [Online]. Available: http://www.windowsazure.com/en-us/support/legal/sla/\n- [24] (2020). Amazon EC2 . Accessed: May 2020. [Online]. Available: http://aws.amazon.com/ec2-sla/\n- [25] (2020). Google Gmail . Accessed: May 2020. [Online]. Available: https://developers.google.com/storage/docs/sla\n- [26] (2020). Salesforce . Accessed: May 2020. [Online]. Available: https://www.salesforce.com/editions-pricing/service-cloud/\n- [27] O. Beaumont, L. Eyraud-Dubois, and H. LarchevŒque, ''Reliable service allocation in clouds,'' in Proc. IEEE 27th Int. Symp. Parallel Distrib. Process. , May 2013, pp. 55GLYPH<21>66.\n- [28] A. B. M. B. Alam, M. Zulkernine, and A. Haque, ''A reliability-based resource allocation approach for cloud computing,'' in Proc. IEEE 7th Int. Symp. Cloud Service Comput. (SC) , Kanazawa, Japan, Nov. 2017, pp. 249GLYPH<21>252.",
    "context": "The documents primarily focus on evaluating and optimizing resource allocation, reliability, and service level agreements within cloud computing environments.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      10
    ],
    "id": "aa550c849656866bbb10699f72402ece02975c352e4cae1fc07d44a13936f281"
  },
  {
    "text": "- [29] E. Bauer and R. Adams, Reliability and Availability of Cloud Computing . Hoboken, NJ, USA: Wiley, 2012.\n- [30] M. Jammal, A. Kanso, and A. Shami, ''CHASE: Component high availability-aware scheduler in cloud computing environment,'' in Proc. IEEE 8th Int. Conf. Cloud Comput. , Jun. 2015, pp. 477GLYPH<21>484.\n- [31] R. Buyya, R. Ranjan, and R. N. Calheiros, ''Modeling and simulation of scalable cloud computing environments and the CloudSim toolkit: Challenges and opportunities,'' in Proc. Int. Conf. High Perform. Comput. Simulation , Jun. 2009, pp. 1GLYPH<21>11.\n- [32] F. M. Alturkistani and S. S. Alaboodi, ''An analytical model for availability evaluation of cloud service provisioning system,'' Int. J. Adv. Comput. Sci. Appl. , vol. 8, no. 6, pp. 240GLYPH<21>247, 2017.\n- [33] B. G. Batista, J. C. Estrella, C. H. G. Ferreira, D. M. L. Filho, L. H. V. Nakamura, S. Reiff-Marganiec, M. J. Santana, and R. H. C. Santana, ''Performance evaluation of resource management in cloud computing environments,'' PLoS ONE , vol. 10, no. 11, 2015, Art. no. e0141914.\n- [34] S. Meng, X. Qiu, L. Luo, H. Xu, and M. Lei, ''Reliability simulation in cloud computing system,'' Int. J. Performability Eng. , vol. 14, no. 9, p. 2015, 2018.\n- [35] A. B. M. B. Alam, T. Halabi, A. Haque, and M. Zulkernine, ''Reliabilitybased formation of cloud federations using game theory,'' in Proc. IEEE Global Commun. Conf. (GLOBECOM) , Taipei, Taiwan, Dec. 2020, pp. 1GLYPH<21>6.\n- [36] Y.-S. Dai, B. Yang, J. Dongarra, and G. Zhang, ''Cloud service reliability: Modeling and analysis,'' in Proc. 15th IEEE PaciGLYPH<28>c Rim Int. Symp. Dependable Comput. , Nov. 2009, pp. 1GLYPH<21>17.\n- [37] N. Limrungsi, J. Zhao, Y. Xiang, T. Lan, H. H. Huang, and S. Subramaniam, ''Providing reliability as an elastic service in cloud computing,'' in Proc. IEEE Int. Conf. Commun. (ICC) , Jun. 2012, pp. 2912GLYPH<21>2917.\n- [38] H. Kellerer, U. Pferschy, and D. Pisinger, ''Multidimensional knapsack problems,'' in Knapsack Problems . Berlin, Germany: Springer, 2004, pp. 235GLYPH<21>283.\n- [39] T. Murata and H. Ishibuchi, ''MOGA: Multi-objective genetic algorithms,'' in Proc. IEEE Int. Conf. Evol. Comput. , vol. 1, Nov. 1995, pp. 289GLYPH<21>294.\n- [40] A. E. Eiben and J. E. Smith, Introduction to Evolutionary Computing (Natural Computing Series). New York, NY, USA: Springer-Verlag, 2008.\n- A. B. M. BODRUL ALAM (Member, IEEE) received the B.Sc. degree (Hons.) in computer science from Jahangirnagar University, Bangladesh, in 2005, the M.Sc. degree from the Department of Computer Science, Ryerson University, Canada, in 2012, and the Ph.D. degree from Queen's University, Canada, in 2020. He is currently working as a Postdoctoral Researcher with the Department of Computer Science, Lakehead University, Thunder Bay, ON, Canada. Before joining\nRyerson University, he served as a full-time Lecturer with the Department of Computer Science, Stamford University, Bangladesh, for GLYPH<28>ve years. He has published several peer-reviewed journals and conference papers. He also collaborated with two other universities: Western University and the University of Winnipeg while conducting his research. His research interests include optimization, mathematical modeling, reliability analysis, reliable resource management, VM placement, VM migration, survivable network design, and cloud computing.\nZUBAIR MD. FADLULLAH (Senior Member, IEEE) was an Associate Professor at the Graduate School of Information Sciences (GSIS), Tohoku University, Japan, from 2017 to 2019. He is currently an Associate Professor with the Computer Science Department, Lakehead University, and the Research Chair of Thunder Bay Regional Health Research Institute (TBRHRI), Thunder Bay, ON, Canada. His research interests include the areas of emerging communication systems, such as 5G\nnew radio and beyond, deep learning applications on solving computer science and communication system problems, UAV-based systems, smart health technology, cyber security, game theory, smart grid, and emerging communication systems. He is a Senior Member of IEEE Communications Society (ComSoc). He received several best paper awards at conferences, including IEEE/ACM IWCMC, IEEE GLOBECOM, and IEEE IC-NIDC. He is currently an Editor of IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY (TVT), IEEE Network magazine, IEEE ACCESS, IEEE OPEN JOURNAL OF THE COMMUNICATIONS SOCIETY, and Ad Hoc & Sensor Wireless Networks (AHSWN) journal.\nSALIMUR CHOUDHURY (Senior Member, IEEE) received the Ph.D. degree in computing from Queen's University, Kingston, ON, Canada, in 2012. He is currently an Assistant Professor with the Department of Computer Science, Lakehead University, Thunder Bay, ON. He is also the Director of the Lakehead Optimization Research Group (LORG). His research interests include designing algorithms for wireless communications, optimization, cellular automata, and approximation algorithms. He was the Technical Program Chair of the SGIoT 2017, SGIoT 2018, and iThings 2018 conferences. He is also an Editor of Parallel Processing Letters .\n\nProvides a foundational overview of cloud computing reliability and related research areas, including modeling, simulation, and analysis.",
    "original_text": "- [29] E. Bauer and R. Adams, Reliability and Availability of Cloud Computing . Hoboken, NJ, USA: Wiley, 2012.\n- [30] M. Jammal, A. Kanso, and A. Shami, ''CHASE: Component high availability-aware scheduler in cloud computing environment,'' in Proc. IEEE 8th Int. Conf. Cloud Comput. , Jun. 2015, pp. 477GLYPH<21>484.\n- [31] R. Buyya, R. Ranjan, and R. N. Calheiros, ''Modeling and simulation of scalable cloud computing environments and the CloudSim toolkit: Challenges and opportunities,'' in Proc. Int. Conf. High Perform. Comput. Simulation , Jun. 2009, pp. 1GLYPH<21>11.\n- [32] F. M. Alturkistani and S. S. Alaboodi, ''An analytical model for availability evaluation of cloud service provisioning system,'' Int. J. Adv. Comput. Sci. Appl. , vol. 8, no. 6, pp. 240GLYPH<21>247, 2017.\n- [33] B. G. Batista, J. C. Estrella, C. H. G. Ferreira, D. M. L. Filho, L. H. V. Nakamura, S. Reiff-Marganiec, M. J. Santana, and R. H. C. Santana, ''Performance evaluation of resource management in cloud computing environments,'' PLoS ONE , vol. 10, no. 11, 2015, Art. no. e0141914.\n- [34] S. Meng, X. Qiu, L. Luo, H. Xu, and M. Lei, ''Reliability simulation in cloud computing system,'' Int. J. Performability Eng. , vol. 14, no. 9, p. 2015, 2018.\n- [35] A. B. M. B. Alam, T. Halabi, A. Haque, and M. Zulkernine, ''Reliabilitybased formation of cloud federations using game theory,'' in Proc. IEEE Global Commun. Conf. (GLOBECOM) , Taipei, Taiwan, Dec. 2020, pp. 1GLYPH<21>6.\n- [36] Y.-S. Dai, B. Yang, J. Dongarra, and G. Zhang, ''Cloud service reliability: Modeling and analysis,'' in Proc. 15th IEEE PaciGLYPH<28>c Rim Int. Symp. Dependable Comput. , Nov. 2009, pp. 1GLYPH<21>17.\n- [37] N. Limrungsi, J. Zhao, Y. Xiang, T. Lan, H. H. Huang, and S. Subramaniam, ''Providing reliability as an elastic service in cloud computing,'' in Proc. IEEE Int. Conf. Commun. (ICC) , Jun. 2012, pp. 2912GLYPH<21>2917.\n- [38] H. Kellerer, U. Pferschy, and D. Pisinger, ''Multidimensional knapsack problems,'' in Knapsack Problems . Berlin, Germany: Springer, 2004, pp. 235GLYPH<21>283.\n- [39] T. Murata and H. Ishibuchi, ''MOGA: Multi-objective genetic algorithms,'' in Proc. IEEE Int. Conf. Evol. Comput. , vol. 1, Nov. 1995, pp. 289GLYPH<21>294.\n- [40] A. E. Eiben and J. E. Smith, Introduction to Evolutionary Computing (Natural Computing Series). New York, NY, USA: Springer-Verlag, 2008.\n- A. B. M. BODRUL ALAM (Member, IEEE) received the B.Sc. degree (Hons.) in computer science from Jahangirnagar University, Bangladesh, in 2005, the M.Sc. degree from the Department of Computer Science, Ryerson University, Canada, in 2012, and the Ph.D. degree from Queen's University, Canada, in 2020. He is currently working as a Postdoctoral Researcher with the Department of Computer Science, Lakehead University, Thunder Bay, ON, Canada. Before joining\nRyerson University, he served as a full-time Lecturer with the Department of Computer Science, Stamford University, Bangladesh, for GLYPH<28>ve years. He has published several peer-reviewed journals and conference papers. He also collaborated with two other universities: Western University and the University of Winnipeg while conducting his research. His research interests include optimization, mathematical modeling, reliability analysis, reliable resource management, VM placement, VM migration, survivable network design, and cloud computing.\nZUBAIR MD. FADLULLAH (Senior Member, IEEE) was an Associate Professor at the Graduate School of Information Sciences (GSIS), Tohoku University, Japan, from 2017 to 2019. He is currently an Associate Professor with the Computer Science Department, Lakehead University, and the Research Chair of Thunder Bay Regional Health Research Institute (TBRHRI), Thunder Bay, ON, Canada. His research interests include the areas of emerging communication systems, such as 5G\nnew radio and beyond, deep learning applications on solving computer science and communication system problems, UAV-based systems, smart health technology, cyber security, game theory, smart grid, and emerging communication systems. He is a Senior Member of IEEE Communications Society (ComSoc). He received several best paper awards at conferences, including IEEE/ACM IWCMC, IEEE GLOBECOM, and IEEE IC-NIDC. He is currently an Editor of IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY (TVT), IEEE Network magazine, IEEE ACCESS, IEEE OPEN JOURNAL OF THE COMMUNICATIONS SOCIETY, and Ad Hoc & Sensor Wireless Networks (AHSWN) journal.\nSALIMUR CHOUDHURY (Senior Member, IEEE) received the Ph.D. degree in computing from Queen's University, Kingston, ON, Canada, in 2012. He is currently an Assistant Professor with the Department of Computer Science, Lakehead University, Thunder Bay, ON. He is also the Director of the Lakehead Optimization Research Group (LORG). His research interests include designing algorithms for wireless communications, optimization, cellular automata, and approximation algorithms. He was the Technical Program Chair of the SGIoT 2017, SGIoT 2018, and iThings 2018 conferences. He is also an Editor of Parallel Processing Letters .",
    "context": "Provides a foundational overview of cloud computing reliability and related research areas, including modeling, simulation, and analysis.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      10,
      11
    ],
    "id": "9003f1fa4ab4dc44fd0fe78f9853c8b559e0b70ca7de02d59759fc2378aacafa"
  },
  {
    "text": "Received February 7, 2022, accepted February 22, 2022, date of publication March 2, 2022, date of current version March 11, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.31561 18\n\nProvides publication and DOI information.",
    "original_text": "Received February 7, 2022, accepted February 22, 2022, date of publication March 2, 2022, date of current version March 11, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.31561 18",
    "context": "Provides publication and DOI information.",
    "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
    "pages": [
      1
    ],
    "id": "ced7da322a4c089bc6e250f16b3cce510dc9cafef75170bb71275124b8a4ec10"
  },
  {
    "text": "SANGHO YU 1 , SANG MIN WON 1 , HYOUNG WON BAAC 1 , DONGHEE SON 1 , AND CHANGHWAN SHIN 2 , (Senior Member, IEEE)\n1 Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon 16419, Republic of Korea 2 School of Electrical Engineering, Korea University, Seoul 02841, Republic of Korea\nCorresponding authors: Changhwan Shin (cshin@korea.ac.kr) and Donghee Son (daniel3600@g.skku.edu)\nThis work was supported in part by the National Research Foundation of Korea (NRF) Grant funded by the Korea Government Ministry of Science and ICT (MSIT) under Grant 2020R1A2C1009063; and in part by the Korea Medical Device Development Fund Grant funded by the Korean Government (the Ministry of Trade, Industry and Energy, the Ministry of Health & Welfare, and the Ministry of Food and Drug Safety) under Grant 202012D28.\n- ABSTRACT To design a device that is robust to process-induced random variation, this study proposes a machine-learning-based predictive model that can simulate the electrical characteristics of FinFETs with process-induced line-edge roughness. This model, i.e., a Bayesian neural network (BNN) model with horseshoe priors (Horseshoe-BNN), can signiGLYPH<28>cantly reduce the simulation time (as compared to the conventional technology computer-aided design (TCAD) simulation method) in a sufGLYPH<28>ciently accurate manner. Moreover, this model can perform autonomous model selection over the most compact layer size, which is necessary when the amount of data must be limited. The mean absolute percentage error for the mean and standard deviation of the drain-to-source current . IDS / were GLYPH<24> 0.5% and GLYPH<24> 6%, respectively. By estimating the distribution of the current-voltage characteristics, the distributions of the other device metrics, such as off-state leakage current and threshold voltage, can be estimated as well.\nINDEX TERMS Line edge roughness (LER), process-induced random variation, Bayesian neural network, automatic model selection.\n\nProposes a Bayesian neural network model for simulating FinFET electrical characteristics, reducing simulation time and enabling automatic model selection for limited data.",
    "original_text": "SANGHO YU 1 , SANG MIN WON 1 , HYOUNG WON BAAC 1 , DONGHEE SON 1 , AND CHANGHWAN SHIN 2 , (Senior Member, IEEE)\n1 Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon 16419, Republic of Korea 2 School of Electrical Engineering, Korea University, Seoul 02841, Republic of Korea\nCorresponding authors: Changhwan Shin (cshin@korea.ac.kr) and Donghee Son (daniel3600@g.skku.edu)\nThis work was supported in part by the National Research Foundation of Korea (NRF) Grant funded by the Korea Government Ministry of Science and ICT (MSIT) under Grant 2020R1A2C1009063; and in part by the Korea Medical Device Development Fund Grant funded by the Korean Government (the Ministry of Trade, Industry and Energy, the Ministry of Health & Welfare, and the Ministry of Food and Drug Safety) under Grant 202012D28.\n- ABSTRACT To design a device that is robust to process-induced random variation, this study proposes a machine-learning-based predictive model that can simulate the electrical characteristics of FinFETs with process-induced line-edge roughness. This model, i.e., a Bayesian neural network (BNN) model with horseshoe priors (Horseshoe-BNN), can signiGLYPH<28>cantly reduce the simulation time (as compared to the conventional technology computer-aided design (TCAD) simulation method) in a sufGLYPH<28>ciently accurate manner. Moreover, this model can perform autonomous model selection over the most compact layer size, which is necessary when the amount of data must be limited. The mean absolute percentage error for the mean and standard deviation of the drain-to-source current . IDS / were GLYPH<24> 0.5% and GLYPH<24> 6%, respectively. By estimating the distribution of the current-voltage characteristics, the distributions of the other device metrics, such as off-state leakage current and threshold voltage, can be estimated as well.\nINDEX TERMS Line edge roughness (LER), process-induced random variation, Bayesian neural network, automatic model selection.",
    "context": "Proposes a Bayesian neural network model for simulating FinFET electrical characteristics, reducing simulation time and enabling automatic model selection for limited data.",
    "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
    "pages": [
      1
    ],
    "id": "e9078adc0f19121c9bdf19b95484326ad38c44565abca94f0181966dca6de015"
  },
  {
    "text": "As the lateral/vertical dimension of transistors in integrated circuits (ICs) (e.g., channel length, channel width, junction depth, etc.) has been scaled down, the process-induced randomvariation of device parameters becomes signiGLYPH<28>cant. This would signiGLYPH<28>cantly damage the yield of devices. Processinduced line-edge-roughness (LER) is one type of random variation sources, and it induces a signiGLYPH<28>cant amount of variation in aggressively scaled transistors. Because the amplitude of LER did not shrink as much as the feature size shrinkage, the portion of LER in the nominal/physical channel length and width became signiGLYPH<28>cant, resulting in a larger variation in IDS GLYPH<0> VGS characteristics of devices. Hence, the LER in device has become a primary limit when the physical dimensions of device has been scaled down. In this regard, precise proGLYPH<28>ling of the impact of LER on the device performance has become\nThe associate editor coordinating the review of this manuscript and approving it for publication was Anisul Haque.\nan indispensable requisite for designing a variation-immune device. To date, technology computer-aided design (TCAD) simulation has been used to evaluate the impact of LER on device characteristics; however, it takes a signiGLYPH<28>cant amount of time. Thousands of devices should be simulated to determine the exact amount of LER-induced variation (in real, a few weeks even for a single LER proGLYPH<28>le). For this reason, a novel approach for less time-consuming simulation is necessary. In this context, we proposed a linear regression model in [1]; however, this linear regression model was limited in dealing with nonlinearity. Conversely, artiGLYPH<28>cial neural network (ANN) is the most popular machine-learning model because it shows a powerful ability to GLYPH<28>nd patterns as well as to detect trends in complex real-world data. By stacking multiple layers (in detail, neurons/nodes aggregations that receive inputs and compute/give outputs, based on their predeGLYPH<28>ned simple non-linear functions), an ANN can well deGLYPH<28>ne highly nonlinear functions and capture non-linear dependencies in data. However, standard ANN models are prone\nto overGLYPH<28>tting and exhibit poor generalization performance. In addition, they tend to make unjustiGLYPH<28>ed over-conGLYPH<28>dent predictions for the inputs far away from the training data, which can sometimes result in suboptimal predictions [2]. In particular, in the regime of small data (where the uncertainty of data is severe), ANNs are prone to poorly specify their weights. To solve these problems, the Bayesian neural network (BNN) has received lots of attention. The BNNs treat their weights as random variables (described by distributions), and thereby capturing uncertainties in the training data and their weights. This study employed BNN models to predict the IDS GLYPH<0> VGS characteristics in various FinFETs with different device structures. In real, there is a demand for FinFET devices with multiple heights [3], [4]. The BNN model proposed in this work assigns horseshoe priors [5] as a prior distribution, which we call the Horseshoe-BNN (HS-BNN). The prior (i.e., prior distribution) is a prior belief of the weight distribution's conGLYPH<28>guration before observing the data. According to the Bayesian theorem, the beliefs are updated after the data observance [6]. Herein, depending on which prior distribution is to be speciGLYPH<28>ed, it leads to a different model training process. While the Gaussian prior is the most commonly-used prior, the horseshoe distribution has been chosen to introduce sparsity and shrinkage over the weights. The horseshoe prior makes it available the model selection over a number of nodes and can reduce arduous/tiresome work in optimizing the layer sizes.\n\nDetails the limitations of linear regression and proposes a Bayesian neural network (HS-BNN) for predicting FinFET device characteristics, addressing issues of over-fitting and uncertainty in data.",
    "original_text": "As the lateral/vertical dimension of transistors in integrated circuits (ICs) (e.g., channel length, channel width, junction depth, etc.) has been scaled down, the process-induced randomvariation of device parameters becomes signiGLYPH<28>cant. This would signiGLYPH<28>cantly damage the yield of devices. Processinduced line-edge-roughness (LER) is one type of random variation sources, and it induces a signiGLYPH<28>cant amount of variation in aggressively scaled transistors. Because the amplitude of LER did not shrink as much as the feature size shrinkage, the portion of LER in the nominal/physical channel length and width became signiGLYPH<28>cant, resulting in a larger variation in IDS GLYPH<0> VGS characteristics of devices. Hence, the LER in device has become a primary limit when the physical dimensions of device has been scaled down. In this regard, precise proGLYPH<28>ling of the impact of LER on the device performance has become\nThe associate editor coordinating the review of this manuscript and approving it for publication was Anisul Haque.\nan indispensable requisite for designing a variation-immune device. To date, technology computer-aided design (TCAD) simulation has been used to evaluate the impact of LER on device characteristics; however, it takes a signiGLYPH<28>cant amount of time. Thousands of devices should be simulated to determine the exact amount of LER-induced variation (in real, a few weeks even for a single LER proGLYPH<28>le). For this reason, a novel approach for less time-consuming simulation is necessary. In this context, we proposed a linear regression model in [1]; however, this linear regression model was limited in dealing with nonlinearity. Conversely, artiGLYPH<28>cial neural network (ANN) is the most popular machine-learning model because it shows a powerful ability to GLYPH<28>nd patterns as well as to detect trends in complex real-world data. By stacking multiple layers (in detail, neurons/nodes aggregations that receive inputs and compute/give outputs, based on their predeGLYPH<28>ned simple non-linear functions), an ANN can well deGLYPH<28>ne highly nonlinear functions and capture non-linear dependencies in data. However, standard ANN models are prone\nto overGLYPH<28>tting and exhibit poor generalization performance. In addition, they tend to make unjustiGLYPH<28>ed over-conGLYPH<28>dent predictions for the inputs far away from the training data, which can sometimes result in suboptimal predictions [2]. In particular, in the regime of small data (where the uncertainty of data is severe), ANNs are prone to poorly specify their weights. To solve these problems, the Bayesian neural network (BNN) has received lots of attention. The BNNs treat their weights as random variables (described by distributions), and thereby capturing uncertainties in the training data and their weights. This study employed BNN models to predict the IDS GLYPH<0> VGS characteristics in various FinFETs with different device structures. In real, there is a demand for FinFET devices with multiple heights [3], [4]. The BNN model proposed in this work assigns horseshoe priors [5] as a prior distribution, which we call the Horseshoe-BNN (HS-BNN). The prior (i.e., prior distribution) is a prior belief of the weight distribution's conGLYPH<28>guration before observing the data. According to the Bayesian theorem, the beliefs are updated after the data observance [6]. Herein, depending on which prior distribution is to be speciGLYPH<28>ed, it leads to a different model training process. While the Gaussian prior is the most commonly-used prior, the horseshoe distribution has been chosen to introduce sparsity and shrinkage over the weights. The horseshoe prior makes it available the model selection over a number of nodes and can reduce arduous/tiresome work in optimizing the layer sizes.",
    "context": "Details the limitations of linear regression and proposes a Bayesian neural network (HS-BNN) for predicting FinFET device characteristics, addressing issues of over-fitting and uncertainty in data.",
    "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
    "pages": [
      1,
      2
    ],
    "id": "975d97ababdbc33169d57b2a71bc113ba3f2811d21281758d0961d84c45ea489"
  },
  {
    "text": "Various FinFETs with different structures were designed on the basis of [7]GLYPH<21>[10], and thereafter, 3D LER proGLYPH<28>les on those FinFETs were applied/simulated using the MATLAB model (which was proposed in [11]) and Sentaurus TCAD. Specifically, the 3D LER sequences (i.e., points of the randomly rough surfaces) were generated by the MATLAB model with three main parameters for LER [i.e., RMS amplitude ( GLYPH<27> ), x(y)-axis correlation length .GLYPH<24> X ( GLYPH<24> Y / ), and roughness exponent ( GLYPH<11> )], and the sequences were imported into the TCAD tool. Herein, the RMS amplitude ( GLYPH<27> ) indicates the standard deviation of the LER amplitudes, and it is often referred to as the LER or LER magnitude. Note that GLYPH<27> is the major factor for LER. The correlation length ( GLYPH<24> ) is corresponding to the wavelength of LER proGLYPH<28>le, and the roughness exponent ( GLYPH<11> ) quantitatively indicates the way how high-frequency components in LER proGLYPH<28>le diminishes. The device parameters for FinFET (including the parameters for LER and device structure) are summarized in Table 1. Figure 1(a) shows an isometric view and current density distribution of a FinFET with Lg D 20 nm ; WGLYPH<28>n D 7 nm ; HGLYPH<28>n D 42 nm . Note that the feature parameters for LER used in the FinFET is as follows: GLYPH<27> D 0 : 5 nm ; GLYPH<24> X D 20 nm ; GLYPH<24> Y D 50 nm ; GLYPH<11> D 1. Figure 1(b) shows the IDS GLYPH<0> VGS characteristics with 100 sample devices. Herein, the GLYPH<28>n width variation by LER causes the variations in the IDS GLYPH<0> VGS characteristics [12]. It is noteworthy that the 3D TCAD device simulations were run using various\nTABLE 1. Device parameters used in this work.\nCorrelation length along y direction (5x), 1 = 1~500 nm. Roughness exponent, 1 = 0.1~1.3\n(a)\nFIGURE 1. (a) Isometric view of FinFET with LER (left) and its current density distribution (right), and (b) simulated I DS -V GS of nominal device and 100 sample devices with LER. The parameters for LER and FinFET device structure is GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1, and Lg D 20 nm, W fin D 7 nm, H fin D 42 nm, respectively.\nphysics models, i.e., the ShockleyGLYPH<21>ReadGLYPH<21>Hall model for carrier generation and recombination, the Old Slotboom model for bandgap narrowing, the Lombardi model for thin-layer mobility, and the density gradient quantization model for quantum mechanics effects.\nWe have noted that creating a number of data is very time-consuming because of a long TCAD simulation run\nFIGURE 2. Probability density of Horseshoe prior and Gaussian prior.\ntime. This led to a dilemma when satisfying (i) the number of LER proGLYPH<28>les or (ii) the number of sample devices with each LER proGLYPH<28>le. In other words, to appropriately estimate the distribution of electrical characteristics, the number of sample devices should be sufGLYPH<28>cient, but at the same time, the number of LER proGLYPH<28>les is quite important. In considering those two aspects, 169 different datasets were composed for training and testing the model, and each dataset consisted of 50 different sample FinFETs with identical LER and device parameters. Eighteen types of FinFET device structure were chosen based on [7]GLYPH<21>[10], and the LER parameters were randomly selected in the relevant ranges shown in Table 1.\nAs mentioned in the previous study [1], the distribution of log . IDS / for a given VGS can be considered as a normal distribution in terms of small kurtosis and skewness values. From this perspective, the mean and standard deviation of log . IDS / (denoted as GLYPH<22> IDS and GLYPH<27> IDS , respectively) were selected as target variables. And gate voltage . VGS / , LER parameters (i.e., RMS amplitude ( GLYPH<27> ), correlation length along the x-axis ( GLYPH<24> X ), correlation length along the y-axis .GLYPH<24> Y / , and roughness exponent ( GLYPH<11> )) and device structure parameters (i.e., gate length ( Lg ), GLYPH<28>n width ( WGLYPH<28>n ), and GLYPH<28>n height ( HGLYPH<28>n )) were selected as feature variables to predict the LER-induced variation in the IDS-VGS characteristics of various FinFET structures.\n\nDetails the methodology for simulating LER-induced variations in FinFET characteristics, including the selection of LER parameters, device structure, and the use of TCAD simulations and physics models.",
    "original_text": "Various FinFETs with different structures were designed on the basis of [7]GLYPH<21>[10], and thereafter, 3D LER proGLYPH<28>les on those FinFETs were applied/simulated using the MATLAB model (which was proposed in [11]) and Sentaurus TCAD. Specifically, the 3D LER sequences (i.e., points of the randomly rough surfaces) were generated by the MATLAB model with three main parameters for LER [i.e., RMS amplitude ( GLYPH<27> ), x(y)-axis correlation length .GLYPH<24> X ( GLYPH<24> Y / ), and roughness exponent ( GLYPH<11> )], and the sequences were imported into the TCAD tool. Herein, the RMS amplitude ( GLYPH<27> ) indicates the standard deviation of the LER amplitudes, and it is often referred to as the LER or LER magnitude. Note that GLYPH<27> is the major factor for LER. The correlation length ( GLYPH<24> ) is corresponding to the wavelength of LER proGLYPH<28>le, and the roughness exponent ( GLYPH<11> ) quantitatively indicates the way how high-frequency components in LER proGLYPH<28>le diminishes. The device parameters for FinFET (including the parameters for LER and device structure) are summarized in Table 1. Figure 1(a) shows an isometric view and current density distribution of a FinFET with Lg D 20 nm ; WGLYPH<28>n D 7 nm ; HGLYPH<28>n D 42 nm . Note that the feature parameters for LER used in the FinFET is as follows: GLYPH<27> D 0 : 5 nm ; GLYPH<24> X D 20 nm ; GLYPH<24> Y D 50 nm ; GLYPH<11> D 1. Figure 1(b) shows the IDS GLYPH<0> VGS characteristics with 100 sample devices. Herein, the GLYPH<28>n width variation by LER causes the variations in the IDS GLYPH<0> VGS characteristics [12]. It is noteworthy that the 3D TCAD device simulations were run using various\nTABLE 1. Device parameters used in this work.\nCorrelation length along y direction (5x), 1 = 1~500 nm. Roughness exponent, 1 = 0.1~1.3\n(a)\nFIGURE 1. (a) Isometric view of FinFET with LER (left) and its current density distribution (right), and (b) simulated I DS -V GS of nominal device and 100 sample devices with LER. The parameters for LER and FinFET device structure is GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1, and Lg D 20 nm, W fin D 7 nm, H fin D 42 nm, respectively.\nphysics models, i.e., the ShockleyGLYPH<21>ReadGLYPH<21>Hall model for carrier generation and recombination, the Old Slotboom model for bandgap narrowing, the Lombardi model for thin-layer mobility, and the density gradient quantization model for quantum mechanics effects.\nWe have noted that creating a number of data is very time-consuming because of a long TCAD simulation run\nFIGURE 2. Probability density of Horseshoe prior and Gaussian prior.\ntime. This led to a dilemma when satisfying (i) the number of LER proGLYPH<28>les or (ii) the number of sample devices with each LER proGLYPH<28>le. In other words, to appropriately estimate the distribution of electrical characteristics, the number of sample devices should be sufGLYPH<28>cient, but at the same time, the number of LER proGLYPH<28>les is quite important. In considering those two aspects, 169 different datasets were composed for training and testing the model, and each dataset consisted of 50 different sample FinFETs with identical LER and device parameters. Eighteen types of FinFET device structure were chosen based on [7]GLYPH<21>[10], and the LER parameters were randomly selected in the relevant ranges shown in Table 1.\nAs mentioned in the previous study [1], the distribution of log . IDS / for a given VGS can be considered as a normal distribution in terms of small kurtosis and skewness values. From this perspective, the mean and standard deviation of log . IDS / (denoted as GLYPH<22> IDS and GLYPH<27> IDS , respectively) were selected as target variables. And gate voltage . VGS / , LER parameters (i.e., RMS amplitude ( GLYPH<27> ), correlation length along the x-axis ( GLYPH<24> X ), correlation length along the y-axis .GLYPH<24> Y / , and roughness exponent ( GLYPH<11> )) and device structure parameters (i.e., gate length ( Lg ), GLYPH<28>n width ( WGLYPH<28>n ), and GLYPH<28>n height ( HGLYPH<28>n )) were selected as feature variables to predict the LER-induced variation in the IDS-VGS characteristics of various FinFET structures.",
    "context": "Details the methodology for simulating LER-induced variations in FinFET characteristics, including the selection of LER parameters, device structure, and the use of TCAD simulations and physics models.",
    "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
    "pages": [
      2,
      3
    ],
    "id": "48dad3f022235b00c7e6e3980ea3e5af9bc9b1bcdb345fc023f80f9f8a477a8c"
  },
  {
    "text": "Predictive models are just a function in the form of y D f ( x ) that GLYPH<28>ts the given data D D GLYPH<8>GLYPH<0> xn ; y n GLYPH<1>GLYPH<9> N 1 , and they are generalized/extended to the cases that have not been seen in training. For example, the function form of a linear regression model is y D P wixi C b . However, this model has its own limit for determining the nonlinear relationship between x and y . Conversely, an ANN with an L hidden layer is in the form of y D W . L C 1 / h . L / C b . L C 1 / , where W l is a matrix of weights between layer ( l GLYPH<0> 1 ) and layer ( l ) in the size of R . Kl GLYPH<0> 1 C 1 / × Kl , and h . l / is a matrix of\nFIGURE 3. Architecture of the HS-BNN, predicting the mean and standard deviation of log(I DS ) in various FinFET structures with arbitrary LER profiles.\nFIGURE 4. Diagram of K-fold cross-validation.\nFIGURE 5. MAPE, RMSE, MAE, and PLL values of each model along different numbers of nodes, showing model selection over the number of HS-BNN nodes.\noutputs from layer ( l ) . Herein, Kl is the number of units in layer l , h . l / D a GLYPH<0> W . l / h . l GLYPH<0> 1 / C b GLYPH<1> ; h . 0 / D x ; and a ( · ) is an activation function for each hidden layer. In the case of the ANN with three hidden layers, the function is\nFIGURE 6. (a) Predicted standard deviation and (b) predicted mean of log(I OFF ) for FinFET structure of Lg D 20 nm, H fin D 42 nm, W fin D 7 nm with the LER profile of GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1. Note that each parameter (i.e., GLYPH<27> , GLYPH<24> X , GLYPH<24> Y , GLYPH<11> , Lg, H fin , and W fin ) is varied in relevant ranges.\ny D W 4 a GLYPH<0> W 3 a GLYPH<0> W 2 a GLYPH<0> W 1 x C b 1 GLYPH<1> C b 2 GLYPH<1> C b 3 GLYPH<1> C b 4 . The ANN model is a composite function of multiple activation functions (i.e., activation integral) [13], which can make it complex nonlinear regression predictions.\nA Bayesian neural network is an extended standard ANN with Bayesian/posterior inference [14]. While the ANN is a deterministic model, the BNN is a stochastic/probabilistic model. The BNN incorporates uncertainty in modeling tasks by introducing distributions over the weights W GLYPH<24> p ( W ) (i.e., W D GLYPH<8> W l GLYPH<9> L C 1 1 , a set of weight matrices over all the layers) [15]. By doing so, BNNs can demonstrate/reconGLYPH<28>gure epistemic uncertainty when the amount of data is limited. However, at the same time, there are inherent neural network problems like over\n(under)-parameterization. The larger(smaller) the number of data points is, the larger(smaller) the number of required nodes is. When the number of nodes is much larger(smaller) than that estimated from the data, the variance around the BNN's predictions would be too large(small). The layer sizes should be as large as necessary. Especially, in the case of estimating the LER-induced variation, data were bound to be small, so that over-parameterization had to be prevented. In this regard, this study assigns horseshoe priors over weights using Bayesian inference. The horseshoe prior is given by: GLYPH<16> w . l / i ; j j GLYPH<28> i ; v GLYPH<17> ∼ N GLYPH<0> 0 ;GLYPH<28> 2 i v 2 GLYPH<1> where GLYPH<28> i ∼ C C . 0 ; b 0 / and v ∼ C C GLYPH<0> 0 ; bg GLYPH<1> . Herein, w . l / i ; j is the weight connecting the i th node of layer ( l -1) and j th node of\nFIGURE 7. (a) Predicted standard deviation and (b) mean of V TH distribution for FinFET structure of Lg D 20 nm, H fin D 42 nm, W fin D 7 nm with the LER profile of GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1. Note that each parameter (i.e., GLYPH<27> , GLYPH<24> X , GLYPH<24> Y , GLYPH<11> , Lg, H fin , and W fin ) is varied in relevant ranges.\nlayer . l / , and C C . 0 ; b / is a half-Cauchy distribution, and b 0 and bg are shrinkage parameters. The horseshoe prior can introduce shrinkage and sparsity over the weights and bring model selection that automatically GLYPH<28>nds the most compact neural network structure (i.e., GLYPH<28>nding the best number of nodes). This is due to the two features of the horseshoe distribution: (1) a tall spike at zero and (2) heavy (relatively GLYPH<29>at) tails (see Figure 2). The tall spike at zero promotes shrinkage of the weights, which seems unnecessary in making predictions. However, its heavy tail allows large weights to avoid shrinkage.\nThe complete architecture of the HS-BNN is illustrated in Figure 3: Three hidden layers with a RELU activation function, horseshoe priors for the weights into the hidden layers, and a Gaussian prior for the weights into the output layer. The Gaussian prior was used to prevent overGLYPH<28>tting [16].\n\nDetails the architecture and Bayesian inference methods used in the HS-BNN model, including weight priors and their impact on model complexity.",
    "original_text": "Predictive models are just a function in the form of y D f ( x ) that GLYPH<28>ts the given data D D GLYPH<8>GLYPH<0> xn ; y n GLYPH<1>GLYPH<9> N 1 , and they are generalized/extended to the cases that have not been seen in training. For example, the function form of a linear regression model is y D P wixi C b . However, this model has its own limit for determining the nonlinear relationship between x and y . Conversely, an ANN with an L hidden layer is in the form of y D W . L C 1 / h . L / C b . L C 1 / , where W l is a matrix of weights between layer ( l GLYPH<0> 1 ) and layer ( l ) in the size of R . Kl GLYPH<0> 1 C 1 / × Kl , and h . l / is a matrix of\nFIGURE 3. Architecture of the HS-BNN, predicting the mean and standard deviation of log(I DS ) in various FinFET structures with arbitrary LER profiles.\nFIGURE 4. Diagram of K-fold cross-validation.\nFIGURE 5. MAPE, RMSE, MAE, and PLL values of each model along different numbers of nodes, showing model selection over the number of HS-BNN nodes.\noutputs from layer ( l ) . Herein, Kl is the number of units in layer l , h . l / D a GLYPH<0> W . l / h . l GLYPH<0> 1 / C b GLYPH<1> ; h . 0 / D x ; and a ( · ) is an activation function for each hidden layer. In the case of the ANN with three hidden layers, the function is\nFIGURE 6. (a) Predicted standard deviation and (b) predicted mean of log(I OFF ) for FinFET structure of Lg D 20 nm, H fin D 42 nm, W fin D 7 nm with the LER profile of GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1. Note that each parameter (i.e., GLYPH<27> , GLYPH<24> X , GLYPH<24> Y , GLYPH<11> , Lg, H fin , and W fin ) is varied in relevant ranges.\ny D W 4 a GLYPH<0> W 3 a GLYPH<0> W 2 a GLYPH<0> W 1 x C b 1 GLYPH<1> C b 2 GLYPH<1> C b 3 GLYPH<1> C b 4 . The ANN model is a composite function of multiple activation functions (i.e., activation integral) [13], which can make it complex nonlinear regression predictions.\nA Bayesian neural network is an extended standard ANN with Bayesian/posterior inference [14]. While the ANN is a deterministic model, the BNN is a stochastic/probabilistic model. The BNN incorporates uncertainty in modeling tasks by introducing distributions over the weights W GLYPH<24> p ( W ) (i.e., W D GLYPH<8> W l GLYPH<9> L C 1 1 , a set of weight matrices over all the layers) [15]. By doing so, BNNs can demonstrate/reconGLYPH<28>gure epistemic uncertainty when the amount of data is limited. However, at the same time, there are inherent neural network problems like over\n(under)-parameterization. The larger(smaller) the number of data points is, the larger(smaller) the number of required nodes is. When the number of nodes is much larger(smaller) than that estimated from the data, the variance around the BNN's predictions would be too large(small). The layer sizes should be as large as necessary. Especially, in the case of estimating the LER-induced variation, data were bound to be small, so that over-parameterization had to be prevented. In this regard, this study assigns horseshoe priors over weights using Bayesian inference. The horseshoe prior is given by: GLYPH<16> w . l / i ; j j GLYPH<28> i ; v GLYPH<17> ∼ N GLYPH<0> 0 ;GLYPH<28> 2 i v 2 GLYPH<1> where GLYPH<28> i ∼ C C . 0 ; b 0 / and v ∼ C C GLYPH<0> 0 ; bg GLYPH<1> . Herein, w . l / i ; j is the weight connecting the i th node of layer ( l -1) and j th node of\nFIGURE 7. (a) Predicted standard deviation and (b) mean of V TH distribution for FinFET structure of Lg D 20 nm, H fin D 42 nm, W fin D 7 nm with the LER profile of GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1. Note that each parameter (i.e., GLYPH<27> , GLYPH<24> X , GLYPH<24> Y , GLYPH<11> , Lg, H fin , and W fin ) is varied in relevant ranges.\nlayer . l / , and C C . 0 ; b / is a half-Cauchy distribution, and b 0 and bg are shrinkage parameters. The horseshoe prior can introduce shrinkage and sparsity over the weights and bring model selection that automatically GLYPH<28>nds the most compact neural network structure (i.e., GLYPH<28>nding the best number of nodes). This is due to the two features of the horseshoe distribution: (1) a tall spike at zero and (2) heavy (relatively GLYPH<29>at) tails (see Figure 2). The tall spike at zero promotes shrinkage of the weights, which seems unnecessary in making predictions. However, its heavy tail allows large weights to avoid shrinkage.\nThe complete architecture of the HS-BNN is illustrated in Figure 3: Three hidden layers with a RELU activation function, horseshoe priors for the weights into the hidden layers, and a Gaussian prior for the weights into the output layer. The Gaussian prior was used to prevent overGLYPH<28>tting [16].",
    "context": "Details the architecture and Bayesian inference methods used in the HS-BNN model, including weight priors and their impact on model complexity.",
    "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
    "pages": [
      3,
      4,
      5
    ],
    "id": "aad5d978d36fc9f91ba5ca5bf315bf6e3d4b31aa73ac15df9c8fb7c64dd98ca0"
  },
  {
    "text": "The HS-BNN predictions were evaluated using K-fold crossvalidation [splitting data in a K number of sections, and then, iteratively using one of the sections as a test set and the others as a training set (see Figure 4)]. This method is expected to be a proper choice when data are limited, by ensuring all data to be used as a test set at least once, which makes less biased evaluation and well estimates how a model will perform in general cases [17], [18]. Mean absolute percentage errors (MAPEs), root-mean-squared errors (RMSEs), mean absolute errors (MAEs) (herein, note that the lower they are,\nTABLE 2. MAPE and RMSE values of each model.\nTABLE 3. MAPE, RMSE, MAE, and PLL values of each model.\nthe better they are), and predictive log likelihoods (PLLs) (herein, note that the higher it is, the better it is) are calculated in each test set. Afterwards, the averages of those are the representative evaluation process values.\nThe overall prediction performance was improved, as compared against the previous work [1] which is the Bayesian linear regression (BLR) model (see Table 2 ). Note that the HS-BNN model for this comparison is only for the FinFET with Lg D 14 nm, WGLYPH<28>n D 7 nm, HGLYPH<28>n D 30 nm because the BLR model made predictions only for the corresponding FinFET structure. Table 2 summarizes the MAPE and RMSEof the two target variables (i.e., GLYPH<22> IDS and GLYPH<27> IDS) of the HS-BNN and BLR. The predictions of both GLYPH<22> IDS and GLYPH<27> IDS were much improved over those in the previous work. The prediction for GLYPH<22> IDS was improved (i.e., 0.55% vs. 0.81%), but there was signiGLYPH<28>cant prediction improvement for GLYPH<27> IDS (i.e., 6.66% vs. 19.59%).\nTable 3 and Figure 5 show the results of HS-BNN model for various FinFET structures. To verify the beneGLYPH<28>t of the HS-BNN model, we compared it with the Gaussian-BNN model. Table 3 summarizes the predictive performance, showing that the HS-BNN showed much better results. Figure 5 demonstrates the performance of the model selection with respect to the layer sizes. For the number of nodes over 200, the HS-BNN's prediction for GLYPH<27> IDS showed almost the same results (MAPEs GLYPH<24> 7%), while the Gaussian-BNN showed totally different results for different numbers of nodes. It is noteworthy that it is a log-scale for the MAPE, RMSE, and MAE in Figure 5. This indicates that, as mentioned in [19], even if the number of nodes was excessively overestimated, the horseshoe prior made it possible to GLYPH<28>nd the most compact layer sizes. However, it is meaningful, not to merely make a simple comparison of predictive performance, but to focus on the fact that the HS-BNN achieved the model selection while making a GLYPH<28>ne predictive performance. This is because the number of nodes for every hidden layer is speciGLYPH<28>ed as the same. Manual layer size optimization was not performed, and the optimization was performed automatically\nin the HS-BNN. Thus, it is an open question whether the HS-BNNis better than all the Gaussian-BNNs. However, it is certain that the HS-BNN has saved a signiGLYPH<28>cant optimization time.\nFigures 6 and 7 show the predictions of the mean and standard deviation of log off-state current [log(IOFF)] and threshold voltage (VTH) of the FinFET with Lg D 20 nm, WGLYPH<28>n D 7 nm, HGLYPH<28>n D 42 nm, and an LER proGLYPH<28>le [i.e., GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1]. By GLYPH<28>xing all the parameters other than a single parameter, GLYPH<27> log(IOFF), GLYPH<22> log(IOFF), GLYPH<27> VTH, and GLYPH<22> VTH were predicted.\nThe device performance trends shown in Figures 6 and 7 seem to be well matched to the general trends, when considering several studies shown in [20]GLYPH<21>[25]. Using this HS-BNN model, we can show how both the standard deviation and mean of the electrical characteristics vary with arbitrary LER proGLYPH<28>les in various FinFET structures.\n\nEvaluates HS-BNN model performance using K-fold crossvalidation and metrics like MAPE, RMSE, and PLL, demonstrating improved prediction accuracy compared to previous Bayesian linear regression (BLR) models, particularly for GLYPH<27> IDS.",
    "original_text": "The HS-BNN predictions were evaluated using K-fold crossvalidation [splitting data in a K number of sections, and then, iteratively using one of the sections as a test set and the others as a training set (see Figure 4)]. This method is expected to be a proper choice when data are limited, by ensuring all data to be used as a test set at least once, which makes less biased evaluation and well estimates how a model will perform in general cases [17], [18]. Mean absolute percentage errors (MAPEs), root-mean-squared errors (RMSEs), mean absolute errors (MAEs) (herein, note that the lower they are,\nTABLE 2. MAPE and RMSE values of each model.\nTABLE 3. MAPE, RMSE, MAE, and PLL values of each model.\nthe better they are), and predictive log likelihoods (PLLs) (herein, note that the higher it is, the better it is) are calculated in each test set. Afterwards, the averages of those are the representative evaluation process values.\nThe overall prediction performance was improved, as compared against the previous work [1] which is the Bayesian linear regression (BLR) model (see Table 2 ). Note that the HS-BNN model for this comparison is only for the FinFET with Lg D 14 nm, WGLYPH<28>n D 7 nm, HGLYPH<28>n D 30 nm because the BLR model made predictions only for the corresponding FinFET structure. Table 2 summarizes the MAPE and RMSEof the two target variables (i.e., GLYPH<22> IDS and GLYPH<27> IDS) of the HS-BNN and BLR. The predictions of both GLYPH<22> IDS and GLYPH<27> IDS were much improved over those in the previous work. The prediction for GLYPH<22> IDS was improved (i.e., 0.55% vs. 0.81%), but there was signiGLYPH<28>cant prediction improvement for GLYPH<27> IDS (i.e., 6.66% vs. 19.59%).\nTable 3 and Figure 5 show the results of HS-BNN model for various FinFET structures. To verify the beneGLYPH<28>t of the HS-BNN model, we compared it with the Gaussian-BNN model. Table 3 summarizes the predictive performance, showing that the HS-BNN showed much better results. Figure 5 demonstrates the performance of the model selection with respect to the layer sizes. For the number of nodes over 200, the HS-BNN's prediction for GLYPH<27> IDS showed almost the same results (MAPEs GLYPH<24> 7%), while the Gaussian-BNN showed totally different results for different numbers of nodes. It is noteworthy that it is a log-scale for the MAPE, RMSE, and MAE in Figure 5. This indicates that, as mentioned in [19], even if the number of nodes was excessively overestimated, the horseshoe prior made it possible to GLYPH<28>nd the most compact layer sizes. However, it is meaningful, not to merely make a simple comparison of predictive performance, but to focus on the fact that the HS-BNN achieved the model selection while making a GLYPH<28>ne predictive performance. This is because the number of nodes for every hidden layer is speciGLYPH<28>ed as the same. Manual layer size optimization was not performed, and the optimization was performed automatically\nin the HS-BNN. Thus, it is an open question whether the HS-BNNis better than all the Gaussian-BNNs. However, it is certain that the HS-BNN has saved a signiGLYPH<28>cant optimization time.\nFigures 6 and 7 show the predictions of the mean and standard deviation of log off-state current [log(IOFF)] and threshold voltage (VTH) of the FinFET with Lg D 20 nm, WGLYPH<28>n D 7 nm, HGLYPH<28>n D 42 nm, and an LER proGLYPH<28>le [i.e., GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1]. By GLYPH<28>xing all the parameters other than a single parameter, GLYPH<27> log(IOFF), GLYPH<22> log(IOFF), GLYPH<27> VTH, and GLYPH<22> VTH were predicted.\nThe device performance trends shown in Figures 6 and 7 seem to be well matched to the general trends, when considering several studies shown in [20]GLYPH<21>[25]. Using this HS-BNN model, we can show how both the standard deviation and mean of the electrical characteristics vary with arbitrary LER proGLYPH<28>les in various FinFET structures.",
    "context": "Evaluates HS-BNN model performance using K-fold crossvalidation and metrics like MAPE, RMSE, and PLL, demonstrating improved prediction accuracy compared to previous Bayesian linear regression (BLR) models, particularly for GLYPH<27> IDS.",
    "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
    "pages": [
      5,
      6
    ],
    "id": "907ee6bd1249aa8cfc54c14f66a6290d0804ea2687ca417b39b85f6825d3e492"
  },
  {
    "text": "A Bayesian neural network model with horseshoe priors (HS-BNN) has been proposed for the design of LERimmune FinFETs. This model can quantitatively estimate the LER-induced random variation of IDS-VGS characteristics in various FinFET structures within a few seconds, which is much shorter than the conventional TCAD simulation running time. With the capability of neural networks to deGLYPH<28>ne highly nonlinear functions, the prediction has been improved [vs. the Bayesian linear regression model [1] (i.e., GLYPH<24> 6 % vs. GLYPH<24> 19 %)]. Moreover, the model selection performance over the most compact layer size was veriGLYPH<28>ed, which is essential when the amount of data is very limited.\n\nQuantitatively estimates LER-induced random variation in FinFET characteristics significantly faster than TCAD simulation, and validates model selection for compact layer sizes.",
    "original_text": "A Bayesian neural network model with horseshoe priors (HS-BNN) has been proposed for the design of LERimmune FinFETs. This model can quantitatively estimate the LER-induced random variation of IDS-VGS characteristics in various FinFET structures within a few seconds, which is much shorter than the conventional TCAD simulation running time. With the capability of neural networks to deGLYPH<28>ne highly nonlinear functions, the prediction has been improved [vs. the Bayesian linear regression model [1] (i.e., GLYPH<24> 6 % vs. GLYPH<24> 19 %)]. Moreover, the model selection performance over the most compact layer size was veriGLYPH<28>ed, which is essential when the amount of data is very limited.",
    "context": "Quantitatively estimates LER-induced random variation in FinFET characteristics significantly faster than TCAD simulation, and validates model selection for compact layer sizes.",
    "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
    "pages": [
      6
    ],
    "id": "a46e0e09053ca021f82200a107eaef751089702af5e42cd98622737900981c6e"
  },
  {
    "text": "- [1] S. Yu and C. Shin, ''Quantitative evaluation of process-induced lineedge roughness in FinFET: Bayesian regression model,'' Semicond. Sci. Technol. , vol. 32, no. 2, 2021, Art. no. 025020.\n- [2] L. Valentin Jospin, W. Buntine, F. Boussaid, H. Laga, and M. Bennamoun, ''Hands-on Bayesian neural networksGLYPH<21>A tutorial for deep learning users,'' 2020, arXiv:2007.06823 .\n- [3] R. Courtland, ''3-D transistors with different in heights make better memory cells,'' IEEE Spectr. , Jun. 2013. [Online]. Available: https://spectrum.ieee.org/3d-transistors-with-different-GLYPH<28>ns-heights-makebetter-memory-cells\n- [4] I. Aller and B. Rainey, ''Multi-height FinFETs,'' U.S. Patent 6 909 147 B2, Jun. 21, 2005.\n- [5] C. M. Carvalho, N. G. Polson, and J. G. Scott, ''Handling sparsity via the horseshoe,'' presented at the Artif. Intell. Statist., 2009. [Online]. Available: http://procee ings.mlr.press/v5/carvalho09a\n- [6] J. Miguel HernÆndez-Lobato and R. P. Adams, ''Probabilistic backpropagation for scalable learning of Bayesian neural networks,'' 2015, arXiv:1502.05336 .\n- [7] M. Badaroglu. (2018). International Roadmap for Device and Systems More Moore . [Online]. Available: https://irds.ieee.org/editions/2018\n- [8] A. Allan. (2016). International Roadmap for Device and Systems More Moore . [Online]. Available: https://irds.ieee.org/editions/2016\n- [9] WikiChip. Technology Node . Accessed: Mar. 15, 2021. [Online]. Available: https://en.wikichip.org/wiki/technology_node\n- [10] Wikipedia. Semiconductor Device Fabrication . Accessed: Mar. 15, 2021. [Online]. Available: https://en.wikipedia.org/wiki/Semiconductor_ device_fabrication\n- [11] S. Oh and C. Shin, ''3-D quasi-atomistic model for line edge roughness in nonplanar MOSFETs,'' IEEE Trans. Electron Devices , vol. 63, no. 12, pp. 4617GLYPH<21>4623, Dec. 2016.\n- [12] C.-Y. Chen, W.-T. Huang, and Y. Li, ''Electrical characteristic and power consumption GLYPH<29>uctuations of trapezoidal bulk FinFET devices and circuits induced by random line edge roughness,'' in Proc. 16th Int. Symp. Quality Electron. Design , Mar. 2015, pp. 61GLYPH<21>64, doi: 10.1109/ISQED.2015.7085399.\n- [13] Z. Ma, ''The function representation of artiGLYPH<28>cial neural network,'' 2019, arXiv:1908.10493 .\n- [14] J. Gordon and J. Miguel HernÆndez-Lobato, ''Bayesian semisupervised learning with deep generative models,'' 2017, arXiv:1706.09751 .\n- [15] H. Overweg, A.-L. Popkes, A. Ercole, Y. Li, J. Miguel HernÆndez-Lobato, Y. Zaykov, and C. Zhang, ''Interpretable outcome prediction with sparse Bayesian neural networks in intensive care,'' 2019, arXiv:1905.02599 .\n- [16] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, ''Weight uncertainty in neural networks,'' 2015, arXiv:1505.05424 .\n- [17] S. Shinmura, ''Comparison of linear discriminant function by K-fold cross validation,'' Data Anal. , vol. 2014, pp. 1GLYPH<21>6, Sep. 2014.\n- [18] J. Brownlee. (2018). A Gentle Introduction to K-Fold CrossValidation . Machine Learning Mastery. [Online]. Available: https:// machinelearningmastery.com/k-fold-cross-validation/\n- [19] S. Ghosh, J. Yao, and F. Doshi-Velez, ''Model selection in Bayesian neural networks via horseshoe priors,'' J. Mach. Learn. Res. , vol. 20, no. 182, pp. 1GLYPH<21>46, 2019.\n- [20] A. Asenov, S. Kaya, and A. R. Brown, ''Intrinsic parameter GLYPH<29>uctuations in decananometer MOSFETs introduced by gate line edge roughness,'' IEEE Trans. Electron Devices , vol. 50, no. 5, pp. 1254GLYPH<21>1260, May 2003.\n- [21] M. Wong, K. D. Holland, S. Anderson, and S. Rizw, ''Impact of shortwavelength and long-wavelength line-edge roughness on the variability of ultrascaled FinFETs,'' IEEE Trans. Electron Devices , vol. 64, no. 3, pp. 1231GLYPH<21>1238, Mar. 2017.\n- [22] Y. Ma, H. J. Levinson, and T. Wallow, ''Line edge roughness impact on critical dimension variation,'' Proc. SPIE , vol. 6518, Apr. 2007, Art. no. 651824.\n- [23] K. Patel, T. Wallow, H. J. Levinson, and C. J. Spanos, ''Comparative study of line width roughness (LWR) in next-generation lithography (NGL) processes,'' Proc. SPIE , vol. 7640, Mar. 2010, Art. no. 76400T.\n- [24] Y. Ban, ''Electrical impact of line-edge roughness on sub-45 nm node standard cell,'' J. Micro/Nanolithography, MEMS, MOEMS , vol. 9, no. 4, 2009, Art. no. 041206.\n- [25] E. Liu, K. Lutker-Lee, Q. Lou, Y.-M. Chen, A. Raley, and P. Biolsi, ''Line edge roughness (LER) reduction strategies for EUV self-aligned double patterning (SADP),'' Proc. SPIE , vol. 11615, Apr. 2021, Art. no. 1161506.\nSANGHO YU received the M.S. degree in electronic and electrical engineering from Sungkyunkwan University (SKKU), Suwon, Republic of Korea, in 2021. He is currently working as a Researcher with SKKU. His current research interests include process-induced random variation and machine learning model.\nSANG MIN WON received the B.S., M.S., and Ph.D. degrees in electrical and computer engineering from the University of Illinois at UrbanaGLYPH<21> Champaign. He is currently an Assistant Professor with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include sensors/stimulators with unique applications in advanced biomedical and/or health monitoring systems.\nHYOUNG WON BAAC received the B.S. degree (Hons.) in electronic engineering from Sungkyunkwan University, Suwon, Republic of Korea, in 1999, and the Ph.D. degree in electrical engineering and computer sciences from the University of Michigan, Ann Arbor, MI, USA, in 2011. He is currently an Associate Professor with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include opti- cal/acoustic/electrical sensors and systems for biomedical therapy, healthcare, and non-destructive evaluation.\nDONGHEE SON received the Ph.D. degree in chemical and biological engineering from Seoul National University, in 2015. He is currently an Assistant Professor with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include soft GLYPH<29>exible electronic devices and systems.\n\nThe document focuses on the impact of process-induced random variation, specifically line edge roughness (LER), in semiconductor device fabrication, particularly concerning FinFET technology. It explores various modeling techniques (Bayesian regression, quasi-atomistic models) and investigates the effects of LER on device characteristics and variability, referencing research from SKKU and related institutions.",
    "original_text": "- [1] S. Yu and C. Shin, ''Quantitative evaluation of process-induced lineedge roughness in FinFET: Bayesian regression model,'' Semicond. Sci. Technol. , vol. 32, no. 2, 2021, Art. no. 025020.\n- [2] L. Valentin Jospin, W. Buntine, F. Boussaid, H. Laga, and M. Bennamoun, ''Hands-on Bayesian neural networksGLYPH<21>A tutorial for deep learning users,'' 2020, arXiv:2007.06823 .\n- [3] R. Courtland, ''3-D transistors with different in heights make better memory cells,'' IEEE Spectr. , Jun. 2013. [Online]. Available: https://spectrum.ieee.org/3d-transistors-with-different-GLYPH<28>ns-heights-makebetter-memory-cells\n- [4] I. Aller and B. Rainey, ''Multi-height FinFETs,'' U.S. Patent 6 909 147 B2, Jun. 21, 2005.\n- [5] C. M. Carvalho, N. G. Polson, and J. G. Scott, ''Handling sparsity via the horseshoe,'' presented at the Artif. Intell. Statist., 2009. [Online]. Available: http://procee ings.mlr.press/v5/carvalho09a\n- [6] J. Miguel HernÆndez-Lobato and R. P. Adams, ''Probabilistic backpropagation for scalable learning of Bayesian neural networks,'' 2015, arXiv:1502.05336 .\n- [7] M. Badaroglu. (2018). International Roadmap for Device and Systems More Moore . [Online]. Available: https://irds.ieee.org/editions/2018\n- [8] A. Allan. (2016). International Roadmap for Device and Systems More Moore . [Online]. Available: https://irds.ieee.org/editions/2016\n- [9] WikiChip. Technology Node . Accessed: Mar. 15, 2021. [Online]. Available: https://en.wikichip.org/wiki/technology_node\n- [10] Wikipedia. Semiconductor Device Fabrication . Accessed: Mar. 15, 2021. [Online]. Available: https://en.wikipedia.org/wiki/Semiconductor_ device_fabrication\n- [11] S. Oh and C. Shin, ''3-D quasi-atomistic model for line edge roughness in nonplanar MOSFETs,'' IEEE Trans. Electron Devices , vol. 63, no. 12, pp. 4617GLYPH<21>4623, Dec. 2016.\n- [12] C.-Y. Chen, W.-T. Huang, and Y. Li, ''Electrical characteristic and power consumption GLYPH<29>uctuations of trapezoidal bulk FinFET devices and circuits induced by random line edge roughness,'' in Proc. 16th Int. Symp. Quality Electron. Design , Mar. 2015, pp. 61GLYPH<21>64, doi: 10.1109/ISQED.2015.7085399.\n- [13] Z. Ma, ''The function representation of artiGLYPH<28>cial neural network,'' 2019, arXiv:1908.10493 .\n- [14] J. Gordon and J. Miguel HernÆndez-Lobato, ''Bayesian semisupervised learning with deep generative models,'' 2017, arXiv:1706.09751 .\n- [15] H. Overweg, A.-L. Popkes, A. Ercole, Y. Li, J. Miguel HernÆndez-Lobato, Y. Zaykov, and C. Zhang, ''Interpretable outcome prediction with sparse Bayesian neural networks in intensive care,'' 2019, arXiv:1905.02599 .\n- [16] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, ''Weight uncertainty in neural networks,'' 2015, arXiv:1505.05424 .\n- [17] S. Shinmura, ''Comparison of linear discriminant function by K-fold cross validation,'' Data Anal. , vol. 2014, pp. 1GLYPH<21>6, Sep. 2014.\n- [18] J. Brownlee. (2018). A Gentle Introduction to K-Fold CrossValidation . Machine Learning Mastery. [Online]. Available: https:// machinelearningmastery.com/k-fold-cross-validation/\n- [19] S. Ghosh, J. Yao, and F. Doshi-Velez, ''Model selection in Bayesian neural networks via horseshoe priors,'' J. Mach. Learn. Res. , vol. 20, no. 182, pp. 1GLYPH<21>46, 2019.\n- [20] A. Asenov, S. Kaya, and A. R. Brown, ''Intrinsic parameter GLYPH<29>uctuations in decananometer MOSFETs introduced by gate line edge roughness,'' IEEE Trans. Electron Devices , vol. 50, no. 5, pp. 1254GLYPH<21>1260, May 2003.\n- [21] M. Wong, K. D. Holland, S. Anderson, and S. Rizw, ''Impact of shortwavelength and long-wavelength line-edge roughness on the variability of ultrascaled FinFETs,'' IEEE Trans. Electron Devices , vol. 64, no. 3, pp. 1231GLYPH<21>1238, Mar. 2017.\n- [22] Y. Ma, H. J. Levinson, and T. Wallow, ''Line edge roughness impact on critical dimension variation,'' Proc. SPIE , vol. 6518, Apr. 2007, Art. no. 651824.\n- [23] K. Patel, T. Wallow, H. J. Levinson, and C. J. Spanos, ''Comparative study of line width roughness (LWR) in next-generation lithography (NGL) processes,'' Proc. SPIE , vol. 7640, Mar. 2010, Art. no. 76400T.\n- [24] Y. Ban, ''Electrical impact of line-edge roughness on sub-45 nm node standard cell,'' J. Micro/Nanolithography, MEMS, MOEMS , vol. 9, no. 4, 2009, Art. no. 041206.\n- [25] E. Liu, K. Lutker-Lee, Q. Lou, Y.-M. Chen, A. Raley, and P. Biolsi, ''Line edge roughness (LER) reduction strategies for EUV self-aligned double patterning (SADP),'' Proc. SPIE , vol. 11615, Apr. 2021, Art. no. 1161506.\nSANGHO YU received the M.S. degree in electronic and electrical engineering from Sungkyunkwan University (SKKU), Suwon, Republic of Korea, in 2021. He is currently working as a Researcher with SKKU. His current research interests include process-induced random variation and machine learning model.\nSANG MIN WON received the B.S., M.S., and Ph.D. degrees in electrical and computer engineering from the University of Illinois at UrbanaGLYPH<21> Champaign. He is currently an Assistant Professor with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include sensors/stimulators with unique applications in advanced biomedical and/or health monitoring systems.\nHYOUNG WON BAAC received the B.S. degree (Hons.) in electronic engineering from Sungkyunkwan University, Suwon, Republic of Korea, in 1999, and the Ph.D. degree in electrical engineering and computer sciences from the University of Michigan, Ann Arbor, MI, USA, in 2011. He is currently an Associate Professor with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include opti- cal/acoustic/electrical sensors and systems for biomedical therapy, healthcare, and non-destructive evaluation.\nDONGHEE SON received the Ph.D. degree in chemical and biological engineering from Seoul National University, in 2015. He is currently an Assistant Professor with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include soft GLYPH<29>exible electronic devices and systems.",
    "context": "The document focuses on the impact of process-induced random variation, specifically line edge roughness (LER), in semiconductor device fabrication, particularly concerning FinFET technology. It explores various modeling techniques (Bayesian regression, quasi-atomistic models) and investigates the effects of LER on device characteristics and variability, referencing research from SKKU and related institutions.",
    "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
    "pages": [
      6,
      7
    ],
    "id": "0c99593db324f7e427a5fe2b52c6ac15308dc333385e0c7476ca77a6a3545827"
  },
  {
    "text": "CHANGHWAN SHIN (Senior Member, IEEE) received the B.S. degree (Hons.) in electrical engineering from Korea University, Seoul, South Korea, in 2006, and the Ph.D. degree in electrical engineering and computer sciences from the University of California at Berkeley, Berkeley, CA, USA, in 2011. Since 2017, he has been the Board of Directors with SK Hynix. He is currently Professor with the School of Electrical Engineering, Korea University. His current research interests include advanced CMOS device designs and their applications to memory/logic devices.\n\nDetails Changhwan Shin’s academic background and current position at Korea University and SK Hynix, highlighting his research interests in CMOS device design.",
    "original_text": "CHANGHWAN SHIN (Senior Member, IEEE) received the B.S. degree (Hons.) in electrical engineering from Korea University, Seoul, South Korea, in 2006, and the Ph.D. degree in electrical engineering and computer sciences from the University of California at Berkeley, Berkeley, CA, USA, in 2011. Since 2017, he has been the Board of Directors with SK Hynix. He is currently Professor with the School of Electrical Engineering, Korea University. His current research interests include advanced CMOS device designs and their applications to memory/logic devices.",
    "context": "Details Changhwan Shin’s academic background and current position at Korea University and SK Hynix, highlighting his research interests in CMOS device design.",
    "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
    "pages": [
      7
    ],
    "id": "bb12d010210110042d07877954fddc21c85dcbc9145af6a0edc2c6a844c89b57"
  },
  {
    "text": "Received May 11, 2021, accepted June 6, 2021, date of publication June 11, 2021, date of current version June 22, 2021.\nDigital Object Identifier 10.1 109/ACCESS.2021.3088461\n\nProvides publication and version information.",
    "original_text": "Received May 11, 2021, accepted June 6, 2021, date of publication June 11, 2021, date of current version June 22, 2021.\nDigital Object Identifier 10.1 109/ACCESS.2021.3088461",
    "context": "Provides publication and version information.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      1
    ],
    "id": "13edd023ab6e240494f754af75c8884c586b1b4c9469f03c99a3a9a6939ba1b6"
  },
  {
    "text": "Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon 16419, Republic of Korea\nCorresponding author: Changhwan Shin (cshin@skku.edu)\nThis work was supported in part by the National Research Foundation of Korea (NRF) grant funded by the Korea Government Ministry of Science and ICT (MSIT) under Grant 2020R1A2C1009063, and in part by the Future Semiconductor Device Technology Development Program under Grant 20003551 funded by the Ministry of Trade, Industry and Energy (MOTIE) and the Korea Semiconductor Research Consortium (KSRC).\n- ABSTRACT Line-edge-roughness (LER) is one of undesirable process-induced random variation sources. LER is mostly occurred in the process of photo-lithography and etching, and it provokes random variation in performance of transistors such as metal oxide semiconductor GLYPH<28>eld effect transistor (MOSFET), GLYPH<28>n-shaped GLYPH<28>eld effect transistor (FinFET), and gate-all-around GLYPH<28>eld effect transistor (GAAFET). LER was analyzed/characterized with technology computer-aided design (TCAD), but it is fundamentally very time consuming. To tackle this issue, machine learning (ML)-based method is proposed in this work. LER parameters (i.e., amplitude, and correlation length X, Y) are provided as inputs. Then, artiGLYPH<28>cial neural network (ANN) predicts 7-parameters [i.e., off-state leakage current (Ioff), saturation drain current (Idsat), linear drain current (Idlin), low drain current (Idlo), high drain current (Idhi), saturation threshold voltage (Vtsat), and linear threshold voltage (Vtlin)] which are usually used to evaluate the performance of FinFET. First, how datasets for training process of ANN were generated is explained. Next, the evaluation method for probabilistic problem is introduced. Finally, the architecture of ANN, training process and our new proposition is presented. It turned out that the prediction results (i.e., non-Gaussian distribution of device performance metrics) obtained from the ANN were very similar to that from TCAD in the respect of both qualitative and quantitative comparison.\nLine edge roughness, process-induced random variation, FinFET, machine learning,\n- INDEX TERMS artiGLYPH<28>cial neural network.\n\nIntroduces a machine learning approach to predict FinFET performance metrics affected by line edge roughness, comparing the results to TCAD simulations.",
    "original_text": "Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon 16419, Republic of Korea\nCorresponding author: Changhwan Shin (cshin@skku.edu)\nThis work was supported in part by the National Research Foundation of Korea (NRF) grant funded by the Korea Government Ministry of Science and ICT (MSIT) under Grant 2020R1A2C1009063, and in part by the Future Semiconductor Device Technology Development Program under Grant 20003551 funded by the Ministry of Trade, Industry and Energy (MOTIE) and the Korea Semiconductor Research Consortium (KSRC).\n- ABSTRACT Line-edge-roughness (LER) is one of undesirable process-induced random variation sources. LER is mostly occurred in the process of photo-lithography and etching, and it provokes random variation in performance of transistors such as metal oxide semiconductor GLYPH<28>eld effect transistor (MOSFET), GLYPH<28>n-shaped GLYPH<28>eld effect transistor (FinFET), and gate-all-around GLYPH<28>eld effect transistor (GAAFET). LER was analyzed/characterized with technology computer-aided design (TCAD), but it is fundamentally very time consuming. To tackle this issue, machine learning (ML)-based method is proposed in this work. LER parameters (i.e., amplitude, and correlation length X, Y) are provided as inputs. Then, artiGLYPH<28>cial neural network (ANN) predicts 7-parameters [i.e., off-state leakage current (Ioff), saturation drain current (Idsat), linear drain current (Idlin), low drain current (Idlo), high drain current (Idhi), saturation threshold voltage (Vtsat), and linear threshold voltage (Vtlin)] which are usually used to evaluate the performance of FinFET. First, how datasets for training process of ANN were generated is explained. Next, the evaluation method for probabilistic problem is introduced. Finally, the architecture of ANN, training process and our new proposition is presented. It turned out that the prediction results (i.e., non-Gaussian distribution of device performance metrics) obtained from the ANN were very similar to that from TCAD in the respect of both qualitative and quantitative comparison.\nLine edge roughness, process-induced random variation, FinFET, machine learning,\n- INDEX TERMS artiGLYPH<28>cial neural network.",
    "context": "Introduces a machine learning approach to predict FinFET performance metrics affected by line edge roughness, comparing the results to TCAD simulations.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      1
    ],
    "id": "67ee8ff59e145ba0f792599de7302dd62ece3933ee9becd56d809fc78b410611"
  },
  {
    "text": "Over the past a few decades, complementary metal oxide semiconductor (CMOS) technology has been evolved with advanced techniques such as stress engineering in 90 nm technology node [1], high-k/metal-gate (HK/MG) in 45 nm technology node [2], and three-dimensional advanced device structure in 22 nm technology node [3]. Those new techniques have enabled the physical dimension of metal oxide semiconductor GLYPH<28>eld effect transistor (MOSFET) to be successfully scaled down, resulting in the improved functions of integrated circuit (IC) per cost. However, there still exists secondary effects in aggressively scaled MOSFETs, and they should be overcome. Especially, one of those challenges, i.e., process-induced random variation, which randomly cause\nThe associate editor coordinating the review of this manuscript and approving it for publication was Kalyan Koley .\nvariation in transistor performance metrics such as threshold voltage, on-state drive current, and off-state leakage current, become signiGLYPH<28>cant as CMOS technology is evolved [4]. The primary causes of process-induced random variation can be classiGLYPH<28>ed as (i) line edge roughness (LER), (ii) random dopant GLYPH<29>uctuation (RDF), and (iii) work function variation (WFV) [5]. Among them, because LER can affect the other random variation sources (i.e., RDF and WFV) by inducing the deformation of device structure [6], it would degrade the device performance more severely. With the most radical shift in device architecture at 22 nm node, i.e., from planar bulk MOSFET to three-dimensional GLYPH<28>n-shaped GLYPH<28>eld effect transistor (FinFET), the process-induced random variation becomes much more severe [7]. As expected that a more 45 complicated device structure such as multiple bridge channel GLYPH<28>eld effect transistor (MBCFET), stacked nano-wire FET, complementary FET (CFET) would be adopted at 3 nm node\n[9], understanding and analyzing the impact of LER on device performance is essential for designing variation-robust silicon device.\nTo understand and quantify the impact of LER on device performance, TCAD has been used so far. TCAD simulation is, however, fundamentally very time-consuming. As another approach, compact model [10] has been used to overcome the time-inefGLYPH<28>ciency of using TCAD tools. However, the compact model for LER is still based on 2-D analysis, despite that 3-D analysis for LER is necessary [7]. As an alternative way to avoid those obstacles, we have focused on machine learning (ML)-based artiGLYPH<28>cial neural network (ANN). Machine Learning technology has already been in spotlight in various GLYPH<28>elds such as geology, and biology [11]GLYPH<21>[13]. Especially in semiconductor technologies, a number of studies have been reported in many branches such as fabrication [14], [15], optimization [16], and modeling [17]. Following this trend, we suggested a ML-based ANN model in our previous work [18]. However, it turned out that the ML-based ANN had limits, in that target performance metrics were assumed to follow multi-variate Gaussian distribution only. This assumption would cause some inevitable errors. Moreover, the ANN model has its own intrinsic limit in estimating the other performance metrics such as Idlo , and Idhi (which, in real, do not follow Gaussian distribution). Therefore, we would like to develop and propose an upgraded ANN model with enhanced accuracy (note that this new ANN model can overcome the limit mentioned above).\nIn this work, we show the way how FinFETs with LER are simulated as well as how those data are preprocessed for training process of ANN. Afterwards, the evaluation method used to assess the results of proposed work is introduced. Finally, it is shown how ANN was composed and built, including its geometrical structure, hyper parameters, and the process of grafting probability.\n\nHighlights the challenges of process-induced random variation in advanced CMOS technology and proposes a new, upgraded ANN model to address limitations in existing approaches.",
    "original_text": "Over the past a few decades, complementary metal oxide semiconductor (CMOS) technology has been evolved with advanced techniques such as stress engineering in 90 nm technology node [1], high-k/metal-gate (HK/MG) in 45 nm technology node [2], and three-dimensional advanced device structure in 22 nm technology node [3]. Those new techniques have enabled the physical dimension of metal oxide semiconductor GLYPH<28>eld effect transistor (MOSFET) to be successfully scaled down, resulting in the improved functions of integrated circuit (IC) per cost. However, there still exists secondary effects in aggressively scaled MOSFETs, and they should be overcome. Especially, one of those challenges, i.e., process-induced random variation, which randomly cause\nThe associate editor coordinating the review of this manuscript and approving it for publication was Kalyan Koley .\nvariation in transistor performance metrics such as threshold voltage, on-state drive current, and off-state leakage current, become signiGLYPH<28>cant as CMOS technology is evolved [4]. The primary causes of process-induced random variation can be classiGLYPH<28>ed as (i) line edge roughness (LER), (ii) random dopant GLYPH<29>uctuation (RDF), and (iii) work function variation (WFV) [5]. Among them, because LER can affect the other random variation sources (i.e., RDF and WFV) by inducing the deformation of device structure [6], it would degrade the device performance more severely. With the most radical shift in device architecture at 22 nm node, i.e., from planar bulk MOSFET to three-dimensional GLYPH<28>n-shaped GLYPH<28>eld effect transistor (FinFET), the process-induced random variation becomes much more severe [7]. As expected that a more 45 complicated device structure such as multiple bridge channel GLYPH<28>eld effect transistor (MBCFET), stacked nano-wire FET, complementary FET (CFET) would be adopted at 3 nm node\n[9], understanding and analyzing the impact of LER on device performance is essential for designing variation-robust silicon device.\nTo understand and quantify the impact of LER on device performance, TCAD has been used so far. TCAD simulation is, however, fundamentally very time-consuming. As another approach, compact model [10] has been used to overcome the time-inefGLYPH<28>ciency of using TCAD tools. However, the compact model for LER is still based on 2-D analysis, despite that 3-D analysis for LER is necessary [7]. As an alternative way to avoid those obstacles, we have focused on machine learning (ML)-based artiGLYPH<28>cial neural network (ANN). Machine Learning technology has already been in spotlight in various GLYPH<28>elds such as geology, and biology [11]GLYPH<21>[13]. Especially in semiconductor technologies, a number of studies have been reported in many branches such as fabrication [14], [15], optimization [16], and modeling [17]. Following this trend, we suggested a ML-based ANN model in our previous work [18]. However, it turned out that the ML-based ANN had limits, in that target performance metrics were assumed to follow multi-variate Gaussian distribution only. This assumption would cause some inevitable errors. Moreover, the ANN model has its own intrinsic limit in estimating the other performance metrics such as Idlo , and Idhi (which, in real, do not follow Gaussian distribution). Therefore, we would like to develop and propose an upgraded ANN model with enhanced accuracy (note that this new ANN model can overcome the limit mentioned above).\nIn this work, we show the way how FinFETs with LER are simulated as well as how those data are preprocessed for training process of ANN. Afterwards, the evaluation method used to assess the results of proposed work is introduced. Finally, it is shown how ANN was composed and built, including its geometrical structure, hyper parameters, and the process of grafting probability.",
    "context": "Highlights the challenges of process-induced random variation in advanced CMOS technology and proposes a new, upgraded ANN model to address limitations in existing approaches.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      1,
      2
    ],
    "id": "67cc20048dc953ca4401bea493596bb83b8a68e769d8f220e5c6b739a717009b"
  },
  {
    "text": "Asdone in the previous study [18], 3-D quasi atomistic model for line edge roughness was used [19]. Three parameters (i.e., 1 , 3 x, and 3 y) are used to describe and reconGLYPH<28>gure LER proGLYPH<28>le. The physical meaning of those parameters are as follows [see Fig 2]:\n- (i) Amplitude ( 1 ): it indicates the rms (root-meansquared) value of surface roughness.\n- (ii) Correlation length ( 3 ): it means how line edge is closely correlated with its neighboring edge. A larger 3 indicates a smoother line.\nFIGURE 1. A bird's-eye view of FinFET with a three-dimensional line-edge-roughness (LER) on its sidewall. LER parameters used in this example are as follows: 1 D 0 : 5 nm, 3 x D 20 nm, 3 y D 50 nm, GLYPH<11> D 1, and 2 D 0.\nFIGURE 2. Examples of roughness amplitude when (a) 3 D 10, and (b) 1 D 0.5.\nIn Eq. (1), as shown at the bottom of the page, 3 x and 3 y are the correlation length along x-direction and y-direction, respectively [see Fig. 1]. 2 indicates the relation between x-direction and y-direction.\nUsing parameters mentioned above and two-dimensional auto covariance function (ACVF), we have simulated FinFET with MATLAB and TCAD Sentaurus Structure Editor and Device [19], [20]. The detailed steps how to create a rough sidewall surface of FinFET are provided in [18], [19].\nNominal device parameters of FinFET is summarized in Table 1. Drift-diffusion simulation for the FinFETs with surface roughness are executed, using various models such as doping-dependent mobility model, thin-layer mobility model for carrier transport, the Shockley-Read-Hall (SRH) model for generation and recombination, high GLYPH<28>eld saturation model\n<!-- formula-not-decoded -->\nTABLE 1. Nominal device parameters of FinFET [8].\nTABLE 2. Performance metrics.\nfor velocity saturation, and a density-gradient quantization model for quantum-mechanical effects.\n\nDetails the methodology for simulating FinFET sidewall roughness, including specific parameters, simulation tools, and underlying models.",
    "original_text": "Asdone in the previous study [18], 3-D quasi atomistic model for line edge roughness was used [19]. Three parameters (i.e., 1 , 3 x, and 3 y) are used to describe and reconGLYPH<28>gure LER proGLYPH<28>le. The physical meaning of those parameters are as follows [see Fig 2]:\n- (i) Amplitude ( 1 ): it indicates the rms (root-meansquared) value of surface roughness.\n- (ii) Correlation length ( 3 ): it means how line edge is closely correlated with its neighboring edge. A larger 3 indicates a smoother line.\nFIGURE 1. A bird's-eye view of FinFET with a three-dimensional line-edge-roughness (LER) on its sidewall. LER parameters used in this example are as follows: 1 D 0 : 5 nm, 3 x D 20 nm, 3 y D 50 nm, GLYPH<11> D 1, and 2 D 0.\nFIGURE 2. Examples of roughness amplitude when (a) 3 D 10, and (b) 1 D 0.5.\nIn Eq. (1), as shown at the bottom of the page, 3 x and 3 y are the correlation length along x-direction and y-direction, respectively [see Fig. 1]. 2 indicates the relation between x-direction and y-direction.\nUsing parameters mentioned above and two-dimensional auto covariance function (ACVF), we have simulated FinFET with MATLAB and TCAD Sentaurus Structure Editor and Device [19], [20]. The detailed steps how to create a rough sidewall surface of FinFET are provided in [18], [19].\nNominal device parameters of FinFET is summarized in Table 1. Drift-diffusion simulation for the FinFETs with surface roughness are executed, using various models such as doping-dependent mobility model, thin-layer mobility model for carrier transport, the Shockley-Read-Hall (SRH) model for generation and recombination, high GLYPH<28>eld saturation model\n<!-- formula-not-decoded -->\nTABLE 1. Nominal device parameters of FinFET [8].\nTABLE 2. Performance metrics.\nfor velocity saturation, and a density-gradient quantization model for quantum-mechanical effects.",
    "context": "Details the methodology for simulating FinFET sidewall roughness, including specific parameters, simulation tools, and underlying models.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      2,
      3
    ],
    "id": "ba77fab3e371f17c8d4404f048c643f41e908f572dac971dcfcd1e0016e09b90"
  },
  {
    "text": "To train, validate, and test the ANN model that we have newly built up, two kinds of datasets were separately generated: (1) The GLYPH<28>rst kind of dataset has 130 datasets. Each dataset contains the performance metrics of 50 different FinFETs, so that total 6,500 FinFETs are used in 130 datasets. 70% of them are used as the training datasets, and the others are used as the validation datasets. (2) The second kind of dataset has 10 different datasets. Each dataset contains the performance metrics of 250 different FinFETs, so that total 2,500 FinFETs are used in 10 datasets.\nThe LER parameters are chosen in the range below V\nAmplitude ( 1 )\nV 0 : 1 nm GLYPH<24> 0 : 8 nm\nCorrelation length X( 3 x) V 10 nm GLYPH<24> 100 nm\nCorrelation length Y( 3 y) V 20 nm GLYPH<24> 200 nm\nThe reference parameter set ( 1 D 0.5 nm, 3 x D 20 nm, 3 y D 50 nm) is obtained, based on experimental results [21]GLYPH<21>[24]. Then, the range is selected on the basis of the reference parameter set. Note that the performance metrics of transistor were extracted from simulated drain current versus gate voltage (Id-vs.-Vg) characteristic. The details on the device performance metrics are summarized in Table 2. These metrics are extracted using the Sentaurus TCAD inspect [20].\n\nDetails the creation and composition of datasets used for ANN model training and validation, including specific parameter ranges for transistor performance metrics.",
    "original_text": "To train, validate, and test the ANN model that we have newly built up, two kinds of datasets were separately generated: (1) The GLYPH<28>rst kind of dataset has 130 datasets. Each dataset contains the performance metrics of 50 different FinFETs, so that total 6,500 FinFETs are used in 130 datasets. 70% of them are used as the training datasets, and the others are used as the validation datasets. (2) The second kind of dataset has 10 different datasets. Each dataset contains the performance metrics of 250 different FinFETs, so that total 2,500 FinFETs are used in 10 datasets.\nThe LER parameters are chosen in the range below V\nAmplitude ( 1 )\nV 0 : 1 nm GLYPH<24> 0 : 8 nm\nCorrelation length X( 3 x) V 10 nm GLYPH<24> 100 nm\nCorrelation length Y( 3 y) V 20 nm GLYPH<24> 200 nm\nThe reference parameter set ( 1 D 0.5 nm, 3 x D 20 nm, 3 y D 50 nm) is obtained, based on experimental results [21]GLYPH<21>[24]. Then, the range is selected on the basis of the reference parameter set. Note that the performance metrics of transistor were extracted from simulated drain current versus gate voltage (Id-vs.-Vg) characteristic. The details on the device performance metrics are summarized in Table 2. These metrics are extracted using the Sentaurus TCAD inspect [20].",
    "context": "Details the creation and composition of datasets used for ANN model training and validation, including specific parameter ranges for transistor performance metrics.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      3
    ],
    "id": "ebd31e6ca79e50e00bd8ac62e6e03f71b4d485ce9b7a2d16c9c21eb800778c85"
  },
  {
    "text": "To quantitatively verify the distribution of values obtained from the new ANN model, we used earth-mover's distance (EMD) score (or be referred to as Wasserstein metric\nFIGURE 3. Conceptual diagram for showing the mixture of multivariate normal distributions.\nin mathematics). The EMD score is used to measure how two probability distributions are different from each other [25]. The deGLYPH<28>nition of the score is ''The minimal amount of work needed to transform one distribution to another distribution''. The EMD score can be calculated following the steps below:\nStep I: Calculate the difference of cumulative distribution function (CDF) of TCAD datasets and ANN prediction datasets.\nStep II: Normalize the calculated value in Step I.\nIn this work, we used Gaussian kernel density estimation (KDE) to estimate CDF of datasets. The EMD score is ''0'' when two distributions are exactly identical.\n\nDetails the calculation and interpretation of the earth-mover's distance (EMD) score used to compare datasets.",
    "original_text": "To quantitatively verify the distribution of values obtained from the new ANN model, we used earth-mover's distance (EMD) score (or be referred to as Wasserstein metric\nFIGURE 3. Conceptual diagram for showing the mixture of multivariate normal distributions.\nin mathematics). The EMD score is used to measure how two probability distributions are different from each other [25]. The deGLYPH<28>nition of the score is ''The minimal amount of work needed to transform one distribution to another distribution''. The EMD score can be calculated following the steps below:\nStep I: Calculate the difference of cumulative distribution function (CDF) of TCAD datasets and ANN prediction datasets.\nStep II: Normalize the calculated value in Step I.\nIn this work, we used Gaussian kernel density estimation (KDE) to estimate CDF of datasets. The EMD score is ''0'' when two distributions are exactly identical.",
    "context": "Details the calculation and interpretation of the earth-mover's distance (EMD) score used to compare datasets.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      3
    ],
    "id": "061a8d07cee4b5796e315775572201e0a08ef619d49126f342a9283bcfbd91ea"
  },
  {
    "text": "Different from the previous study [10], the mixture of multivariate normal distributions (MVN) is used in this work. By using the mixture of MVNs, we can respond to many other unknown distribution shapes. It is trivial that performance metrics of transistor approximately follow Gaussian distribution [26]GLYPH<21>[28]. However, triggered by many non-ideal effects in transistors (i.e., short-channel effects [29]), there are some skewness, kurtosis, and/or non-linear correlation in-between the performance metrics. Moreover, distribution shapes are also quite different, for each LER parameter. For those reasons, the mixture of MVNs was used to deal with the non-ideal cases. A conceptual diagram for the mixture of MVNs is shown in Fig. 3.\nTo determine the number of components (i.e., MVN distributions) used in generating the mixture of MVNs, an optimization was GLYPH<28>rst done. V alidation datasets were used in the training process, to GLYPH<28>nd the best working model. In Fig. 4, it can be noted that the validation loss was minimized with the number of components of 11 at 7,800 epochs. This means that the ANN model with 11 MVNs works best to describe the distribution of performance metrics. The optimized ANN with the mixture of MVNs has 3 neurons for the input layer, 81 neurons for the GLYPH<28>rst hidden layer, 162 neurons for the second hidden layer, 324 for the third hidden layer, and 324 neurons for the output layer. This output neurons are\nFIGURE 4. Validation loss vs. the number of epochs, when the number of distribution used to mixture is varied from 9 to 13.\nFIGURE 5. The flow chart how to build/train/test the ANN model.\nconnected to probabilistic layer with the mixture of multivariate normal distribution, for the generation of power density function (PDF) of output variables (performance metrics).\n\nJustifies the use of a mixture of MVNs to account for non-Gaussian performance metrics and establishes the optimal number of components (11) for the ANN model.",
    "original_text": "Different from the previous study [10], the mixture of multivariate normal distributions (MVN) is used in this work. By using the mixture of MVNs, we can respond to many other unknown distribution shapes. It is trivial that performance metrics of transistor approximately follow Gaussian distribution [26]GLYPH<21>[28]. However, triggered by many non-ideal effects in transistors (i.e., short-channel effects [29]), there are some skewness, kurtosis, and/or non-linear correlation in-between the performance metrics. Moreover, distribution shapes are also quite different, for each LER parameter. For those reasons, the mixture of MVNs was used to deal with the non-ideal cases. A conceptual diagram for the mixture of MVNs is shown in Fig. 3.\nTo determine the number of components (i.e., MVN distributions) used in generating the mixture of MVNs, an optimization was GLYPH<28>rst done. V alidation datasets were used in the training process, to GLYPH<28>nd the best working model. In Fig. 4, it can be noted that the validation loss was minimized with the number of components of 11 at 7,800 epochs. This means that the ANN model with 11 MVNs works best to describe the distribution of performance metrics. The optimized ANN with the mixture of MVNs has 3 neurons for the input layer, 81 neurons for the GLYPH<28>rst hidden layer, 162 neurons for the second hidden layer, 324 for the third hidden layer, and 324 neurons for the output layer. This output neurons are\nFIGURE 4. Validation loss vs. the number of epochs, when the number of distribution used to mixture is varied from 9 to 13.\nFIGURE 5. The flow chart how to build/train/test the ANN model.\nconnected to probabilistic layer with the mixture of multivariate normal distribution, for the generation of power density function (PDF) of output variables (performance metrics).",
    "context": "Justifies the use of a mixture of MVNs to account for non-Gaussian performance metrics and establishes the optimal number of components (11) for the ANN model.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      3,
      4
    ],
    "id": "162dab2f62129f1b2441783099b32df98f5b8926a1457eb43ad2f723462cd3f3"
  },
  {
    "text": "Unlike the proposed ANN in the previous section, another simple ANN was built only for estimating mean and standard deviation. With this simple ANN, the performance metrics were standardized in the training process of ANN with the mixture of MVNs [see Fig. 5]. By using additional simple ANN, we can limit the role of ANN with the mixture of MVNs, to just estimate the shape of probability distribution. Thus, the time spent to train the ANN model can be largely reduced (see Table 3). The simple ANN has 3 neurons for the input layer, 3 neurons for the GLYPH<28>rst hidden layer, 7 neurons for the second hidden layer, 14 neurons for the third hidden layer, and 14 neurons for the output layer. To compare this work against the non-separated ANN (note that the ANN model\nTABLE 3. Time spent to train ANN model.\ncan predict the mean, standard deviation, and the shape of distribution, at once), the same work mentioned in Part A was repeated to GLYPH<28>nd the optimized ANN (i.e., the number of MVNs, training epochs, etc.).\n\nProvides a simplified ANN model for estimating distribution shape, reducing training time compared to the primary ANN model.",
    "original_text": "Unlike the proposed ANN in the previous section, another simple ANN was built only for estimating mean and standard deviation. With this simple ANN, the performance metrics were standardized in the training process of ANN with the mixture of MVNs [see Fig. 5]. By using additional simple ANN, we can limit the role of ANN with the mixture of MVNs, to just estimate the shape of probability distribution. Thus, the time spent to train the ANN model can be largely reduced (see Table 3). The simple ANN has 3 neurons for the input layer, 3 neurons for the GLYPH<28>rst hidden layer, 7 neurons for the second hidden layer, 14 neurons for the third hidden layer, and 14 neurons for the output layer. To compare this work against the non-separated ANN (note that the ANN model\nTABLE 3. Time spent to train ANN model.\ncan predict the mean, standard deviation, and the shape of distribution, at once), the same work mentioned in Part A was repeated to GLYPH<28>nd the optimized ANN (i.e., the number of MVNs, training epochs, etc.).",
    "context": "Provides a simplified ANN model for estimating distribution shape, reducing training time compared to the primary ANN model.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      4
    ],
    "id": "0683b94871fa28ca24ee920927d7e7580363e697b18b1ef0198bf02ea2bd7c9c"
  },
  {
    "text": "The weight matrices and bias vectors of ANN model are updated for the given/speciGLYPH<28>ed number of iterations. These matrices and vectors determine the output of ANN model. The probabilistic layer attached to output neurons returns the PDF of variables while training process. Thus, conventional mean-squared error cannot be used as loss function. Instead, ''Negative log likelihood'' (Negloglik) was used as a loss function [see Eq. (2)]. The training process is executed to minimize this loss function. That is, training ANN becomes the process of Maximum Likelihood Estimation (MLE) [30].\n<!-- formula-not-decoded -->\nIn Eq. (2), P(x) and Q(x) denotes the PDF of observation and hypothesis, respectively.\nUsing Adam Optimizer [31], the training process was executed for 14,880 epochs (84 sec) for mean and standard deviation of ANN model and 7,800 epochs (101 sec) for the mixture of MVNs ANN model. Both models are trained with the learning rate of 10 GLYPH<0> 4 , and then optimized to prevent the occurrence of over-GLYPH<28>tting by using validation datasets. ReLU was used as the activation function for both models. Note that the ANN model was built using the TensorGLYPH<29>ow 2.0 and TensorGLYPH<29>ow-probability python library [32], [33].\n\nDetails the ANN model training process, including the use of Negloglik as a loss function, Maximum Likelihood Estimation, Adam Optimizer, ReLU activation, and TensorGLYPH<29>ow libraries.",
    "original_text": "The weight matrices and bias vectors of ANN model are updated for the given/speciGLYPH<28>ed number of iterations. These matrices and vectors determine the output of ANN model. The probabilistic layer attached to output neurons returns the PDF of variables while training process. Thus, conventional mean-squared error cannot be used as loss function. Instead, ''Negative log likelihood'' (Negloglik) was used as a loss function [see Eq. (2)]. The training process is executed to minimize this loss function. That is, training ANN becomes the process of Maximum Likelihood Estimation (MLE) [30].\n<!-- formula-not-decoded -->\nIn Eq. (2), P(x) and Q(x) denotes the PDF of observation and hypothesis, respectively.\nUsing Adam Optimizer [31], the training process was executed for 14,880 epochs (84 sec) for mean and standard deviation of ANN model and 7,800 epochs (101 sec) for the mixture of MVNs ANN model. Both models are trained with the learning rate of 10 GLYPH<0> 4 , and then optimized to prevent the occurrence of over-GLYPH<28>tting by using validation datasets. ReLU was used as the activation function for both models. Note that the ANN model was built using the TensorGLYPH<29>ow 2.0 and TensorGLYPH<29>ow-probability python library [32], [33].",
    "context": "Details the ANN model training process, including the use of Negloglik as a loss function, Maximum Likelihood Estimation, Adam Optimizer, ReLU activation, and TensorGLYPH<29>ow libraries.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      4
    ],
    "id": "cee1f43f262f5b67a8a64cb41b5a475b435d0a7ce1cd04b532a8618ab6a6131a"
  },
  {
    "text": "Based on the PDF determined by the mixture of MVNs, the standardized prediction data was randomly extracted. Then, these standardized values are recovered with the predicted mean and standard deviation to the original scale (see Fig. 5).\nFig. 6 shows the comparison between the previous work, non-separated ANN, and this work, for a given LER of 1 D 0.505 nm, 3 x D 48.62 nm, 3 y D 67.99 nm. As shown in Fig. 5(a, b), previous work with plain MVN cannot predict metrics with non-linear correlation, skewness and kurtosis. On the other hand, ANN with mixture of MVN [see Fig. 6(c-f)] successfully predicts skewness, kurtosis, and non-linear correlation, which is distinctly different from plain MVN. EMD score also proves that prediction accuracy is highly improved (0.0170 vs 0.00928). As shown in Fig. 6(c-f), there is no signiGLYPH<28>cant performance degradation\nFIGURE 6. Histograms of (a, b) previous work, (c, d) non-separated model, and (e, f) this work. EMD score of (b), (d), and (f) is 0.0170, 0.00785, and 0.00928, respectively.\nTABLE 4. Comparison of EMD score between this work and non-separated ANN model.\nFIGURE 7. Pair plot of performance metrics for 1 D 0.505 nm, 3 x D 48.62 nm, 3 y D 67.99 nm.\nbetween non-separated ANN model and this work, in spite of huge time saving (1412 sec to 185 sec) [see Table 3].\nThe pair plots for 1 D 0.505 nm, 3 x D 48.62 nm, 3 y D 67.99 nm, and 1 D 0.690 nm, 3 x D 85.09 nm,\n3 y D 80.98 nm are shown in [See Fig. 7 and Fig. 8]. We can notify that distribution for each parameter (a diagonal line) and correlation between parameters (the rest except for a diagonal line) are well predicted.\nVtlin\nFIGURE 8. Pair plot of performance metrics 1 D 0.690 nm, 3 x D 85.09 nm, 3 y D 80.98 nm.\nTable 4 shows the EMD score comparison of performance metric. In the respect of quantitative analysis, it is shown that this work shows quite similar performance with non-separated ANN, when the amplitude of LER proGLYPH<28>le is around 0.5 or 0.6. However, it shows enhanced performance in a wider range than the non-separated ANN model. This work not only predicts the mean and standard deviation at a high level with the additional simple ANN, but also\nshows higher consistency for speciGLYPH<28>c points such as the tail of distribution by using the mixture of MVNs.\n\nDemonstrates improved prediction accuracy and consistency, particularly in tail distribution prediction, compared to a non-separated ANN model.",
    "original_text": "Based on the PDF determined by the mixture of MVNs, the standardized prediction data was randomly extracted. Then, these standardized values are recovered with the predicted mean and standard deviation to the original scale (see Fig. 5).\nFig. 6 shows the comparison between the previous work, non-separated ANN, and this work, for a given LER of 1 D 0.505 nm, 3 x D 48.62 nm, 3 y D 67.99 nm. As shown in Fig. 5(a, b), previous work with plain MVN cannot predict metrics with non-linear correlation, skewness and kurtosis. On the other hand, ANN with mixture of MVN [see Fig. 6(c-f)] successfully predicts skewness, kurtosis, and non-linear correlation, which is distinctly different from plain MVN. EMD score also proves that prediction accuracy is highly improved (0.0170 vs 0.00928). As shown in Fig. 6(c-f), there is no signiGLYPH<28>cant performance degradation\nFIGURE 6. Histograms of (a, b) previous work, (c, d) non-separated model, and (e, f) this work. EMD score of (b), (d), and (f) is 0.0170, 0.00785, and 0.00928, respectively.\nTABLE 4. Comparison of EMD score between this work and non-separated ANN model.\nFIGURE 7. Pair plot of performance metrics for 1 D 0.505 nm, 3 x D 48.62 nm, 3 y D 67.99 nm.\nbetween non-separated ANN model and this work, in spite of huge time saving (1412 sec to 185 sec) [see Table 3].\nThe pair plots for 1 D 0.505 nm, 3 x D 48.62 nm, 3 y D 67.99 nm, and 1 D 0.690 nm, 3 x D 85.09 nm,\n3 y D 80.98 nm are shown in [See Fig. 7 and Fig. 8]. We can notify that distribution for each parameter (a diagonal line) and correlation between parameters (the rest except for a diagonal line) are well predicted.\nVtlin\nFIGURE 8. Pair plot of performance metrics 1 D 0.690 nm, 3 x D 85.09 nm, 3 y D 80.98 nm.\nTable 4 shows the EMD score comparison of performance metric. In the respect of quantitative analysis, it is shown that this work shows quite similar performance with non-separated ANN, when the amplitude of LER proGLYPH<28>le is around 0.5 or 0.6. However, it shows enhanced performance in a wider range than the non-separated ANN model. This work not only predicts the mean and standard deviation at a high level with the additional simple ANN, but also\nshows higher consistency for speciGLYPH<28>c points such as the tail of distribution by using the mixture of MVNs.",
    "context": "Demonstrates improved prediction accuracy and consistency, particularly in tail distribution prediction, compared to a non-separated ANN model.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      4,
      5,
      6,
      7,
      8
    ],
    "id": "d5f0ec052fdd40c9f5c8a0e7e5ed2fa80518ec3d41592c76c7d3c079e744d28f"
  },
  {
    "text": "We have proposed newly developed ANN models with enhanced accuracy. A ML-based model [14] was GLYPH<28>rst suggested to estimate LER-induced random variation, and its simulation time was shorter than using compact models. Herein, compared against the previous ML-based model [14], the newly proposed ANN models have shortened the simulation time by GLYPH<24> 6 times (from 1,191 seconds to 185 seconds). Especially, non-Gaussian features of device performance metrics' distribution (i.e., skewness, kurtosis, and non-linear correlation) are successfully predicted while the previous ML model only did with a shape of Gaussian distribution and linear correlation. Thus, the accuracy of the ANN model is signiGLYPH<28>cantly improved in the respect of both quantitative and qualitative comparisons. Especially, we extend the prediction target from 4 parameters such as Ioff, Idsat, Vtsat, and SS (Subthreshold Swing) to 7 parameters such as Ioff, Idsat, Idlin, Idlo , Idhi , Vtsat, and Vtlin. This enables simulating electrical behavior of transistor as well as DC behavior of digital circuit blocks such as SRAM bit cell [34]. This work can pave a new road to analyzing the impact of LER, and thereby, to timely design the process integration for integrated circuits.\n\nDemonstrates a significant improvement in simulation speed and predictive accuracy for transistor behavior and circuit block analysis.",
    "original_text": "We have proposed newly developed ANN models with enhanced accuracy. A ML-based model [14] was GLYPH<28>rst suggested to estimate LER-induced random variation, and its simulation time was shorter than using compact models. Herein, compared against the previous ML-based model [14], the newly proposed ANN models have shortened the simulation time by GLYPH<24> 6 times (from 1,191 seconds to 185 seconds). Especially, non-Gaussian features of device performance metrics' distribution (i.e., skewness, kurtosis, and non-linear correlation) are successfully predicted while the previous ML model only did with a shape of Gaussian distribution and linear correlation. Thus, the accuracy of the ANN model is signiGLYPH<28>cantly improved in the respect of both quantitative and qualitative comparisons. Especially, we extend the prediction target from 4 parameters such as Ioff, Idsat, Vtsat, and SS (Subthreshold Swing) to 7 parameters such as Ioff, Idsat, Idlin, Idlo , Idhi , Vtsat, and Vtlin. This enables simulating electrical behavior of transistor as well as DC behavior of digital circuit blocks such as SRAM bit cell [34]. This work can pave a new road to analyzing the impact of LER, and thereby, to timely design the process integration for integrated circuits.",
    "context": "Demonstrates a significant improvement in simulation speed and predictive accuracy for transistor behavior and circuit block analysis.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      8
    ],
    "id": "e5641ee5926b64216a040b28fefa9fc5eab8f7053062fe751b20c411f0aee4cf"
  },
  {
    "text": "The EDA Tool was supported by the IC Design Education Center (IDEC), Republic of Korea.\n\nDetails the funding source and organization behind the EDA Tool.",
    "original_text": "The EDA Tool was supported by the IC Design Education Center (IDEC), Republic of Korea.",
    "context": "Details the funding source and organization behind the EDA Tool.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      8
    ],
    "id": "8e527ea97e0d87289b7b8525dfb4f6fc1bbb0a48744f093c80e090402abf1f52"
  },
  {
    "text": "- [1] K. Mistry, M. Armstrong, C. Auth, S. Cea, T. Coan, T. Ghani, T. Hoffmann, A. Murthy, J. Sandford, R. Shaheed, K. Zawadzki, K. Zhang, S. Thompson, and M. Bohr, ''Delaying forever: Uniaxial strained silicon transistors in a 90 nm CMOS technology,'' in Proc. Dig. Tech. Papers. Symp. VLSI Technol. , 2004, pp. 50GLYPH<21>51.\n- [2] C. Auth et al. , ''45 nm high-k C metal gate strain-enhanced transistors,'' in Proc. Symp. VLSI Technol. , Jun. 2008, pp. 128GLYPH<21>129.\n- [3] C. Auth et al. , ''A 22 nm high performance and low-power CMOS technology featuring fully-depleted tri-gate transistors, self-aligned contacts and high density MIM capacitors,'' in Proc. Symp. VLSI Technol. (VLSIT) , Jun. 2012, pp. 131GLYPH<21>132.\n- [4] K. Agarwal and S. Nassif, ''The impact of random device variation on SRAMcell stability in sub-90-nm CMOS technologies,'' IEEE Trans. Very Large Scale Integr. (VLSI) Syst. , vol. 16, no. 1, pp. 86GLYPH<21>97, Jan. 2008.\n- [5] S. Markov, A. S. M. Zain, B. Cheng, and A. Asenov, ''Statistical variability in scaled generations of n-channel UTB-FD-SOI MOSFETs under the inGLYPH<29>uence of RDF, LER, OTF and MGG,'' in Proc. IEEE Int. SOI Conf. (SOI) , Oct. 2012, pp. 1GLYPH<21>2.\n- [6] G. Leung and C. O. Chui, ''Interactions between line edge roughness and random dopant GLYPH<29>uctuation in nonplanar GLYPH<28>eld-effect transistor variability,'' IEEE Trans. Electron Devices , vol. 60, no. 10, pp. 3277GLYPH<21>3284, Oct. 2013.\n- [7] W.-T. Huang and Y. Li, ''The impact of GLYPH<28>n/sidewall/gate line edge roughness on trapezoidal bulk FinFET devices,'' in Proc. Int. Conf. Simul. Semiconductor Processes Devices (SISPAD) , Sep. 2014, pp. 281GLYPH<21>284.\n- [8] M. Badaroglu. (2018). International Roadmap for Device and Systems (IRDS) . [Online]. Available: http://irds.ieee.org\n- [9] G. Bae et al. , ''3 nm GAA technology featuring multi-bridge-channel FET for low power and high performance applications,'' in IEDM Tech. Dig. , Dec. 2018, pp. 28.7.1GLYPH<21>28.7.4.\n- [10] X. Jiang, X. Wang, R. Wang, B. Cheng, A. Asenov, and R. Huang, ''Predictive compact modeling of random variations in FinFET technology for 16/14 nm node and beyond,'' in IEDM Tech. Dig. , Dec. 2015, pp. 28.3.1GLYPH<21>28.3.4.\n- [11] M. H. A. Banna, K. A. Taher, M. S. Kaiser, M. Mahmud, M. S. Rahman, A. S. M. S. Hosen, and G. H. Cho, ''Application of artiGLYPH<28>cial intelligence in predicting earthquakes: State-of-the-art and future challenges,'' IEEE Access , vol. 8, pp. 192880GLYPH<21>192923, 2020.\n- [12] M. B. T. Noor, N. Z. Zenia, M. S. Kaiser, S. A. Mamun, and M. Mahmud, ''Application of deep learning in detecting neurological disorders from magnetic resonance images: A survey on the detection of Alzheimer's disease, Parkinson's disease and schizophrenia,'' Brain Informat. , vol. 7, no. 1, pp. 1GLYPH<21>21, Dec. 2020.\n- [13] A. Whata and C. Chimedza, ''Deep learning for SARS COV-2 genome sequences,'' IEEE Access , vol. 9, pp. 59597GLYPH<21>59611, 2021.\n- [14] H.-C. Choi, H. Yun, J.-S. Yoon, and R.-H. Baek, ''Neural approach for modeling and optimizing Si-MOSFET manufacturing,'' IEEE Access , vol. 8, pp. 159351GLYPH<21>159370, 2020.\n- [15] D. Jiang, W. Lin, and N. Raghavan, ''A Gaussian mixture model clustering ensemble regressor for semiconductor manufacturing GLYPH<28>nal test yield prediction,'' IEEE Access , vol. 9, pp. 22253GLYPH<21>22263, 2021.\n- [16] J.-S. Yoon, S. Lee, H. Yun, and R.-H. Baek, ''Digital/analog performance optimization of vertical nanowire FETs using machine learning,'' IEEE Access , vol. 9, pp. 29071GLYPH<21>29077, 2021.\n- [17] K. Ko, J. K. Lee, and H. Shin, ''Variability-aware machine learning strategy for 3-D NAND GLYPH<29>ash memories,'' IEEE Trans. Electron Devices , vol. 67, no. 4, pp. 1575GLYPH<21>1580, Apr. 2020.\n- [18] J. Lim and C. Shin, ''Machine learning (ML)-based model to characterize the line edge roughness (LER)-induced random variation in FinFET,'' IEEE Access , vol. 8, pp. 158237GLYPH<21>158242, 2020.\n- [19] S. Oh and C. Shin, ''3-D quasi-atomistic model for line edge roughness in nonplanar MOSFETs,'' IEEE Trans. Electron Devices , vol. 63, no. 12, pp. 4617GLYPH<21>4623, Dec. 2016.\n- [20] TCAD Sentaurus? User Guide Version P-2019.03 , Synopsys, Inc., Mountain View, CA, USA, 2019.\n- [21] C. Shin, Variation-Aware Advanced CMOS Devices and SRAM , vol. 56. Dordrecht, The Netherlands: Springer, 2016.\n- [22] E. Dornel, T. Ernst, J. C. BarbØ, J. M. Hartmann, V. Delaye, F. Aussenac, C. Vizioz, S. Borel, V . MafGLYPH<28>ni-Alvaro, C. Isheden, and J. Foucher, ''Hydrogen annealing of arrays of planar and vertically stacked Si nanowires,'' Appl. Phys. Lett. , vol. 91, no. 23, Dec. 2007, Art. no. 233502.\n- [23] T. Tezuka, N. Hirashita, Y. Moriyama, N. Sugiyama, K. Usuda, E. Toyoda, K. Murayama, and S.-I. Takagi, ''110-facets formation by hydrogen thermal etching on sidewalls of Si and strained-Si GLYPH<28>n structures,'' Appl. Phys. Lett. , vol. 92, no. 19, May 2008, Art. no. 191903.\n- [24] Y. Ma, H. J. Levinson, and T. Wallow, ''Line edge roughness impact on critical dimension variation,'' Proc. SPIE , vol. 6518, Apr. 2007, Art. no. 651824.\n- [25] Y. Rubner, C. Tomasi, and L. J. Guibas, ''A metric for distributions with applications to image databases,'' in Proc. 6th Int. Conf. Comput. Vis. , Jan. 1998, pp. 59GLYPH<21>66.\n\nThe collection of papers focuses on semiconductor device variability and manufacturing challenges, primarily related to scaling down transistor dimensions. Key themes include the impact of line edge roughness (LER), random device variations, and the application of machine learning techniques for predicting and mitigating these issues in technologies ranging from 90nm to 3nm.",
    "original_text": "- [1] K. Mistry, M. Armstrong, C. Auth, S. Cea, T. Coan, T. Ghani, T. Hoffmann, A. Murthy, J. Sandford, R. Shaheed, K. Zawadzki, K. Zhang, S. Thompson, and M. Bohr, ''Delaying forever: Uniaxial strained silicon transistors in a 90 nm CMOS technology,'' in Proc. Dig. Tech. Papers. Symp. VLSI Technol. , 2004, pp. 50GLYPH<21>51.\n- [2] C. Auth et al. , ''45 nm high-k C metal gate strain-enhanced transistors,'' in Proc. Symp. VLSI Technol. , Jun. 2008, pp. 128GLYPH<21>129.\n- [3] C. Auth et al. , ''A 22 nm high performance and low-power CMOS technology featuring fully-depleted tri-gate transistors, self-aligned contacts and high density MIM capacitors,'' in Proc. Symp. VLSI Technol. (VLSIT) , Jun. 2012, pp. 131GLYPH<21>132.\n- [4] K. Agarwal and S. Nassif, ''The impact of random device variation on SRAMcell stability in sub-90-nm CMOS technologies,'' IEEE Trans. Very Large Scale Integr. (VLSI) Syst. , vol. 16, no. 1, pp. 86GLYPH<21>97, Jan. 2008.\n- [5] S. Markov, A. S. M. Zain, B. Cheng, and A. Asenov, ''Statistical variability in scaled generations of n-channel UTB-FD-SOI MOSFETs under the inGLYPH<29>uence of RDF, LER, OTF and MGG,'' in Proc. IEEE Int. SOI Conf. (SOI) , Oct. 2012, pp. 1GLYPH<21>2.\n- [6] G. Leung and C. O. Chui, ''Interactions between line edge roughness and random dopant GLYPH<29>uctuation in nonplanar GLYPH<28>eld-effect transistor variability,'' IEEE Trans. Electron Devices , vol. 60, no. 10, pp. 3277GLYPH<21>3284, Oct. 2013.\n- [7] W.-T. Huang and Y. Li, ''The impact of GLYPH<28>n/sidewall/gate line edge roughness on trapezoidal bulk FinFET devices,'' in Proc. Int. Conf. Simul. Semiconductor Processes Devices (SISPAD) , Sep. 2014, pp. 281GLYPH<21>284.\n- [8] M. Badaroglu. (2018). International Roadmap for Device and Systems (IRDS) . [Online]. Available: http://irds.ieee.org\n- [9] G. Bae et al. , ''3 nm GAA technology featuring multi-bridge-channel FET for low power and high performance applications,'' in IEDM Tech. Dig. , Dec. 2018, pp. 28.7.1GLYPH<21>28.7.4.\n- [10] X. Jiang, X. Wang, R. Wang, B. Cheng, A. Asenov, and R. Huang, ''Predictive compact modeling of random variations in FinFET technology for 16/14 nm node and beyond,'' in IEDM Tech. Dig. , Dec. 2015, pp. 28.3.1GLYPH<21>28.3.4.\n- [11] M. H. A. Banna, K. A. Taher, M. S. Kaiser, M. Mahmud, M. S. Rahman, A. S. M. S. Hosen, and G. H. Cho, ''Application of artiGLYPH<28>cial intelligence in predicting earthquakes: State-of-the-art and future challenges,'' IEEE Access , vol. 8, pp. 192880GLYPH<21>192923, 2020.\n- [12] M. B. T. Noor, N. Z. Zenia, M. S. Kaiser, S. A. Mamun, and M. Mahmud, ''Application of deep learning in detecting neurological disorders from magnetic resonance images: A survey on the detection of Alzheimer's disease, Parkinson's disease and schizophrenia,'' Brain Informat. , vol. 7, no. 1, pp. 1GLYPH<21>21, Dec. 2020.\n- [13] A. Whata and C. Chimedza, ''Deep learning for SARS COV-2 genome sequences,'' IEEE Access , vol. 9, pp. 59597GLYPH<21>59611, 2021.\n- [14] H.-C. Choi, H. Yun, J.-S. Yoon, and R.-H. Baek, ''Neural approach for modeling and optimizing Si-MOSFET manufacturing,'' IEEE Access , vol. 8, pp. 159351GLYPH<21>159370, 2020.\n- [15] D. Jiang, W. Lin, and N. Raghavan, ''A Gaussian mixture model clustering ensemble regressor for semiconductor manufacturing GLYPH<28>nal test yield prediction,'' IEEE Access , vol. 9, pp. 22253GLYPH<21>22263, 2021.\n- [16] J.-S. Yoon, S. Lee, H. Yun, and R.-H. Baek, ''Digital/analog performance optimization of vertical nanowire FETs using machine learning,'' IEEE Access , vol. 9, pp. 29071GLYPH<21>29077, 2021.\n- [17] K. Ko, J. K. Lee, and H. Shin, ''Variability-aware machine learning strategy for 3-D NAND GLYPH<29>ash memories,'' IEEE Trans. Electron Devices , vol. 67, no. 4, pp. 1575GLYPH<21>1580, Apr. 2020.\n- [18] J. Lim and C. Shin, ''Machine learning (ML)-based model to characterize the line edge roughness (LER)-induced random variation in FinFET,'' IEEE Access , vol. 8, pp. 158237GLYPH<21>158242, 2020.\n- [19] S. Oh and C. Shin, ''3-D quasi-atomistic model for line edge roughness in nonplanar MOSFETs,'' IEEE Trans. Electron Devices , vol. 63, no. 12, pp. 4617GLYPH<21>4623, Dec. 2016.\n- [20] TCAD Sentaurus? User Guide Version P-2019.03 , Synopsys, Inc., Mountain View, CA, USA, 2019.\n- [21] C. Shin, Variation-Aware Advanced CMOS Devices and SRAM , vol. 56. Dordrecht, The Netherlands: Springer, 2016.\n- [22] E. Dornel, T. Ernst, J. C. BarbØ, J. M. Hartmann, V. Delaye, F. Aussenac, C. Vizioz, S. Borel, V . MafGLYPH<28>ni-Alvaro, C. Isheden, and J. Foucher, ''Hydrogen annealing of arrays of planar and vertically stacked Si nanowires,'' Appl. Phys. Lett. , vol. 91, no. 23, Dec. 2007, Art. no. 233502.\n- [23] T. Tezuka, N. Hirashita, Y. Moriyama, N. Sugiyama, K. Usuda, E. Toyoda, K. Murayama, and S.-I. Takagi, ''110-facets formation by hydrogen thermal etching on sidewalls of Si and strained-Si GLYPH<28>n structures,'' Appl. Phys. Lett. , vol. 92, no. 19, May 2008, Art. no. 191903.\n- [24] Y. Ma, H. J. Levinson, and T. Wallow, ''Line edge roughness impact on critical dimension variation,'' Proc. SPIE , vol. 6518, Apr. 2007, Art. no. 651824.\n- [25] Y. Rubner, C. Tomasi, and L. J. Guibas, ''A metric for distributions with applications to image databases,'' in Proc. 6th Int. Conf. Comput. Vis. , Jan. 1998, pp. 59GLYPH<21>66.",
    "context": "The collection of papers focuses on semiconductor device variability and manufacturing challenges, primarily related to scaling down transistor dimensions. Key themes include the impact of line edge roughness (LER), random device variations, and the application of machine learning techniques for predicting and mitigating these issues in technologies ranging from 90nm to 3nm.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      8
    ],
    "id": "bc21eaee24a6e02ab8f0a8bcc2cdf5629cd92485aa12a4b2fe235cf07f488da0"
  },
  {
    "text": "- [26] A. Asenov, S. Kaya, and A. R. Brown, ''Intrinsic parameter GLYPH<29>uctuations in decananometer MOSFETs introduced by gate line edge roughness,'' IEEE Trans. Electron Devices , vol. 50, no. 5, pp. 1254GLYPH<21>1260, May 2003.\n- [27] J. Min and C. Shin, ''Study of line edge roughness on various types of gateall-around GLYPH<28>eld effect transistor,'' Semicond. Sci. Technol. , vol. 35, no. 1, Jan. 2020, Art. no. 015004.\n- [28] Y.-N. Chen, C.-J. Chen, M.-L. Fan, V. Hu, P. Su, and C.-T. Chuang, ''Impacts of work function variation and line-edge roughness on TFET and FinFET devices and 32-bit CLA circuits,'' J. Low Power Electron. Appl. , vol. 5, no. 2, pp. 101GLYPH<21>115, May 2015.\n- [29] S. Kaya, A. Brown, A. Asenov, D. Magot, and T. LintonI, ''Analysis of statistical GLYPH<29>uctuations due to line edge roughness in sub-0.1 GLYPH<22> mMOSFETs,'' in Simulation of Semiconductor Processes and Devices . Vienna, Austria: Springer, 2001, pp. 78GLYPH<21>81.\n- [30] J.-X. Pan and K.-T. Fang, ''Maximum likelihood estimation,'' in Growth Curve Models and Statistical Diagnostics . New York, NY, USA: Springer, 2002, pp. 77GLYPH<21>158.\n- [31] D. P. Kingma and J. Ba, ''Adam: A method for stochastic optimization,'' 2014, arXiv:1412.6980 . [Online]. Available: http://arxiv.org/abs/1412.6980\n- [32] M. Abadi et al. , ''TensorFlow: Large-scale machine learning on heterogeneous distributed systems,'' 2016, arXiv:1603.04467 . [Online]. Available: https://arxiv.org/abs/1603.04467\n- [33] M. Abadi et al. , ''TensorFlow: A system for large-scale machine learning,'' in Proc. 12th USENIX Symp. Operating Syst. Design Implement. (OSDI) , 2016, pp. 265GLYPH<21>283.\n- [34] A. E. Carlson, ''Device and circuit techniques for reducing variation in nanoscale SRAM,'' Univ, California, Berkeley, CA, USA, Tech. Rep., 2008, pp. 23GLYPH<21>51.\nJAEHYUK LIM received the B.S. degree in electronic and electrical engineering from Sungkyunkwan University (SKKU), Suwon, Republic of Korea, in 2020, where he is currently pursuing the M.S. degree in electrical and computer engineering. His current research interests include machine learning and process induced random variations.\nJINWOONG LEE received the B.S. degree from the Department of Physics, KyungHee University (KHU), Seoul, Republic of Korea, in 2017. He is currently pursuing the M.S. degree in electrical and computer engineering with Sungkyunkwan University (SKKU). His current research interests include machine learning and process induced random variations.\nCHANGHWAN SHIN (Senior Member, IEEE) received the B.S. degree (Hons.) in electrical engineering from Korea University, Seoul, Republic of Korea, in 2006, and the Ph.D. degree in electrical engineering and computer sciences from the University of California at Berkeley, in 2011. Since 2017, he has been with the Board of Directors in SK Hynix. He is currently with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include advanced CMOS device designs and their applications to memory/logic devices.\n\nProvides research on process-induced random variations and related device design techniques.",
    "original_text": "- [26] A. Asenov, S. Kaya, and A. R. Brown, ''Intrinsic parameter GLYPH<29>uctuations in decananometer MOSFETs introduced by gate line edge roughness,'' IEEE Trans. Electron Devices , vol. 50, no. 5, pp. 1254GLYPH<21>1260, May 2003.\n- [27] J. Min and C. Shin, ''Study of line edge roughness on various types of gateall-around GLYPH<28>eld effect transistor,'' Semicond. Sci. Technol. , vol. 35, no. 1, Jan. 2020, Art. no. 015004.\n- [28] Y.-N. Chen, C.-J. Chen, M.-L. Fan, V. Hu, P. Su, and C.-T. Chuang, ''Impacts of work function variation and line-edge roughness on TFET and FinFET devices and 32-bit CLA circuits,'' J. Low Power Electron. Appl. , vol. 5, no. 2, pp. 101GLYPH<21>115, May 2015.\n- [29] S. Kaya, A. Brown, A. Asenov, D. Magot, and T. LintonI, ''Analysis of statistical GLYPH<29>uctuations due to line edge roughness in sub-0.1 GLYPH<22> mMOSFETs,'' in Simulation of Semiconductor Processes and Devices . Vienna, Austria: Springer, 2001, pp. 78GLYPH<21>81.\n- [30] J.-X. Pan and K.-T. Fang, ''Maximum likelihood estimation,'' in Growth Curve Models and Statistical Diagnostics . New York, NY, USA: Springer, 2002, pp. 77GLYPH<21>158.\n- [31] D. P. Kingma and J. Ba, ''Adam: A method for stochastic optimization,'' 2014, arXiv:1412.6980 . [Online]. Available: http://arxiv.org/abs/1412.6980\n- [32] M. Abadi et al. , ''TensorFlow: Large-scale machine learning on heterogeneous distributed systems,'' 2016, arXiv:1603.04467 . [Online]. Available: https://arxiv.org/abs/1603.04467\n- [33] M. Abadi et al. , ''TensorFlow: A system for large-scale machine learning,'' in Proc. 12th USENIX Symp. Operating Syst. Design Implement. (OSDI) , 2016, pp. 265GLYPH<21>283.\n- [34] A. E. Carlson, ''Device and circuit techniques for reducing variation in nanoscale SRAM,'' Univ, California, Berkeley, CA, USA, Tech. Rep., 2008, pp. 23GLYPH<21>51.\nJAEHYUK LIM received the B.S. degree in electronic and electrical engineering from Sungkyunkwan University (SKKU), Suwon, Republic of Korea, in 2020, where he is currently pursuing the M.S. degree in electrical and computer engineering. His current research interests include machine learning and process induced random variations.\nJINWOONG LEE received the B.S. degree from the Department of Physics, KyungHee University (KHU), Seoul, Republic of Korea, in 2017. He is currently pursuing the M.S. degree in electrical and computer engineering with Sungkyunkwan University (SKKU). His current research interests include machine learning and process induced random variations.\nCHANGHWAN SHIN (Senior Member, IEEE) received the B.S. degree (Hons.) in electrical engineering from Korea University, Seoul, Republic of Korea, in 2006, and the Ph.D. degree in electrical engineering and computer sciences from the University of California at Berkeley, in 2011. Since 2017, he has been with the Board of Directors in SK Hynix. He is currently with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include advanced CMOS device designs and their applications to memory/logic devices.",
    "context": "Provides research on process-induced random variations and related device design techniques.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      8,
      9
    ],
    "id": "7ea35574b1bed28c4123d815c2ace9ef3f0b6600f70908f3af114855f4d6894f"
  },
  {
    "text": "Received June 20, 2020, accepted July 6, 2020, date of publication July 9, 2020, date of current version July 22, 2020.\nDigital Object Identifier 10.1 109/ACCESS.2020.3008195\n\nProvides publication and version information.",
    "original_text": "Received June 20, 2020, accepted July 6, 2020, date of publication July 9, 2020, date of current version July 22, 2020.\nDigital Object Identifier 10.1 109/ACCESS.2020.3008195",
    "context": "Provides publication and version information.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      1
    ],
    "id": "8a95b9951edb4fadfa691dd448ba0164867ec693e842d3e52c37ef242dcd62d8"
  },
  {
    "text": "Department of Electronic Engineering, Inha University, Incheon 22212, South Korea\nCorresponding author: Byung Cheol Song (bcsong@inha.ac.kr)\nThis work was supported in part by the Industrial Technology Innovation Program funded by the Ministry of Trade, Industry & Energy (MI, South Korea) (Development on Deep Learning based 4K30P Edge Computing enabled IP Camera System) under Grant 20006483, in part by the Institute of Information & Communications Technology Planning & Evaluation (IITP) funded by the Korea Government (MSIT) (ArtiGLYPH<28>cial Intelligence Convergence Research Center, Inha University) under Grant 2020-0-01389, and in part by the Industrial Technology Innovation Program through the Ministry of Trade, Industry, and Energy (MI, South Korea) (Development of Human-Friendly Human-Robot Interaction Technologies Using Human Internal Emotional States) under Grant 10073154.\n- ABSTRACT Singular value decomposition (SVD) is a popular technique to extract essential information by reducing the dimension of a feature set. SVD is able to analyze a vast matrix in spite of a relatively low computational cost. However, singular vectors produced by SVD have been seldom used in convolutional neural networks (CNNs). This is because the inherent properties of singular vectors such as sign ambiguity and manifold features make CNNs difGLYPH<28>cult to learn singular vectors. In order to overcome the limitations, this paper analyzes the undesirable properties of singular vectors and presents the transformation of singular vectors into Euclidean space as a smart solution. If the singular vectors are transformed to follow Euclidean geometry, SVD can be used for pooling to maintain the feature information well, which is called singular vector pooling (SVP). Since SVP can extract essential information from a feature map, it is robust against adversarial attacks in comparison to global average pooling. Thus, SVP shows a quantitative performance improvement of about 36% for the CIFAR10 dataset. In addition, we applied SVP to a knowledge distillation scheme that uses singular vectors in a restricted manner. As a result, SVP improved the performance by up to 1.7% for the CIFAR100 dataset.\nINDEX TERMS Neural networks, pattern analysis, principal component analysis.\n\nDetails funding sources and introduces singular vector pooling (SVP) as a method to improve neural network performance, citing quantitative improvements on CIFAR10 and CIFAR100 datasets.",
    "original_text": "Department of Electronic Engineering, Inha University, Incheon 22212, South Korea\nCorresponding author: Byung Cheol Song (bcsong@inha.ac.kr)\nThis work was supported in part by the Industrial Technology Innovation Program funded by the Ministry of Trade, Industry & Energy (MI, South Korea) (Development on Deep Learning based 4K30P Edge Computing enabled IP Camera System) under Grant 20006483, in part by the Institute of Information & Communications Technology Planning & Evaluation (IITP) funded by the Korea Government (MSIT) (ArtiGLYPH<28>cial Intelligence Convergence Research Center, Inha University) under Grant 2020-0-01389, and in part by the Industrial Technology Innovation Program through the Ministry of Trade, Industry, and Energy (MI, South Korea) (Development of Human-Friendly Human-Robot Interaction Technologies Using Human Internal Emotional States) under Grant 10073154.\n- ABSTRACT Singular value decomposition (SVD) is a popular technique to extract essential information by reducing the dimension of a feature set. SVD is able to analyze a vast matrix in spite of a relatively low computational cost. However, singular vectors produced by SVD have been seldom used in convolutional neural networks (CNNs). This is because the inherent properties of singular vectors such as sign ambiguity and manifold features make CNNs difGLYPH<28>cult to learn singular vectors. In order to overcome the limitations, this paper analyzes the undesirable properties of singular vectors and presents the transformation of singular vectors into Euclidean space as a smart solution. If the singular vectors are transformed to follow Euclidean geometry, SVD can be used for pooling to maintain the feature information well, which is called singular vector pooling (SVP). Since SVP can extract essential information from a feature map, it is robust against adversarial attacks in comparison to global average pooling. Thus, SVP shows a quantitative performance improvement of about 36% for the CIFAR10 dataset. In addition, we applied SVP to a knowledge distillation scheme that uses singular vectors in a restricted manner. As a result, SVP improved the performance by up to 1.7% for the CIFAR100 dataset.\nINDEX TERMS Neural networks, pattern analysis, principal component analysis.",
    "context": "Details funding sources and introduces singular vector pooling (SVP) as a method to improve neural network performance, citing quantitative improvements on CIFAR10 and CIFAR100 datasets.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      1
    ],
    "id": "e48cea70177b6e462537498a97a4f63745db576d881284fe22cb5a1f7aeb5815"
  },
  {
    "text": "Principal component analysis (PCA) is a technique used for the identiGLYPH<28>cation of a smaller number of uncorrelated variables known as principal components from a larger set of data. Several principal components have most information of a given data, so the dimension of the data can be signiGLYPH<28>cantly reduced or compressed by choosing only some of the principal components. Therefore, PCA is a beneGLYPH<28>cial technique for analyzing high-dimensional data. Among PCA techniques, singular value decomposition (SVD) is most popular because it can produce global solutions to various problems in computer vision, pattern recognition, and machine learning [1]GLYPH<21>[3]. As the derivative function of SVD was developed [4], SVD has been applied even to convolutional neural networks (CNNs) that can deal with very high-dimensional data. For instance, SVD was employed to obtain a unique square root of a symmetric positive deGLYPH<28>nite (SPD) matrix for subspace analysis [5]GLYPH<21>[7] or higher-order pooling [4], [8]GLYPH<21>[10].\nThe associate editor coordinating the review of this manuscript and approving it for publication was Claudio Cusano .\nPrevious works show that SVD can effectively extract essential information from feature maps. However, since singular vectors have undesirable properties such as sign ambiguity [11] and manifold features, they have been seldom studied to be directly learned in spite that they have feature map's essential information. In order to avoid undesirable properties, conventional approaches adopted an SPD matrix consisting of singular vectors. However, since they should expand the compressed feature's dimension again, they require additional computational and memory cost.\nLee et al. proposed a knowledge distillation method using SVD (KD-SVD) the harmful properties of singular vectors and deGLYPH<28>ned singular vectors as knowledge [12]. But this algorithm has a limitation that it needs guidance from a teacher network.\nIn this paper, we deGLYPH<28>ne a singular vector as a feature vector and make its information directly learnable without re-composition and guidance. To do this, we analyze two problematic properties of singular vectors and proposes solutions to effectively remove the unfavorable properties based on the analysis. The sign ambiguity is eliminated so that singular vectors with similar information are gathered, and then singular vectors are converted from manifold to Euclidean space. Here, Euclidean geometry that is useful for ordinary learning schemes is employed. This procedure does not simply make singular vectors learnable but transform them to follow Euclidean geometry. Thus, SVD can be easily analyzed with layers based on Euclidean geometry.\nAlso, we propose a method called singular vector pooling (SVP), which is the only pooling method using SVD to our knowledge. SVP is basically robust to noise due to the nature of SVD. For example, in adversarial leaning with the CIFAR10 dataset, SVP provides about 36% better performance than global average pooling (GAP). As shown in the feature distributions of Fig. 1, SVP has smaller intra-class variance and larger inter-class variance than GAP, which is a great advantage of SVP. In addition, when the SVP is applied to KD-SVD, which has learned singular vectors relatively naively, the performance of a student network can be improved by about 1.7% for the CIFAR100 dataset. The contribution of this paper is summarized as follows:\n- GLYPH<15> This paper analyzes the properties of disturbing learning of singular vectors. Then, we propose skills to eliminate the unfavorable properties and present a method to transform singular vectors on non-Euclidean space to Euclidean space for effective learning.\n- GLYPH<15> We propose SVP as a novel pooling method using SVD, and prove that applying SVP to CNN's feature maps is very useful for obtaining essential information.\n- GLYPH<15> In addition, this paper shows that since singular vectors preserve feature map's information well, they are not only robust to adversarial attack, but also effective in knowledge distillation.\n\nAnalyzes singular vector properties, proposes methods to eliminate unfavorable characteristics, and introduces a novel pooling method (SVP) for improved learning and knowledge distillation.",
    "original_text": "Principal component analysis (PCA) is a technique used for the identiGLYPH<28>cation of a smaller number of uncorrelated variables known as principal components from a larger set of data. Several principal components have most information of a given data, so the dimension of the data can be signiGLYPH<28>cantly reduced or compressed by choosing only some of the principal components. Therefore, PCA is a beneGLYPH<28>cial technique for analyzing high-dimensional data. Among PCA techniques, singular value decomposition (SVD) is most popular because it can produce global solutions to various problems in computer vision, pattern recognition, and machine learning [1]GLYPH<21>[3]. As the derivative function of SVD was developed [4], SVD has been applied even to convolutional neural networks (CNNs) that can deal with very high-dimensional data. For instance, SVD was employed to obtain a unique square root of a symmetric positive deGLYPH<28>nite (SPD) matrix for subspace analysis [5]GLYPH<21>[7] or higher-order pooling [4], [8]GLYPH<21>[10].\nThe associate editor coordinating the review of this manuscript and approving it for publication was Claudio Cusano .\nPrevious works show that SVD can effectively extract essential information from feature maps. However, since singular vectors have undesirable properties such as sign ambiguity [11] and manifold features, they have been seldom studied to be directly learned in spite that they have feature map's essential information. In order to avoid undesirable properties, conventional approaches adopted an SPD matrix consisting of singular vectors. However, since they should expand the compressed feature's dimension again, they require additional computational and memory cost.\nLee et al. proposed a knowledge distillation method using SVD (KD-SVD) the harmful properties of singular vectors and deGLYPH<28>ned singular vectors as knowledge [12]. But this algorithm has a limitation that it needs guidance from a teacher network.\nIn this paper, we deGLYPH<28>ne a singular vector as a feature vector and make its information directly learnable without re-composition and guidance. To do this, we analyze two problematic properties of singular vectors and proposes solutions to effectively remove the unfavorable properties based on the analysis. The sign ambiguity is eliminated so that singular vectors with similar information are gathered, and then singular vectors are converted from manifold to Euclidean space. Here, Euclidean geometry that is useful for ordinary learning schemes is employed. This procedure does not simply make singular vectors learnable but transform them to follow Euclidean geometry. Thus, SVD can be easily analyzed with layers based on Euclidean geometry.\nAlso, we propose a method called singular vector pooling (SVP), which is the only pooling method using SVD to our knowledge. SVP is basically robust to noise due to the nature of SVD. For example, in adversarial leaning with the CIFAR10 dataset, SVP provides about 36% better performance than global average pooling (GAP). As shown in the feature distributions of Fig. 1, SVP has smaller intra-class variance and larger inter-class variance than GAP, which is a great advantage of SVP. In addition, when the SVP is applied to KD-SVD, which has learned singular vectors relatively naively, the performance of a student network can be improved by about 1.7% for the CIFAR100 dataset. The contribution of this paper is summarized as follows:\n- GLYPH<15> This paper analyzes the properties of disturbing learning of singular vectors. Then, we propose skills to eliminate the unfavorable properties and present a method to transform singular vectors on non-Euclidean space to Euclidean space for effective learning.\n- GLYPH<15> We propose SVP as a novel pooling method using SVD, and prove that applying SVP to CNN's feature maps is very useful for obtaining essential information.\n- GLYPH<15> In addition, this paper shows that since singular vectors preserve feature map's information well, they are not only robust to adversarial attack, but also effective in knowledge distillation.",
    "context": "Analyzes singular vector properties, proposes methods to eliminate unfavorable characteristics, and introduces a novel pooling method (SVP) for improved learning and knowledge distillation.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      1,
      2
    ],
    "id": "9af4ca68d2a7af19771c7857a1d2a536d09fdac354acc2005e4b4b3354785181"
  },
  {
    "text": "As one of the most popular PCA techniques, SVD is often used to reduce the dimension of a matrix or to extract essential information from a matrix or dataset. Even in the GLYPH<28>eld of machine learning, the use of SVD is expanding more and more [4], [5], [8]GLYPH<21>[10], [12]GLYPH<21>[15]. For example, SVD has been applied to a typical CNN as the derivative of SVD has been mathematically derived recently. The most common example is the computation of a unique square root of an SPD matrix. This method is used for various purposes, such as\nFIGURE 1. Comparison of feature vectors obtained by global average pooling and singular vector pooling.\ncovariance pooling [4], [8], [14], [15] and manifold embedding [5], [8]GLYPH<21>[10]. In addition, there are several cases where a good property that singular vector(s) obtained from SVD has the compressed information is used for global pooling.\nSince global pooling was GLYPH<28>rst introduced in [16], it has evolved in various ways, such as employing trainable variables [17] and adopting non-linear functions [18], [19]. On the other hand, a global pooling method using SVD was proposed [20], which was based on covariance pooling. Covariance pooling obtains the covariance matrix FF T of a feature matrix F and extracts meaningful information from covariance. Reference [4] showed that if matrix logarithm is obtained by SVD, tangent space mapping is applicable. Recently, various covariance pooling methods have been proposed, inspired by [4]. For instance, [14] presented a method to solve the instability of SVD gradients, and [8] reduced the computational cost of SVD through Newton-Schulz iteration. Also, [15] showed that covariance pooling can be effectively applied in the middle of the network.\nHowever, very few studies directly use singular vectors in deep learning. As far as we know, KD-SVD [12] may be the GLYPH<28>rst deep learning technique that uses essential information of singular vectors directly. In KD-SVD, singular vectors are extracted from two feature maps by SVD, a correlation between two singular vectors is computed, and the correlation is distilled as knowledge. To facilitate the learning of singular vectors, KD-SVD uses a teacher network as a guide. However, such an approach is not applicable to general architectures other than the student-teacher network where the teacher network plays a guide role.\nAccording to our survey, the use of SVD in deep learning is continuously being studied, but there is no way to learn singular vectors directly. On the other hand, it is also true that many methods using singular vectors for various computer vision tasks have been studied before the deep learning era [11], [21], [22]. We argue that the reason for this recent research trend is that although singular vectors have good information, there are some bad properties making difGLYPH<28>cult to learn them with common deep learning schemes. Therefore, by analyzing and removing these properties, we intend to propose a way that singular vectors can be utilized in deep learning scheme. Ultimately, the proposed algorithm will provide an opportunity to use SVD in various machine learning applications.\nFIGURE 2. An example of CNN architecture using SVP. At the bottom of the figure, we show the process of transforming the singular vectors obtained through SVD prior to learning and the process of changing the feature distribution through each process.\n\nDetails the application of SVD in deep learning, specifically focusing on KD-SVD and the challenges in directly learning singular vectors, aiming to overcome these challenges for broader deep learning applications.",
    "original_text": "As one of the most popular PCA techniques, SVD is often used to reduce the dimension of a matrix or to extract essential information from a matrix or dataset. Even in the GLYPH<28>eld of machine learning, the use of SVD is expanding more and more [4], [5], [8]GLYPH<21>[10], [12]GLYPH<21>[15]. For example, SVD has been applied to a typical CNN as the derivative of SVD has been mathematically derived recently. The most common example is the computation of a unique square root of an SPD matrix. This method is used for various purposes, such as\nFIGURE 1. Comparison of feature vectors obtained by global average pooling and singular vector pooling.\ncovariance pooling [4], [8], [14], [15] and manifold embedding [5], [8]GLYPH<21>[10]. In addition, there are several cases where a good property that singular vector(s) obtained from SVD has the compressed information is used for global pooling.\nSince global pooling was GLYPH<28>rst introduced in [16], it has evolved in various ways, such as employing trainable variables [17] and adopting non-linear functions [18], [19]. On the other hand, a global pooling method using SVD was proposed [20], which was based on covariance pooling. Covariance pooling obtains the covariance matrix FF T of a feature matrix F and extracts meaningful information from covariance. Reference [4] showed that if matrix logarithm is obtained by SVD, tangent space mapping is applicable. Recently, various covariance pooling methods have been proposed, inspired by [4]. For instance, [14] presented a method to solve the instability of SVD gradients, and [8] reduced the computational cost of SVD through Newton-Schulz iteration. Also, [15] showed that covariance pooling can be effectively applied in the middle of the network.\nHowever, very few studies directly use singular vectors in deep learning. As far as we know, KD-SVD [12] may be the GLYPH<28>rst deep learning technique that uses essential information of singular vectors directly. In KD-SVD, singular vectors are extracted from two feature maps by SVD, a correlation between two singular vectors is computed, and the correlation is distilled as knowledge. To facilitate the learning of singular vectors, KD-SVD uses a teacher network as a guide. However, such an approach is not applicable to general architectures other than the student-teacher network where the teacher network plays a guide role.\nAccording to our survey, the use of SVD in deep learning is continuously being studied, but there is no way to learn singular vectors directly. On the other hand, it is also true that many methods using singular vectors for various computer vision tasks have been studied before the deep learning era [11], [21], [22]. We argue that the reason for this recent research trend is that although singular vectors have good information, there are some bad properties making difGLYPH<28>cult to learn them with common deep learning schemes. Therefore, by analyzing and removing these properties, we intend to propose a way that singular vectors can be utilized in deep learning scheme. Ultimately, the proposed algorithm will provide an opportunity to use SVD in various machine learning applications.\nFIGURE 2. An example of CNN architecture using SVP. At the bottom of the figure, we show the process of transforming the singular vectors obtained through SVD prior to learning and the process of changing the feature distribution through each process.",
    "context": "Details the application of SVD in deep learning, specifically focusing on KD-SVD and the challenges in directly learning singular vectors, aiming to overcome these challenges for broader deep learning applications.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      2,
      3
    ],
    "id": "0c33a10f3144f7d81394a5d861d7c37943e24f9e62bf73be57f536e0e140bc41"
  },
  {
    "text": "We theoretically analyze the undesirable properties that generally make the learning of singular vectors difGLYPH<28>cult. Based on the theoretical analysis, we propose SVP as a solution to overcome the limitations of using singular vectors. Fig. 2 shows the overall structure of SVP, which is a sort of post-processing for learning singular vectors. Feature maps obtained from CNN are GLYPH<28>rst decomposed by SVD, and then the output singular vectors are transformed to facilitate learning by SVP. Finally, the transformed information is classiGLYPH<28>ed.\n\nDetails the SVP algorithm as a post-processing step for singular vector learning.",
    "original_text": "We theoretically analyze the undesirable properties that generally make the learning of singular vectors difGLYPH<28>cult. Based on the theoretical analysis, we propose SVP as a solution to overcome the limitations of using singular vectors. Fig. 2 shows the overall structure of SVP, which is a sort of post-processing for learning singular vectors. Feature maps obtained from CNN are GLYPH<28>rst decomposed by SVD, and then the output singular vectors are transformed to facilitate learning by SVP. Finally, the transformed information is classiGLYPH<28>ed.",
    "context": "Details the SVP algorithm as a post-processing step for singular vector learning.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      3
    ],
    "id": "8412b8c408d036af00a80def43cbc48e74cabc894234da49bdc924f902cee7dd"
  },
  {
    "text": "Assume that a feature map obtained by a CNN has a spatial shape of H GLYPH<2> W and a feature depth of D , and it can be transformed into a matrix form. Then, the feature map of matrix form can be interpreted as a set composed of HWD -dimensional vectors. We deGLYPH<28>ne this as F . Decomposing F using SVD results in:\n<!-- formula-not-decoded -->\nLeft-hand matrix U has feature speciGLYPH<28>c information, right-hand matrix V has global feature information, and the central matrix 6 indicates singular value magnitudes [1]. Since the singular values are decomposed in descending order, SVD of the same matrices always yields the same 6 . On the other hand, U and V are created in pairs as follows:\n<!-- formula-not-decoded -->\nwhere\n<!-- formula-not-decoded -->\nEq. (2) indicates that singular vectors with identical information can be randomly distributed in two different positions due to random variable pk . This phenomenon is called sign ambiguity [11]. This is an undesirable property that must be removed because it makes learning singular vectors difGLYPH<28>cult. Meanwhile, each singular vector is a unit vector with a norm of 1. In other words, singular vectors are the points on a unit hypersphereGLYPH<22>i.e., a manifold. Therefore, learning singular vectors with a generic neural network and Euclidean geometry can be very inefGLYPH<28>cient or impossible. This is the second problem. In summary, singular vectors have two fundamental problems as follows.\n- GLYPH<15> Singular vectors with similar information are randomly distributed in two areas due to sign ambiguity.\n- GLYPH<15> It is inefGLYPH<28>cient to learn singular vectors on manifold space through a general neural network scheme.\n\nHighlights the challenges of using SVD for singular vector learning due to sign ambiguity and inefﬁciency on manifold space.",
    "original_text": "Assume that a feature map obtained by a CNN has a spatial shape of H GLYPH<2> W and a feature depth of D , and it can be transformed into a matrix form. Then, the feature map of matrix form can be interpreted as a set composed of HWD -dimensional vectors. We deGLYPH<28>ne this as F . Decomposing F using SVD results in:\n<!-- formula-not-decoded -->\nLeft-hand matrix U has feature speciGLYPH<28>c information, right-hand matrix V has global feature information, and the central matrix 6 indicates singular value magnitudes [1]. Since the singular values are decomposed in descending order, SVD of the same matrices always yields the same 6 . On the other hand, U and V are created in pairs as follows:\n<!-- formula-not-decoded -->\nwhere\n<!-- formula-not-decoded -->\nEq. (2) indicates that singular vectors with identical information can be randomly distributed in two different positions due to random variable pk . This phenomenon is called sign ambiguity [11]. This is an undesirable property that must be removed because it makes learning singular vectors difGLYPH<28>cult. Meanwhile, each singular vector is a unit vector with a norm of 1. In other words, singular vectors are the points on a unit hypersphereGLYPH<22>i.e., a manifold. Therefore, learning singular vectors with a generic neural network and Euclidean geometry can be very inefGLYPH<28>cient or impossible. This is the second problem. In summary, singular vectors have two fundamental problems as follows.\n- GLYPH<15> Singular vectors with similar information are randomly distributed in two areas due to sign ambiguity.\n- GLYPH<15> It is inefGLYPH<28>cient to learn singular vectors on manifold space through a general neural network scheme.",
    "context": "Highlights the challenges of using SVD for singular vector learning due to sign ambiguity and inefﬁciency on manifold space.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      3
    ],
    "id": "4a5cab8b87efe1ae58e0ae6e270551604be45019f31cf289d3709867f5afcc9a"
  },
  {
    "text": "We GLYPH<28>rst describe a solution for learning singular vectors with the manifold property. Since the manifold is a unit hypersphere, it can be mapped to Euclidean space by disentangling each coordinate through a speciGLYPH<28>c coordinate conversion. As a result, it becomes possible to use SVD as a layer of CNN because it is easier to learn singular vectors based on Euclidean geometry and to transform them. (see Sec. III-E).\nHowever, since each coordinate in the spherical space is bounded, the singular vector distribution can be discontinuous near the boundary. This phenomenon can negatively affect the feature embedding of CNN. Therefore, rotation is applied so that the singular vectors are not located near the boundary before coordinate conversion. (see Sec. III-C).\nNext, we present a method to remove the sign ambiguity phenomenon. Suppose that singular vectors are distributed in two regions around a speciGLYPH<28>c center vector due to sign ambiguity. Similarity of each singular vector is calculated based on the center vector. By reversing the sign of a singular vector with negative similarity, the sign ambiguity can be resolved. This method is very simple and efGLYPH<28>cient as in Sec. III-D.\nOn the other hand, it is not easy to determine an accurate center vector because the singular vector distribution can change as learning progresses. Thus, we propose a way to approximate the center vector during learning. This process is performed after coordinate conversion for convenient calculation. (see Sec. III-F)\n\nDetails the methods for learning singular vectors with manifold properties, addressing boundary issues and sign ambiguity through coordinate conversion and center vector approximation.",
    "original_text": "We GLYPH<28>rst describe a solution for learning singular vectors with the manifold property. Since the manifold is a unit hypersphere, it can be mapped to Euclidean space by disentangling each coordinate through a speciGLYPH<28>c coordinate conversion. As a result, it becomes possible to use SVD as a layer of CNN because it is easier to learn singular vectors based on Euclidean geometry and to transform them. (see Sec. III-E).\nHowever, since each coordinate in the spherical space is bounded, the singular vector distribution can be discontinuous near the boundary. This phenomenon can negatively affect the feature embedding of CNN. Therefore, rotation is applied so that the singular vectors are not located near the boundary before coordinate conversion. (see Sec. III-C).\nNext, we present a method to remove the sign ambiguity phenomenon. Suppose that singular vectors are distributed in two regions around a speciGLYPH<28>c center vector due to sign ambiguity. Similarity of each singular vector is calculated based on the center vector. By reversing the sign of a singular vector with negative similarity, the sign ambiguity can be resolved. This method is very simple and efGLYPH<28>cient as in Sec. III-D.\nOn the other hand, it is not easy to determine an accurate center vector because the singular vector distribution can change as learning progresses. Thus, we propose a way to approximate the center vector during learning. This process is performed after coordinate conversion for convenient calculation. (see Sec. III-F)",
    "context": "Details the methods for learning singular vectors with manifold properties, addressing boundary issues and sign ambiguity through coordinate conversion and center vector approximation.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      3
    ],
    "id": "98ef740ce6a0ea099f42e369f568298bba0b935dd31045ef14496c77d77c9a0c"
  },
  {
    "text": "Mapping of singular vectors in the spherical coordinate system is equivalent to converting them from non-Euclidean space to Euclidean space. However, when singular vectors\nFIGURE 3. Discontinuity and rotation effects in the spherical coordinate system. Red, green, yellow, and blue lines indicate a discontinuity boundary, SP1, SP2, and rotation, respectively. (a) Discontinuity in spherical coordinates (b) SP1 and SP2 before rotation (c) SP1 and SP2 after rotation.\nare directly mapped to spherical coordinates, a discontinuity problem may occur.\nLet . r ; GLYPH<30> 1 ; GLYPH<30> 2 ; : : : ; GLYPH<30> N GLYPH<0> 1 / denote N -dimensional spherical coordinates. Each component of .GLYPH<30> 1 ; GLYPH<30> 2 ; : : : ; GLYPH<30> N GLYPH<0> 2 / has a bounded range of [0 ; GLYPH<25> ], and GLYPH<30> N GLYPH<0> 1 is limited to [ GLYPH<0> GLYPH<25> ; GLYPH<25> ], which means that discontinuity may occur at the boundary of each component, as shown in Fig. 3(a). For convenience, the three-dimensional space is assumed in the GLYPH<28>gure. If two feature vectors v and v t are present near the discontinuity boundary, as in Fig. 3(b), we can consider two different shortest paths: one through the discontinuity boundary (SP1) and another to avoid the discontinuity boundary (SP2). Therefore, the learning efGLYPH<28>ciency may be reduced because singular vectors can practically move to longer paths such as SP2 rather than the real shortest path (SP1) during the learning process. Thus, singular vectors should be kept as distant from the discontinuity boundary as possible. In other words, the singular vector distribution must be intentionally centered in the spherical coordinate system. The center of the spherical coordinate system GLYPH<8> GLYPH<25> 2 ; GLYPH<25> 2 ; : : : ; GLYPH<25> 2 ; 0 GLYPH<9> is converted to f 0 ; : : : ; 0 ; 1 ; 0 g of the Cartesian coordinate using spherical to Cartesian coordinates conversion equation in [23].\nAs a result, the center of the singular vector distribution v c must be rotated so that it becomes the coordinate center, e v c which is f 0 ; : : : ; 0 ; 1 ; 0 g . How to obtain v c will be given in Sec. III-F. Then, the rotation matrix R is computed by using the well-known Rodrigues rotation as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere k is the direction of the rotation axis. Using the rotation matrix of Eq. (6), e V is obtained by:\n<!-- formula-not-decoded -->\nWe can obtain e U by applying the same process to U .\nAs shown in Fig. 3(c), the shortest path and the real path now coincide. However, if singular vector variance is too high, some singular vectors still exist near the discontinuity boundary. Fortunately, SVD computes a least-square solution which is not sparse, and regularization skills such as weight\ndecay and batch normalization [24] also control feature map's variance to some extent. So, we can ignore this high variance problem. Therefore, the discontinuity problem is eliminated so that a singular vector can be effectively learned. However, the two problems of singular vectors mentioned in Section 3.1 still remain unsolved.\n\nDetails the discontinuity problem in spherical coordinates and proposes a method for centering singular vectors to mitigate it, referencing a rotation process and regularization techniques.",
    "original_text": "Mapping of singular vectors in the spherical coordinate system is equivalent to converting them from non-Euclidean space to Euclidean space. However, when singular vectors\nFIGURE 3. Discontinuity and rotation effects in the spherical coordinate system. Red, green, yellow, and blue lines indicate a discontinuity boundary, SP1, SP2, and rotation, respectively. (a) Discontinuity in spherical coordinates (b) SP1 and SP2 before rotation (c) SP1 and SP2 after rotation.\nare directly mapped to spherical coordinates, a discontinuity problem may occur.\nLet . r ; GLYPH<30> 1 ; GLYPH<30> 2 ; : : : ; GLYPH<30> N GLYPH<0> 1 / denote N -dimensional spherical coordinates. Each component of .GLYPH<30> 1 ; GLYPH<30> 2 ; : : : ; GLYPH<30> N GLYPH<0> 2 / has a bounded range of [0 ; GLYPH<25> ], and GLYPH<30> N GLYPH<0> 1 is limited to [ GLYPH<0> GLYPH<25> ; GLYPH<25> ], which means that discontinuity may occur at the boundary of each component, as shown in Fig. 3(a). For convenience, the three-dimensional space is assumed in the GLYPH<28>gure. If two feature vectors v and v t are present near the discontinuity boundary, as in Fig. 3(b), we can consider two different shortest paths: one through the discontinuity boundary (SP1) and another to avoid the discontinuity boundary (SP2). Therefore, the learning efGLYPH<28>ciency may be reduced because singular vectors can practically move to longer paths such as SP2 rather than the real shortest path (SP1) during the learning process. Thus, singular vectors should be kept as distant from the discontinuity boundary as possible. In other words, the singular vector distribution must be intentionally centered in the spherical coordinate system. The center of the spherical coordinate system GLYPH<8> GLYPH<25> 2 ; GLYPH<25> 2 ; : : : ; GLYPH<25> 2 ; 0 GLYPH<9> is converted to f 0 ; : : : ; 0 ; 1 ; 0 g of the Cartesian coordinate using spherical to Cartesian coordinates conversion equation in [23].\nAs a result, the center of the singular vector distribution v c must be rotated so that it becomes the coordinate center, e v c which is f 0 ; : : : ; 0 ; 1 ; 0 g . How to obtain v c will be given in Sec. III-F. Then, the rotation matrix R is computed by using the well-known Rodrigues rotation as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere k is the direction of the rotation axis. Using the rotation matrix of Eq. (6), e V is obtained by:\n<!-- formula-not-decoded -->\nWe can obtain e U by applying the same process to U .\nAs shown in Fig. 3(c), the shortest path and the real path now coincide. However, if singular vector variance is too high, some singular vectors still exist near the discontinuity boundary. Fortunately, SVD computes a least-square solution which is not sparse, and regularization skills such as weight\ndecay and batch normalization [24] also control feature map's variance to some extent. So, we can ignore this high variance problem. Therefore, the discontinuity problem is eliminated so that a singular vector can be effectively learned. However, the two problems of singular vectors mentioned in Section 3.1 still remain unsolved.",
    "context": "Details the discontinuity problem in spherical coordinates and proposes a method for centering singular vectors to mitigate it, referencing a rotation process and regularization techniques.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      3,
      4
    ],
    "id": "e473d704fd3f305edc8be733a6418f2acc3ab0dd7ae8cc7d20a6a947ca7c68be"
  },
  {
    "text": "U and V in Eq. (1) exist as a pair, and one of them is used as a reference. We adopt an approach of aligning e V based on e U as a reference. Let e u k be the k -th singular vector. Then e u k and GLYPH<0> e u k exist concurrently with the same information but opposite signs. Therefore, a domain in which a singular vector exists can be divided into a positive half-sphere that is decomposed in the forward direction and a negative half-sphere that is decomposed in the reverse direction. Thus, we eliminate the randomness property by multiplying a singular vector on the negative half-sphere by GLYPH<0> 1. In the previous section, the center vector of the singular vector distribution is rotated to e u c , which is f 0 ; 0 ; : : : ; 0 ; 1 ; 0 g . So we can verify the hyper-sphere where e u k is in according to the sign of e uHW GLYPH<0> 1 ; k . Finally, O U and O V that are arranged based on e u c are deGLYPH<28>ned as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nSince the sign ambiguity phenomenon in O U and O V has been removed, O U and O V are easier to analyze than the original e U and e V .\n\nClarifies the methodology for analyzing singular vectors and removes sign ambiguity, leading to simpler analysis.",
    "original_text": "U and V in Eq. (1) exist as a pair, and one of them is used as a reference. We adopt an approach of aligning e V based on e U as a reference. Let e u k be the k -th singular vector. Then e u k and GLYPH<0> e u k exist concurrently with the same information but opposite signs. Therefore, a domain in which a singular vector exists can be divided into a positive half-sphere that is decomposed in the forward direction and a negative half-sphere that is decomposed in the reverse direction. Thus, we eliminate the randomness property by multiplying a singular vector on the negative half-sphere by GLYPH<0> 1. In the previous section, the center vector of the singular vector distribution is rotated to e u c , which is f 0 ; 0 ; : : : ; 0 ; 1 ; 0 g . So we can verify the hyper-sphere where e u k is in according to the sign of e uHW GLYPH<0> 1 ; k . Finally, O U and O V that are arranged based on e u c are deGLYPH<28>ned as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nSince the sign ambiguity phenomenon in O U and O V has been removed, O U and O V are easier to analyze than the original e U and e V .",
    "context": "Clarifies the methodology for analyzing singular vectors and removes sign ambiguity, leading to simpler analysis.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      4
    ],
    "id": "5af1b979401dce08bea1c289fa93b9ce0c0c34adbd78c050d2bc01dfa47edc8e"
  },
  {
    "text": "To convert the coordinate system of O V , it is rewritten as:\n<!-- formula-not-decoded -->\nThen, the D -dimensional transformation of O V in Cartesian coordinates into spherical coordinates is deGLYPH<28>ned as follows:\n<!-- formula-not-decoded -->\nwhere\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nIn Eq. (13), GLYPH<15> is a constant to prevent zero-division and is set to 10 GLYPH<0> 3 . So, N V on the spherical coordinate system is obtained. However, the 1 st derivative of the arccosine in Eq. (12) can easily diverge, as shown in Eq. (14).\n<!-- formula-not-decoded -->\nSo, we approximate the arccosine to GLYPH<0> z C GLYPH<25> 2 by the GLYPH<28>rst order Taylor expansion. Next, we add a constant term for keeping a zero-centered feature. Finally, Eq. (12) is re-written by\n<!-- formula-not-decoded -->\nAs a result, O V is converted to N V that can be learned by a generic neural network. At the same manner, N U is derived.\n\nDetails the mathematical transformation of coordinate systems (O V to N V) using a Taylor expansion to avoid divergence and enables learning via a neural network.",
    "original_text": "To convert the coordinate system of O V , it is rewritten as:\n<!-- formula-not-decoded -->\nThen, the D -dimensional transformation of O V in Cartesian coordinates into spherical coordinates is deGLYPH<28>ned as follows:\n<!-- formula-not-decoded -->\nwhere\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nIn Eq. (13), GLYPH<15> is a constant to prevent zero-division and is set to 10 GLYPH<0> 3 . So, N V on the spherical coordinate system is obtained. However, the 1 st derivative of the arccosine in Eq. (12) can easily diverge, as shown in Eq. (14).\n<!-- formula-not-decoded -->\nSo, we approximate the arccosine to GLYPH<0> z C GLYPH<25> 2 by the GLYPH<28>rst order Taylor expansion. Next, we add a constant term for keeping a zero-centered feature. Finally, Eq. (12) is re-written by\n<!-- formula-not-decoded -->\nAs a result, O V is converted to N V that can be learned by a generic neural network. At the same manner, N U is derived.",
    "context": "Details the mathematical transformation of coordinate systems (O V to N V) using a Taylor expansion to avoid divergence and enables learning via a neural network.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      4,
      5
    ],
    "id": "b7043fe9d636a147413ffa99723c6cc58e39d76fe6924560d1cb8d2206781bef"
  },
  {
    "text": "In this section, the process of estimating the center vector mentioned in Sec. III-C is described. Since this process has to be explained in connection with Sec. III-D, it is described with u c instead of v c . The proposed method rotates u around u c , moves the center of the distribution to f 0 ; 0 ; : : : ; 0 ; 1 ; 0 g , and determines which half-sphere it belongs to through the sign of e uHW GLYPH<0> 1 ; k . If u c is inaccurate, this process will be erroneous. Thereby, the sign ambiguity problem cannot be solved properly. To observe this phenomenon, the distribution of N u obtained from u c is shown in the GLYPH<28>rst row of Fig. 4. Here u c is experimentally set to a unit vector with all components of the same size. We can GLYPH<28>nd that the distributions of the two half-spheres are not aligned correctly because of the sign ambiguity problem caused by inaccurate u c . To obtain the precise u c , we suggest learning of u c . Since we are hard to understand the statistical characteristics of singular vectors due to two inherent bad properties, we learn u c based on N u . Note that each component of features produced by neural networks follows the Gaussian distribution in general. So, assuming that each component of the aligned N u has the Gaussian distribution, u c is learned through KL-divergence [25] of Eq. (16) so that the assumption is correct.\n<!-- formula-not-decoded -->\nwhere Norm GLYPH<16> GLYPH<22> N u b ; k ; GLYPH<27> 2 N u b ; k GLYPH<17> indicates a Gaussian distribution. Furthermore, GLYPH<27> 2 N u b ; k D var GLYPH<0> N u b ; k GLYPH<1> , and mean GLYPH<22> N u b ; k is empirically set to 0. B is the batch size. As shown in Fig. 4, in order to minimize L KL N u b ; k , two distributions of N u get merged and form a single Gaussian distribution, which means that the sign ambiguity problem is solved. Therefore, the proposed method can estimate u c overcoming two bad properties of singular vectors. In the same manner, v c is estimated by minimizing L KL N v b ; k . Note that L KL N u b ; k and L KL N v b ; k are loss functions for learning u c and v c only and do not affect the network parameters.\n\nDetails the process of estimating the center vector (u c) and addresses the sign ambiguity problem through a learning-based approach, demonstrating how minimizing KL-divergence resolves the issue and enabling accurate estimation of both u c and v c.",
    "original_text": "In this section, the process of estimating the center vector mentioned in Sec. III-C is described. Since this process has to be explained in connection with Sec. III-D, it is described with u c instead of v c . The proposed method rotates u around u c , moves the center of the distribution to f 0 ; 0 ; : : : ; 0 ; 1 ; 0 g , and determines which half-sphere it belongs to through the sign of e uHW GLYPH<0> 1 ; k . If u c is inaccurate, this process will be erroneous. Thereby, the sign ambiguity problem cannot be solved properly. To observe this phenomenon, the distribution of N u obtained from u c is shown in the GLYPH<28>rst row of Fig. 4. Here u c is experimentally set to a unit vector with all components of the same size. We can GLYPH<28>nd that the distributions of the two half-spheres are not aligned correctly because of the sign ambiguity problem caused by inaccurate u c . To obtain the precise u c , we suggest learning of u c . Since we are hard to understand the statistical characteristics of singular vectors due to two inherent bad properties, we learn u c based on N u . Note that each component of features produced by neural networks follows the Gaussian distribution in general. So, assuming that each component of the aligned N u has the Gaussian distribution, u c is learned through KL-divergence [25] of Eq. (16) so that the assumption is correct.\n<!-- formula-not-decoded -->\nwhere Norm GLYPH<16> GLYPH<22> N u b ; k ; GLYPH<27> 2 N u b ; k GLYPH<17> indicates a Gaussian distribution. Furthermore, GLYPH<27> 2 N u b ; k D var GLYPH<0> N u b ; k GLYPH<1> , and mean GLYPH<22> N u b ; k is empirically set to 0. B is the batch size. As shown in Fig. 4, in order to minimize L KL N u b ; k , two distributions of N u get merged and form a single Gaussian distribution, which means that the sign ambiguity problem is solved. Therefore, the proposed method can estimate u c overcoming two bad properties of singular vectors. In the same manner, v c is estimated by minimizing L KL N v b ; k . Note that L KL N u b ; k and L KL N v b ; k are loss functions for learning u c and v c only and do not affect the network parameters.",
    "context": "Details the process of estimating the center vector (u c) and addresses the sign ambiguity problem through a learning-based approach, demonstrating how minimizing KL-divergence resolves the issue and enabling accurate estimation of both u c and v c.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      5
    ],
    "id": "cc76a4baade3dbd9df09debf62c0bad7df09f12d10c1ffa7c8ac66858786ce11"
  },
  {
    "text": "We performed three experiments to verify the proposed method. First, we evaluated the performance of SVP in a CNN structure when using SVD. Because classiGLYPH<28>cation task outputs only class information, we use one singular vector\nFIGURE 4. Learning process of singular vectors which are converted to the spherical coordinate. The left column is plotting the last two components and the right column is the distribution. The black line shows a Gaussian distribution, and the blue and yellow lines indicate the distributions of singular vectors in two half-spheres obtained by applying the proposed method. During the learning process, it gradually gets closer to the Gaussian distribution, indicating that the basis is well learned.\nfor the classiGLYPH<28>cation task. Second, we applied the proposed method to KD-SVD, which is a representative network using singular vectors. Since KD-SVD compresses CNN's feature maps with several dominant singular vectors, the performance of the student network improves. In this experiment, weadditionally show an effect of using multiple singular vectors for extracting important information from feature maps. Third, the performance of the proposed method is analyzed through an ablation study.\n\nVerifies the proposed method's effectiveness in CNN structures and demonstrates improvements with KD-SVD and multiple singular vectors.",
    "original_text": "We performed three experiments to verify the proposed method. First, we evaluated the performance of SVP in a CNN structure when using SVD. Because classiGLYPH<28>cation task outputs only class information, we use one singular vector\nFIGURE 4. Learning process of singular vectors which are converted to the spherical coordinate. The left column is plotting the last two components and the right column is the distribution. The black line shows a Gaussian distribution, and the blue and yellow lines indicate the distributions of singular vectors in two half-spheres obtained by applying the proposed method. During the learning process, it gradually gets closer to the Gaussian distribution, indicating that the basis is well learned.\nfor the classiGLYPH<28>cation task. Second, we applied the proposed method to KD-SVD, which is a representative network using singular vectors. Since KD-SVD compresses CNN's feature maps with several dominant singular vectors, the performance of the student network improves. In this experiment, weadditionally show an effect of using multiple singular vectors for extracting important information from feature maps. Third, the performance of the proposed method is analyzed through an ablation study.",
    "context": "Verifies the proposed method's effectiveness in CNN structures and demonstrates improvements with KD-SVD and multiple singular vectors.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      5
    ],
    "id": "00c79b643eb8046e6f72a9958c56c406901c14c40df1cde35c960dc4f4e2bef2"
  },
  {
    "text": "The datasets used in the following experiments are CIFAR10, CIFAR100 [26], Tiny-ImageNet, and ImageNet2012 [27], which are all normalized to have values of [-0.5, 0.5]. CIFAR10 and CIFAR100 have 10 and 100 labels, respectively, and are compact datasets consisting of 50,000 color images of 32 GLYPH<2> 32 pixels. Because these datasets require relatively little cost, they are used for basic veriGLYPH<28>cation of the proposed method. Tiny-ImageNet is a medium-sized dataset with 100,000 color images of 64 GLYPH<2> 64 pixels and has 200 labels. ImageNet2012 is a large dataset with more than 1.5 million high-resolution color images and 1,000 labels. The last two datasets are used to prove that the proposed method works well even for huge datasets with large resolutions.\nA particular augmentation is applied to each dataset. Horizontal random GLYPH<29>ip is applied to all the datasets. Also, the images of CIFAR10 and CIFAR100 are zero-padded by 4 pixels, and the images of Tiny-ImageNet [27] are zero-padded by 8 pixels. Then, the zero-padded images are randomly cropped to the original size. In case of ImageNet2012, all\nFIGURE 5. The block diagrams of the networks introduced in this paper.\nthe images are resized to 256 GLYPH<2> 256, and then the training image are produced by randomly cropping the resized images to 224 GLYPH<2> 224. The test set is constructed by cutting the central area of each image to 224 GLYPH<2> 224.\nAll algorithms are implemented using TensorGLYPH<29>ow [28], and their network architectures are shown in Fig. 5. 'CONV' and 'Max Pool' indicate convolutional layer and max pooling layer, respectively. 'FC' stands for fully connected layer. Also H GLYPH<2> W is the kernel size, D is the output depth. s means stride and the same layer repeats by N . (Label) indicates the number of labels in the target dataset. The activation function for convolution layers and all FC layers except the end of the network is ReLU [29]. In Fig. 5(a), 5(b) and 5(c), one of SVP, global average pooling (GAP) [16], global max pooling (GMP) [18], and matrix power normalized covariance pooling (MPN) [8] is adopted for pooling block. Also, in Fig. 5(a), 5(b), 5(c) and 5(d), batch normalization [24] is used next to the convolutional layer. The dotted gray boxes in Fig. 5(e) and 5(f) are the layer modules for knowledge distillation. The input and output feature maps of this module are sensed and applied to both the authentic KD-SVD [12] and the KD-SVP where SVP was applied to KD-SVD.\nThe weights of all networks are initialized with He's initialization [30], and L 2 regularization is applied. A stochastic gradient descent (SGD) [31] is used as the optimizer, and a Nesterov accelerated gradient [32] is applied. All numerical values in the tables and GLYPH<28>gures are the averages of all GLYPH<28>ve trials.\n\nDetails the datasets used, preprocessing steps, and network architecture, including specific layer types, activation functions, and regularization techniques.",
    "original_text": "The datasets used in the following experiments are CIFAR10, CIFAR100 [26], Tiny-ImageNet, and ImageNet2012 [27], which are all normalized to have values of [-0.5, 0.5]. CIFAR10 and CIFAR100 have 10 and 100 labels, respectively, and are compact datasets consisting of 50,000 color images of 32 GLYPH<2> 32 pixels. Because these datasets require relatively little cost, they are used for basic veriGLYPH<28>cation of the proposed method. Tiny-ImageNet is a medium-sized dataset with 100,000 color images of 64 GLYPH<2> 64 pixels and has 200 labels. ImageNet2012 is a large dataset with more than 1.5 million high-resolution color images and 1,000 labels. The last two datasets are used to prove that the proposed method works well even for huge datasets with large resolutions.\nA particular augmentation is applied to each dataset. Horizontal random GLYPH<29>ip is applied to all the datasets. Also, the images of CIFAR10 and CIFAR100 are zero-padded by 4 pixels, and the images of Tiny-ImageNet [27] are zero-padded by 8 pixels. Then, the zero-padded images are randomly cropped to the original size. In case of ImageNet2012, all\nFIGURE 5. The block diagrams of the networks introduced in this paper.\nthe images are resized to 256 GLYPH<2> 256, and then the training image are produced by randomly cropping the resized images to 224 GLYPH<2> 224. The test set is constructed by cutting the central area of each image to 224 GLYPH<2> 224.\nAll algorithms are implemented using TensorGLYPH<29>ow [28], and their network architectures are shown in Fig. 5. 'CONV' and 'Max Pool' indicate convolutional layer and max pooling layer, respectively. 'FC' stands for fully connected layer. Also H GLYPH<2> W is the kernel size, D is the output depth. s means stride and the same layer repeats by N . (Label) indicates the number of labels in the target dataset. The activation function for convolution layers and all FC layers except the end of the network is ReLU [29]. In Fig. 5(a), 5(b) and 5(c), one of SVP, global average pooling (GAP) [16], global max pooling (GMP) [18], and matrix power normalized covariance pooling (MPN) [8] is adopted for pooling block. Also, in Fig. 5(a), 5(b), 5(c) and 5(d), batch normalization [24] is used next to the convolutional layer. The dotted gray boxes in Fig. 5(e) and 5(f) are the layer modules for knowledge distillation. The input and output feature maps of this module are sensed and applied to both the authentic KD-SVD [12] and the KD-SVP where SVP was applied to KD-SVD.\nThe weights of all networks are initialized with He's initialization [30], and L 2 regularization is applied. A stochastic gradient descent (SGD) [31] is used as the optimizer, and a Nesterov accelerated gradient [32] is applied. All numerical values in the tables and GLYPH<28>gures are the averages of all GLYPH<28>ve trials.",
    "context": "Details the datasets used, preprocessing steps, and network architecture, including specific layer types, activation functions, and regularization techniques.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      5,
      6
    ],
    "id": "4c75524b7630f60d6eec171cd1306599bed278b713001658dd306f2fe4fa6113"
  },
  {
    "text": "Most CNNs increase the size of the receptive GLYPH<28>eld and reduce the size of the feature map through pooling. GMP and GAP are the most popular pooling methods, but both methods suffer from the inherent loss of information. SVP can improve the performance of a given network because it can reduce the size of feature maps while keeping as much of their information as possible.\nThe base network used for this experiment is ResNet32 [30] which described in Fig. 5(a). We analyzed the results by replacing GAP of ResNet-32 with GMP [18] and SVP, respectively. In case of CIFAR10, learning is proceeded for 200 epochs and the initial learning rate is equal to 10 GLYPH<0> 2 , which is reduced by 0.1 times at 100 and 150 epochs. The batch size is set to 128, and the weight decay of L 2 regularization is GLYPH<28>xed to 10 GLYPH<0> 4 . ImageNet2012 is trained for 90 epochs, and the initial learning rate is set to 10 GLYPH<0> 2 , and the learning rate is reduced by 0.1 times at 30 and 60 epochs. The batch size in this experiment is 128 and the weight decay of L 2 regularization is GLYPH<28>xed to 5 GLYPH<2> 10 GLYPH<0> 4 .\nTABLE 1. The comparison of the proposed SVP with GAP and GMP for CIFAR10 dataset. 'ACC' indicates accuracy, 'Silh' indicates silhouette score, and T indicates forward time for each pooling layer.\nTable 1 shows the experimental results for CIFAR10. We can GLYPH<28>nd the validation accuracy of SVP is 0.67% and 0.91% better than that of GAP and GMP, respectively. However, this is not a big improvement and almost same with MPN. Nevertheless, the silhouette score [33] of SVP on training dataset is better by 0.444, 0.510, and 0.551 than that of GAP, GMP, and MPN, respectively. SVP shows excellent silhouette scores that are more than twice those of the\nFIGURE 6. The distribution of feature vectors obtained by several pooling techniques for CIFAR10 training set.\nother pooling methods. This indicate that SVP makes feature vectors get a high inter-class variance and a low intra-class variance. Also, the proposed method has a lower forward time than MPN, which is a light-weight second-order pooling method. Therefore, we can GLYPH<28>nd that the proposed method can provide better clustering performance with less computation time.\nAlso, to illustrate the clustering performance clearly, we performed the same experiment through a network with an additional FC layer having a two-dimensional output as shown in Fig. 5(b). The network is learned for CIFAR10 dataset until overGLYPH<28>tting occurs in order to discriminate clusters clearly. The feature distributions are shown in Fig. 6. The feature distributions of GAP, GMP, and MPN have very large intra-class variation, and the features tend to gather near the origin. That is, their inter-class variation is low. However, the feature distribution of SVP shows better separation than those of GAP, GMP, and MPN. This means that SVP makes a given network operate more robustly to noise and attack. In addition, we show the evaluation results for ImageNet2012 through ResNet-18 of Fig. 5(c) to generalize the proposed method (see Table 2). We could observe a trend similar to the results in CIFAR10 dataset, which means that SVP extracts essential information well even with large feature maps.\nTABLE 2. The comparison of SVP with GAP for ImageNet2012 dataset.\n\nCompares SVP pooling to GAP and GMP, demonstrating improved silhouette scores and feature separation in CIFAR10, and showing similar trends on ImageNet2012, highlighting its robustness and efficiency.",
    "original_text": "Most CNNs increase the size of the receptive GLYPH<28>eld and reduce the size of the feature map through pooling. GMP and GAP are the most popular pooling methods, but both methods suffer from the inherent loss of information. SVP can improve the performance of a given network because it can reduce the size of feature maps while keeping as much of their information as possible.\nThe base network used for this experiment is ResNet32 [30] which described in Fig. 5(a). We analyzed the results by replacing GAP of ResNet-32 with GMP [18] and SVP, respectively. In case of CIFAR10, learning is proceeded for 200 epochs and the initial learning rate is equal to 10 GLYPH<0> 2 , which is reduced by 0.1 times at 100 and 150 epochs. The batch size is set to 128, and the weight decay of L 2 regularization is GLYPH<28>xed to 10 GLYPH<0> 4 . ImageNet2012 is trained for 90 epochs, and the initial learning rate is set to 10 GLYPH<0> 2 , and the learning rate is reduced by 0.1 times at 30 and 60 epochs. The batch size in this experiment is 128 and the weight decay of L 2 regularization is GLYPH<28>xed to 5 GLYPH<2> 10 GLYPH<0> 4 .\nTABLE 1. The comparison of the proposed SVP with GAP and GMP for CIFAR10 dataset. 'ACC' indicates accuracy, 'Silh' indicates silhouette score, and T indicates forward time for each pooling layer.\nTable 1 shows the experimental results for CIFAR10. We can GLYPH<28>nd the validation accuracy of SVP is 0.67% and 0.91% better than that of GAP and GMP, respectively. However, this is not a big improvement and almost same with MPN. Nevertheless, the silhouette score [33] of SVP on training dataset is better by 0.444, 0.510, and 0.551 than that of GAP, GMP, and MPN, respectively. SVP shows excellent silhouette scores that are more than twice those of the\nFIGURE 6. The distribution of feature vectors obtained by several pooling techniques for CIFAR10 training set.\nother pooling methods. This indicate that SVP makes feature vectors get a high inter-class variance and a low intra-class variance. Also, the proposed method has a lower forward time than MPN, which is a light-weight second-order pooling method. Therefore, we can GLYPH<28>nd that the proposed method can provide better clustering performance with less computation time.\nAlso, to illustrate the clustering performance clearly, we performed the same experiment through a network with an additional FC layer having a two-dimensional output as shown in Fig. 5(b). The network is learned for CIFAR10 dataset until overGLYPH<28>tting occurs in order to discriminate clusters clearly. The feature distributions are shown in Fig. 6. The feature distributions of GAP, GMP, and MPN have very large intra-class variation, and the features tend to gather near the origin. That is, their inter-class variation is low. However, the feature distribution of SVP shows better separation than those of GAP, GMP, and MPN. This means that SVP makes a given network operate more robustly to noise and attack. In addition, we show the evaluation results for ImageNet2012 through ResNet-18 of Fig. 5(c) to generalize the proposed method (see Table 2). We could observe a trend similar to the results in CIFAR10 dataset, which means that SVP extracts essential information well even with large feature maps.\nTABLE 2. The comparison of SVP with GAP for ImageNet2012 dataset.",
    "context": "Compares SVP pooling to GAP and GMP, demonstrating improved silhouette scores and feature separation in CIFAR10, and showing similar trends on ImageNet2012, highlighting its robustness and efficiency.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      6,
      7
    ],
    "id": "33456d172209fdb82cdd35e5857d51a327b14ebb670d68fc25f3c43e07fe4de2"
  },
  {
    "text": "We next examined the robustness of SVP against adversarial attacks. The attack methods include the fast gradient sign method (FGSM) [34] and basic iterative method (BIM) [35]. FGSM is a gradient-based adversarial attack and one of the simplest and most effective attack methods. The image X adv generated by FGSM is deGLYPH<28>ned by:\n<!-- formula-not-decoded -->\nwhere X is an input image, ytrue is the label, and J stands for the Jacobian. The attack rate GLYPH<11> is set to 0.01 in this experiment.\nSince BIM performs FGSM iteratively, BIM generally attacks more strongly than FGSM. The I -th image generated by BIM is deGLYPH<28>ned by Eqs. (18) and (19):\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere GLYPH<12> is the clipping threshold. The following values are used in this experiment: GLYPH<11> D 0 : 01, GLYPH<12> D 0 : 002, and T D 5. All hyper-parameters are set as in Sec. IV-B.\nTABLE 3. Experimental result on white-box adversarial attack. The first three rows correspond to natural training, and the second three rows are the result of adversarial training.\nTo compare the results of Sec. IV-B, the CIFAR10 dataset was employed in this experiment, using ResNet-32 in Fig. 5(a) and ResNet-32-plot in Fig. 5(b) Table 3 shows the white-box attack results for several pooling methods. For FGSM with natural and adversarial training, SVP shows 36.07%, 28.18%, and 34.21% higher accuracies than GAP, GMP, and MPN, respectively. Overall, SVP provides the best performance for all other cases. The performance improvement is interpreted through the high silhouette score of the feature vector obtained by SVP. Fig 7 plots the feature vector distributions by employing the network used in Fig. 6. In the other pooling methods, the clusters are scattered due to the attack, but SVP maintains a relatively strong inter-cluster\nFIGURE 7. The distribution of feature vectors obtained by several pooling techniques for an adversarial attacked CIFAR10 training set.\nTABLE 4. Experimental results on black-box adversarial attack. The first three rows represent the natural training, and the second three rows represent the adversarial training results.\ndistance. Furthermore, Fig. 8 shows the performance changes according to the attack rates. As the attack rate increases, the gaps between SVP and the other pooling methods increase. For example, when GLYPH<11> is 0.02, SVP shows 40.99%, 33.60%, and 40.62% better than GAP, GMP, and MPN, respectively.\nA black-box attack is a method of training and testing a target network using attacked data generated by an arbitrary network. The network used for attacking is VGG-16 [36], and the experimental results are shown in Table 4. SVP still outperforms GAP, GMP, and MPN, but the gap is somewhat reduced. This occurs because SVP is located at the end of the network and it is difGLYPH<28>cult for it to extract essential information from a feature map that has already been destroyed. The experiments with adversarial attacks prove that the proposed method is robust to noise. Despite the limitations, the proposed method makes the given network robust against noise without requiring additional techniques for singular vectors.\n\nDetails the robustness of SVP against adversarial attacks, presenting experimental results on both white-box and black-box attacks and highlighting its superior performance compared to other pooling methods.",
    "original_text": "We next examined the robustness of SVP against adversarial attacks. The attack methods include the fast gradient sign method (FGSM) [34] and basic iterative method (BIM) [35]. FGSM is a gradient-based adversarial attack and one of the simplest and most effective attack methods. The image X adv generated by FGSM is deGLYPH<28>ned by:\n<!-- formula-not-decoded -->\nwhere X is an input image, ytrue is the label, and J stands for the Jacobian. The attack rate GLYPH<11> is set to 0.01 in this experiment.\nSince BIM performs FGSM iteratively, BIM generally attacks more strongly than FGSM. The I -th image generated by BIM is deGLYPH<28>ned by Eqs. (18) and (19):\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere GLYPH<12> is the clipping threshold. The following values are used in this experiment: GLYPH<11> D 0 : 01, GLYPH<12> D 0 : 002, and T D 5. All hyper-parameters are set as in Sec. IV-B.\nTABLE 3. Experimental result on white-box adversarial attack. The first three rows correspond to natural training, and the second three rows are the result of adversarial training.\nTo compare the results of Sec. IV-B, the CIFAR10 dataset was employed in this experiment, using ResNet-32 in Fig. 5(a) and ResNet-32-plot in Fig. 5(b) Table 3 shows the white-box attack results for several pooling methods. For FGSM with natural and adversarial training, SVP shows 36.07%, 28.18%, and 34.21% higher accuracies than GAP, GMP, and MPN, respectively. Overall, SVP provides the best performance for all other cases. The performance improvement is interpreted through the high silhouette score of the feature vector obtained by SVP. Fig 7 plots the feature vector distributions by employing the network used in Fig. 6. In the other pooling methods, the clusters are scattered due to the attack, but SVP maintains a relatively strong inter-cluster\nFIGURE 7. The distribution of feature vectors obtained by several pooling techniques for an adversarial attacked CIFAR10 training set.\nTABLE 4. Experimental results on black-box adversarial attack. The first three rows represent the natural training, and the second three rows represent the adversarial training results.\ndistance. Furthermore, Fig. 8 shows the performance changes according to the attack rates. As the attack rate increases, the gaps between SVP and the other pooling methods increase. For example, when GLYPH<11> is 0.02, SVP shows 40.99%, 33.60%, and 40.62% better than GAP, GMP, and MPN, respectively.\nA black-box attack is a method of training and testing a target network using attacked data generated by an arbitrary network. The network used for attacking is VGG-16 [36], and the experimental results are shown in Table 4. SVP still outperforms GAP, GMP, and MPN, but the gap is somewhat reduced. This occurs because SVP is located at the end of the network and it is difGLYPH<28>cult for it to extract essential information from a feature map that has already been destroyed. The experiments with adversarial attacks prove that the proposed method is robust to noise. Despite the limitations, the proposed method makes the given network robust against noise without requiring additional techniques for singular vectors.",
    "context": "Details the robustness of SVP against adversarial attacks, presenting experimental results on both white-box and black-box attacks and highlighting its superior performance compared to other pooling methods.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      8,
      7
    ],
    "id": "4b1097296e4f342c435ec2f7e4ac692fedb233cfec826260aa3bf380d0cb1371"
  },
  {
    "text": "The feature vectors obtained by KD-SVP are easier to analyze than original singular vectors. In KD-SVD, the sign ambiguity is removed by aligning the singular vectors of the student network based on the singular vector of the teacher network. In addition, KD-SVD learns only similar features to learn the manifold features and deGLYPH<28>nes the correlation by a radial basis function (RBF), which has smaller values as the distance becomes larger. In other words, singular vectors\nFIGURE 8. The performance change according to the intensity of adversarial attack.\nare post-processed in a very naive manner in KD-SVD. Therefore, if SVP plays a post-processing role, the learning accuracy will improve. The correlation between feature vectors obtained by SVP is computed by RBF, similarly to KD-SVD.TheCIFAR100andTiny-ImageNet are used in this experiment. The teacher network is VGG-16, and the student network is a condensed network that uses only one convolution of the same depth as VGG-16, which are described in Fig. 5(e) and 5(f). KD-SVD and KD-SVP are all learned with the same hyper-parameters. Their learning progressed by 200 epochs, with an initial learning rate of 10 GLYPH<0> 2 and a reduction of 0.1 in 100 and 150 epochs. Also, the batch size is set to 128 and the weight decay of L 2 regularization is 10 GLYPH<0> 4 .\nTABLE 5. Performance of KD-SVP according to the number of singular vectors.\nTable 5 shows that when the proposed method is applied to KD-SVD, the performance is improved by 1.69% for CIFAR100 and 0.97% for Tiny-ImageNet. This experiment shows that singular vectors are learned much better by KD-SVP. Thus, the experiment demonstrates that the\nTABLE 6. Performance of KD-SVP according to the number of singular vectors.\nTABLE 7. Performance change whenever each step of SVP is removed.\nFIGURE 9. Performance comparison whenever removing each step of SVP. Red triangle means NaN.\napplication of SVP signiGLYPH<28>cantly improves the performance of a given network. Next, we analyze the performance of KD-SVP according to the number of used singular vectors. Table 6 shows that since more singular vectors give more information, they improve the performance of the student network.\n\nDemonstrates that KD-SVP improves network performance by learning singular vectors more effectively than KD-SVD, leading to performance gains when using more singular vectors.",
    "original_text": "The feature vectors obtained by KD-SVP are easier to analyze than original singular vectors. In KD-SVD, the sign ambiguity is removed by aligning the singular vectors of the student network based on the singular vector of the teacher network. In addition, KD-SVD learns only similar features to learn the manifold features and deGLYPH<28>nes the correlation by a radial basis function (RBF), which has smaller values as the distance becomes larger. In other words, singular vectors\nFIGURE 8. The performance change according to the intensity of adversarial attack.\nare post-processed in a very naive manner in KD-SVD. Therefore, if SVP plays a post-processing role, the learning accuracy will improve. The correlation between feature vectors obtained by SVP is computed by RBF, similarly to KD-SVD.TheCIFAR100andTiny-ImageNet are used in this experiment. The teacher network is VGG-16, and the student network is a condensed network that uses only one convolution of the same depth as VGG-16, which are described in Fig. 5(e) and 5(f). KD-SVD and KD-SVP are all learned with the same hyper-parameters. Their learning progressed by 200 epochs, with an initial learning rate of 10 GLYPH<0> 2 and a reduction of 0.1 in 100 and 150 epochs. Also, the batch size is set to 128 and the weight decay of L 2 regularization is 10 GLYPH<0> 4 .\nTABLE 5. Performance of KD-SVP according to the number of singular vectors.\nTable 5 shows that when the proposed method is applied to KD-SVD, the performance is improved by 1.69% for CIFAR100 and 0.97% for Tiny-ImageNet. This experiment shows that singular vectors are learned much better by KD-SVP. Thus, the experiment demonstrates that the\nTABLE 6. Performance of KD-SVP according to the number of singular vectors.\nTABLE 7. Performance change whenever each step of SVP is removed.\nFIGURE 9. Performance comparison whenever removing each step of SVP. Red triangle means NaN.\napplication of SVP signiGLYPH<28>cantly improves the performance of a given network. Next, we analyze the performance of KD-SVP according to the number of used singular vectors. Table 6 shows that since more singular vectors give more information, they improve the performance of the student network.",
    "context": "Demonstrates that KD-SVP improves network performance by learning singular vectors more effectively than KD-SVD, leading to performance gains when using more singular vectors.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      8,
      9
    ],
    "id": "4ad93a66dfa9bc84888159ae9a90b07e99098a28137029112b4eda0dd9f3805e"
  },
  {
    "text": "The effect of each part of the proposed SVP on the overall performance of a given network is analyzed. SVP consists of rotation (step 1), removing sign ambiguity (step 2), and coordinate conversion (step 3). The model used for this experiment is ResNet-18, and CIFAR100 is chosen as the dataset. Wemeasuredthe performance changes by removing each part one by one. Table 7 shows the results, and Fig. 9 shows the training plots.\nOf course, whenever each step is omitted, the overall performance deteriorates. If step 1 is omitted (rotation), the sign ambiguity cannot be eliminated accurately because the singular vectors on the two half-spheres are not superimposed on each other. The performance decreases by about 1.46% because the error increases as the distance from the center of the coordinates deviates. However, the performance degradation is smaller than when omitting other steps. When the coordinate conversion step is removed, manifold features are not sufGLYPH<28>ciently learned, which resulted in performance degradation by 5.01%. This occurred because singular vectors that are deGLYPH<28>ned in non-Euclidean space are hard to learn by Euclidean geometry. Finally, if the sign ambiguity removal of step 2 is absent, learning is impossible. This experiment shows that each step of SVP is indispensable for proper learning of singular vectors.\n\nDetails the impact of each step of the SVP process on network performance, demonstrating the necessity of each step for optimal singular vector learning.",
    "original_text": "The effect of each part of the proposed SVP on the overall performance of a given network is analyzed. SVP consists of rotation (step 1), removing sign ambiguity (step 2), and coordinate conversion (step 3). The model used for this experiment is ResNet-18, and CIFAR100 is chosen as the dataset. Wemeasuredthe performance changes by removing each part one by one. Table 7 shows the results, and Fig. 9 shows the training plots.\nOf course, whenever each step is omitted, the overall performance deteriorates. If step 1 is omitted (rotation), the sign ambiguity cannot be eliminated accurately because the singular vectors on the two half-spheres are not superimposed on each other. The performance decreases by about 1.46% because the error increases as the distance from the center of the coordinates deviates. However, the performance degradation is smaller than when omitting other steps. When the coordinate conversion step is removed, manifold features are not sufGLYPH<28>ciently learned, which resulted in performance degradation by 5.01%. This occurred because singular vectors that are deGLYPH<28>ned in non-Euclidean space are hard to learn by Euclidean geometry. Finally, if the sign ambiguity removal of step 2 is absent, learning is impossible. This experiment shows that each step of SVP is indispensable for proper learning of singular vectors.",
    "context": "Details the impact of each step of the SVP process on network performance, demonstrating the necessity of each step for optimal singular vector learning.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      9
    ],
    "id": "7f8eccd03d9917c715b580e59a70274cf9cf3c98461677fe6fd0aa6074778a08"
  },
  {
    "text": "One advantage of the proposed method is that converting singular vectors into Euclidean space enables to use Euclidean geometry. The merit makes a generic CNN learn the singular vectors. As another advantage, the proposed SVP is superior to other techniques that treat singular vectors as manifold features in terms of scalability. SVP outperforms conventional pooling methods such as GAP and GMP, and also it is more robust against noise. In addition, SVP is experimentally proven to provide further performance enhancement when it is applied to an existing knowledge distillation scheme such as [17]. On the other hand, SVP has a disadvantage that its computational complexity is somewhat higher than that of general pooling methods. Also, SVD requires burdensome computations, so it may be difGLYPH<28>cult to apply it directly to an embedded system or a mobile environment.\n\nHighlights the method’s scalability and performance benefits compared to existing techniques, while acknowledging computational complexity limitations.",
    "original_text": "One advantage of the proposed method is that converting singular vectors into Euclidean space enables to use Euclidean geometry. The merit makes a generic CNN learn the singular vectors. As another advantage, the proposed SVP is superior to other techniques that treat singular vectors as manifold features in terms of scalability. SVP outperforms conventional pooling methods such as GAP and GMP, and also it is more robust against noise. In addition, SVP is experimentally proven to provide further performance enhancement when it is applied to an existing knowledge distillation scheme such as [17]. On the other hand, SVP has a disadvantage that its computational complexity is somewhat higher than that of general pooling methods. Also, SVD requires burdensome computations, so it may be difGLYPH<28>cult to apply it directly to an embedded system or a mobile environment.",
    "context": "Highlights the method’s scalability and performance benefits compared to existing techniques, while acknowledging computational complexity limitations.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      9
    ],
    "id": "c935dde40e31dd09914acd77f8120dacf1ebdd556e807b16008ccd49b22cc8bd"
  },
  {
    "text": "SVDisone of the most important techniques in many areas of high-level data manipulation. Therefore, if SVD can be used in deep learning methods such as CNN, it could be useful for various purposes. However, singular vectors decomposed by SVD have not been widely used in CNNs yet, as they are generally difGLYPH<28>cult to handle. This study presented a starting point for effectively using singular vectors with essential information in deep learning. Although the proposed SVP has a simple form, it is very useful because it is about 36% more robust to adversarial attacks than GAP and produces 1.69% more improvement in knowledge distillation than the naGLYPH<239>ve solution [12]. Future work should be done to improve the performance of CNNs further by devising ways of more effectively using the information of singular vectors.\n\nIntroduces a novel approach (SVP) for utilizing singular vectors in CNNs, demonstrating improved robustness and performance compared to existing methods.",
    "original_text": "SVDisone of the most important techniques in many areas of high-level data manipulation. Therefore, if SVD can be used in deep learning methods such as CNN, it could be useful for various purposes. However, singular vectors decomposed by SVD have not been widely used in CNNs yet, as they are generally difGLYPH<28>cult to handle. This study presented a starting point for effectively using singular vectors with essential information in deep learning. Although the proposed SVP has a simple form, it is very useful because it is about 36% more robust to adversarial attacks than GAP and produces 1.69% more improvement in knowledge distillation than the naGLYPH<239>ve solution [12]. Future work should be done to improve the performance of CNNs further by devising ways of more effectively using the information of singular vectors.",
    "context": "Introduces a novel approach (SVP) for utilizing singular vectors in CNNs, demonstrating improved robustness and performance compared to existing methods.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      9
    ],
    "id": "273675854b71eb18e7ee710ed2c3006c09a73416d18c5410cc47473b7721e415"
  },
  {
    "text": "- [1] O. Alter, P. O. Brown, and D. Botstein, ''Singular value decomposition for genome-wide expression data processing and modeling,'' Proc. Nat. Acad. Sci. USA , vol. 97, no. 18, pp. 10101GLYPH<21>10106, Aug. 2000.\n- [2] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus, ''Exploiting linear structure within convolutional networks for efGLYPH<28>cient evaluation,'' in Proc. Adv. Neural Inf. Process. Syst. , 2014, pp. 1269GLYPH<21>1277.\n- [3] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl, ''Incremental singular value decomposition algorithms for highly scalable recommender systems,'' in Proc. 5th Int. Conf. Comput. Inf. Sci. , 2002, pp. 27GLYPH<21>28.\n- [4] C. Ionescu, O. Vantzos, and C. Sminchisescu, ''Matrix backpropagation for deep networks with structured layers,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , Dec. 2015, pp. 2965GLYPH<21>2973.\n- [5] Z. Huang, J. Wu, and L. Van Gool, ''Building deep networks on Grassmann manifolds,'' 2016, arXiv:1611.05742 . [Online]. Available: http://arxiv.org/abs/1611.05742\n- [6] R. Ranftl and V. Koltun, ''Deep fundamental matrix estimation,'' in Proc. Eur. Conf. Comput. Vis. (ECCV) , Sep. 2018, pp. 284GLYPH<21>299.\n\nSummarizes techniques for dimensionality reduction and matrix analysis applied to deep learning and genomics.",
    "original_text": "- [1] O. Alter, P. O. Brown, and D. Botstein, ''Singular value decomposition for genome-wide expression data processing and modeling,'' Proc. Nat. Acad. Sci. USA , vol. 97, no. 18, pp. 10101GLYPH<21>10106, Aug. 2000.\n- [2] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus, ''Exploiting linear structure within convolutional networks for efGLYPH<28>cient evaluation,'' in Proc. Adv. Neural Inf. Process. Syst. , 2014, pp. 1269GLYPH<21>1277.\n- [3] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl, ''Incremental singular value decomposition algorithms for highly scalable recommender systems,'' in Proc. 5th Int. Conf. Comput. Inf. Sci. , 2002, pp. 27GLYPH<21>28.\n- [4] C. Ionescu, O. Vantzos, and C. Sminchisescu, ''Matrix backpropagation for deep networks with structured layers,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , Dec. 2015, pp. 2965GLYPH<21>2973.\n- [5] Z. Huang, J. Wu, and L. Van Gool, ''Building deep networks on Grassmann manifolds,'' 2016, arXiv:1611.05742 . [Online]. Available: http://arxiv.org/abs/1611.05742\n- [6] R. Ranftl and V. Koltun, ''Deep fundamental matrix estimation,'' in Proc. Eur. Conf. Comput. Vis. (ECCV) , Sep. 2018, pp. 284GLYPH<21>299.",
    "context": "Summarizes techniques for dimensionality reduction and matrix analysis applied to deep learning and genomics.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      9
    ],
    "id": "a0fa410fc42efa47853e5134f94eeba979773ebca2504106a2bfa2fa22f2e0e4"
  },
  {
    "text": "- [7] S. Suwajanakorn, N. Snavely, J. J. Tompson, and M. Norouzi, ''Discovery of latent 3d keypoints via end-to-end geometric reasoning,'' in Proc. Adv. Neural Inf. Process. Syst. , 2018, pp. 2059GLYPH<21>2070.\n- [8] P. Li, J. Xie, Q. Wang, and Z. Gao, ''Towards faster training of global covariance pooling networks by iterative matrix square root normalization,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , Jun. 2018, pp. 947GLYPH<21>955.\n- [9] Q. Wang, P. Li, and L. Zhang, ''G2DeNet: Global Gaussian distribution embedding network and its application to visual recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jul. 2017, pp. 2730GLYPH<21>2739.\n- [10] X. Wei, Y. Zhang, Y. Gong, J. Zhang, and N. Zheng, ''Grassmann pooling as compact homogeneous bilinear pooling for GLYPH<28>ne-grained visual classiGLYPH<28>cation,'' in Proc. Eur. Conf. Comput. Vis. (ECCV) , Sep. 2018, pp. 355GLYPH<21>370.\n- [11] R. Bro, E. Acar, and T. G. Kolda, ''Resolving the sign ambiguity in the singular value decomposition,'' J. Chemometrics, A J. Chemometrics Soc. , vol. 22, no. 2, pp. 135GLYPH<21>140, 2008.\n- [12] S. H. Lee, D. H. Kim, and B. C. Song, ''Self-supervised knowledge distillation using singular value decomposition,'' in Proc. Eur. Conf. Comput. Vis. Springer, 2018, pp. 339GLYPH<21>354.\n- [13] L. Song, B. Du, L. Zhang, L. Zhang, J. Wu, and X. Li, ''Nonlocal patch based T-SVD for image inpainting: Algorithm and error analysis,'' in Proc. 32nd AAAI Conf. Artif. Intell. , 2018, pp. 2419GLYPH<21>2426.\n- [14] T.-Y. Lin and S. Maji, ''Improved bilinear pooling with CNNs,'' 2017, arXiv:1707.06772 . [Online]. Available: http://arxiv.org/abs/1707.06772\n- [15] Z. Gao, J. Xie, Q. Wang, and P. Li, ''Global second-order pooling convolutional networks,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2019, pp. 3024GLYPH<21>3033.\n- [16] M. Lin, Q. Chen, and S. Yan, ''Network in network,'' 2013, arXiv:1312.4400 . [Online]. Available: http://arxiv.org/abs/1312.4400\n- [17] X. Zhang and X. Zhang, ''Global learnable pooling with enhancing distinctive feature for image classiGLYPH<28>cation,'' IEEE Access , vol. 8, pp. 98539GLYPH<21>98547, 2020.\n- [18] H. Azizpour, A. S. Razavian, J. Sullivan, A. Maki, and S. Carlsson, ''From generic to speciGLYPH<28>c deep representations for visual recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW) , Jun. 2015, pp. 36GLYPH<21>45.\n- [19] B. Zhang, Q. Zhao, W. Feng, and S. Lyu, ''AlphaMEX: A smarter global pooling method for convolutional neural networks,'' Neurocomputing , vol. 321, pp. 36GLYPH<21>48, Dec. 2018.\n- [20] T.-Y. Lin, A. RoyChowdhury, and S. Maji, ''Bilinear CNN models for GLYPH<28>negrained visual recognition,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , Dec. 2015, pp. 1449GLYPH<21>1457.\n- [21] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, ''Bm3D image denoising with shape-adaptive principal component analysis,'' Tech. Rep., 2009.\n- [22] L. Hazelhoff, J. Han, and P. H. N. de With, ''Video-based fall detection in the home using principal component analysis,'' in Proc. Int. Conf. Adv. Concepts Intell. Vis. Syst. , Springer, 2008, pp. 298GLYPH<21>309.\n- [23] L. E. Blumenson, ''A derivation of n-dimensional spherical coordinates,'' Amer. Math. Monthly , vol. 67, no. 1, pp. 63GLYPH<21>66, Jan. 1960. [Online]. Available: http://www.jstor.org/stable/2308932\n- [24] S. Ioffe and C. Szegedy, ''Batch normalization: Accelerating deep network training by reducing internal covariate shift,'' in Int. Conf. Mach. Learn. , 2015, pp. 448GLYPH<21>456.\n- [25] S. Kullback and R. A. Leibler, ''On information and sufGLYPH<28>ciency,'' Ann. Math. Statist. , vol. 22, no. 1, pp. 79GLYPH<21>86, 1951.\n- [26] A. Krizhevsky and G. Hinton, ''Learning multiple layers of features from tiny images,'' Citeseer, Tech. Rep., 2009.\n- [27] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ''ImageNet classiGLYPH<28>cation with deep convolutional neural networks,'' in Proc. Adv. Neural Inf. Process. Syst. , 2012, pp. 1097GLYPH<21>1105.\n- [28] M. Abadi et al. , ''TensorFlow: A system for large-scale machine learning,'' in Proc. OSDI , vol. 16. 2016, pp. 265GLYPH<21>283.\n- [29] V. Nair and G. E. Hinton, ''RectiGLYPH<28>ed linear units improve restricted Boltzmann machines,'' in Proc. 27th Int. Conf. Mach. Learn. (ICML) , 2010, pp. 807GLYPH<21>814.\n- [30] K. He, X. Zhang, S. Ren, and J. Sun, ''Deep residual learning for image recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2016, pp. 770GLYPH<21>778.\n- [31] J. Kiefer and J. Wolfowitz, ''Stochastic estimation of the maximum of a regression function,'' Ann. Math. Statist. , vol. 23, no. 3, pp. 462GLYPH<21>466, Sep. 1952.\n- [32] Y. Nesterov, ''A method for unconstrained convex minimization problem with the rate of convergence o (1/k ^ 2),'' in Proc. Doklady AN USSR , vol. 269, 1983, pp. 543GLYPH<21>547.\n- [33] P. J. Rousseeuw, ''Silhouettes: A graphical aid to the interpretation and validation of cluster analysis,'' J. Comput. Appl. Math. , vol. 20, pp. 53GLYPH<21>65, Nov. 1987.\n\nThe document focuses on various techniques related to pooling, normalization, and dimensionality reduction, primarily within the context of deep learning for image recognition.",
    "original_text": "- [7] S. Suwajanakorn, N. Snavely, J. J. Tompson, and M. Norouzi, ''Discovery of latent 3d keypoints via end-to-end geometric reasoning,'' in Proc. Adv. Neural Inf. Process. Syst. , 2018, pp. 2059GLYPH<21>2070.\n- [8] P. Li, J. Xie, Q. Wang, and Z. Gao, ''Towards faster training of global covariance pooling networks by iterative matrix square root normalization,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , Jun. 2018, pp. 947GLYPH<21>955.\n- [9] Q. Wang, P. Li, and L. Zhang, ''G2DeNet: Global Gaussian distribution embedding network and its application to visual recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jul. 2017, pp. 2730GLYPH<21>2739.\n- [10] X. Wei, Y. Zhang, Y. Gong, J. Zhang, and N. Zheng, ''Grassmann pooling as compact homogeneous bilinear pooling for GLYPH<28>ne-grained visual classiGLYPH<28>cation,'' in Proc. Eur. Conf. Comput. Vis. (ECCV) , Sep. 2018, pp. 355GLYPH<21>370.\n- [11] R. Bro, E. Acar, and T. G. Kolda, ''Resolving the sign ambiguity in the singular value decomposition,'' J. Chemometrics, A J. Chemometrics Soc. , vol. 22, no. 2, pp. 135GLYPH<21>140, 2008.\n- [12] S. H. Lee, D. H. Kim, and B. C. Song, ''Self-supervised knowledge distillation using singular value decomposition,'' in Proc. Eur. Conf. Comput. Vis. Springer, 2018, pp. 339GLYPH<21>354.\n- [13] L. Song, B. Du, L. Zhang, L. Zhang, J. Wu, and X. Li, ''Nonlocal patch based T-SVD for image inpainting: Algorithm and error analysis,'' in Proc. 32nd AAAI Conf. Artif. Intell. , 2018, pp. 2419GLYPH<21>2426.\n- [14] T.-Y. Lin and S. Maji, ''Improved bilinear pooling with CNNs,'' 2017, arXiv:1707.06772 . [Online]. Available: http://arxiv.org/abs/1707.06772\n- [15] Z. Gao, J. Xie, Q. Wang, and P. Li, ''Global second-order pooling convolutional networks,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2019, pp. 3024GLYPH<21>3033.\n- [16] M. Lin, Q. Chen, and S. Yan, ''Network in network,'' 2013, arXiv:1312.4400 . [Online]. Available: http://arxiv.org/abs/1312.4400\n- [17] X. Zhang and X. Zhang, ''Global learnable pooling with enhancing distinctive feature for image classiGLYPH<28>cation,'' IEEE Access , vol. 8, pp. 98539GLYPH<21>98547, 2020.\n- [18] H. Azizpour, A. S. Razavian, J. Sullivan, A. Maki, and S. Carlsson, ''From generic to speciGLYPH<28>c deep representations for visual recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW) , Jun. 2015, pp. 36GLYPH<21>45.\n- [19] B. Zhang, Q. Zhao, W. Feng, and S. Lyu, ''AlphaMEX: A smarter global pooling method for convolutional neural networks,'' Neurocomputing , vol. 321, pp. 36GLYPH<21>48, Dec. 2018.\n- [20] T.-Y. Lin, A. RoyChowdhury, and S. Maji, ''Bilinear CNN models for GLYPH<28>negrained visual recognition,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , Dec. 2015, pp. 1449GLYPH<21>1457.\n- [21] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, ''Bm3D image denoising with shape-adaptive principal component analysis,'' Tech. Rep., 2009.\n- [22] L. Hazelhoff, J. Han, and P. H. N. de With, ''Video-based fall detection in the home using principal component analysis,'' in Proc. Int. Conf. Adv. Concepts Intell. Vis. Syst. , Springer, 2008, pp. 298GLYPH<21>309.\n- [23] L. E. Blumenson, ''A derivation of n-dimensional spherical coordinates,'' Amer. Math. Monthly , vol. 67, no. 1, pp. 63GLYPH<21>66, Jan. 1960. [Online]. Available: http://www.jstor.org/stable/2308932\n- [24] S. Ioffe and C. Szegedy, ''Batch normalization: Accelerating deep network training by reducing internal covariate shift,'' in Int. Conf. Mach. Learn. , 2015, pp. 448GLYPH<21>456.\n- [25] S. Kullback and R. A. Leibler, ''On information and sufGLYPH<28>ciency,'' Ann. Math. Statist. , vol. 22, no. 1, pp. 79GLYPH<21>86, 1951.\n- [26] A. Krizhevsky and G. Hinton, ''Learning multiple layers of features from tiny images,'' Citeseer, Tech. Rep., 2009.\n- [27] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ''ImageNet classiGLYPH<28>cation with deep convolutional neural networks,'' in Proc. Adv. Neural Inf. Process. Syst. , 2012, pp. 1097GLYPH<21>1105.\n- [28] M. Abadi et al. , ''TensorFlow: A system for large-scale machine learning,'' in Proc. OSDI , vol. 16. 2016, pp. 265GLYPH<21>283.\n- [29] V. Nair and G. E. Hinton, ''RectiGLYPH<28>ed linear units improve restricted Boltzmann machines,'' in Proc. 27th Int. Conf. Mach. Learn. (ICML) , 2010, pp. 807GLYPH<21>814.\n- [30] K. He, X. Zhang, S. Ren, and J. Sun, ''Deep residual learning for image recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2016, pp. 770GLYPH<21>778.\n- [31] J. Kiefer and J. Wolfowitz, ''Stochastic estimation of the maximum of a regression function,'' Ann. Math. Statist. , vol. 23, no. 3, pp. 462GLYPH<21>466, Sep. 1952.\n- [32] Y. Nesterov, ''A method for unconstrained convex minimization problem with the rate of convergence o (1/k ^ 2),'' in Proc. Doklady AN USSR , vol. 269, 1983, pp. 543GLYPH<21>547.\n- [33] P. J. Rousseeuw, ''Silhouettes: A graphical aid to the interpretation and validation of cluster analysis,'' J. Comput. Appl. Math. , vol. 20, pp. 53GLYPH<21>65, Nov. 1987.",
    "context": "The document focuses on various techniques related to pooling, normalization, and dimensionality reduction, primarily within the context of deep learning for image recognition.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      10
    ],
    "id": "c10f7d05286c2b9c2497493b90acaaac51d81b878e9df7360703bc83457849d2"
  },
  {
    "text": "- [34] I. J. Goodfellow, J. Shlens, and C. Szegedy, ''Explaining and harnessing adversarial examples,'' 2014, arXiv:1412.6572 . [Online]. Available: http://arxiv.org/abs/1412.6572\n- [35] A. Kurakin, I. Goodfellow, and S. Bengio, ''Adversarial examples in the physical world,'' 2016, arXiv:1607.02533 . [Online]. Available: http://arxiv.org/abs/1607.02533\n- [36] K. Simonyan and A. Zisserman, ''Very deep convolutional networks for large-scale image recognition,'' 2014, arXiv:1409.1556 . [Online]. Available: http://arxiv.org/abs/1409.1556\nSEUNGHYUN LEE (Associate Member, IEEE) received the B.S. degree in electronic engineering from Inha University, Incheon, South Korea, in 2017, where he is currently pursuing the Ph.D. degree. His research interests include computer vision and machine learning.\nBYUNG CHEOL SONG (Senior Member, IEEE) received the B.S., M.S., and Ph.D. degrees in electrical engineering from the Korea Advanced Institute of Science and Technology, Daejeon, South Korea, in 1994, 1996, and 2001, respectively. From 2001 to 2008, he was a Senior Engineer with the Digital Media Research and Development Center, Samsung Electronics Company Ltd., Suwon, South Korea. He joined the Department of Electronic Engineering, Inha University, Incheon,\nSouth Korea, in 2008, where he is currently a Professor. His research interests include general areas of image processing and computer vision.\n\nLists foundational research on adversarial examples and provides biographical information about the authors.",
    "original_text": "- [34] I. J. Goodfellow, J. Shlens, and C. Szegedy, ''Explaining and harnessing adversarial examples,'' 2014, arXiv:1412.6572 . [Online]. Available: http://arxiv.org/abs/1412.6572\n- [35] A. Kurakin, I. Goodfellow, and S. Bengio, ''Adversarial examples in the physical world,'' 2016, arXiv:1607.02533 . [Online]. Available: http://arxiv.org/abs/1607.02533\n- [36] K. Simonyan and A. Zisserman, ''Very deep convolutional networks for large-scale image recognition,'' 2014, arXiv:1409.1556 . [Online]. Available: http://arxiv.org/abs/1409.1556\nSEUNGHYUN LEE (Associate Member, IEEE) received the B.S. degree in electronic engineering from Inha University, Incheon, South Korea, in 2017, where he is currently pursuing the Ph.D. degree. His research interests include computer vision and machine learning.\nBYUNG CHEOL SONG (Senior Member, IEEE) received the B.S., M.S., and Ph.D. degrees in electrical engineering from the Korea Advanced Institute of Science and Technology, Daejeon, South Korea, in 1994, 1996, and 2001, respectively. From 2001 to 2008, he was a Senior Engineer with the Digital Media Research and Development Center, Samsung Electronics Company Ltd., Suwon, South Korea. He joined the Department of Electronic Engineering, Inha University, Incheon,\nSouth Korea, in 2008, where he is currently a Professor. His research interests include general areas of image processing and computer vision.",
    "context": "Lists foundational research on adversarial examples and provides biographical information about the authors.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      10
    ],
    "id": "ea24210c3bc8284895b1be69dfd57772134a6182003235bf06121d1002e11a62"
  },
  {
    "text": "Daewoon Seong , Deokmin Jeon , Ruchire Eranga Wijesinghe , Kibeom Park , Hyeree Kim , Euimin Lee , Mansik Jeon , Member, IEEE , and Jeehyun Kim , Member, IEEE\nAbstract -The primary optimization of the imaging speed of optical coherence tomography (OCT) has been keenly studied. In order to overcome the major speed limitation of spectral-domain OCT (SD-OCT), we developed an ultrahigh-speed SD-OCT system, with an A-scan rate of up to 1 MHz, using the method of space-time-division multiplexing (STDM). Multicameras comprising a single spectrometer were implemented in the developed ultrahigh-speed STDM method to eliminate the dead time of operation, whereas STDM was simultaneously employed to enable wide-range scanning measurements at a high speed. By successfully integrating the developed STDM method with GPU parallel processing, 8 vol/s for an image range of 250 × 250 × 2048 pixels (9 × 4.5 × 5 mm) was achieved, with an adjustable volume rate according to the required scanning speed and range. The examined STDM-OCT results of the customized optical thin film confirmed its feasibility for various fields that require rapid and wide-field scanning.\nstructures [1], [2]. Due to the method's potential resolution merits, OCT has been widely employed in various applications, such as ophthalmology [3], dentistry [4], [5], otolaryngology [6], [7], dermatology [8], [9], and even industrial fields [10], [11]. In terms of imaging speed, the development of high-speed real-time OCT has been required in order to observe morphological time variation of biological tissues with minimal motion artifacts [12], [13] and to fit a limited inspection time in industrial applications [14], [15].\nIndex Terms -Fourier optical signal processing, parallel processing, space-time-division multiplexing (STDM), spectraldomain optical coherence tomography (OCT), ultrahigh-speed imaging system.\n\nDetails the development and implementation of an ultrahigh-speed STDM OCT system achieving 8 vol/s for rapid imaging of optical thin films, highlighting its potential across diverse applications.",
    "original_text": "Daewoon Seong , Deokmin Jeon , Ruchire Eranga Wijesinghe , Kibeom Park , Hyeree Kim , Euimin Lee , Mansik Jeon , Member, IEEE , and Jeehyun Kim , Member, IEEE\nAbstract -The primary optimization of the imaging speed of optical coherence tomography (OCT) has been keenly studied. In order to overcome the major speed limitation of spectral-domain OCT (SD-OCT), we developed an ultrahigh-speed SD-OCT system, with an A-scan rate of up to 1 MHz, using the method of space-time-division multiplexing (STDM). Multicameras comprising a single spectrometer were implemented in the developed ultrahigh-speed STDM method to eliminate the dead time of operation, whereas STDM was simultaneously employed to enable wide-range scanning measurements at a high speed. By successfully integrating the developed STDM method with GPU parallel processing, 8 vol/s for an image range of 250 × 250 × 2048 pixels (9 × 4.5 × 5 mm) was achieved, with an adjustable volume rate according to the required scanning speed and range. The examined STDM-OCT results of the customized optical thin film confirmed its feasibility for various fields that require rapid and wide-field scanning.\nstructures [1], [2]. Due to the method's potential resolution merits, OCT has been widely employed in various applications, such as ophthalmology [3], dentistry [4], [5], otolaryngology [6], [7], dermatology [8], [9], and even industrial fields [10], [11]. In terms of imaging speed, the development of high-speed real-time OCT has been required in order to observe morphological time variation of biological tissues with minimal motion artifacts [12], [13] and to fit a limited inspection time in industrial applications [14], [15].\nIndex Terms -Fourier optical signal processing, parallel processing, space-time-division multiplexing (STDM), spectraldomain optical coherence tomography (OCT), ultrahigh-speed imaging system.",
    "context": "Details the development and implementation of an ultrahigh-speed STDM OCT system achieving 8 vol/s for rapid imaging of optical thin films, highlighting its potential across diverse applications.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      1
    ],
    "id": "4ea2f6384dd79e950f4f62a1dd74f65ac9807fad9e8607a908444bbba6eec2eb"
  },
  {
    "text": "O PTICAL coherence tomography (OCT) is a noninvasive and high-resolution imaging technique that enables 2-D imaging and 3-D imaging to measure morphological\nManuscript received January 27, 2021; revised March 27, 2021; accepted April 6, 2021. Date of publication April 16, 2021; date of current version May 4, 2021. This work was supported in part by the Bio & Medical Technology Development Program of the NRF funded by the Korean government (the Ministry of Science, ICT and Future Planning) under Grant 2017M3A9E2065282 and in part by the Korea Medical Device Development Fund grant funded by the Korea government (the Ministry of Science and ICT, the Ministry of Trade, Industry and Energy, the Ministry of Health & Welfare, the Ministry of Food and Drug Safety) under Grant KMDF-PR202009010055, 202011C13. The Associate Editor coordinating the review process was Jing Lei. (Daewoon Seong and Deokmin Jeon contributed equally to this work.) (Corresponding authors: Mansik Jeon; Jeehyun Kim.)\nDaewoon Seong, Deokmin Jeon, Hyeree Kim, Euimin Lee, Mansik Jeon, and Jeehyun Kim are with the School of Electronic and Electrical Engineering, College of IT Engineering, Kyungpook National University, Daegu 41566, South Korea (e-mail: smc7095@knu.ac.kr; dmjeon@knu.ac.kr; hleeworld@gmail.com; augustmini@knu.ac.kr; msjeon@knu.ac.kr; jeehk@ knu.ac.kr).\nRuchire Eranga Wijesinghe is with the Department of Materials and Mechanical Technology, Faculty of Technology, University of Sri Jayewardenepura, Homagama 10200, Sri Lanka (e-mail: erangawijesinghe@sjp.ac.lk). Kibeom Park is with the Department of Biomedical Engineering, Ulsan National Institute of Science and Technology, Ulsan 44919, South Korea (e-mail: pepl116@unist.ac.kr).\nThis article has supplementary downloadable material available at https://doi.org/10.1109/TIM.2021.3073701, provided by the authors.\nDigital Object Identifier 10.1109/TIM.2021.3073701\nThe imaging speed of the initially developed time-domain OCT (TD-OCT) was limited due to the motor-based scanning mechanism. To overcome this drawback, the Fourier-domain OCT (FD-OCT) was developed to directly obtain depthresolved spectra without unnecessary movement of mirror and with an improved system sensitivity [16], [17]. Swept-source OCT (SS-OCT) is one of the FD-OCT methods, which has a sufficient imaging speed and detects the spectrum of each wavelength by a photodetector that is mainly influenced by the sweeping rate of the wavelength-tunable light source [18], [19], whereas SS-OCT systems with an MHz high A-scan rate have been presented in previous reports [20]-[22]. Although SS-OCT provides the reduced sensitivity roll-off and high-speed A-scan rate [16], [23], axial resolution is slightly degraded, because of the longer wavelength of the source, and the cost of development is expansive compared to spectral-domain OCT (SD-OCT) [24], [25].\nSD-OCT is a widely applied and comparatively economical imaging technique with an exceptional resolution. However, the low speed of the detector has a direct negative impact on the system speed [26]. Thus, multiple attempts were made to enhance the system speed, where those studies are mainly focused on shortening the integration time of the line-scan camera, which needs to sacrifice the detectable maximum number of detector pixels [27]. To further improve the A-line rate, multiple line-scan cameras were used, and their operating times were conversely controlled to use the dead time for capturing [26], [27]. Based on this method, a 1-MHz A-line rate SD-OCT system was developed using four separate cameras [28]. However, the conventional multicamerabased system requires multiple independent spectrometers as much as a number of line-scan cameras, causing inevitable errors of spectrum difference at each spectrometer [26]-[28]. In addition, optical demultiplexer-based FD-OCT, which is used as a spectral analyzer, was introduced to further improve\nthe scanning speed up to multi-MHz [29], [30]. Nevertheless, the interference spectrum is diffracted by the limited channels of the demultiplexer, which attenuates the signal and decreases the resolution of frequency interval [29]. Moreover, multiple data acquisition (DAQ) boards and digitizers are required to improve the speed, leading to an increase in the total system prices of optical demultiplexer-based OCT [30].\nAs an alternative approach to enhancing the speed, the MHz streak-mode OCT, which utilizes an area-scan camera with a resonant scanner in the spectrometer to extend the integration time, was proposed. However, the signal-to-noise ratio (SNR) is much degraded compared with the conventional OCT, whereas nonuniformed exposure of the camera reduces the utilization of the duty cycle as well [31]. In addition, a parallel-OCT-based high-speed system, which improves the illumination power by applying a line focus to the sample, was reported in [32] and [33]. However, the limitations of sensitivity and imaging depth were nonnegligible [32], [34]. Moreover, as an alternative method for improving the scanning rate, multispatial scanning method-based OCT was developed, which utilized wavelength filters to split the beam of the sample arm; however, the axial resolution was reduced because of a higher number of scanning channel [35]. In addition, the space-division multiplexing (SDM) OCT, which illuminates the sample with multiple imaging beams with different optical delays, was also demonstrated [36], [37].\nIn this study, we developed ultrahigh-speed SD-OCT by optimizing the A-line rate up to 1 MHz, which was achieved by space-time-division multiplexing (STDM). To precisely synchronize the operating time and improve the processing speed, a control software platform was developed using C ++ with the Compute Unified Device Architecture (CUDA). The system performance, as an aspect of SDM, was quantitatively evaluated by sensitivity roll-off of each camera, and cross-sectional images were obtained. In addition, the time-division multiplexing (TDM) method, which improves the operating speed and minimizes the alignment error, was verified by the A-scan profiling results of the image-merging process. Moreover, to verify the capability of high-speed STDM-OCT for industrial applications, laboratory-customized subsurface structures of multilayered optical thin films (OTFs) were examined. Therefore, the demonstrated high-speed SD-OCT system has a potential for various applications, where high-speed scanning is essential, such as clinical and biomedical research areas.\n\nDetails the evolution of OCT imaging techniques, focusing on speed improvements and associated challenges (e.g., spectral resolution, cost).",
    "original_text": "O PTICAL coherence tomography (OCT) is a noninvasive and high-resolution imaging technique that enables 2-D imaging and 3-D imaging to measure morphological\nManuscript received January 27, 2021; revised March 27, 2021; accepted April 6, 2021. Date of publication April 16, 2021; date of current version May 4, 2021. This work was supported in part by the Bio & Medical Technology Development Program of the NRF funded by the Korean government (the Ministry of Science, ICT and Future Planning) under Grant 2017M3A9E2065282 and in part by the Korea Medical Device Development Fund grant funded by the Korea government (the Ministry of Science and ICT, the Ministry of Trade, Industry and Energy, the Ministry of Health & Welfare, the Ministry of Food and Drug Safety) under Grant KMDF-PR202009010055, 202011C13. The Associate Editor coordinating the review process was Jing Lei. (Daewoon Seong and Deokmin Jeon contributed equally to this work.) (Corresponding authors: Mansik Jeon; Jeehyun Kim.)\nDaewoon Seong, Deokmin Jeon, Hyeree Kim, Euimin Lee, Mansik Jeon, and Jeehyun Kim are with the School of Electronic and Electrical Engineering, College of IT Engineering, Kyungpook National University, Daegu 41566, South Korea (e-mail: smc7095@knu.ac.kr; dmjeon@knu.ac.kr; hleeworld@gmail.com; augustmini@knu.ac.kr; msjeon@knu.ac.kr; jeehk@ knu.ac.kr).\nRuchire Eranga Wijesinghe is with the Department of Materials and Mechanical Technology, Faculty of Technology, University of Sri Jayewardenepura, Homagama 10200, Sri Lanka (e-mail: erangawijesinghe@sjp.ac.lk). Kibeom Park is with the Department of Biomedical Engineering, Ulsan National Institute of Science and Technology, Ulsan 44919, South Korea (e-mail: pepl116@unist.ac.kr).\nThis article has supplementary downloadable material available at https://doi.org/10.1109/TIM.2021.3073701, provided by the authors.\nDigital Object Identifier 10.1109/TIM.2021.3073701\nThe imaging speed of the initially developed time-domain OCT (TD-OCT) was limited due to the motor-based scanning mechanism. To overcome this drawback, the Fourier-domain OCT (FD-OCT) was developed to directly obtain depthresolved spectra without unnecessary movement of mirror and with an improved system sensitivity [16], [17]. Swept-source OCT (SS-OCT) is one of the FD-OCT methods, which has a sufficient imaging speed and detects the spectrum of each wavelength by a photodetector that is mainly influenced by the sweeping rate of the wavelength-tunable light source [18], [19], whereas SS-OCT systems with an MHz high A-scan rate have been presented in previous reports [20]-[22]. Although SS-OCT provides the reduced sensitivity roll-off and high-speed A-scan rate [16], [23], axial resolution is slightly degraded, because of the longer wavelength of the source, and the cost of development is expansive compared to spectral-domain OCT (SD-OCT) [24], [25].\nSD-OCT is a widely applied and comparatively economical imaging technique with an exceptional resolution. However, the low speed of the detector has a direct negative impact on the system speed [26]. Thus, multiple attempts were made to enhance the system speed, where those studies are mainly focused on shortening the integration time of the line-scan camera, which needs to sacrifice the detectable maximum number of detector pixels [27]. To further improve the A-line rate, multiple line-scan cameras were used, and their operating times were conversely controlled to use the dead time for capturing [26], [27]. Based on this method, a 1-MHz A-line rate SD-OCT system was developed using four separate cameras [28]. However, the conventional multicamerabased system requires multiple independent spectrometers as much as a number of line-scan cameras, causing inevitable errors of spectrum difference at each spectrometer [26]-[28]. In addition, optical demultiplexer-based FD-OCT, which is used as a spectral analyzer, was introduced to further improve\nthe scanning speed up to multi-MHz [29], [30]. Nevertheless, the interference spectrum is diffracted by the limited channels of the demultiplexer, which attenuates the signal and decreases the resolution of frequency interval [29]. Moreover, multiple data acquisition (DAQ) boards and digitizers are required to improve the speed, leading to an increase in the total system prices of optical demultiplexer-based OCT [30].\nAs an alternative approach to enhancing the speed, the MHz streak-mode OCT, which utilizes an area-scan camera with a resonant scanner in the spectrometer to extend the integration time, was proposed. However, the signal-to-noise ratio (SNR) is much degraded compared with the conventional OCT, whereas nonuniformed exposure of the camera reduces the utilization of the duty cycle as well [31]. In addition, a parallel-OCT-based high-speed system, which improves the illumination power by applying a line focus to the sample, was reported in [32] and [33]. However, the limitations of sensitivity and imaging depth were nonnegligible [32], [34]. Moreover, as an alternative method for improving the scanning rate, multispatial scanning method-based OCT was developed, which utilized wavelength filters to split the beam of the sample arm; however, the axial resolution was reduced because of a higher number of scanning channel [35]. In addition, the space-division multiplexing (SDM) OCT, which illuminates the sample with multiple imaging beams with different optical delays, was also demonstrated [36], [37].\nIn this study, we developed ultrahigh-speed SD-OCT by optimizing the A-line rate up to 1 MHz, which was achieved by space-time-division multiplexing (STDM). To precisely synchronize the operating time and improve the processing speed, a control software platform was developed using C ++ with the Compute Unified Device Architecture (CUDA). The system performance, as an aspect of SDM, was quantitatively evaluated by sensitivity roll-off of each camera, and cross-sectional images were obtained. In addition, the time-division multiplexing (TDM) method, which improves the operating speed and minimizes the alignment error, was verified by the A-scan profiling results of the image-merging process. Moreover, to verify the capability of high-speed STDM-OCT for industrial applications, laboratory-customized subsurface structures of multilayered optical thin films (OTFs) were examined. Therefore, the demonstrated high-speed SD-OCT system has a potential for various applications, where high-speed scanning is essential, such as clinical and biomedical research areas.",
    "context": "Details the evolution of OCT imaging techniques, focusing on speed improvements and associated challenges (e.g., spectral resolution, cost).",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      1,
      2
    ],
    "id": "6991b79b0cb635b6784e1ade00e5d6c098f38b7976f778723783d5e3ea51cac8"
  },
  {
    "text": "The optical configuration of the STDM-OCT system is illustrated in Fig. 1(a). The light from the broadband light source (SLD-371-HP3, Superlum, Ireland), with a center wavelength of 838 nm, a full-width at half-maximum of 81 nm, and optical power of 27.2 mW, was transmitted to a 50:50 fiber coupler (TW850R5A2, Thorlabs, USA) and evenly distributed to each interferometer. Each interferometer is identically comprised of a 75:25 fiber coupler (TW850R3A2, Thorlabs, USA), a reference arm, and a sample arm. The reference arm consisted of the collimator (F780APC-850, Thorlabs, USA),\nFig. 1. (a) Optical configuration of the developed STDM-OCT system. (b) Photograph of multiscanners for the implementation of SDM. (c) Photograph of multicameras comprising the single spectrometer for TDM. BS: beam splitter; C: collimator; DG: diffraction grating; FC: fiber coupler; GVS: galvanometer scanner; L: lens; LSC: line-scan camera; LMS: linear-motor stage; M: mirror; S: sample; and SLD: superluminescent diode.\nlens (AC508-100-B, Thorlabs, USA), and mirror (PF10-03P01, Thorlabs, USA). Both the sample and reference arm optics were maintained equivalently, whereas 2-D and 3-D scans were obtained using a galvanometer scanner (GVS002, Thorlabs, USA). In the case of scanning OTF sample, a linear-motor stage (TS-P, YAMAHA, Japan) was used instead of a y -axis galvo scanner to achieve an accurate translation speed for 3-D inspection. To yield an ultrafast A-scan rate, the high-precision SDM method was used, whereas an adequate physical optical path difference between the fiber couplers of each interferometer was accurately regulated. The space-divided coherence signal was generated and properly transmitted to the spectrometer without any distortion. The utilized spectrometer was customized for STDM-OCT, where the conventional spectrometer described in [38] was optically modulated with the addition of beam splitter and additional line-scan camera enabling ultrahigh-speed A-scan rate up to 1 MHz. The diffracted interference signal by diffraction grating (WP-HD1800/840-50.8, Wasatch Photonics, USA) was split up by a beam splitter (BS032, Thorlabs, USA) and separately passed into two line-scan cameras with a resolution of 2048 pixels (e2v OCTOPLUS, TELEDYNE e2v, U.K.), which are the principal components for the application of the TDM technique described in Section II-B. Two frame grabbers (APX-3326A, AVAL DATA, Japan) and a DAQ board (NI PCIe-6363, National Instrument Corporation, USA) were utilized to precisely control the hardware compositions. The photograph of the multiscanners for the implementation of the SDM is presented in Fig. 1(b). In addition, the photograph of\nFig. 2. (a) Software flowchart of STDM-OCT. (b) Timing diagram of synchronized trigger signal sequences. API: application program interface; GPU: graphics processing unit.\na single spectrometer comprised of multicameras, as an aspect of TDM, is presented in Fig. 1(c).\n\nDetails the optical configuration of the STDM-OCT system, including component specifications and the implementation of SDM and TDM techniques.",
    "original_text": "The optical configuration of the STDM-OCT system is illustrated in Fig. 1(a). The light from the broadband light source (SLD-371-HP3, Superlum, Ireland), with a center wavelength of 838 nm, a full-width at half-maximum of 81 nm, and optical power of 27.2 mW, was transmitted to a 50:50 fiber coupler (TW850R5A2, Thorlabs, USA) and evenly distributed to each interferometer. Each interferometer is identically comprised of a 75:25 fiber coupler (TW850R3A2, Thorlabs, USA), a reference arm, and a sample arm. The reference arm consisted of the collimator (F780APC-850, Thorlabs, USA),\nFig. 1. (a) Optical configuration of the developed STDM-OCT system. (b) Photograph of multiscanners for the implementation of SDM. (c) Photograph of multicameras comprising the single spectrometer for TDM. BS: beam splitter; C: collimator; DG: diffraction grating; FC: fiber coupler; GVS: galvanometer scanner; L: lens; LSC: line-scan camera; LMS: linear-motor stage; M: mirror; S: sample; and SLD: superluminescent diode.\nlens (AC508-100-B, Thorlabs, USA), and mirror (PF10-03P01, Thorlabs, USA). Both the sample and reference arm optics were maintained equivalently, whereas 2-D and 3-D scans were obtained using a galvanometer scanner (GVS002, Thorlabs, USA). In the case of scanning OTF sample, a linear-motor stage (TS-P, YAMAHA, Japan) was used instead of a y -axis galvo scanner to achieve an accurate translation speed for 3-D inspection. To yield an ultrafast A-scan rate, the high-precision SDM method was used, whereas an adequate physical optical path difference between the fiber couplers of each interferometer was accurately regulated. The space-divided coherence signal was generated and properly transmitted to the spectrometer without any distortion. The utilized spectrometer was customized for STDM-OCT, where the conventional spectrometer described in [38] was optically modulated with the addition of beam splitter and additional line-scan camera enabling ultrahigh-speed A-scan rate up to 1 MHz. The diffracted interference signal by diffraction grating (WP-HD1800/840-50.8, Wasatch Photonics, USA) was split up by a beam splitter (BS032, Thorlabs, USA) and separately passed into two line-scan cameras with a resolution of 2048 pixels (e2v OCTOPLUS, TELEDYNE e2v, U.K.), which are the principal components for the application of the TDM technique described in Section II-B. Two frame grabbers (APX-3326A, AVAL DATA, Japan) and a DAQ board (NI PCIe-6363, National Instrument Corporation, USA) were utilized to precisely control the hardware compositions. The photograph of the multiscanners for the implementation of the SDM is presented in Fig. 1(b). In addition, the photograph of\nFig. 2. (a) Software flowchart of STDM-OCT. (b) Timing diagram of synchronized trigger signal sequences. API: application program interface; GPU: graphics processing unit.\na single spectrometer comprised of multicameras, as an aspect of TDM, is presented in Fig. 1(c).",
    "context": "Details the optical configuration of the STDM-OCT system, including component specifications and the implementation of SDM and TDM techniques.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      2,
      3
    ],
    "id": "cf9c4932434016213bdc8d8bcb5ea35d96ebf5fdef8bf065a4f5a41733cab90c"
  },
  {
    "text": "The developed software algorithm used for STDM-OCT is presented in Fig. 2(a). The developed custom control programming platform is built using C ++ , CUDA, and Qt. Before the initiation of optical scanning, parameter setting and initialization process, in which the scanning mode was selected and the range was matched to the properties of the sample, were performed. In addition, memories are automatically allocated according to previously initiated values. After the initialization stage, the STDM-OCT engine operation is initiated simultaneously with optical scanning and camera acquisition. The acquired raw signal is transferred to the graphics processing unit (GPU) (from host memory to GPU memory) and processed via the CUDA-based signal processing stage, which includes k-domain linearization, the fast Fourier transform, and log scaling. Through the GPU acceleration, data transmission, processing, and displaying process are properly operated in real-time without buffer overflow. After completing the signal processing of the obtained data in one cycle of x-galvo on GPU, the status of the engine (B-mode completed) is transferred and updated to the user interface (UI) via the callback function in the application programming interface (API). When the UI receives the B-mode completed signal, it starts the 'Get image' process by utilizing a callback function in the API for a memory copy of processed data in the OCT engine and displays the image. According to the API-based source code design, precise status synchronization and highly efficient source code management were achieved by loose coupling between the UI and OCT engine.\nTo control the hardware compositions in order to successfully achieve a 1-MHz A-line rate, synchronization with precise triggering is essentially required. The working sequence is represented by the main trigger, signals of two scanners, and two cameras, as presented in Fig. 2(b). To initiate the starting signal from UI, the DAQ board generates the main trigger, which ensures synchronized timing of the start of implementation. The operations of both scanners and frame grabbers are started simultaneously at the rising edge of the main trigger.\nTo enhance the speed of STDM-OCT, the SDM method, which simultaneously operates the multiscanners, was implemented. Although two sample arms were composed separately, the x -axis and y -axis galvo scanners of each sample arm were controlled similarly with an equal timing by triangular and square waves. Since the raster scanning method was applied to STDM-OCT, the x -axis scanner bidirectionally scans, while the y -axis scanner moves one step in synchronization at the end of each direction (shown in Fig. 2(b) as forward scanning and backward scanning). The path lengths of each scanner were precisely controlled to multiplex the dual scanning data in a single imaging depth. In the case of frame grabbers, identical but reversed trigger sequences of 250 kHz with 50% duty cycle were transferred to each camera to fully utilize the duty cycle without a dead time, which is called TDM in the proposed scheme. Therefore, two continuous A-lines were captured in one period of camera trigger; it guarantees 500 kHz of the effective imaging rate, which is twice faster than the maximum sampling rate of the line-scan camera. These captured A-lines of each camera were alternately composed of B-scan images. Therefore, the speed of STDM-OCT was further increased to 1 MHz by successfully implementing the integrated STDM method.\n\nDetails the software algorithm and hardware synchronization required to achieve a 1 MHz A-line rate through integrated STDM method.",
    "original_text": "The developed software algorithm used for STDM-OCT is presented in Fig. 2(a). The developed custom control programming platform is built using C ++ , CUDA, and Qt. Before the initiation of optical scanning, parameter setting and initialization process, in which the scanning mode was selected and the range was matched to the properties of the sample, were performed. In addition, memories are automatically allocated according to previously initiated values. After the initialization stage, the STDM-OCT engine operation is initiated simultaneously with optical scanning and camera acquisition. The acquired raw signal is transferred to the graphics processing unit (GPU) (from host memory to GPU memory) and processed via the CUDA-based signal processing stage, which includes k-domain linearization, the fast Fourier transform, and log scaling. Through the GPU acceleration, data transmission, processing, and displaying process are properly operated in real-time without buffer overflow. After completing the signal processing of the obtained data in one cycle of x-galvo on GPU, the status of the engine (B-mode completed) is transferred and updated to the user interface (UI) via the callback function in the application programming interface (API). When the UI receives the B-mode completed signal, it starts the 'Get image' process by utilizing a callback function in the API for a memory copy of processed data in the OCT engine and displays the image. According to the API-based source code design, precise status synchronization and highly efficient source code management were achieved by loose coupling between the UI and OCT engine.\nTo control the hardware compositions in order to successfully achieve a 1-MHz A-line rate, synchronization with precise triggering is essentially required. The working sequence is represented by the main trigger, signals of two scanners, and two cameras, as presented in Fig. 2(b). To initiate the starting signal from UI, the DAQ board generates the main trigger, which ensures synchronized timing of the start of implementation. The operations of both scanners and frame grabbers are started simultaneously at the rising edge of the main trigger.\nTo enhance the speed of STDM-OCT, the SDM method, which simultaneously operates the multiscanners, was implemented. Although two sample arms were composed separately, the x -axis and y -axis galvo scanners of each sample arm were controlled similarly with an equal timing by triangular and square waves. Since the raster scanning method was applied to STDM-OCT, the x -axis scanner bidirectionally scans, while the y -axis scanner moves one step in synchronization at the end of each direction (shown in Fig. 2(b) as forward scanning and backward scanning). The path lengths of each scanner were precisely controlled to multiplex the dual scanning data in a single imaging depth. In the case of frame grabbers, identical but reversed trigger sequences of 250 kHz with 50% duty cycle were transferred to each camera to fully utilize the duty cycle without a dead time, which is called TDM in the proposed scheme. Therefore, two continuous A-lines were captured in one period of camera trigger; it guarantees 500 kHz of the effective imaging rate, which is twice faster than the maximum sampling rate of the line-scan camera. These captured A-lines of each camera were alternately composed of B-scan images. Therefore, the speed of STDM-OCT was further increased to 1 MHz by successfully implementing the integrated STDM method.",
    "context": "Details the software algorithm and hardware synchronization required to achieve a 1 MHz A-line rate through integrated STDM method.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      3
    ],
    "id": "15b1bae1f79839dad95fb6f3e1ec83694bced71c46e7ee2597f48321b93b4d38"
  },
  {
    "text": "The image merging process of STDM-OCT with description and generalized configuration of each step is shown in Fig. 3. Since we implemented raster scanning (forward scanning and backward scanning described in Fig. 2 in detail) to fully utilize the duty cycle, the image merging process to obtain the combined B-scan image of each direction is essentially required. As shown in Fig. 3(a) and (b), the B-scan data of both cameras were comprised with both back-and-forth scanning data. We use black for camera #1 and red for camera #2 to distinguish the data of each camera and select representative lines (L1-L4). Fig. 3(c) and (d) demonstrates the merged image of both directions (L1 and L3 for forward and L2 and L4 for backward), and the positions of each line are determined by the description. According to the case of forward scanning presented in Fig. 3(c), the odd-numbered A-lines of merged B-scan images were captured by camera #1 and the even-numbered A-lines by camera #2. As a case of backward scanning shown in Fig. 3(d), a flipping process is additionally required to compensate for the reversed scanning direction of the x -axis scanner after performing the identical merging process of forward scanning. Therefore, the flipping step is applied to Fig. 3(d) and has obtained completed B-scan data, as shown in Fig. 3(e). As an aspect of time-consuming, image merging proceeds in real-time with scanning, data processing, and displaying.\n\nDetails the raster scanning process and image merging steps for combining B-scan data from two cameras, including a flipping process for backward scanning.",
    "original_text": "The image merging process of STDM-OCT with description and generalized configuration of each step is shown in Fig. 3. Since we implemented raster scanning (forward scanning and backward scanning described in Fig. 2 in detail) to fully utilize the duty cycle, the image merging process to obtain the combined B-scan image of each direction is essentially required. As shown in Fig. 3(a) and (b), the B-scan data of both cameras were comprised with both back-and-forth scanning data. We use black for camera #1 and red for camera #2 to distinguish the data of each camera and select representative lines (L1-L4). Fig. 3(c) and (d) demonstrates the merged image of both directions (L1 and L3 for forward and L2 and L4 for backward), and the positions of each line are determined by the description. According to the case of forward scanning presented in Fig. 3(c), the odd-numbered A-lines of merged B-scan images were captured by camera #1 and the even-numbered A-lines by camera #2. As a case of backward scanning shown in Fig. 3(d), a flipping process is additionally required to compensate for the reversed scanning direction of the x -axis scanner after performing the identical merging process of forward scanning. Therefore, the flipping step is applied to Fig. 3(d) and has obtained completed B-scan data, as shown in Fig. 3(e). As an aspect of time-consuming, image merging proceeds in real-time with scanning, data processing, and displaying.",
    "context": "Details the raster scanning process and image merging steps for combining B-scan data from two cameras, including a flipping process for backward scanning.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      3
    ],
    "id": "d3c74186766da8fc2f192329f10adfad3900c8f52ab3085782f6d19a2bfaa15e"
  },
  {
    "text": "Fig. 4 demonstrates the UI of the presented STDM-OCT system. UI was largely divided into two parts: signal displaying [see Fig. 4(a)-(e)] and parameter setting [see Fig. 4(f)]. Fig. 4(a) and (b) shows the real-time cross-sectional OCT image of cameras #1 and #2, respectively. Since SDM was\nFig. 3. Detailed description and generalized configuration of flowchart for imaging merging process of STDM-OCT. (a) and (b) Before merging state. (c) and (d) Merged state of (a) and (b). (e) Flipped state of (d).\napplied in the presented system, Fig. 4(a) and (b) is composed of the dual signals obtained from two different scanners. Fig. 4(c) and (d) provides the extracted en face information of two scanners in real-time measured at different positions according to the preset region of interest in the depth direction. In addition, Fig. 4(e) simultaneously shows the OCT raw signals obtained from both cameras. The operational parameters of the STMD-OCT are demonstrated in Fig. 4(f), which are consisted of the scanning range of both axes, the scanning mode, and the region of interest. The actual operating video of the developed software is shown in Video 1 the Supplementary Material (two-axis galvanometer scanner) and Video 2 in the Supplementary Material (one-axis galvanometer scanner with the linear motor stage).\nFig. 4. UI of the STDM-OCT system. (a) OCT cross-sectional view of camera #1. (b) OCT cross-sectional view of camera #2. (c) Extracted en face view of scanner #1. (d) Extracted en face view of scanner #2. (e) Processed raw signals of both cameras. (f) Parameter setting part for STMD-OCT acquisition.\n\nDetails the STDM-OCT system's user interface, including real-time imaging, en face views, processed signals, and parameter settings.",
    "original_text": "Fig. 4 demonstrates the UI of the presented STDM-OCT system. UI was largely divided into two parts: signal displaying [see Fig. 4(a)-(e)] and parameter setting [see Fig. 4(f)]. Fig. 4(a) and (b) shows the real-time cross-sectional OCT image of cameras #1 and #2, respectively. Since SDM was\nFig. 3. Detailed description and generalized configuration of flowchart for imaging merging process of STDM-OCT. (a) and (b) Before merging state. (c) and (d) Merged state of (a) and (b). (e) Flipped state of (d).\napplied in the presented system, Fig. 4(a) and (b) is composed of the dual signals obtained from two different scanners. Fig. 4(c) and (d) provides the extracted en face information of two scanners in real-time measured at different positions according to the preset region of interest in the depth direction. In addition, Fig. 4(e) simultaneously shows the OCT raw signals obtained from both cameras. The operational parameters of the STMD-OCT are demonstrated in Fig. 4(f), which are consisted of the scanning range of both axes, the scanning mode, and the region of interest. The actual operating video of the developed software is shown in Video 1 the Supplementary Material (two-axis galvanometer scanner) and Video 2 in the Supplementary Material (one-axis galvanometer scanner with the linear motor stage).\nFig. 4. UI of the STDM-OCT system. (a) OCT cross-sectional view of camera #1. (b) OCT cross-sectional view of camera #2. (c) Extracted en face view of scanner #1. (d) Extracted en face view of scanner #2. (e) Processed raw signals of both cameras. (f) Parameter setting part for STMD-OCT acquisition.",
    "context": "Details the STDM-OCT system's user interface, including real-time imaging, en face views, processed signals, and parameter settings.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      3,
      4
    ],
    "id": "c37432cb7ec48ac732498b94125ef511665e7c9c2f641d955e48ca15a5f60eb3"
  },
  {
    "text": "To confirm the feasibility of STDM-OCT for industrial inspection applications, an OTF sample was fabricated. The internal structure of the OTF is largely divided into four layers: a protective film, a transparent film, a deco film, and a base film. The measured thicknesses of each layer were 100, 250, 150, and 100 µ m, respectively. Moreover, the measured total thickness of OTF was 700 µ m, including the vacuum gaps between each layer, which are 40, 20, and 40 µ m. Since the refractive indices of the sublayers in the OTF were different, customized OTF as an industrial sample is appropriate for evaluating the applicability of a high-speed STDM-OCT system. Moreover, the measured total surface area of the sample was 68 × 140 mm, which is suitable for the inspection of the internal structure using wide-field STDM-OCT.\n\nDetails the fabrication and specifications of the OTF sample used to assess STDM-OCT feasibility for industrial inspection.",
    "original_text": "To confirm the feasibility of STDM-OCT for industrial inspection applications, an OTF sample was fabricated. The internal structure of the OTF is largely divided into four layers: a protective film, a transparent film, a deco film, and a base film. The measured thicknesses of each layer were 100, 250, 150, and 100 µ m, respectively. Moreover, the measured total thickness of OTF was 700 µ m, including the vacuum gaps between each layer, which are 40, 20, and 40 µ m. Since the refractive indices of the sublayers in the OTF were different, customized OTF as an industrial sample is appropriate for evaluating the applicability of a high-speed STDM-OCT system. Moreover, the measured total surface area of the sample was 68 × 140 mm, which is suitable for the inspection of the internal structure using wide-field STDM-OCT.",
    "context": "Details the fabrication and specifications of the OTF sample used to assess STDM-OCT feasibility for industrial inspection.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      4
    ],
    "id": "d8512fd8bf3c05b2b8d3a55c87a1f69f76d16350ef631e687088a4d32668dc5b"
  },
  {
    "text": "To quantitatively analyze the performance of STDM-OCT, we evaluated the presented system as an aspect of SDM and TDM. First, the sensitivity of each camera at a 1-MHz A-line rate was measured at every 100 pixels (0.44-mm interval) in the range of 100th-800th, as presented in Fig. 5(a) and (b). In addition, each value of measured sensitivity was shown in Table I as well. Since the developed single spectrometer consists of two line-scan cameras using a beam splitter to identically divide the coherent signal for each detector, the measured sensitivity roll-off values of the two independently detected signals confirmed the almost equivalent performance. The averaged sensitivity difference between the two cameras is 2.5 dB, and the measured peak sensitivities (at the 100th pixel) were 139 and 137 dB, respectively. To demonstrate the degree of image quality degradation in th depth direction, B-scan images of IR-card were obtained, which are shown in Fig. 5(c) and (d) while varying depth position from the zero-path delay to 2.6 mm. The measured pixel range of B-scan images in the depth direction was distinguished by edge colors (red and blue). Following the edge colors, Fig. 5(c) and (d), which were STDM-OCT images of IR-card, was measured at 0-300 and 300-600, respectively. In addition, we obtained the combined\nFig. 5. Quantitative analysis of STDM-OCT performance. (a) Sensitivity fall-off graph of camera #1. (b) Sensitivity fall-off graph of camera #2. (c) and (d) Cross-sectional images of IR-card obtained at different depth positions according to the edge color to evaluate the performance of the SDM. (e) Combined A-scan profile corresponding to the yellow dashed line of Fig. 5(c) and (d).\nTABLE I MEASURED SENSITIVITY ROLL-OFF OF EACH CAMERA\n\n100, Measured sensitivity [dB.Camera #1 = 139. 100, Measured sensitivity [dB.Camera #2 = 137. 200, Measured sensitivity [dB.Camera #1 = 135. 200, Measured sensitivity [dB.Camera #2 = 132. 300, Measured sensitivity [dB.Camera #1 = 129. 300, Measured sensitivity [dB.Camera #2 = 126. 400, Measured sensitivity [dB.Camera #1 = 118. 400, Measured sensitivity [dB.Camera #2 = 120. 500, Measured sensitivity [dB.Camera #1 = 111. 500, Measured sensitivity [dB.Camera #2 = 108. 600, Measured sensitivity [dB.Camera #1 = 108. 600, Measured sensitivity [dB.Camera #2 = 105. 700, Measured sensitivity [dB.Camera #1 = 99. 700, Measured sensitivity [dB.Camera #2 = 96. 800, Measured sensitivity [dB.Camera #1 = 91. 800, Measured sensitivity [dB.Camera #2 = 86\ndepth-dependent A-scan profile [as presented in Fig. 5(e)], which was extracted from the depth information indicated in the yellow dashed line in Fig. 5(c) and (d). As emphasized in Fig. 5(e), four intensity peaks corresponding to each B-scan image (indicated in red and blue (1)-(4) intensity peaks) were extracted from the yellow dashed line in Fig. 5(c) and (d) for an intensity comparison of cross sections. The averaged intensity of the four blue intensity peaks [depth intensity corresponds to Fig. 5(d)] is 10.6%, which decreased compared with the averaged intensity of the four red intensity peaks [depth intensity corresponds to Fig. 5(c)]. However, the internal sublayers of the IR card were obviously observed in both graphs. These results indicated that the SDM method simultaneously utilizes a full detectable range of camera pixels, which can be applied to the STDM-OCT. Moreover, Table II demonstrates the result of quantitative performance evaluation of the developed STMD-OCT for various categories, including scanning speed, resolution, and imaging depth.\nFurthermore, to quantitatively assess the TDM method applied to STDM-OCT, we independently obtained two B-scan images from each camera (before merging) presented in Fig. 6(a) and (b), which are consisted of bidirectional images (demonstrated in Fig. 2(b) as forward scanning and backward scanning). To accurately obtain the scanned data with TDM, the image-merging process described in\nFig. 6. Verification of the result of image merging process of STMD-OCT, as described in Section II-C. (a) and (b) Cross-sectional IR detection card images of cameras #1 and #2. (c) and (d) Merged images of (a) and (b). (e) Flipped image of (d). (f)-(i) A-scan profiling results of L1 (and L1') to L4 (and L4') indicated in (a)-(d).\n\nA-scan rate, Obtained value = MHz. Scanning range (representative), Obtained value = 250 250 x 2048 pixels x 4.5 x 5 mm). 3D volume rate (representative), Obtained value = 8 volls. Sensitivity, Obtained value = 139 dB. 10 dB roll-off depth, Obtained value = 1.3 mm. Lateral resolution, Obtained value = . Axial resolution (in air), Obtained value = . Imaging depth (in air), Obtained value = 5 mm\nTABLE II OBTAINED QUANTITATIVE PERFORMANCE OF STDM-OCT\nSection II-C was essentially required. In order to clearly elucidate the merging process of TDM, four lines (L1-L4) in Fig. 6(a) and (b) were representatively selected. L1 and L3 are the 300th A-lines of the B-scan image, which is part of forward scanning [first to 500th in Fig. 6(a) and (b)] obtained by each camera. L2 and L4 are the 800th A-lines, which were included in backward scanning [500th to 1000th in Fig. 6(a) and (b)]. L1 and L3 (300th) and L2 and L4 (800th) are the same A-line positions of B-scan images, but the obtained structural information is different because cameras were alternatively operated, whereas the galvanometer scanner performed continuous scanning, which can be simultaneously verified by the A-scan profiling results in Fig. 6(e)-(h). Following the image merging process described\nFig. 7. (a) Top view of the OTF indicated by five yellow dashed lines (L1-L5). (b)-(f) Corresponding cross-sectional OCT images of the OTF samples sequentially measured at five representative lines presented in Fig. 7(a) as L1-L5. Both the red and blue bars, as presented in (b)-(f), represent the space-divided pixel range of each scanner, which covers 0-300 and 300-600, respectively.\nin Section II-C, L1 and L3, which are both 300th lines of B-scan images (forward scanning), were matched to L1' (599th) and L3' (600th) in Fig. 6(c), respectively. In addition, the finally obtained B-scan image of backward direction is shown in Fig. 6(e), which is the result of applying the flipping step to Fig. 6(d). In this way, L2 and L4, which are both 800th lines, corresponded to L2' (400th) and L4' (399th) in Fig. 6(d). Hence, these results indicate that the TDM utilizing sequentially operated multicameras in a single spectrometer was accurately implemented to STDM-OCT in order to achieve a 1-MHz A-line sampling rate.\n\nQuantitative analysis of STDM-OCT performance, including sensitivity fall-off measurements and B-scan image degradation evaluation to demonstrate SDM and TDM capabilities.",
    "original_text": "To quantitatively analyze the performance of STDM-OCT, we evaluated the presented system as an aspect of SDM and TDM. First, the sensitivity of each camera at a 1-MHz A-line rate was measured at every 100 pixels (0.44-mm interval) in the range of 100th-800th, as presented in Fig. 5(a) and (b). In addition, each value of measured sensitivity was shown in Table I as well. Since the developed single spectrometer consists of two line-scan cameras using a beam splitter to identically divide the coherent signal for each detector, the measured sensitivity roll-off values of the two independently detected signals confirmed the almost equivalent performance. The averaged sensitivity difference between the two cameras is 2.5 dB, and the measured peak sensitivities (at the 100th pixel) were 139 and 137 dB, respectively. To demonstrate the degree of image quality degradation in th depth direction, B-scan images of IR-card were obtained, which are shown in Fig. 5(c) and (d) while varying depth position from the zero-path delay to 2.6 mm. The measured pixel range of B-scan images in the depth direction was distinguished by edge colors (red and blue). Following the edge colors, Fig. 5(c) and (d), which were STDM-OCT images of IR-card, was measured at 0-300 and 300-600, respectively. In addition, we obtained the combined\nFig. 5. Quantitative analysis of STDM-OCT performance. (a) Sensitivity fall-off graph of camera #1. (b) Sensitivity fall-off graph of camera #2. (c) and (d) Cross-sectional images of IR-card obtained at different depth positions according to the edge color to evaluate the performance of the SDM. (e) Combined A-scan profile corresponding to the yellow dashed line of Fig. 5(c) and (d).\nTABLE I MEASURED SENSITIVITY ROLL-OFF OF EACH CAMERA\n\n100, Measured sensitivity [dB.Camera #1 = 139. 100, Measured sensitivity [dB.Camera #2 = 137. 200, Measured sensitivity [dB.Camera #1 = 135. 200, Measured sensitivity [dB.Camera #2 = 132. 300, Measured sensitivity [dB.Camera #1 = 129. 300, Measured sensitivity [dB.Camera #2 = 126. 400, Measured sensitivity [dB.Camera #1 = 118. 400, Measured sensitivity [dB.Camera #2 = 120. 500, Measured sensitivity [dB.Camera #1 = 111. 500, Measured sensitivity [dB.Camera #2 = 108. 600, Measured sensitivity [dB.Camera #1 = 108. 600, Measured sensitivity [dB.Camera #2 = 105. 700, Measured sensitivity [dB.Camera #1 = 99. 700, Measured sensitivity [dB.Camera #2 = 96. 800, Measured sensitivity [dB.Camera #1 = 91. 800, Measured sensitivity [dB.Camera #2 = 86\ndepth-dependent A-scan profile [as presented in Fig. 5(e)], which was extracted from the depth information indicated in the yellow dashed line in Fig. 5(c) and (d). As emphasized in Fig. 5(e), four intensity peaks corresponding to each B-scan image (indicated in red and blue (1)-(4) intensity peaks) were extracted from the yellow dashed line in Fig. 5(c) and (d) for an intensity comparison of cross sections. The averaged intensity of the four blue intensity peaks [depth intensity corresponds to Fig. 5(d)] is 10.6%, which decreased compared with the averaged intensity of the four red intensity peaks [depth intensity corresponds to Fig. 5(c)]. However, the internal sublayers of the IR card were obviously observed in both graphs. These results indicated that the SDM method simultaneously utilizes a full detectable range of camera pixels, which can be applied to the STDM-OCT. Moreover, Table II demonstrates the result of quantitative performance evaluation of the developed STMD-OCT for various categories, including scanning speed, resolution, and imaging depth.\nFurthermore, to quantitatively assess the TDM method applied to STDM-OCT, we independently obtained two B-scan images from each camera (before merging) presented in Fig. 6(a) and (b), which are consisted of bidirectional images (demonstrated in Fig. 2(b) as forward scanning and backward scanning). To accurately obtain the scanned data with TDM, the image-merging process described in\nFig. 6. Verification of the result of image merging process of STMD-OCT, as described in Section II-C. (a) and (b) Cross-sectional IR detection card images of cameras #1 and #2. (c) and (d) Merged images of (a) and (b). (e) Flipped image of (d). (f)-(i) A-scan profiling results of L1 (and L1') to L4 (and L4') indicated in (a)-(d).\n\nA-scan rate, Obtained value = MHz. Scanning range (representative), Obtained value = 250 250 x 2048 pixels x 4.5 x 5 mm). 3D volume rate (representative), Obtained value = 8 volls. Sensitivity, Obtained value = 139 dB. 10 dB roll-off depth, Obtained value = 1.3 mm. Lateral resolution, Obtained value = . Axial resolution (in air), Obtained value = . Imaging depth (in air), Obtained value = 5 mm\nTABLE II OBTAINED QUANTITATIVE PERFORMANCE OF STDM-OCT\nSection II-C was essentially required. In order to clearly elucidate the merging process of TDM, four lines (L1-L4) in Fig. 6(a) and (b) were representatively selected. L1 and L3 are the 300th A-lines of the B-scan image, which is part of forward scanning [first to 500th in Fig. 6(a) and (b)] obtained by each camera. L2 and L4 are the 800th A-lines, which were included in backward scanning [500th to 1000th in Fig. 6(a) and (b)]. L1 and L3 (300th) and L2 and L4 (800th) are the same A-line positions of B-scan images, but the obtained structural information is different because cameras were alternatively operated, whereas the galvanometer scanner performed continuous scanning, which can be simultaneously verified by the A-scan profiling results in Fig. 6(e)-(h). Following the image merging process described\nFig. 7. (a) Top view of the OTF indicated by five yellow dashed lines (L1-L5). (b)-(f) Corresponding cross-sectional OCT images of the OTF samples sequentially measured at five representative lines presented in Fig. 7(a) as L1-L5. Both the red and blue bars, as presented in (b)-(f), represent the space-divided pixel range of each scanner, which covers 0-300 and 300-600, respectively.\nin Section II-C, L1 and L3, which are both 300th lines of B-scan images (forward scanning), were matched to L1' (599th) and L3' (600th) in Fig. 6(c), respectively. In addition, the finally obtained B-scan image of backward direction is shown in Fig. 6(e), which is the result of applying the flipping step to Fig. 6(d). In this way, L2 and L4, which are both 800th lines, corresponded to L2' (400th) and L4' (399th) in Fig. 6(d). Hence, these results indicate that the TDM utilizing sequentially operated multicameras in a single spectrometer was accurately implemented to STDM-OCT in order to achieve a 1-MHz A-line sampling rate.",
    "context": "Quantitative analysis of STDM-OCT performance, including sensitivity fall-off measurements and B-scan image degradation evaluation to demonstrate SDM and TDM capabilities.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      4,
      5,
      6
    ],
    "id": "84309a28efbe4a24cc08b12ceff3fd4ca0939a5e3cfd896d1850b36e57153299"
  },
  {
    "text": "To demonstrate the applicability of STDM-OCT for industrial applications, laboratory-customized OTF was initially evaluated as a representative sample of industrial applications. Fabricated OTF sample was placed on a linear-motor stage, and we continued with the 2000 frames of B-scan with 70µ m intervals in the OTF inspection (totally, 2000 × 2000 × 2048 pixels), which consumes 8 s for the whole-range scanning of the sample. The obtained results of the OTF inspection through STDM-OCT are presented in Fig. 7. A photograph of the OTF is presented in Fig. 7(a), indicating five representative scanning positions (L1-L5). Moreover, the cross section extracted locations were 140th, 500th, 1000th, 1500th, and 1900th lines, which were measured at 0.56, 2, 4, 6, and 7.6 s, respectively. Cross-sectional images of the OTF measured by STDM-OCT-based inspection system from L1 to L5 are sequentially presented in Fig. 7(b)-(f). In addition, the scanning ranges of each scanner used for SDM are indicated by edge color and color bar (red and blue) in Fig. 7(a). As presented in Fig. 7(b)-(f), the layer structure of the OTF is well distinguished at every measuring position (L1-L5) during the whole-range scanning of the sample. Therefore, these results reveal that the STDM-OCT with a 1-MHz A-line rate is feasible for industrial applications, ensuring high accuracy and reliable quality with a minimum inspection time.\n\nDemonstrates the feasibility of STDM-OCT for industrial applications through detailed inspection results and scanning parameters.",
    "original_text": "To demonstrate the applicability of STDM-OCT for industrial applications, laboratory-customized OTF was initially evaluated as a representative sample of industrial applications. Fabricated OTF sample was placed on a linear-motor stage, and we continued with the 2000 frames of B-scan with 70µ m intervals in the OTF inspection (totally, 2000 × 2000 × 2048 pixels), which consumes 8 s for the whole-range scanning of the sample. The obtained results of the OTF inspection through STDM-OCT are presented in Fig. 7. A photograph of the OTF is presented in Fig. 7(a), indicating five representative scanning positions (L1-L5). Moreover, the cross section extracted locations were 140th, 500th, 1000th, 1500th, and 1900th lines, which were measured at 0.56, 2, 4, 6, and 7.6 s, respectively. Cross-sectional images of the OTF measured by STDM-OCT-based inspection system from L1 to L5 are sequentially presented in Fig. 7(b)-(f). In addition, the scanning ranges of each scanner used for SDM are indicated by edge color and color bar (red and blue) in Fig. 7(a). As presented in Fig. 7(b)-(f), the layer structure of the OTF is well distinguished at every measuring position (L1-L5) during the whole-range scanning of the sample. Therefore, these results reveal that the STDM-OCT with a 1-MHz A-line rate is feasible for industrial applications, ensuring high accuracy and reliable quality with a minimum inspection time.",
    "context": "Demonstrates the feasibility of STDM-OCT for industrial applications through detailed inspection results and scanning parameters.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      6
    ],
    "id": "9b5994418ebd01657e49306db316087747fb9205564c6a3bb2fca0c2e31e174d"
  },
  {
    "text": "Here, we demonstrated the SD-OCT of a 1-MHz A-line rate and introduced a single-spectrometer-based STDM method. Various approaches to enhance imaging speed have been keenly studied and demonstrated. Compared with the conventional method of using a multiple-camera-based system [28], the proposed STDM-OCT of the current research not only resolves the alignment difference error of each spectrometer but also minimizes power loss and effectively economizes the total cost of the system (single-spectrometer-based system configuration). Furthermore, STDM-OCT partially provides enhanced SNR, sensitivity (more than 130 dB), and imaging depth (more than 4 mm) with cost-effective development in comparison with the optical demultiplexer-based method [29], [30], the streak-mode OCT [31], and the parallel OCT [32], [34]. In addition, as an aspect of system price, which is a factor to broaden applications of OCT, SD-OCT has obvious merit compared to SS-OCT. Based on this fact, the developed STDM-OCT provides a 1-MHz A-scan rate, which reduces the inspection time, with comparably low cost to develop compared to SS-OCT. Furthermore, SS-OCT provides a high-speed A-scan rate; however, to the best of our knowledge, the maximum sweeping rate of the commercial swept source is up to 400 kHz, which can be sufficiently obtained by the developed STDM-OCT.\nIn the case of the SDM method applied to STDM-OCT, two scanners were separately used, where the path lengths were differently maintained to ensure a wide scanning range of the OTF sample. However, single-scanner-based conventional SDM can also be applied to match the sample properties. As an aspect of 3-D volumetric imaging by integrating GPU parallel processing with the STDM method, 8 vol/s for 250 × 250 × 2048 pixels (9 × 4.5 × 5 mm) was achieved, which covers a much wider scanning range maintaining a high speed compared with the conventional real-time SD-OCT systems [39]-[41]. Furthermore, the volume rate of STDM-OCT can be freely adjusted according to experimental conditions, such as 2 vol/s for 16 × 8 × 5 mm and 0.5 vol/s for 30 × 16 × 5 mm. In addition, the scanning speed and range can be further increased by applying a multiple-STDM method, which employs additional scanners and cameras. Since we applied GPU acceleration, signal processing and displaying (including cross-sectional images and extracted each scanner's en face images) are processed in real time. Only when the image save process is proceeded, displaying B-scan images are updated with preset interval, as shown in Video 2 in the Supplementary Material, while signal processing and saving are processed in real time. In addition, each scanner's spliced B-scan images can be obtained by applying our extracting method described in [42].\nHere, a customized OTF was chosen as a sample to confirm the feasibility of the presented ultrahigh-speed OCT system\nin industrial applications for the measurement of the final product to inspect the cracks, which causes huge losses and damage to the product. As an aspect of the thickness of the sample, the path length of the reference arm is adjustable to match the focal point without overlapping each scanner's pixel position. Based on the obtained results, STDM-OCT demonstrates the possibility of applying to various fields, which requires high-speed systems, including industrial product manufacturing and inspecting, such as ultrathin glass ( ∼ 100 µ m), polarizer ( ∼ 300 µ m), glass substrate ( ∼ 500 µ m), OTF ( ∼ 700 µ m), and light guide plate ( ∼ 1 mm), which are within an imaging range of the proposed system (4.4 mm) and even biomedical applications. Furthermore, the successful utilization of higher-speed data-transferring techniques and processing techniques and faster line-scan cameras can further enhance the speed of the system beyond the developed 1-MHz\nA-scan rate.\n\nDemonstrates the feasibility of a high-speed, cost-effective OCT system for industrial applications, specifically for detecting cracks in thin materials and expanding potential applications to biomedical imaging.",
    "original_text": "Here, we demonstrated the SD-OCT of a 1-MHz A-line rate and introduced a single-spectrometer-based STDM method. Various approaches to enhance imaging speed have been keenly studied and demonstrated. Compared with the conventional method of using a multiple-camera-based system [28], the proposed STDM-OCT of the current research not only resolves the alignment difference error of each spectrometer but also minimizes power loss and effectively economizes the total cost of the system (single-spectrometer-based system configuration). Furthermore, STDM-OCT partially provides enhanced SNR, sensitivity (more than 130 dB), and imaging depth (more than 4 mm) with cost-effective development in comparison with the optical demultiplexer-based method [29], [30], the streak-mode OCT [31], and the parallel OCT [32], [34]. In addition, as an aspect of system price, which is a factor to broaden applications of OCT, SD-OCT has obvious merit compared to SS-OCT. Based on this fact, the developed STDM-OCT provides a 1-MHz A-scan rate, which reduces the inspection time, with comparably low cost to develop compared to SS-OCT. Furthermore, SS-OCT provides a high-speed A-scan rate; however, to the best of our knowledge, the maximum sweeping rate of the commercial swept source is up to 400 kHz, which can be sufficiently obtained by the developed STDM-OCT.\nIn the case of the SDM method applied to STDM-OCT, two scanners were separately used, where the path lengths were differently maintained to ensure a wide scanning range of the OTF sample. However, single-scanner-based conventional SDM can also be applied to match the sample properties. As an aspect of 3-D volumetric imaging by integrating GPU parallel processing with the STDM method, 8 vol/s for 250 × 250 × 2048 pixels (9 × 4.5 × 5 mm) was achieved, which covers a much wider scanning range maintaining a high speed compared with the conventional real-time SD-OCT systems [39]-[41]. Furthermore, the volume rate of STDM-OCT can be freely adjusted according to experimental conditions, such as 2 vol/s for 16 × 8 × 5 mm and 0.5 vol/s for 30 × 16 × 5 mm. In addition, the scanning speed and range can be further increased by applying a multiple-STDM method, which employs additional scanners and cameras. Since we applied GPU acceleration, signal processing and displaying (including cross-sectional images and extracted each scanner's en face images) are processed in real time. Only when the image save process is proceeded, displaying B-scan images are updated with preset interval, as shown in Video 2 in the Supplementary Material, while signal processing and saving are processed in real time. In addition, each scanner's spliced B-scan images can be obtained by applying our extracting method described in [42].\nHere, a customized OTF was chosen as a sample to confirm the feasibility of the presented ultrahigh-speed OCT system\nin industrial applications for the measurement of the final product to inspect the cracks, which causes huge losses and damage to the product. As an aspect of the thickness of the sample, the path length of the reference arm is adjustable to match the focal point without overlapping each scanner's pixel position. Based on the obtained results, STDM-OCT demonstrates the possibility of applying to various fields, which requires high-speed systems, including industrial product manufacturing and inspecting, such as ultrathin glass ( ∼ 100 µ m), polarizer ( ∼ 300 µ m), glass substrate ( ∼ 500 µ m), OTF ( ∼ 700 µ m), and light guide plate ( ∼ 1 mm), which are within an imaging range of the proposed system (4.4 mm) and even biomedical applications. Furthermore, the successful utilization of higher-speed data-transferring techniques and processing techniques and faster line-scan cameras can further enhance the speed of the system beyond the developed 1-MHz\nA-scan rate.",
    "context": "Demonstrates the feasibility of a high-speed, cost-effective OCT system for industrial applications, specifically for detecting cracks in thin materials and expanding potential applications to biomedical imaging.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      6,
      7
    ],
    "id": "f440435aeaf72d17f912c1200fd3bbf254c3cb4d5807b9d56c88583ac35bfb15"
  },
  {
    "text": "In conclusion, the ultrahigh-speed SD-OCT system up to a 1-MHz A-scan rate was achieved by implementing the developed STDM method. We used multicameras in single spectrometer to resolve the existing limitations of the TDM method, such as alignment errors of each spectrometer, power loss of detected interference signal caused by the use of multiple couplers, and sacrificing the spectral resolution of the camera. In addition, multiscanners were simultaneously used to implement the SDM to cover a wide scanning range. To accurately and rapidly control the hardware operation timing while employing scanners and cameras with high performance, a C ++ -based software platform with CUDA and Qt was developed and utilized to improve the data processing speed. The quantitatively analyzed results successfully demonstrate the effective implementation of the STDM method for achieving a 1-MHz A-scan rate. In addition, we inspected the OTF as a sample to confirm the feasibility of the developed STDM-OCT system for industrial applications. The obtained results of the implemented STDM method demonstrate the feasibility of the presented system for both real-time volumetric imaging and wide-field scanning at a fast speed according to the required conditions. Hence, the proposed ultrahighspeed STDM-OCT shows promising results encouraging the application of SD-OCT to various fields, including biomedical and industrial measurement fields, such as the noninvasive quality test of the final product and defect inspection, essentially requiring fast scanning speed, while maintaining the advantages of the conventional SD-OCT system.\n\nDemonstrates the successful implementation of the STDM method for achieving a 1-MHz A-scan rate and confirms the system's feasibility for industrial applications like defect inspection.",
    "original_text": "In conclusion, the ultrahigh-speed SD-OCT system up to a 1-MHz A-scan rate was achieved by implementing the developed STDM method. We used multicameras in single spectrometer to resolve the existing limitations of the TDM method, such as alignment errors of each spectrometer, power loss of detected interference signal caused by the use of multiple couplers, and sacrificing the spectral resolution of the camera. In addition, multiscanners were simultaneously used to implement the SDM to cover a wide scanning range. To accurately and rapidly control the hardware operation timing while employing scanners and cameras with high performance, a C ++ -based software platform with CUDA and Qt was developed and utilized to improve the data processing speed. The quantitatively analyzed results successfully demonstrate the effective implementation of the STDM method for achieving a 1-MHz A-scan rate. In addition, we inspected the OTF as a sample to confirm the feasibility of the developed STDM-OCT system for industrial applications. The obtained results of the implemented STDM method demonstrate the feasibility of the presented system for both real-time volumetric imaging and wide-field scanning at a fast speed according to the required conditions. Hence, the proposed ultrahighspeed STDM-OCT shows promising results encouraging the application of SD-OCT to various fields, including biomedical and industrial measurement fields, such as the noninvasive quality test of the final product and defect inspection, essentially requiring fast scanning speed, while maintaining the advantages of the conventional SD-OCT system.",
    "context": "Demonstrates the successful implementation of the STDM method for achieving a 1-MHz A-scan rate and confirms the system's feasibility for industrial applications like defect inspection.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      7
    ],
    "id": "1698910a20c6de3626ade4bc5134e2509393b6534482b8b3eaa71924680f3903"
  },
  {
    "text": "- [1] D. Huang et al. , 'Optical coherence tomography,' Science , vol. 254, no. 5035, pp. 1178-1181, 1991.\n- [2] J. M. Schmitt, 'Optical coherence tomography (OCT): A review,' IEEE J. Sel. Topics Quantum Electron. , vol. 5, no. 4, pp. 1205-1215, Jul. 1999.\n- [3] W. Drexler, U. Morgner, R. K. Ghanta, F. X. Kärtner, J. S. Schuman, and J. G. Fujimoto, 'Ultrahigh-resolution ophthalmic optical coherence tomography,' Nature Med. , vol. 7, no. 4, pp. 502-507, 2001.\n- [4] Y. S. Hsieh et al. , 'Dental optical coherence tomography,' Sensors , vol. 13, no. 7, pp. 8928-8949, Jul. 2013.\n- [5] J. Lee et al. , 'Decalcification using ethylenediaminetetraacetic acid for clear microstructure imaging of cochlea through optical coherence tomography,' J. Biomed. Opt. , vol. 21, no. 8, Mar. 2016, Art. no. 081204.\n- [6] S. Van der Jeught, J. J. J. Dirckx, J. R. M. Aerts, A. Bradu, A. G. Podoleanu, and J. A. N. Buytaert, 'Full-field thickness distribution of human tympanic membrane obtained with optical coherence tomography,' J. Assoc. Res. Otolaryngol. , vol. 14, no. 4, pp. 483-494, Aug. 2013.\n- [7] D. Seong et al. , ' In situ characterization of micro-vibration in natural latex membrane resembling tympanic membrane functionally using optical Doppler tomography,' Sensors , vol. 20, no. 1, p. 64, Dec. 2019.\n- [8] F. Liu et al. , 'A flexible touch-based fingerprint acquisition device and a benchmark database using optical coherence tomography,' IEEE Trans. Instrum. Meas. , vol. 69, no. 9, pp. 6518-6529, Sep. 2020.\n- [9] H. Sun et al. , 'Synchronous fingerprint acquisition system based on total internal reflection and optical coherence tomography,' IEEE Trans. Instrum. Meas. , vol. 69, no. 10, pp. 8452-8465, Oct. 2020.\n- [10] M. D. Duncan, M. Bashkansky, and J. Reintjes, 'Subsurface defect detection in materials using optical coherence tomography,' Opt. Exp. , vol. 2, pp. 540-545, Jun. 1998.\n- [11] Z. Chen, Y. Shen, W. Bao, P. Li, X. Wang, and Z. Ding, 'Identification of surface defects on glass by parallel spectral domain optical coherence tomography,' Opt. Exp. , vol. 23, no. 18, pp. 23634-23646, 2015.\n- [12] A. F. Low, G. J. Tearney, B. E. Bouma, and I.-K. Jang, 'Technology insight: Optical coherence tomography-Current status and future development,' Nature Clin. Pract. Cardiovascular Med. , vol. 3, no. 3, pp. 154-162, Mar. 2006.\n- [13] M. Gargesha, M. W. Jenkins, D. L. Wilson, and A. M. Rollins, 'High temporal resolution OCT using image-based retrospective gating,' Opt. Exp. , vol. 17, pp. 10786-10799, Jun. 2009.\n- [14] M. Bashkansky, M. Duncan, M. Kahn, D. Lewis, and J. Reintjes, 'Subsurface defect detection in ceramics by high-speed high-resolution optical coherent tomography,' Opt. Lett. , vol. 22, pp. 61-63, Jan. 1997.\n- [15] N. H. Cho, K. Park, J.-Y. Kim, Y. Jung, and J. Kim, 'Quantitative assessment of touch-screen panel by nondestructive inspection with three-dimensional real-time display optical coherence tomography,' Opt. Lasers Eng. , vol. 68, pp. 50-57, May 2015.\n- [16] M. A. Choma, M. V. Sarunic, C. Yang, and J. A. Izatt, 'Sensitivity advantage of swept source and Fourier domain optical coherence tomography,' Opt. Exp. , vol. 11, no. 18, pp. 2183-2189, 2003.\n- [17] R. Leitgeb, C. K. Hitzenberger, and A. F. Fercher, 'Performance of Fourier domain vs. time domain optical coherence tomography,' Opt. Exp. , vol. 11, no. 8, pp. 889-894, Apr. 2003.\n- [18] S. Chinn, E. Swanson, and J. Fujimoto, 'Optical coherence tomography using a frequency-tunable optical source,' Opt. Lett. , vol. 22, pp. 340-342, Mar. 1997.\n- [19] T. Klein and R. Huber, 'High-speed OCT light sources and systems,' Biomed. Opt. Exp. , vol. 8, pp. 828-859, Feb. 2017.\n- [20] T. Klein, W. Wieser, C. M. Eigenwillig, B. R. Biedermann, and R. Huber, 'Megahertz OCT for ultrawide-field retinal imaging with a 1050 nm Fourier domain mode-locked laser,' Opt. Exp. , vol. 19, pp. 3044-3062, Feb. 2011.\n- [21] T. Klein, W. Wieser, L. Reznicek, A. Neubauer, A. Kampik, and R. Huber, 'Multi-MHz retinal OCT,' Biomed. Opt. Exp. , vol. 4, pp. 1890-1908, Oct. 2013.\n- [22] J. P. Kolb, T. Pfeiffer, M. Eibl, H. Hakert, and R. Huber, 'Highresolution retinal swept source optical coherence tomography with an ultra-wideband Fourier-domain mode-locked laser at MHz A-scan rates,' Biomed. Opt. Exp. , vol. 9, pp. 120-130, Jan. 2018.\n- [23] F. Lavinsky and D. Lavinsky, 'Novel perspectives on swept-source optical coherence tomography,' Int. J. Retina Vitreous , vol. 2, no. 1, pp. 1-11, Dec. 2016.\n- [24] B. Potsaid et al. , 'Ultrahigh speed 1050nm swept source/Fourier domain OCT retinal and anterior segment imaging at 100,000 to 400,000 axial scans per second,' Opt. Exp. , vol. 18, pp. 20029-20048, Sep. 2010.\n- [25] A. Y. Alibhai, C. Or, and A. J. Witkin, 'Swept source optical coherence tomography: A review,' Current Ophthalmol. Rep. , vol. 6, pp. 7-16, Mar. 2018.\n- [26] L. An, G. Guan, and R. K. Wang, 'High-speed 1310 nm-band spectral domain optical coherence tomography at 184,000 lines per second,' J. Biomed. Opt. , vol. 16, no. 6, 2011, Art. no. 060506.\n- [27] L. An, P. Li, T. T. Shen, and R. Wang, 'High speed spectral domain optical coherence tomography for retinal imaging at 500,000 A-lines per second,' Biomed. Opt. Exp. , vol. 2, pp. 2770-2783, Oct. 2011.\n- [28] O. P. Kocaoglu, T. L. Turner, Z. Liu, and D. T. Miller, 'Adaptive optics optical coherence tomography at 1 MHz,' Biomed. Opt. Exp. , vol. 5, pp. 4186-4200, Dec. 2014.\n\nProvides a historical overview of optical coherence tomography techniques, including advancements in speed, resolution, and wavelength.",
    "original_text": "- [1] D. Huang et al. , 'Optical coherence tomography,' Science , vol. 254, no. 5035, pp. 1178-1181, 1991.\n- [2] J. M. Schmitt, 'Optical coherence tomography (OCT): A review,' IEEE J. Sel. Topics Quantum Electron. , vol. 5, no. 4, pp. 1205-1215, Jul. 1999.\n- [3] W. Drexler, U. Morgner, R. K. Ghanta, F. X. Kärtner, J. S. Schuman, and J. G. Fujimoto, 'Ultrahigh-resolution ophthalmic optical coherence tomography,' Nature Med. , vol. 7, no. 4, pp. 502-507, 2001.\n- [4] Y. S. Hsieh et al. , 'Dental optical coherence tomography,' Sensors , vol. 13, no. 7, pp. 8928-8949, Jul. 2013.\n- [5] J. Lee et al. , 'Decalcification using ethylenediaminetetraacetic acid for clear microstructure imaging of cochlea through optical coherence tomography,' J. Biomed. Opt. , vol. 21, no. 8, Mar. 2016, Art. no. 081204.\n- [6] S. Van der Jeught, J. J. J. Dirckx, J. R. M. Aerts, A. Bradu, A. G. Podoleanu, and J. A. N. Buytaert, 'Full-field thickness distribution of human tympanic membrane obtained with optical coherence tomography,' J. Assoc. Res. Otolaryngol. , vol. 14, no. 4, pp. 483-494, Aug. 2013.\n- [7] D. Seong et al. , ' In situ characterization of micro-vibration in natural latex membrane resembling tympanic membrane functionally using optical Doppler tomography,' Sensors , vol. 20, no. 1, p. 64, Dec. 2019.\n- [8] F. Liu et al. , 'A flexible touch-based fingerprint acquisition device and a benchmark database using optical coherence tomography,' IEEE Trans. Instrum. Meas. , vol. 69, no. 9, pp. 6518-6529, Sep. 2020.\n- [9] H. Sun et al. , 'Synchronous fingerprint acquisition system based on total internal reflection and optical coherence tomography,' IEEE Trans. Instrum. Meas. , vol. 69, no. 10, pp. 8452-8465, Oct. 2020.\n- [10] M. D. Duncan, M. Bashkansky, and J. Reintjes, 'Subsurface defect detection in materials using optical coherence tomography,' Opt. Exp. , vol. 2, pp. 540-545, Jun. 1998.\n- [11] Z. Chen, Y. Shen, W. Bao, P. Li, X. Wang, and Z. Ding, 'Identification of surface defects on glass by parallel spectral domain optical coherence tomography,' Opt. Exp. , vol. 23, no. 18, pp. 23634-23646, 2015.\n- [12] A. F. Low, G. J. Tearney, B. E. Bouma, and I.-K. Jang, 'Technology insight: Optical coherence tomography-Current status and future development,' Nature Clin. Pract. Cardiovascular Med. , vol. 3, no. 3, pp. 154-162, Mar. 2006.\n- [13] M. Gargesha, M. W. Jenkins, D. L. Wilson, and A. M. Rollins, 'High temporal resolution OCT using image-based retrospective gating,' Opt. Exp. , vol. 17, pp. 10786-10799, Jun. 2009.\n- [14] M. Bashkansky, M. Duncan, M. Kahn, D. Lewis, and J. Reintjes, 'Subsurface defect detection in ceramics by high-speed high-resolution optical coherent tomography,' Opt. Lett. , vol. 22, pp. 61-63, Jan. 1997.\n- [15] N. H. Cho, K. Park, J.-Y. Kim, Y. Jung, and J. Kim, 'Quantitative assessment of touch-screen panel by nondestructive inspection with three-dimensional real-time display optical coherence tomography,' Opt. Lasers Eng. , vol. 68, pp. 50-57, May 2015.\n- [16] M. A. Choma, M. V. Sarunic, C. Yang, and J. A. Izatt, 'Sensitivity advantage of swept source and Fourier domain optical coherence tomography,' Opt. Exp. , vol. 11, no. 18, pp. 2183-2189, 2003.\n- [17] R. Leitgeb, C. K. Hitzenberger, and A. F. Fercher, 'Performance of Fourier domain vs. time domain optical coherence tomography,' Opt. Exp. , vol. 11, no. 8, pp. 889-894, Apr. 2003.\n- [18] S. Chinn, E. Swanson, and J. Fujimoto, 'Optical coherence tomography using a frequency-tunable optical source,' Opt. Lett. , vol. 22, pp. 340-342, Mar. 1997.\n- [19] T. Klein and R. Huber, 'High-speed OCT light sources and systems,' Biomed. Opt. Exp. , vol. 8, pp. 828-859, Feb. 2017.\n- [20] T. Klein, W. Wieser, C. M. Eigenwillig, B. R. Biedermann, and R. Huber, 'Megahertz OCT for ultrawide-field retinal imaging with a 1050 nm Fourier domain mode-locked laser,' Opt. Exp. , vol. 19, pp. 3044-3062, Feb. 2011.\n- [21] T. Klein, W. Wieser, L. Reznicek, A. Neubauer, A. Kampik, and R. Huber, 'Multi-MHz retinal OCT,' Biomed. Opt. Exp. , vol. 4, pp. 1890-1908, Oct. 2013.\n- [22] J. P. Kolb, T. Pfeiffer, M. Eibl, H. Hakert, and R. Huber, 'Highresolution retinal swept source optical coherence tomography with an ultra-wideband Fourier-domain mode-locked laser at MHz A-scan rates,' Biomed. Opt. Exp. , vol. 9, pp. 120-130, Jan. 2018.\n- [23] F. Lavinsky and D. Lavinsky, 'Novel perspectives on swept-source optical coherence tomography,' Int. J. Retina Vitreous , vol. 2, no. 1, pp. 1-11, Dec. 2016.\n- [24] B. Potsaid et al. , 'Ultrahigh speed 1050nm swept source/Fourier domain OCT retinal and anterior segment imaging at 100,000 to 400,000 axial scans per second,' Opt. Exp. , vol. 18, pp. 20029-20048, Sep. 2010.\n- [25] A. Y. Alibhai, C. Or, and A. J. Witkin, 'Swept source optical coherence tomography: A review,' Current Ophthalmol. Rep. , vol. 6, pp. 7-16, Mar. 2018.\n- [26] L. An, G. Guan, and R. K. Wang, 'High-speed 1310 nm-band spectral domain optical coherence tomography at 184,000 lines per second,' J. Biomed. Opt. , vol. 16, no. 6, 2011, Art. no. 060506.\n- [27] L. An, P. Li, T. T. Shen, and R. Wang, 'High speed spectral domain optical coherence tomography for retinal imaging at 500,000 A-lines per second,' Biomed. Opt. Exp. , vol. 2, pp. 2770-2783, Oct. 2011.\n- [28] O. P. Kocaoglu, T. L. Turner, Z. Liu, and D. T. Miller, 'Adaptive optics optical coherence tomography at 1 MHz,' Biomed. Opt. Exp. , vol. 5, pp. 4186-4200, Dec. 2014.",
    "context": "Provides a historical overview of optical coherence tomography techniques, including advancements in speed, resolution, and wavelength.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      7
    ],
    "id": "dfbf8fb189bc44596caa9c63a166ca6e4ed73c970daf4f81de569feb6cca43d0"
  },
  {
    "text": "[29] K. Ohbayashi et al. , '60 MHz A-line rate ultra-high speed Fourierdomain optical coherence tomography,' in Proc. 12th Coherence Domain Opt. Methods Opt. Coherence Tomogr. Biomed. , Feb. 2008, Art. no. 68470M.\n[30] D.-H. Choi, H. Hiro-Oka, K. Shimizu, and K. Ohbayashi, 'Spectral domain optical coherence tomography of multi-MHz A-scan rates at 1310 nm range and real-time 4D-display up to 41 volumes/second,' Biomed. Opt. Exp. , vol. 3, pp. 3067-3086, Dec. 2012.\n[31] R. Wang, J. X. Yun, X. Yuan, R. Goodwin, R. R. Markwald, and B. Z. Gao, 'Megahertz streak-mode Fourier domain optical coherence tomography,' J. Biomed. Opt. , vol. 16, no. 6, 2011, Art. no. 066016.\n[32] J. Barrick, A. Doblas, M. R. Gardner, P. R. Sears, L. E. Ostrowski, and A. L. Oldenburg, 'High-speed and high-sensitivity parallel spectraldomain optical coherence tomography using a supercontinuum light source,' Opt. Lett. , vol. 41, pp. 5620-5623, Dec. 2016.\n[33] B. Fang et al. , 'Full-range line-field optical coherence tomography for high-accuracy measurements of optical lens,' IEEE Trans. Instrum. Meas. , vol. 69, no. 9, pp. 7180-7190, Sep. 2020.\n[34] K.-S. Lee et al. , 'High speed parallel spectral-domain OCT using spectrally encoded line-field illumination,' Appl. Phys. Lett. , vol. 112, no. 4, Jan. 2018, Art. no. 041102.\n[35] T. Mekonnen, A. Kourmatzis, J. Amatoury, and S. Cheng, 'Simultaneous multi-spatial scanning optical coherence tomography (OCT) based on spectrum-slicing of a broadband source,' Meas. Sci. Technol. , vol. 30, no. 4, Feb. 2019, Art. no. 045203.\n[36] C. Zhou, A. Alex, J. Rasakanthan, and Y. Ma, 'Space-division multiplexing optical coherence tomography,' Opt. Exp. , vol. 21, pp. 19219-19227, Aug. 2013.\n[37] Y. Huang, M. Badar, A. Nitkowski, A. Weinroth, N. Tansu, and C. Zhou, 'Wide-field high-speed space-division multiplexing optical coherence tomography using an integrated photonic device,' Biomed. Opt. Exp. , vol. 8, pp. 3856-3867, Aug. 2017.\n[38] D. Seong et al. , 'Dynamic compensation of path length difference in optical coherence tomography by an automatic temperature control system of optical fiber,' IEEE Access , vol. 8, pp. 77501-77510, 2020.\n[39] M. Sylwestrzak, M. Szkulmowski, D. Szlag, and P. Targowski, 'Realtime imaging for spectral optical coherence tomography with massively parallel data processing,' Photon. Lett. Poland , vol. 2, no. 3, pp. 137-139, Oct. 2010.\n[40] K. Zhang and J. U. Kang, 'Real-time 4D signal processing and visualization using graphics processing unit on a regular nonlinear-k Fourierdomain OCT system,' Opt. Exp. , vol. 18, no. 11, pp. 11772-11784, 2010.\n[41] K. Zhang and J. U. Kang, 'Real-time intraoperative 4D full-range FD-OCT based on the dual graphics processing units architecture for microsurgery guidance,' Biomed. Opt. Exp. , vol. 2, pp. 764-770, Apr. 2011.\n[42] S. A. Saleah et al. , 'Integrated quad-scanner strategy-based optical coherence tomography for the whole-directional volumetric imaging of a sample,' Sensors , vol. 21, no. 4, p. 1305, Feb. 2021.\nDaewoon Seong received the B.E. degree from the School of Electronics Engineering, Kyungpook National University, Daegu, Republic of Korea, in 2020.\nHe is currently a MD Researcher with the School of Electronic and Electrical Engineering, Kyungpook National University. His research focuses on developing high-speed and high-resolution optical imaging systems, photoacoustic microscopy, and optical coherence tomography applied to medical fields and industrial applications.\nDeokmin Jeon received the Ph.D. degree in electronics engineering from Kyungpook National University, Daegu, South Korea, in 2020.\nHe is currently with Samsung Electronics Company Ltd., Republic of Korea. His research interests include the development of biomedical and industrial imaging techniques, including optical coherence tomography, parallel computing, and 3-D vision technology.\nRuchire Eranga Wijesinghe received the B.Sc. and Ph.D. degrees in electronics engineering from Kyungpook National University, Daegu, South Korea, in 2012 and 2018, respectively.\nHe is currently a Senior Lecturer with the Department of Materials and Mechanical Technology, University of Sri Jayewardenepura, Homagama, Sri Lanka. His research interests are in the development of high-resolution novel biological and biomedical imaging techniques, including optical coherence tomography and microscopy for clinical utility.\nKibeom Park received the B.Sc. and Ph.D. degrees in electronics engineering from Kyungpook National University, Daegu, South Korea, in 2012 and 2018, respectively.\nHe is currently a Post-Doctoral Researcher with the Department of Department of Biomedical Engineering, Ulsan National Institute of Science and Technology, Ulsan, South Korea. His research interests are in the development of high-throughput optical imaging techniques in biomedical and industrial fields.\nHyeree Kim received the B.E. degree in avionic electronics engineering from Kyungwoon University, Gumi, Republic of Korea, in 2018.\nShe is currently an MS Researcher with the Electronics Engineering Department, Kyungpook National University, Daegu, South Korea. Her research interests are the development of biomedical imaging systems, including optical coherence tomography and optical instrument optimization design.\nEuimin Lee received the B.E. degree from the School of Electronic and Electrical Engineering, Kyungpook National University, Daegu, Republic of Korea, in 2020.\nHe is currently a MD Researcher with the School of Electronics Engineering, Kyungpook National University. His research focuses on development of imaging processing technique and convolutional neural networks using optical coherence tomography applied to industrial and agricultural application.\nMansik Jeon (Member, IEEE) received the Ph.D. degree in electronics engineering from Kyungpook National University, Daegu, Republic of Korea, in 2011.\nHe is currently an Associate Professor with the School of Electronics Engineering, Kyungpook National University. His research interests are in the development of nonionizing and noninvasive novel biomedical imaging techniques, including photoacoustic tomography, photoacoustic microscopy, optical coherence tomography, ultrasonic imaging, handheld scanner, and their clinical applications.\nJeehyun Kim (Member, IEEE) received the Ph.D. degree in biomedical engineering from The University of Texas at Austin, Austin, TX, USA, in 2004.\nHe has worked as a Post-Doctoral Researcher at the Beckman Laser Institute, University of California at Irvine, Irvine, CA, USA. He is currently an Associate Professor with Kyungpook National University, Daegu, Republic of Korea. His research interests are in biomedical imaging and sensing, neuroscience studies using multiphoton microscopy, photoacoustic imaging, and other novel applications of sensors.\n\nDetails of various optical coherence tomography advancements, including high-speed scanning, parallel processing, and 4D imaging techniques, primarily from Kyungpook National University researchers.",
    "original_text": "[29] K. Ohbayashi et al. , '60 MHz A-line rate ultra-high speed Fourierdomain optical coherence tomography,' in Proc. 12th Coherence Domain Opt. Methods Opt. Coherence Tomogr. Biomed. , Feb. 2008, Art. no. 68470M.\n[30] D.-H. Choi, H. Hiro-Oka, K. Shimizu, and K. Ohbayashi, 'Spectral domain optical coherence tomography of multi-MHz A-scan rates at 1310 nm range and real-time 4D-display up to 41 volumes/second,' Biomed. Opt. Exp. , vol. 3, pp. 3067-3086, Dec. 2012.\n[31] R. Wang, J. X. Yun, X. Yuan, R. Goodwin, R. R. Markwald, and B. Z. Gao, 'Megahertz streak-mode Fourier domain optical coherence tomography,' J. Biomed. Opt. , vol. 16, no. 6, 2011, Art. no. 066016.\n[32] J. Barrick, A. Doblas, M. R. Gardner, P. R. Sears, L. E. Ostrowski, and A. L. Oldenburg, 'High-speed and high-sensitivity parallel spectraldomain optical coherence tomography using a supercontinuum light source,' Opt. Lett. , vol. 41, pp. 5620-5623, Dec. 2016.\n[33] B. Fang et al. , 'Full-range line-field optical coherence tomography for high-accuracy measurements of optical lens,' IEEE Trans. Instrum. Meas. , vol. 69, no. 9, pp. 7180-7190, Sep. 2020.\n[34] K.-S. Lee et al. , 'High speed parallel spectral-domain OCT using spectrally encoded line-field illumination,' Appl. Phys. Lett. , vol. 112, no. 4, Jan. 2018, Art. no. 041102.\n[35] T. Mekonnen, A. Kourmatzis, J. Amatoury, and S. Cheng, 'Simultaneous multi-spatial scanning optical coherence tomography (OCT) based on spectrum-slicing of a broadband source,' Meas. Sci. Technol. , vol. 30, no. 4, Feb. 2019, Art. no. 045203.\n[36] C. Zhou, A. Alex, J. Rasakanthan, and Y. Ma, 'Space-division multiplexing optical coherence tomography,' Opt. Exp. , vol. 21, pp. 19219-19227, Aug. 2013.\n[37] Y. Huang, M. Badar, A. Nitkowski, A. Weinroth, N. Tansu, and C. Zhou, 'Wide-field high-speed space-division multiplexing optical coherence tomography using an integrated photonic device,' Biomed. Opt. Exp. , vol. 8, pp. 3856-3867, Aug. 2017.\n[38] D. Seong et al. , 'Dynamic compensation of path length difference in optical coherence tomography by an automatic temperature control system of optical fiber,' IEEE Access , vol. 8, pp. 77501-77510, 2020.\n[39] M. Sylwestrzak, M. Szkulmowski, D. Szlag, and P. Targowski, 'Realtime imaging for spectral optical coherence tomography with massively parallel data processing,' Photon. Lett. Poland , vol. 2, no. 3, pp. 137-139, Oct. 2010.\n[40] K. Zhang and J. U. Kang, 'Real-time 4D signal processing and visualization using graphics processing unit on a regular nonlinear-k Fourierdomain OCT system,' Opt. Exp. , vol. 18, no. 11, pp. 11772-11784, 2010.\n[41] K. Zhang and J. U. Kang, 'Real-time intraoperative 4D full-range FD-OCT based on the dual graphics processing units architecture for microsurgery guidance,' Biomed. Opt. Exp. , vol. 2, pp. 764-770, Apr. 2011.\n[42] S. A. Saleah et al. , 'Integrated quad-scanner strategy-based optical coherence tomography for the whole-directional volumetric imaging of a sample,' Sensors , vol. 21, no. 4, p. 1305, Feb. 2021.\nDaewoon Seong received the B.E. degree from the School of Electronics Engineering, Kyungpook National University, Daegu, Republic of Korea, in 2020.\nHe is currently a MD Researcher with the School of Electronic and Electrical Engineering, Kyungpook National University. His research focuses on developing high-speed and high-resolution optical imaging systems, photoacoustic microscopy, and optical coherence tomography applied to medical fields and industrial applications.\nDeokmin Jeon received the Ph.D. degree in electronics engineering from Kyungpook National University, Daegu, South Korea, in 2020.\nHe is currently with Samsung Electronics Company Ltd., Republic of Korea. His research interests include the development of biomedical and industrial imaging techniques, including optical coherence tomography, parallel computing, and 3-D vision technology.\nRuchire Eranga Wijesinghe received the B.Sc. and Ph.D. degrees in electronics engineering from Kyungpook National University, Daegu, South Korea, in 2012 and 2018, respectively.\nHe is currently a Senior Lecturer with the Department of Materials and Mechanical Technology, University of Sri Jayewardenepura, Homagama, Sri Lanka. His research interests are in the development of high-resolution novel biological and biomedical imaging techniques, including optical coherence tomography and microscopy for clinical utility.\nKibeom Park received the B.Sc. and Ph.D. degrees in electronics engineering from Kyungpook National University, Daegu, South Korea, in 2012 and 2018, respectively.\nHe is currently a Post-Doctoral Researcher with the Department of Department of Biomedical Engineering, Ulsan National Institute of Science and Technology, Ulsan, South Korea. His research interests are in the development of high-throughput optical imaging techniques in biomedical and industrial fields.\nHyeree Kim received the B.E. degree in avionic electronics engineering from Kyungwoon University, Gumi, Republic of Korea, in 2018.\nShe is currently an MS Researcher with the Electronics Engineering Department, Kyungpook National University, Daegu, South Korea. Her research interests are the development of biomedical imaging systems, including optical coherence tomography and optical instrument optimization design.\nEuimin Lee received the B.E. degree from the School of Electronic and Electrical Engineering, Kyungpook National University, Daegu, Republic of Korea, in 2020.\nHe is currently a MD Researcher with the School of Electronics Engineering, Kyungpook National University. His research focuses on development of imaging processing technique and convolutional neural networks using optical coherence tomography applied to industrial and agricultural application.\nMansik Jeon (Member, IEEE) received the Ph.D. degree in electronics engineering from Kyungpook National University, Daegu, Republic of Korea, in 2011.\nHe is currently an Associate Professor with the School of Electronics Engineering, Kyungpook National University. His research interests are in the development of nonionizing and noninvasive novel biomedical imaging techniques, including photoacoustic tomography, photoacoustic microscopy, optical coherence tomography, ultrasonic imaging, handheld scanner, and their clinical applications.\nJeehyun Kim (Member, IEEE) received the Ph.D. degree in biomedical engineering from The University of Texas at Austin, Austin, TX, USA, in 2004.\nHe has worked as a Post-Doctoral Researcher at the Beckman Laser Institute, University of California at Irvine, Irvine, CA, USA. He is currently an Associate Professor with Kyungpook National University, Daegu, Republic of Korea. His research interests are in biomedical imaging and sensing, neuroscience studies using multiphoton microscopy, photoacoustic imaging, and other novel applications of sensors.",
    "context": "Details of various optical coherence tomography advancements, including high-speed scanning, parallel processing, and 4D imaging techniques, primarily from Kyungpook National University researchers.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      8
    ],
    "id": "7c76d1d0daab13a0dfc3dee3b3ee08bc7270633448afa5c55fd51e697a16e4e0"
  },
  {
    "text": "Received 22 January 2023, accepted 11 February 2023, date of publication 22 February 2023, date of current version 27 February 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3246486\n\nSpecifies publication and version history details.",
    "original_text": "Received 22 January 2023, accepted 11 February 2023, date of publication 22 February 2023, date of current version 27 February 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3246486",
    "context": "Specifies publication and version history details.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      1
    ],
    "id": "549a022816caabb6ce69bfb8fd22635e67f0eff447d1e5e2ce0d923347a691f1"
  },
  {
    "text": "TYLER PHILLIPS 1 , (Member, IEEE), LAURENTIU D. MARINOVICI 2 , (Member, IEEE), CRAIG RIEGER 1 , (Senior Member, IEEE), AND ALICE ORRELL 2\nIdaho National Laboratory, Idaho Falls, ID 83415, USA\n1\n2 Pacific Northwest National Laboratory, Richland, WA 99354, USA\nCorresponding author: Laurentiu D. Marinovici (Laurentiu.Marinovici@pnnl.gov)\nThis work was supported in part by the U.S. Department of Energy Wind Energy Technologies Office, in part by the Pacific Northwest National Laboratory under Contract DE-AC05-76RL01830, and in part by the Idaho National Laboratory through the U.S. DOE Idaho Operations Office under Contract DE-AC07-05ID14517.\nABSTRACT The ability to robustly characterize transmission and distribution grid resilience requires the ability to perform time-scale analysis that interweaves communications, control, and power contributions. This consideration is important to ensuring an understanding of how each individual aspect can affect the resulting systemic resilience. The combination of co-simulation of these time-based characteristics and a resilience-specific metric provides a likely method to inform both design planning and implementation/operational goals to ensure resilience in power systems. The Microgrids, Infrastructure Resilience, and Advanced Controls Launchpad co-simulation platform (MIRACL-CSP) has been developed to allow the modular integration of power grid models with control and metrics applications. This paper introduces MIRACL-CSPasafundamental platform to study and improve the resilient operation of a microgrid. It offers a holistic investigation environment for systemically comparing the cyber-physical resilience to natural and manmade events. We emphasize the importance/advantage of intertwining the distribution system simulator GridLAB-D with the Power Distribution Designing for Resilience (PowDDeR) application to analyze the resilience of the St. Mary's microgrid in Alaska. The resilience is evaluated in both short-term (frequency stability) and long-term (energy constrained) metrics. The results of the analysis of the St. Mary's microgrid show that there is a trade-off between the two. As inertia-based generation assets are taken off-line, shortterm resilience drops. However, the long-term resilience is retained longer as less fuel is being used.\nINDEX TERMS Resilience, co-simulation, microgrid, distributed wind, cyber-physical.\n\nIntroduces the MIRACL-CSP platform and presents a co-simulation analysis of the St. Mary's microgrid, highlighting a trade-off between short-term and long-term resilience metrics.",
    "original_text": "TYLER PHILLIPS 1 , (Member, IEEE), LAURENTIU D. MARINOVICI 2 , (Member, IEEE), CRAIG RIEGER 1 , (Senior Member, IEEE), AND ALICE ORRELL 2\nIdaho National Laboratory, Idaho Falls, ID 83415, USA\n1\n2 Pacific Northwest National Laboratory, Richland, WA 99354, USA\nCorresponding author: Laurentiu D. Marinovici (Laurentiu.Marinovici@pnnl.gov)\nThis work was supported in part by the U.S. Department of Energy Wind Energy Technologies Office, in part by the Pacific Northwest National Laboratory under Contract DE-AC05-76RL01830, and in part by the Idaho National Laboratory through the U.S. DOE Idaho Operations Office under Contract DE-AC07-05ID14517.\nABSTRACT The ability to robustly characterize transmission and distribution grid resilience requires the ability to perform time-scale analysis that interweaves communications, control, and power contributions. This consideration is important to ensuring an understanding of how each individual aspect can affect the resulting systemic resilience. The combination of co-simulation of these time-based characteristics and a resilience-specific metric provides a likely method to inform both design planning and implementation/operational goals to ensure resilience in power systems. The Microgrids, Infrastructure Resilience, and Advanced Controls Launchpad co-simulation platform (MIRACL-CSP) has been developed to allow the modular integration of power grid models with control and metrics applications. This paper introduces MIRACL-CSPasafundamental platform to study and improve the resilient operation of a microgrid. It offers a holistic investigation environment for systemically comparing the cyber-physical resilience to natural and manmade events. We emphasize the importance/advantage of intertwining the distribution system simulator GridLAB-D with the Power Distribution Designing for Resilience (PowDDeR) application to analyze the resilience of the St. Mary's microgrid in Alaska. The resilience is evaluated in both short-term (frequency stability) and long-term (energy constrained) metrics. The results of the analysis of the St. Mary's microgrid show that there is a trade-off between the two. As inertia-based generation assets are taken off-line, shortterm resilience drops. However, the long-term resilience is retained longer as less fuel is being used.\nINDEX TERMS Resilience, co-simulation, microgrid, distributed wind, cyber-physical.",
    "context": "Introduces the MIRACL-CSP platform and presents a co-simulation analysis of the St. Mary's microgrid, highlighting a trade-off between short-term and long-term resilience metrics.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      1
    ],
    "id": "b0a9d71205dcdd4cbabf1a8542ffb0b4f5c590fd758112a4c108d931f365e2b1"
  },
  {
    "text": "To assess options in the advancement of modern distribution system (MDS), a set of quantifying metrics are necessary to correlate a value proposition for industry. In alignment with the recently released National Electric Grid Security and Resilience Action Plan, 1 a framework will correlate the overall resilience of MDS options to performance degrading impacts from threats.\nThe associate editor coordinating the review of this manuscript and approving it for publication was R.K. Saket .\n1 https://obamawhitehouse.archives.gov/sites/whitehouse.gov/files/ images/National_Electric_Grid_Action_Plan_06Dec2016.pdf\nWithin the power system community, quantitative metrics have been proposed as mathematical formalism to objectively measure the resilience of a system. Based on how the power system and its associated controls perform, these resilience metrics quantify the impact in direct measures associated with loss of generation, ability to maintain critical functionality, and often an integration of the cyber-physical system characteristics. State-of-the-art resilience assessment and quantification methods are reviewed in [1], [2], and [3]. While [1] focuses on frameworks, resilience curves, and quantitative metrics, [2] presents the challenges faced by researchers and power utilities due to nonstandard frameworks and tries to release that burden by categorizing them.\nIn [3] the authors evaluate and compare common metrics for short- and long-term resilience assessments focusing on microgrids' potential for power system resilience improvement. The review of all proposed resilience metrics for electrical power systems, or any other complex system, is not within the scope of this study. However, an established resilience metric that allows for systemic comparison of distributed systems will be leveraged, as will be differentiated in what follows. The ability to recover from an attack, provided the attack is discovered within a fixed time interval, is quantified through the metric proposed in [4]. Metrics to assess the two stages of smart grid operation, that is the duration before a failure occurs and the recovery time after the event, are presented in [5]. The metrics in [6] are directed towards the hardness and asset health as measures of stress, and towards capacity and efficacy as measures of strain. The works of [7] and [8] introduce a quantitative metrics basis to integrate the cognitive, cyber-physical aspects, which should all be considered when defining solutions for resilience. This approach is applied together with considering the uncertainties of solar and hydro renewables in [9], [10], and [11]. In these studies the performance index that provides the basis for resilience is the system adaptive capacity and its inertia. It considers several MDS design variables including generation or demand response delivery capacity, reactive power, power network topology, and control system architecture.\nResilience of complex systems is not a short-term or long-term measure. It encompasses many time scales from prior to an event to potentially days or weeks after. This is shown notionally by the disturbance and impact resilience evaluation (DIRE) curve in Fig. 1. It can be seen that the resilience is broken into five different time scales, known as the ''R's'' of resilience: recon, resist, respond, recover, and restore. To account for this time-dependent behavior, a dynamic resilience study approach is explored in this paper. In the context of the electrical power grid, resilience depends on supportive and responsive relationships between all the components at the transmission and distribution levels. Mastering a set of capabilities that could help the system respond and adapt to adversity at each individual aspect in a timely and appropriately healthy manner is of utmost importance. For the quantitative metrics to robustly characterize the transmission and distribution grid resilience, there is need for them to capture the interactions of grid components in response to adversity. Therefore, it is imperative to be able to perform time-scale analysis of the intertwined communication, control, and power systems.\nThe study in [12] proposes co-simulation as means at different design stages to analyze the resilience of complex and multi-disciplinary cyber-physical systems. Co-simulation of domain-specific simulators is involved in [13] to study the resilience of microgrids including smart grid technologies connected through cyber networks. Similarly, [14] presents co-simulation as the approach when studying integrated energy systems resilience. Though studying system resilience is among the goals of the co-simulation, no metrics are\nFIGURE 1. The Disturbance and Impact Resilience (DIRE) curve showing ''R's'' or time scales of resilience. Image taken from [11].\nintegrated to evaluate it for multiple scenarios and at different time scales.\nIn this study, co-simulation considers the temporal nature of integrating disparate models to achieve relevant results. The benefit of having the resilience metric in [9], [10], and [11] in parallel is to be able to address in real time short-term and long-term resilience concerns when adaptive capacity and inertia problems arise. The need to integrate disparate models has been the subject of a number of architectures, including the standardized high level architecture, some decades ago. However, the ability to achieve this integration requires that the time scale of the integrated simulation recognizes the artifacts necessary to characterize a judgment, which for resilience considers the man-made and natural threats.\nUnder the Microgrids, Infrastructure Resilience, and Advanced Controls Launchpad (MIRACL) project 2 at PNNL, the MIRACL co-simulation platform (CSP) in [15] has been developed to provide a real-life-like utility operation system incorporating data monitoring and control systems. In this work, MIRACL-CSP is leveraged to convey distribution system simulation measurements to the resilience metrics at run time. Through the holistic approach of cosimulation, the platform developed in this paper offers an environment that allows the study of complex systems under the specific conditions of dynamic use cases, and, moreover, scaling not only the size of the analyzed distribution system, but also the number of adversaries that could affect it.\nThe paper is organized as follows. Section II introduces the particular components of the MIRACL-CSP built for this application. Section III details the use cases, while Section IV discusses the results of the current study. Conclusions end the manuscript in Section V.\n\nEstablishes the need for quantitative metrics to assess modern distribution system resilience, referencing existing research and proposing a co-simulation approach for dynamic analysis.",
    "original_text": "To assess options in the advancement of modern distribution system (MDS), a set of quantifying metrics are necessary to correlate a value proposition for industry. In alignment with the recently released National Electric Grid Security and Resilience Action Plan, 1 a framework will correlate the overall resilience of MDS options to performance degrading impacts from threats.\nThe associate editor coordinating the review of this manuscript and approving it for publication was R.K. Saket .\n1 https://obamawhitehouse.archives.gov/sites/whitehouse.gov/files/ images/National_Electric_Grid_Action_Plan_06Dec2016.pdf\nWithin the power system community, quantitative metrics have been proposed as mathematical formalism to objectively measure the resilience of a system. Based on how the power system and its associated controls perform, these resilience metrics quantify the impact in direct measures associated with loss of generation, ability to maintain critical functionality, and often an integration of the cyber-physical system characteristics. State-of-the-art resilience assessment and quantification methods are reviewed in [1], [2], and [3]. While [1] focuses on frameworks, resilience curves, and quantitative metrics, [2] presents the challenges faced by researchers and power utilities due to nonstandard frameworks and tries to release that burden by categorizing them.\nIn [3] the authors evaluate and compare common metrics for short- and long-term resilience assessments focusing on microgrids' potential for power system resilience improvement. The review of all proposed resilience metrics for electrical power systems, or any other complex system, is not within the scope of this study. However, an established resilience metric that allows for systemic comparison of distributed systems will be leveraged, as will be differentiated in what follows. The ability to recover from an attack, provided the attack is discovered within a fixed time interval, is quantified through the metric proposed in [4]. Metrics to assess the two stages of smart grid operation, that is the duration before a failure occurs and the recovery time after the event, are presented in [5]. The metrics in [6] are directed towards the hardness and asset health as measures of stress, and towards capacity and efficacy as measures of strain. The works of [7] and [8] introduce a quantitative metrics basis to integrate the cognitive, cyber-physical aspects, which should all be considered when defining solutions for resilience. This approach is applied together with considering the uncertainties of solar and hydro renewables in [9], [10], and [11]. In these studies the performance index that provides the basis for resilience is the system adaptive capacity and its inertia. It considers several MDS design variables including generation or demand response delivery capacity, reactive power, power network topology, and control system architecture.\nResilience of complex systems is not a short-term or long-term measure. It encompasses many time scales from prior to an event to potentially days or weeks after. This is shown notionally by the disturbance and impact resilience evaluation (DIRE) curve in Fig. 1. It can be seen that the resilience is broken into five different time scales, known as the ''R's'' of resilience: recon, resist, respond, recover, and restore. To account for this time-dependent behavior, a dynamic resilience study approach is explored in this paper. In the context of the electrical power grid, resilience depends on supportive and responsive relationships between all the components at the transmission and distribution levels. Mastering a set of capabilities that could help the system respond and adapt to adversity at each individual aspect in a timely and appropriately healthy manner is of utmost importance. For the quantitative metrics to robustly characterize the transmission and distribution grid resilience, there is need for them to capture the interactions of grid components in response to adversity. Therefore, it is imperative to be able to perform time-scale analysis of the intertwined communication, control, and power systems.\nThe study in [12] proposes co-simulation as means at different design stages to analyze the resilience of complex and multi-disciplinary cyber-physical systems. Co-simulation of domain-specific simulators is involved in [13] to study the resilience of microgrids including smart grid technologies connected through cyber networks. Similarly, [14] presents co-simulation as the approach when studying integrated energy systems resilience. Though studying system resilience is among the goals of the co-simulation, no metrics are\nFIGURE 1. The Disturbance and Impact Resilience (DIRE) curve showing ''R's'' or time scales of resilience. Image taken from [11].\nintegrated to evaluate it for multiple scenarios and at different time scales.\nIn this study, co-simulation considers the temporal nature of integrating disparate models to achieve relevant results. The benefit of having the resilience metric in [9], [10], and [11] in parallel is to be able to address in real time short-term and long-term resilience concerns when adaptive capacity and inertia problems arise. The need to integrate disparate models has been the subject of a number of architectures, including the standardized high level architecture, some decades ago. However, the ability to achieve this integration requires that the time scale of the integrated simulation recognizes the artifacts necessary to characterize a judgment, which for resilience considers the man-made and natural threats.\nUnder the Microgrids, Infrastructure Resilience, and Advanced Controls Launchpad (MIRACL) project 2 at PNNL, the MIRACL co-simulation platform (CSP) in [15] has been developed to provide a real-life-like utility operation system incorporating data monitoring and control systems. In this work, MIRACL-CSP is leveraged to convey distribution system simulation measurements to the resilience metrics at run time. Through the holistic approach of cosimulation, the platform developed in this paper offers an environment that allows the study of complex systems under the specific conditions of dynamic use cases, and, moreover, scaling not only the size of the analyzed distribution system, but also the number of adversaries that could affect it.\nThe paper is organized as follows. Section II introduces the particular components of the MIRACL-CSP built for this application. Section III details the use cases, while Section IV discusses the results of the current study. Conclusions end the manuscript in Section V.",
    "context": "Establishes the need for quantitative metrics to assess modern distribution system resilience, referencing existing research and proposing a co-simulation approach for dynamic analysis.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      1,
      2
    ],
    "id": "b66bb4a9f4cc49c2872133a43629b241c79547537059621dd1df5da93d97e43b"
  },
  {
    "text": "The Microgrids, Infrastructure Resilience, and Advanced Controls Launchpad co-simulation platform (MIRACL-CSP) [15] was developed to allow operational coordination between distribution system models and distribution system applications and technologies. This is particularly important\nfor cases where more realistic and dynamic data exchange is required while all simulation instances run in a synchronized manner regardless of their time periods, either continuous or discrete.\nMIRACL-CSP offers a modular integration of distribution systems modeled in GridLAB-D [16] and custom-built distribution system monitoring and control applications modeled in Python ™ through the Hierarchical Engine for Large Infrastructure Co-Simulation (HELICS) [17], [18] environment. Fig. 2 illustrates the integration of the power distribution system, control, and resilience modeling aspects for our cosimulation.\nThe software comprising the MIRACL-CSP has been packaged in a Docker image. It ensures rapid application development and testing. Moreover, it offers portability, deployment and execution on different platforms. The MIRACL-CSP depicted in Fig. 2 currently includes :\n- Ubuntu 3 20.4 as the base operating system,\n- HELICS 4 version 3 as the co-simulation environment,\n- GridLAB-D 5 as the distribution system modeling and simulation environment,\n- Python 6 3.9, with appropriate modules, as the wrapper around the utility control center (UCC) applications for monitoring, optimization-based control, and system resiliency metrics calculation with Power Distribution Designing for Resilience 7 (PowDDeR).\n\nDetails the technical specifications and components of the MIRACL-CSP co-simulation platform, emphasizing its modular design and portability.",
    "original_text": "The Microgrids, Infrastructure Resilience, and Advanced Controls Launchpad co-simulation platform (MIRACL-CSP) [15] was developed to allow operational coordination between distribution system models and distribution system applications and technologies. This is particularly important\nfor cases where more realistic and dynamic data exchange is required while all simulation instances run in a synchronized manner regardless of their time periods, either continuous or discrete.\nMIRACL-CSP offers a modular integration of distribution systems modeled in GridLAB-D [16] and custom-built distribution system monitoring and control applications modeled in Python ™ through the Hierarchical Engine for Large Infrastructure Co-Simulation (HELICS) [17], [18] environment. Fig. 2 illustrates the integration of the power distribution system, control, and resilience modeling aspects for our cosimulation.\nThe software comprising the MIRACL-CSP has been packaged in a Docker image. It ensures rapid application development and testing. Moreover, it offers portability, deployment and execution on different platforms. The MIRACL-CSP depicted in Fig. 2 currently includes :\n- Ubuntu 3 20.4 as the base operating system,\n- HELICS 4 version 3 as the co-simulation environment,\n- GridLAB-D 5 as the distribution system modeling and simulation environment,\n- Python 6 3.9, with appropriate modules, as the wrapper around the utility control center (UCC) applications for monitoring, optimization-based control, and system resiliency metrics calculation with Power Distribution Designing for Resilience 7 (PowDDeR).",
    "context": "Details the technical specifications and components of the MIRACL-CSP co-simulation platform, emphasizing its modular design and portability.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      2,
      3
    ],
    "id": "9a8ebcd7292f3abc39991b3b8742971b3c42cc58a18bb9c0e787f5f38f56cf15"
  },
  {
    "text": "At the core of the MIRACL-CSP is HELICS, an open-source co-simulation platform that coordinates off-the-shelf simulators and applications, including electric transmission systems, electric distribution systems, communication systems, market models, and end-use loads [17], [18]. HELICS performs the two main functions of a co-simulation, that is time management and synchronization of simulators, in particular, GridLAB-D and Python federates, as well as data exchanges between them.\n\nDetails the core co-simulation platform (HELICS) and its role in coordinating various simulators for grid analysis.",
    "original_text": "At the core of the MIRACL-CSP is HELICS, an open-source co-simulation platform that coordinates off-the-shelf simulators and applications, including electric transmission systems, electric distribution systems, communication systems, market models, and end-use loads [17], [18]. HELICS performs the two main functions of a co-simulation, that is time management and synchronization of simulators, in particular, GridLAB-D and Python federates, as well as data exchanges between them.",
    "context": "Details the core co-simulation platform (HELICS) and its role in coordinating various simulators for grid analysis.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      3
    ],
    "id": "a828295e05adb81c4fff2f5e36468a2ca64fa3d92f61a305de9bab4dedb7c3ca"
  },
  {
    "text": "GridLAB-D is the power system simulator integrated in the MIRACL-CSP. It models and performs power flow calculations of the distribution system. In GridLAB-D, the distribution system is modeled as a three-phase, unbalanced system and can be simulated in either quasi-steady-state or dynamic modes [16], [19]. Moreover, distributed energy resources (DERs), including diesel and wind turbine generators are also modeled in GridLAB-D.\n3 https://ubuntu.com/\n4 https://www.helics.org/\n5 https://www.gridlabd.org/\n6 https://www.python.org/\n7 https://github.com/IdahoLabUnsupported/PowDDeR\n\nDetails the capabilities and modeling features of GridLAB-D for simulating distribution systems and incorporating DERs.",
    "original_text": "GridLAB-D is the power system simulator integrated in the MIRACL-CSP. It models and performs power flow calculations of the distribution system. In GridLAB-D, the distribution system is modeled as a three-phase, unbalanced system and can be simulated in either quasi-steady-state or dynamic modes [16], [19]. Moreover, distributed energy resources (DERs), including diesel and wind turbine generators are also modeled in GridLAB-D.\n3 https://ubuntu.com/\n4 https://www.helics.org/\n5 https://www.gridlabd.org/\n6 https://www.python.org/\n7 https://github.com/IdahoLabUnsupported/PowDDeR",
    "context": "Details the capabilities and modeling features of GridLAB-D for simulating distribution systems and incorporating DERs.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      3
    ],
    "id": "da05f1bd5e8cdcaa0d838304fb065208f86334d4064c493031219cdd2157c4b5"
  },
  {
    "text": "An application-specific utility monitoring and decisionsupport system is developed in Python [20] as the UCC in Fig. 2. The UCC incorporates an asset data (e.g., DERs, switches, sensors, loads) monitoring and analysis procedure necessary to run the resilience study with the PowDDeR application [21]. It also extracts different wind power generation profiles from measurement data files and dispatches them to the wind turbine model in GridLAB-D.\n\nDetails the development and function of the UCC for resilience study data processing and integration with GridLAB-D.",
    "original_text": "An application-specific utility monitoring and decisionsupport system is developed in Python [20] as the UCC in Fig. 2. The UCC incorporates an asset data (e.g., DERs, switches, sensors, loads) monitoring and analysis procedure necessary to run the resilience study with the PowDDeR application [21]. It also extracts different wind power generation profiles from measurement data files and dispatches them to the wind turbine model in GridLAB-D.",
    "context": "Details the development and function of the UCC for resilience study data processing and integration with GridLAB-D.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      3
    ],
    "id": "05127258b3cbe6c89fc8aec4154949bc81ece0943c07a33f3cb9472fbe7bbfce"
  },
  {
    "text": "PowDDeR is a software application developed to provide a specific resilience metric for a power system. It is based on the systems adaptive capacity and inertia. Given a set of generation assets in the system, PowDDeR captures the systems real-time inertia and the available adaptive capacity in real and reactive power looking forward in time. It gives a measure of a power system's ability to respond to disturbances, either natural, such as weather related, or human, such as cyberphysical attacks.\nThe adaptive capacity of a generation asset is bound by its operational generation limits based on its current operation point and the speed it can ramp up and down its output power. The operational limit is the maximum real and reactive power output capability at any power factor angle θ . The real and reactive power components at any power factor are given by\n<!-- formula-not-decoded -->\nand\n<!-- formula-not-decoded -->\nrespectively. Here S is the apparent power limit calculated with the nameplate real and reactive capacity, given as\n<!-- formula-not-decoded -->\nThe operational limit of the asset must be translated based on its real-time generation in real power, P 0, and reactive power, Q 0, resulting in components given by\n<!-- formula-not-decoded -->\nfor the real power and\n<!-- formula-not-decoded -->\nfor the reactive component. The temporal limits of real/reactive power outputs are defined by the latency λ and ramp rates, as mathematically expressed in equations (6), (7),\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere t is the future time, dP / dt is the real power ramping rate, and dQ / dt is the reactive power ramping rate.\nFIGURE 2. MIRACL-CSP architecture.\nThe adaptive capacity is therefore bound by the real and reactive power component limits given in equations (4) and (5) and the temporal limits defined in equations (6) and (7). This results in a ''manifold'' that represents the adaptive capacity of an asset. The manifold and a more detailed derivation, which covers the aggregation of assets can be found in [9]. Further derivations including uncertainty in solar PV and battery assets have been shown in [10], as well as hydropower generation assets in [11].\nThe inertia of power systems comes from generation units that have rotating masses or stored kinetic energy. The kinetic energy slows the rate of frequency response to disturbances on the system, given as\n<!-- formula-not-decoded -->\nwhere 1 P is the disturbance or difference between generation and load, and H is the inertial constant. A large amount of inertia on the system allows for additional time for generation units to ramp up or down output to arrest the frequency before it reaches the point of under frequency load shed (UFLS) or over frequency generator tripping. The kinetic energy of a generator is given as\n<!-- formula-not-decoded -->\nwhere f 0 , Np , and J are the real-time frequency, number of poles, and the mass moment of inertia, respectively. The kinetic energy can also be evaluated at the maximum and minimum frequency limits of the system to give the available kinetic energy. In the case of UFLS, this is given as\n<!-- formula-not-decoded -->\nThe available kinetic energy is used to find the amount of time it takes for a disturbance to result in a limiting frequency being reached, given as\n<!-- formula-not-decoded -->\nwhere Pd is the size of the power disturbance.\nThe short-term resilience in this paper is based on the ability of the system to arrest frequency prior to an UFLS event. This occurs when a disturbance results in load being greater than the generation. In this scenario, the frequency begins to drop and generation assets begin to increase their output power to re-balance the load. Therefore, the short-term resilience is the maximum size of disturbance the system can withstand and is calculated using the aggregated adaptive capacity in the positive real-power direction and the inertia or time it takes to reach the UFLS after the disturbance. The short-term resilience in regards to the DIRE curve (Fig. 1) relates to the 'resist' phase and its derivation can be seen in more detail in [22].\nThe long-term resilience is defined by the positive real power adaptive capacity over a large time span. It relates to the energy left in the system and gives a measure of the time the system can maintain generation. With regard to the DIRE curve, the long-term resilience is in the time frames of the 'respond' and 'recover' phases. In the St. Mary's microgrid it relates to the amount of time a generator can run based on its remaining fuel, its fuel burn rate, and the generators power output. The simulations ran in this study were over a short time duration of 350 seconds. In order to demonstrate the long-term resilience, the time axis is changed from seconds to hours. The longer time horizon allows a measurable reduction in the amount of fuel remaining, resulting in a change to the long-term resilience.\nIn this study framework, PowDDeR becomes part of the UCC. Through the HELICS Python APIs, the UCC monitors and processes required GridLAB-D asset information. Data is then loaded through the PowDDeR APIs to calculate\nthe adaptive capacity of each asset and of the overall system, and the results are saved into a Hierarchical Data Format Version 5 (HDF5) file for post-processing.\n\nIntroduces PowDDeR, a software for assessing power system resilience based on adaptive capacity and inertia, including formulas for operational limits and kinetic energy.",
    "original_text": "PowDDeR is a software application developed to provide a specific resilience metric for a power system. It is based on the systems adaptive capacity and inertia. Given a set of generation assets in the system, PowDDeR captures the systems real-time inertia and the available adaptive capacity in real and reactive power looking forward in time. It gives a measure of a power system's ability to respond to disturbances, either natural, such as weather related, or human, such as cyberphysical attacks.\nThe adaptive capacity of a generation asset is bound by its operational generation limits based on its current operation point and the speed it can ramp up and down its output power. The operational limit is the maximum real and reactive power output capability at any power factor angle θ . The real and reactive power components at any power factor are given by\n<!-- formula-not-decoded -->\nand\n<!-- formula-not-decoded -->\nrespectively. Here S is the apparent power limit calculated with the nameplate real and reactive capacity, given as\n<!-- formula-not-decoded -->\nThe operational limit of the asset must be translated based on its real-time generation in real power, P 0, and reactive power, Q 0, resulting in components given by\n<!-- formula-not-decoded -->\nfor the real power and\n<!-- formula-not-decoded -->\nfor the reactive component. The temporal limits of real/reactive power outputs are defined by the latency λ and ramp rates, as mathematically expressed in equations (6), (7),\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere t is the future time, dP / dt is the real power ramping rate, and dQ / dt is the reactive power ramping rate.\nFIGURE 2. MIRACL-CSP architecture.\nThe adaptive capacity is therefore bound by the real and reactive power component limits given in equations (4) and (5) and the temporal limits defined in equations (6) and (7). This results in a ''manifold'' that represents the adaptive capacity of an asset. The manifold and a more detailed derivation, which covers the aggregation of assets can be found in [9]. Further derivations including uncertainty in solar PV and battery assets have been shown in [10], as well as hydropower generation assets in [11].\nThe inertia of power systems comes from generation units that have rotating masses or stored kinetic energy. The kinetic energy slows the rate of frequency response to disturbances on the system, given as\n<!-- formula-not-decoded -->\nwhere 1 P is the disturbance or difference between generation and load, and H is the inertial constant. A large amount of inertia on the system allows for additional time for generation units to ramp up or down output to arrest the frequency before it reaches the point of under frequency load shed (UFLS) or over frequency generator tripping. The kinetic energy of a generator is given as\n<!-- formula-not-decoded -->\nwhere f 0 , Np , and J are the real-time frequency, number of poles, and the mass moment of inertia, respectively. The kinetic energy can also be evaluated at the maximum and minimum frequency limits of the system to give the available kinetic energy. In the case of UFLS, this is given as\n<!-- formula-not-decoded -->\nThe available kinetic energy is used to find the amount of time it takes for a disturbance to result in a limiting frequency being reached, given as\n<!-- formula-not-decoded -->\nwhere Pd is the size of the power disturbance.\nThe short-term resilience in this paper is based on the ability of the system to arrest frequency prior to an UFLS event. This occurs when a disturbance results in load being greater than the generation. In this scenario, the frequency begins to drop and generation assets begin to increase their output power to re-balance the load. Therefore, the short-term resilience is the maximum size of disturbance the system can withstand and is calculated using the aggregated adaptive capacity in the positive real-power direction and the inertia or time it takes to reach the UFLS after the disturbance. The short-term resilience in regards to the DIRE curve (Fig. 1) relates to the 'resist' phase and its derivation can be seen in more detail in [22].\nThe long-term resilience is defined by the positive real power adaptive capacity over a large time span. It relates to the energy left in the system and gives a measure of the time the system can maintain generation. With regard to the DIRE curve, the long-term resilience is in the time frames of the 'respond' and 'recover' phases. In the St. Mary's microgrid it relates to the amount of time a generator can run based on its remaining fuel, its fuel burn rate, and the generators power output. The simulations ran in this study were over a short time duration of 350 seconds. In order to demonstrate the long-term resilience, the time axis is changed from seconds to hours. The longer time horizon allows a measurable reduction in the amount of fuel remaining, resulting in a change to the long-term resilience.\nIn this study framework, PowDDeR becomes part of the UCC. Through the HELICS Python APIs, the UCC monitors and processes required GridLAB-D asset information. Data is then loaded through the PowDDeR APIs to calculate\nthe adaptive capacity of each asset and of the overall system, and the results are saved into a Hierarchical Data Format Version 5 (HDF5) file for post-processing.",
    "context": "Introduces PowDDeR, a software for assessing power system resilience based on adaptive capacity and inertia, including formulas for operational limits and kinetic energy.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      3,
      4,
      5
    ],
    "id": "368ce34089706851f334e4b8e727aac6b1cf630b06fa35fb8821b788f412045b"
  },
  {
    "text": "In what follows, the testing methods and design will be presented for an infrastructure based upon an Alaskan renewables integration in the St. Mary's microgrid.\n\nOutlines the testing methods and design for a microgrid integration project.",
    "original_text": "In what follows, the testing methods and design will be presented for an infrastructure based upon an Alaskan renewables integration in the St. Mary's microgrid.",
    "context": "Outlines the testing methods and design for a microgrid integration project.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      5
    ],
    "id": "05fff9ce2ca703392bb2eaf580dafaa72d14814ea64038b0be88a37dd3eda23e"
  },
  {
    "text": "St. Mary's is a remote rural community in western Alaska, located on the Yukon River and served by its own isolated electrical power grid. The St. Mary's power distribution feeder system is a 12.47 kV and 400 V system. The total load of the system can reach a peak of approximately 600 kW, with a minimum load hitting approximately 150 kW. Because it is situated in such a remote location with the associated challenges to supply energy to consumers, the price of energy is rather high, more than double the average U.S. household in 2022 according to [23] and [24]. The power demand of the St. Mary's community is served by three diesel generators listed in Table 1. The generators are assumed to have an inertia constant of 2, have ramping capability of reaching full output in 10 seconds, and their rate of burning fuel for this study are constant.\nTABLE 1. St. Mary's gensets.\nBy January 5, 2019, the Alaska Village Electric Cooperative (A VEC) had installed a 900 kW, 52-meter rotor diameter type IV pitch-controlled wind turbine generator manufactured by Emergya Wind Technologies and started producing power. Schematically illustrated as a single-line diagram in Fig. 3, the St. Mary's power distribution feeder details can be found in [25] and [26].\nLocated in a remote geographical area falling within the transitional climate zone with seasons changing from long, cold winters to shorter, warmer summers, the St. Mary's microgrid is predisposed to seasonal and operational disturbances, such as, failures due to diesel fuel delivery, high wind speeds, or cyber-physical security attacks. Therefore, it is imperative for the owner operators to have access to real-time information about how much uncertainty the system can sustain during its operation given the current operation points and disturbances.\nThe MIRACL-CSP has been designed to build use-case scenarios to evaluate the ability of a distribution system, specifically the St. Mary's microgrid, to resist, adapt, and recover from possible disturbances by measuring its resilience metric at simulation time. This operational\nFIGURE 3. St. Mary's distribution feeder single-line diagram.\nTABLE 2. St. Mary's gensets running capacity.\nresilience metric depends on monitoring the system inertia and aggregating all the generation assets adaptive capacity in real and reactive power domains. According to [9], given the nameplate rated capacity, latency, ramp rates, and energy constraints, the adaptive capacity of an asset can be explored by calculating its control domain in real and reactive power from the current point of operation.\nThe St. Mary's power distribution system in Fig. 3 has been modeled in GridLAB-D [16] as an isolated grid with the diesel generators being represented by the synchronous machine model. The unbalanced operation of three-phase synchronous machines is modeled with a simplified fundamental frequency model in phasor representation. 8\nThe wind turbine generator is modeled as an inverterinterfaced resource operating as a constant real and reactive power generator. 9 This allows the application to emulate the real wind turbine behavior, by dispatching actual measured wind generation profiles in different scenarios.\nThe loads in the St. Mary's microgrid are modeled as constant power loads 10 and have no voltage dependency. They all sum up to approximately 501 kW of real power demand on all three phases.\nIf the wind turbine is inactive, given the current configuration, to supply all loads and cover the line losses, the GridLAB-D powerflow solver calculates that the diesel generators would run according to the data in Table 2.\n8 http://gridlab-d.shoutwiki.com/wiki/Generators_Module_Guide# Diesel_DG_Model\n9 http://gridlab-d.shoutwiki.com/wiki/Inverter\n10 http://gridlab-d.shoutwiki.com/wiki/ZIPload\nFIGURE 4. HELICS integration architecture.\nGridLAB-D models for the diesel generator dynamics do not include parameters to specifically control generator efficiency and priority. Rather, their generation will vary following the bus frequency deviation due to an imbalance in the system supply and demand, which in this case is assumed to be constant.\nFrom [27], it can be concluded that the primary resilience challenge for the St. Mary's microgrid is fuel availability for the diesel generators. Fuel gets delivered by boat on the Yukon River, which is impassable from August through April due to the long and very cold winters. Therefore, life threatening situations could arise during winter as consequences to diesel fuel depletion. The addition of the distributed wind (DW) turbine to the grid reduces the community's dependence on diesel fuel.\nAVEC, as the electric utility serving the city of St. Mary's, has provided wind speed (m/s) and power generation (kW) measurement data for the wind turbine for years 2019, 2020, and 2021.\nWithin MIRACL-CSP, the UCC loads similar profiles and dispatches them to the GridLAB-D inverter-based wind turbine model to follow the generation profile, thus creating more realistic use-case scenarios inside the co-simulation environment.\nTo perform a quantitative analysis of the value of DW, MIRACL-CSP integrates the GridLAB-D model of St. Mary's microgrid with PowDDeR leveraging its capabilities to establish performance under uncertainties. The integration is realized through the HELICS API 11 [28]. In particular, for this co-simulation integration, the GridLAB-D and Python federates are treated as message federates 12 and data exchange is configured using JSON config files 13 by defining\n11 https://docs.helics.org/en/latest/references/api-reference/C_API.html\n12 https://docs.helics.org/en/latest/user-guide/fundamental_topics/ message_federates.html\n13 https://docs.helics.org/en/latest/user-guide/fundamental_topics/ interface_configuration.html#json-configuration\ncorresponding endpoints. 14 Fig. 4 exemplifies how the connection between simulators/federates is realized. Specifically, the monitor endpoint of the UCC federate communicates the wind turbine generation profile loaded from real-life acquired measurements. The inv_WTG_Pref endpoint of the GridLAB-D federate subscribing to those values captures them and feeds them in the distribution system power flow calculations. Similarly, endpoints, such as meter_DG1_real , publish the measured generation of the diesel generators to the HELICS environment to be picked by the UCC federate and loaded into the PowDDeR application for processing and metrics calculation.\n\nDescribes the St. Mary’s microgrid’s reliance on diesel generators and wind turbine, highlighting fuel availability challenges and the use of co-simulation for resilience analysis.",
    "original_text": "St. Mary's is a remote rural community in western Alaska, located on the Yukon River and served by its own isolated electrical power grid. The St. Mary's power distribution feeder system is a 12.47 kV and 400 V system. The total load of the system can reach a peak of approximately 600 kW, with a minimum load hitting approximately 150 kW. Because it is situated in such a remote location with the associated challenges to supply energy to consumers, the price of energy is rather high, more than double the average U.S. household in 2022 according to [23] and [24]. The power demand of the St. Mary's community is served by three diesel generators listed in Table 1. The generators are assumed to have an inertia constant of 2, have ramping capability of reaching full output in 10 seconds, and their rate of burning fuel for this study are constant.\nTABLE 1. St. Mary's gensets.\nBy January 5, 2019, the Alaska Village Electric Cooperative (A VEC) had installed a 900 kW, 52-meter rotor diameter type IV pitch-controlled wind turbine generator manufactured by Emergya Wind Technologies and started producing power. Schematically illustrated as a single-line diagram in Fig. 3, the St. Mary's power distribution feeder details can be found in [25] and [26].\nLocated in a remote geographical area falling within the transitional climate zone with seasons changing from long, cold winters to shorter, warmer summers, the St. Mary's microgrid is predisposed to seasonal and operational disturbances, such as, failures due to diesel fuel delivery, high wind speeds, or cyber-physical security attacks. Therefore, it is imperative for the owner operators to have access to real-time information about how much uncertainty the system can sustain during its operation given the current operation points and disturbances.\nThe MIRACL-CSP has been designed to build use-case scenarios to evaluate the ability of a distribution system, specifically the St. Mary's microgrid, to resist, adapt, and recover from possible disturbances by measuring its resilience metric at simulation time. This operational\nFIGURE 3. St. Mary's distribution feeder single-line diagram.\nTABLE 2. St. Mary's gensets running capacity.\nresilience metric depends on monitoring the system inertia and aggregating all the generation assets adaptive capacity in real and reactive power domains. According to [9], given the nameplate rated capacity, latency, ramp rates, and energy constraints, the adaptive capacity of an asset can be explored by calculating its control domain in real and reactive power from the current point of operation.\nThe St. Mary's power distribution system in Fig. 3 has been modeled in GridLAB-D [16] as an isolated grid with the diesel generators being represented by the synchronous machine model. The unbalanced operation of three-phase synchronous machines is modeled with a simplified fundamental frequency model in phasor representation. 8\nThe wind turbine generator is modeled as an inverterinterfaced resource operating as a constant real and reactive power generator. 9 This allows the application to emulate the real wind turbine behavior, by dispatching actual measured wind generation profiles in different scenarios.\nThe loads in the St. Mary's microgrid are modeled as constant power loads 10 and have no voltage dependency. They all sum up to approximately 501 kW of real power demand on all three phases.\nIf the wind turbine is inactive, given the current configuration, to supply all loads and cover the line losses, the GridLAB-D powerflow solver calculates that the diesel generators would run according to the data in Table 2.\n8 http://gridlab-d.shoutwiki.com/wiki/Generators_Module_Guide# Diesel_DG_Model\n9 http://gridlab-d.shoutwiki.com/wiki/Inverter\n10 http://gridlab-d.shoutwiki.com/wiki/ZIPload\nFIGURE 4. HELICS integration architecture.\nGridLAB-D models for the diesel generator dynamics do not include parameters to specifically control generator efficiency and priority. Rather, their generation will vary following the bus frequency deviation due to an imbalance in the system supply and demand, which in this case is assumed to be constant.\nFrom [27], it can be concluded that the primary resilience challenge for the St. Mary's microgrid is fuel availability for the diesel generators. Fuel gets delivered by boat on the Yukon River, which is impassable from August through April due to the long and very cold winters. Therefore, life threatening situations could arise during winter as consequences to diesel fuel depletion. The addition of the distributed wind (DW) turbine to the grid reduces the community's dependence on diesel fuel.\nAVEC, as the electric utility serving the city of St. Mary's, has provided wind speed (m/s) and power generation (kW) measurement data for the wind turbine for years 2019, 2020, and 2021.\nWithin MIRACL-CSP, the UCC loads similar profiles and dispatches them to the GridLAB-D inverter-based wind turbine model to follow the generation profile, thus creating more realistic use-case scenarios inside the co-simulation environment.\nTo perform a quantitative analysis of the value of DW, MIRACL-CSP integrates the GridLAB-D model of St. Mary's microgrid with PowDDeR leveraging its capabilities to establish performance under uncertainties. The integration is realized through the HELICS API 11 [28]. In particular, for this co-simulation integration, the GridLAB-D and Python federates are treated as message federates 12 and data exchange is configured using JSON config files 13 by defining\n11 https://docs.helics.org/en/latest/references/api-reference/C_API.html\n12 https://docs.helics.org/en/latest/user-guide/fundamental_topics/ message_federates.html\n13 https://docs.helics.org/en/latest/user-guide/fundamental_topics/ interface_configuration.html#json-configuration\ncorresponding endpoints. 14 Fig. 4 exemplifies how the connection between simulators/federates is realized. Specifically, the monitor endpoint of the UCC federate communicates the wind turbine generation profile loaded from real-life acquired measurements. The inv_WTG_Pref endpoint of the GridLAB-D federate subscribing to those values captures them and feeds them in the distribution system power flow calculations. Similarly, endpoints, such as meter_DG1_real , publish the measured generation of the diesel generators to the HELICS environment to be picked by the UCC federate and loaded into the PowDDeR application for processing and metrics calculation.",
    "context": "Describes the St. Mary’s microgrid’s reliance on diesel generators and wind turbine, highlighting fuel availability challenges and the use of co-simulation for resilience analysis.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      5,
      6
    ],
    "id": "fde12340d082042724b65b95d13a5fb9fdacd32170699d66f5ba8570c850048c"
  },
  {
    "text": "With the St. Mary's GridLAB-D model running with a 501 kW peak load demand, the diesel generators need to run at the capacities listed in Table 2. When a DW resource is added to the system, its dynamics are modified according to the variability of wind power generation. Using different wind profiles provided by AVEC, the St. Mary's microgrid can be simulated under various wind conditions to study its resilience in the presence of variable generation and uncertainties. MIRACL-CSP offers the platform to easily create these scenarios and supply the simulated data directly to PowDDeR for resilience metrics calculation during runtime. Two scenarios have been considered, with wind generation profile selected from the A VEC data presenting a decreasing trend over the simulated time, natural uncertainties that could affect the grid, and fuel depletion:\n- S.1: When the wind speed varies from higher to lower speeds (max 11 . 17 m/s, min 4 . 7 m/s, mean 7 . 74 m/s, and standard deviation 1 . 65), analyzing the system resilience through its assets' adaptive capacity can\n14 https://docs.helics.org/en/latest/user-guide/fundamental_topics/ message_federates.html#message-federate-endpoints\nanswer questions related to possible needs for diesel generation curtailment or load shed.\n- S.2: This scenario assumes the same wind profile as S.1: with the addition of large disturbances. These system dynamics force a drastic change in generation to meet the load. The resilience metric is very important in this case. The disturbances here are based on loss of diesel generators due to lack of fuel. However, the loss of these generators could occur from physical degradation, storms, or cyber threats.\n\nEvaluates microgrid resilience under variable wind conditions and potential diesel generator failures, informing load management strategies.",
    "original_text": "With the St. Mary's GridLAB-D model running with a 501 kW peak load demand, the diesel generators need to run at the capacities listed in Table 2. When a DW resource is added to the system, its dynamics are modified according to the variability of wind power generation. Using different wind profiles provided by AVEC, the St. Mary's microgrid can be simulated under various wind conditions to study its resilience in the presence of variable generation and uncertainties. MIRACL-CSP offers the platform to easily create these scenarios and supply the simulated data directly to PowDDeR for resilience metrics calculation during runtime. Two scenarios have been considered, with wind generation profile selected from the A VEC data presenting a decreasing trend over the simulated time, natural uncertainties that could affect the grid, and fuel depletion:\n- S.1: When the wind speed varies from higher to lower speeds (max 11 . 17 m/s, min 4 . 7 m/s, mean 7 . 74 m/s, and standard deviation 1 . 65), analyzing the system resilience through its assets' adaptive capacity can\n14 https://docs.helics.org/en/latest/user-guide/fundamental_topics/ message_federates.html#message-federate-endpoints\nanswer questions related to possible needs for diesel generation curtailment or load shed.\n- S.2: This scenario assumes the same wind profile as S.1: with the addition of large disturbances. These system dynamics force a drastic change in generation to meet the load. The resilience metric is very important in this case. The disturbances here are based on loss of diesel generators due to lack of fuel. However, the loss of these generators could occur from physical degradation, storms, or cyber threats.",
    "context": "Evaluates microgrid resilience under variable wind conditions and potential diesel generator failures, informing load management strategies.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      6,
      7
    ],
    "id": "2074ae87dfb749985b54dd06c7c2efddd3b1136a83d01ae141abfd420d2d45ca"
  },
  {
    "text": "This section provides the testing results following the use case breakdowns previously discussed. During the co-simulation runtime of the St. Mary's microgrid, PowDDeR gathers the current states of the considered system assets, that is diesel generators and the wind turbine, constructs the adaptive capacity manifolds, gets the system inertia, and calculates the short-term and long-term resilience.\n\nPresents testing results and key performance indicators (inertia, resilience) from the St. Mary’s microgrid co-simulation.",
    "original_text": "This section provides the testing results following the use case breakdowns previously discussed. During the co-simulation runtime of the St. Mary's microgrid, PowDDeR gathers the current states of the considered system assets, that is diesel generators and the wind turbine, constructs the adaptive capacity manifolds, gets the system inertia, and calculates the short-term and long-term resilience.",
    "context": "Presents testing results and key performance indicators (inertia, resilience) from the St. Mary’s microgrid co-simulation.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      7
    ],
    "id": "3fda79053d87c5eb265bfa2282b371eea64b1128e3e520d7a1d4222bd2631ce6"
  },
  {
    "text": "The simulated wind and diesel generation output and frequency at the bus from GridLAB-D are shown in Figs. 5a and 5b, respectively. It can be seen that the wind turbine generator (WTG) and diesel generator 1 (DG1) support the majority of the load. The loss of wind generation over the simulation is compensated by the ramping of output of generator 1. However, diesel generators 2 and 3 are both online over the duration of the simulation and can be seen by the inertia plot in Fig. 5c. Here, each generator is supplying inertia to the system. It is assumed that each generator has an inertia constant of 2. Since each generator is running, they are burning fuel at the rate given in Table 1 and the resulting fuel remaining is shown in Fig. 5d.\nThe resulting short-term resilience, which is a measure of the size of disturbance the system can withstand without dropping below a frequency limit of 58Hz, is shown in Fig. 5e. Here, it can be seen that the short-term resilience has small variation and it follows the same profile as the system frequency. As the system frequency falls, the short-term resilience also falls. The long-term resilience is shown in Fig. 5f. Here, it can be seen that the system resilience is the aggregation of each generator and is continually reduced over time. It is based on the available fuel left, the fuel consumption of the generators, and their maximum outputs.\n\nAnalyzes the system's short-term and long-term resilience to disturbances, correlating frequency drops with reduced resilience due to fuel consumption and generator outputs.",
    "original_text": "The simulated wind and diesel generation output and frequency at the bus from GridLAB-D are shown in Figs. 5a and 5b, respectively. It can be seen that the wind turbine generator (WTG) and diesel generator 1 (DG1) support the majority of the load. The loss of wind generation over the simulation is compensated by the ramping of output of generator 1. However, diesel generators 2 and 3 are both online over the duration of the simulation and can be seen by the inertia plot in Fig. 5c. Here, each generator is supplying inertia to the system. It is assumed that each generator has an inertia constant of 2. Since each generator is running, they are burning fuel at the rate given in Table 1 and the resulting fuel remaining is shown in Fig. 5d.\nThe resulting short-term resilience, which is a measure of the size of disturbance the system can withstand without dropping below a frequency limit of 58Hz, is shown in Fig. 5e. Here, it can be seen that the short-term resilience has small variation and it follows the same profile as the system frequency. As the system frequency falls, the short-term resilience also falls. The long-term resilience is shown in Fig. 5f. Here, it can be seen that the system resilience is the aggregation of each generator and is continually reduced over time. It is based on the available fuel left, the fuel consumption of the generators, and their maximum outputs.",
    "context": "Analyzes the system's short-term and long-term resilience to disturbances, correlating frequency drops with reduced resilience due to fuel consumption and generator outputs.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      7
    ],
    "id": "9999a0ba7dd603beba1cad1d62ec225e7536148c5c753b65961eb869705fbf63"
  },
  {
    "text": "The power generation and frequency from the GridLab-D simulation for this scenario are shown in Figs. 6a and 6b, respectively. Again, it can be seen that the wind generation and diesel generator 1 support the majority of the load. However, in this scenario, generator 1 and 3 run out of fuel and are taken off-line at 250 and 75 seconds, respectively. The loss of wind generation is initially compensated by ramping of generator 1. When this generator runs out of fuel, generator 2\nFIGURE 5. Scenario 1, without any large disturbances, there is little change in the short-term resilience while the long-term resilience is continually reduced as fuel is depleted.\nis ramped up to balance the load demand. The loss of generation units has a direct impact to the system inertia, shown in\nLuc\nDowDDeR Iono_term recilience\nFIGURE 6. Scenario 2, the loss of a generator from running out of fuel has a large negative impact on the short-term resilience. However, the rate of reduction of the long-term resilience is slowed.\nFIGURE 7. Short-term and long-term resilience for each scenario. The impact of losing or taking a generator off-line has a reduction of short-term resilience but increases the long-term resilience.\nFig. 6c. When they are taken off-line, they no longer add any inertia to the system.\nIn this case, the resulting short-term resilience is shown in Fig. 6e. Here, it can be seen that the short-term resilience not only follows the frequency of the system, but, more importantly, it takes a large step reduction when a generator is taken off-line. This results in the system not being able to withstand large disturbances before frequency limits are reached. The long-term resilience is shown in Fig. 6f. Here, the long-term resilience contribution of each generator is shown along with system's resilience. The system resilience is actually reduced at a slower rate after each generator is taken off-line. It should be noted that this is only possible as there is adequate generation still online to support the load demand.\n\nIllustrates the impact of generator failures on short-term and long-term system resilience, showing a significant reduction in short-term resilience and a slower reduction in long-term resilience due to continued online generation.",
    "original_text": "The power generation and frequency from the GridLab-D simulation for this scenario are shown in Figs. 6a and 6b, respectively. Again, it can be seen that the wind generation and diesel generator 1 support the majority of the load. However, in this scenario, generator 1 and 3 run out of fuel and are taken off-line at 250 and 75 seconds, respectively. The loss of wind generation is initially compensated by ramping of generator 1. When this generator runs out of fuel, generator 2\nFIGURE 5. Scenario 1, without any large disturbances, there is little change in the short-term resilience while the long-term resilience is continually reduced as fuel is depleted.\nis ramped up to balance the load demand. The loss of generation units has a direct impact to the system inertia, shown in\nLuc\nDowDDeR Iono_term recilience\nFIGURE 6. Scenario 2, the loss of a generator from running out of fuel has a large negative impact on the short-term resilience. However, the rate of reduction of the long-term resilience is slowed.\nFIGURE 7. Short-term and long-term resilience for each scenario. The impact of losing or taking a generator off-line has a reduction of short-term resilience but increases the long-term resilience.\nFig. 6c. When they are taken off-line, they no longer add any inertia to the system.\nIn this case, the resulting short-term resilience is shown in Fig. 6e. Here, it can be seen that the short-term resilience not only follows the frequency of the system, but, more importantly, it takes a large step reduction when a generator is taken off-line. This results in the system not being able to withstand large disturbances before frequency limits are reached. The long-term resilience is shown in Fig. 6f. Here, the long-term resilience contribution of each generator is shown along with system's resilience. The system resilience is actually reduced at a slower rate after each generator is taken off-line. It should be noted that this is only possible as there is adequate generation still online to support the load demand.",
    "context": "Illustrates the impact of generator failures on short-term and long-term system resilience, showing a significant reduction in short-term resilience and a slower reduction in long-term resilience due to continued online generation.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      8,
      7
    ],
    "id": "857842a177a8ea7162d106c3d4d562a8339207dcff726eec25e7ce6a981a2bbc"
  },
  {
    "text": "The results of the short-term and long-term resilience for each scenario are shown in Fig. 7. The key takeaway that can be seen is the trade-off between short-term and longterm resilience. Examining scenario 2, you can see that when a generator goes off-line it has a large negative impact to the short-term resilience because of the reduced inertia and generation ramping capability. However, it has a positive effect on the long-term resilience. This is due to the system no longer burning as much fuel to support the load. It should be noted that this is only possible because the generators are not being run near their maximum generation capability, therefore they can be ramped up to support the load demand.\nAnother key takeaway is that the short-term and long-term resilience metrics do not directly show the impact of wind generation. In the case of the short-term resilience, wind does not directly add inertia to the system and it is ran at its maximum output. In order to add to the short-term resilience, the wind turbine generator can be ran below its maximum capability. The impact to resilience based on the dynamics of different generation assets has been demonstrated in [22]. Here, it was shown that the quick ramping capability of inverter-based generation can have a large impact to maintain frequency stability. For the long-term resilience, wind\ngeneration allows for diesel generators to be taken off-line, conserving the fuel, and therefore increasing the long-term resilience.\n\nHighlights the trade-off between short-term and long-term resilience, demonstrating how wind generation impacts each metric and the role of generator ramping capability.",
    "original_text": "The results of the short-term and long-term resilience for each scenario are shown in Fig. 7. The key takeaway that can be seen is the trade-off between short-term and longterm resilience. Examining scenario 2, you can see that when a generator goes off-line it has a large negative impact to the short-term resilience because of the reduced inertia and generation ramping capability. However, it has a positive effect on the long-term resilience. This is due to the system no longer burning as much fuel to support the load. It should be noted that this is only possible because the generators are not being run near their maximum generation capability, therefore they can be ramped up to support the load demand.\nAnother key takeaway is that the short-term and long-term resilience metrics do not directly show the impact of wind generation. In the case of the short-term resilience, wind does not directly add inertia to the system and it is ran at its maximum output. In order to add to the short-term resilience, the wind turbine generator can be ran below its maximum capability. The impact to resilience based on the dynamics of different generation assets has been demonstrated in [22]. Here, it was shown that the quick ramping capability of inverter-based generation can have a large impact to maintain frequency stability. For the long-term resilience, wind\ngeneration allows for diesel generators to be taken off-line, conserving the fuel, and therefore increasing the long-term resilience.",
    "context": "Highlights the trade-off between short-term and long-term resilience, demonstrating how wind generation impacts each metric and the role of generator ramping capability.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      8,
      9
    ],
    "id": "22a8217c790b081d22c67a9b62d6a51d314ffa43b6e25102b6cd841042de2692"
  },
  {
    "text": "In this paper, to account for the time-dependent behavior of a power distribution system under uncertainties and dynamically study its resilience, a co-simulation platform, also known as MIRACL-CSP, has been developed and detailed. The intended purpose of this platform was to integrate the dynamics of the distribution system with the resilience assessment and quantification metric. As a use case for the proposed platform to quantitatively study the resilience of a microgrid in a holistic manner, we analyzed the resilience of the real world St. Mary's microgrid incorporating a 900 kW wind turbine generator under different scenarios dictated by real-life uncertainties using PowDDeR, a software application providing resilience metrics for power systems based on their adaptive capacity and inertia. The integration of PowDDeR with GridLAB-D, the St. Mary's microgrid power flow simulator, is facilitated by MIRACL-CSP so that the resilience application could monitor and analyze distribution system measurements in real time. The co-simulation environment allowed us to collect system dynamics data to characterize its resilience at different time scales.\nThe resilience results of the St. Mary's microgrid show a trade-off between short-term and long-term resilience. Having diesel generators online add to the short-term resilience as there is more inertia in the system and a quicker ramping capability if a disturbance occurs. However, this results in faster reduction of fuel and therefore a quicker reduction in long-term resilience. The contribution of wind generation has the ability to increase either the short-term or long-term resilience depending on how it is utilized. If it is run at maximum output, diesel generation can be taken off-line. If it is run below its maximum output, the fast ramping capability of inverters allows for increased short-term resilience of the system. Future work includes leveraging MIRACL-CSP as the practical framework for asset owners and operators, and original equipment providers to understand benefit versus risk through co-optimization of resilience and economic considerations for applications. With DW as a resilience enhancement, the resilience metrics can be used as control decisions to benefit the overall system efficiency, and the cost of operation in particular.\nMoreover, MIRACL-CSP serves as a prototype for complex systems integration with dynamic resilience quantitative metrics, and could be adapted and/or extended for studies pertaining to other domains, such as chemical and water infrastructure systems.\n\nDetails the development and application of a co-simulation platform (MIRACL-CSP) for quantifying resilience in the St. Mary’s microgrid, highlighting a trade-off between short-term and long-term resilience influenced by diesel generators and wind generation, and suggesting its potential for broader system integration.",
    "original_text": "In this paper, to account for the time-dependent behavior of a power distribution system under uncertainties and dynamically study its resilience, a co-simulation platform, also known as MIRACL-CSP, has been developed and detailed. The intended purpose of this platform was to integrate the dynamics of the distribution system with the resilience assessment and quantification metric. As a use case for the proposed platform to quantitatively study the resilience of a microgrid in a holistic manner, we analyzed the resilience of the real world St. Mary's microgrid incorporating a 900 kW wind turbine generator under different scenarios dictated by real-life uncertainties using PowDDeR, a software application providing resilience metrics for power systems based on their adaptive capacity and inertia. The integration of PowDDeR with GridLAB-D, the St. Mary's microgrid power flow simulator, is facilitated by MIRACL-CSP so that the resilience application could monitor and analyze distribution system measurements in real time. The co-simulation environment allowed us to collect system dynamics data to characterize its resilience at different time scales.\nThe resilience results of the St. Mary's microgrid show a trade-off between short-term and long-term resilience. Having diesel generators online add to the short-term resilience as there is more inertia in the system and a quicker ramping capability if a disturbance occurs. However, this results in faster reduction of fuel and therefore a quicker reduction in long-term resilience. The contribution of wind generation has the ability to increase either the short-term or long-term resilience depending on how it is utilized. If it is run at maximum output, diesel generation can be taken off-line. If it is run below its maximum output, the fast ramping capability of inverters allows for increased short-term resilience of the system. Future work includes leveraging MIRACL-CSP as the practical framework for asset owners and operators, and original equipment providers to understand benefit versus risk through co-optimization of resilience and economic considerations for applications. With DW as a resilience enhancement, the resilience metrics can be used as control decisions to benefit the overall system efficiency, and the cost of operation in particular.\nMoreover, MIRACL-CSP serves as a prototype for complex systems integration with dynamic resilience quantitative metrics, and could be adapted and/or extended for studies pertaining to other domains, such as chemical and water infrastructure systems.",
    "context": "Details the development and application of a co-simulation platform (MIRACL-CSP) for quantifying resilience in the St. Mary’s microgrid, highlighting a trade-off between short-term and long-term resilience influenced by diesel generators and wind generation, and suggesting its potential for broader system integration.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      9
    ],
    "id": "4e5d0a69a818b29d6b9573d65a66eb648709e9b5212833c5ccfdf1074b084ca7"
  },
  {
    "text": "- [1] S. Afzal, H. Mokhlis, H. A. Illias, N. N. Mansor, and H. Shareef, ''State-of-the-art review on power system resilience and assessment techniques,'' IET Gener., Transmiss. Distribution , vol. 14, no. 25, pp. 6107-6121, Dec. 2020. [Online]. Available: https://ietresearch. onlinelibrary.wiley.com/doi/abs/10.1049/iet-gtd.2020.0531\n- [2] A. Umunnakwe, H. Huang, K. Oikonomou, and K. Davis, ''Quantitative analysis of power systems resilience: Standardization, categorizations, and challenges,'' Renew. Sustain. Energy Rev. , vol. 149, Oct. 2021, Art. no. 111252. [Online]. Available: https://www.sciencedirect.com/ science/article/pii/S1364032121005396\n- [3] A. Younesi, H. Shayeghi, Z. Wang, P. Siano, A. Mehrizi-Sani, and A. Safari, ''Trends in modern power systems resilience: State-ofthe-art review,'' Renew. Sustain. Energy Rev. , vol. 162, Jul. 2022, Art. no. 112397. [Online]. Available: https://www.sciencedirect.com/ science/article/pii/S1364032122003070\n- [4] D. Wei and K. Ji, ''Resilient industrial control system (RICS): Concepts, formulation, metrics, and insights,'' in Proc. 3rd Int. Symp. Resilient Control Syst. , Aug. 2010, pp. 15-22.\n- [5] A. Clark and S. Zonouz, ''Cyber-physical resilience: Definition and assessment metric,'' IEEE Trans. Smart Grid , vol. 10, no. 2, pp. 1671-1684, Mar. 2018.\n- [6] J. D. Taft, ''Electric grid resilience and reliability for grid architecture,'' Pacific Northwest Nat. Lab., Richland, WA, USA, Tech. Rep. PNNL26623, 2017.\n- [7] T. R. McJunkin and C. G. Rieger, ''Electricity distribution system resilient control system metrics,'' in Proc. Resilience Week (RWS) , Sep. 2017, pp. 103-112.\n- [8] C. G. Rieger, ''Resilient control systems practical metrics basis for defining mission impact,'' in Proc. 7th Int. Symp. Resilient Control Syst. (ISRCS) , Aug. 2014, pp. 1-10.\n- [9] T. Phillips, T. McJunkin, C. Rieger, J. Gardner, and H. Mehrpouyan, ''An operational resilience metric for modern power distribution systems,'' in Proc. IEEE 20th Int. Conf. Softw. Qual., Rel. Secur. Companion (QRS-C) , Dec. 2020, pp. 334-342.\n- [10] T. Phillips, T. McJunkin, C. Rieger, J. Gardner, and H. Mehrpouyan, ''A framework for evaluating the resilience contribution of solar PV and battery storage on the grid,'' in Proc. Resilience Week (RWS) , Oct. 2020, pp. 133-139.\n- [11] T. Phillips, V. Chalishazar, T. McJunkin, M. Maharjan, S. M. Shafiul Alam, T. Mosier, and A. Somani, ''A metric framework for evaluating the resilience contribution of hydropower to the grid,'' in Proc. Resilience Week (RWS) , Oct. 2020, pp. 78-85.\n- [12] M. Jackson and J. S. Fitzgerald, ''Towards resilience-explicit modelling and co-simulation of cyber-physical systems,'' in Software Engineering and Formal Methods , A. Cerone and M. Roveri, Eds. Berlin, Germany: Springer, 2018, pp. 361-376.\n- [13] P. T. Mana, K. P. Schneider, W. Du, M. Mukherjee, T. Hardy, and F. K. Tuffner, ''Study of microgrid resilience through co-simulation of power system dynamics and communication systems,'' IEEE Trans. Ind. Informat. , vol. 17, no. 3, pp. 1905-1915, Mar. 2021.\n- [14] K. Hoth, T. Steffen, B. Wiegel, A. Youssfi, D. Babazadeh, M. Venzke, C. Becker, K. Fischer, and V. Turau, ''Holistic simulation approach for optimal operation of smart integrated energy systems under consideration of resilience, economics and sustainability,'' Infrastructures , vol. 6, no. 11, p. 150, Oct. 2021. [Online]. Available: https://www.mdpi.com/24123811/6/11/150\n- [15] B. Bhattarai, L. Marinovici, P. S. Sarker, and A. Orrell, ''MIRACL co-simulation platform for control and operation of distributed wind in microgrid,'' IET Smart Grid , vol. 5, no. 2, pp. 90-100, Apr. 2022. [Online]. Available: https://ietresearch.onlinelibrary.wiley.com/ doi/abs/10.1049/stg2.12054\n- [16] PNNL. (2022). GridLAB-D . [Online]. Available: https://www.gridlabd. org/\n- [17] LNNL. (2022). Hierarchical Engine for Large-Scale Infrastructure CoSimulation . [Online]. Available: https://www.helics.org/\n- [18] B. Palmintier, D. Krishnamurthy, P. Top, S. Smith, J. Daily, and J. Fuller, ''Design of the HELICS high-performance transmissiondistribution-communication-market co-simulation framework,'' in Proc. Workshop Model. Simul. Cyber-Phys. Energy Syst. (MSCPES) , Apr. 2017, pp. 1-6.\n- [19] D. P. Chassin, K. Schneider, and C. Gerkensmeyer, ''GridLAB-D: An open-source power systems modeling and simulation environment,'' in Proc. IEEE/PES Transmiss. Distrib. Conf. Expo. , Apr. 2008, pp. 1-5.\n- [20] (2022). Python . [Online]. Available: https://www.python.org/\n- [21] INL. (2019). PowDDeR . [Online]. Available: https://github.com/ IdahoLabUnsupported/PowDDeR\n\nProvides a collection of research on power system resilience assessment techniques and simulation tools.",
    "original_text": "- [1] S. Afzal, H. Mokhlis, H. A. Illias, N. N. Mansor, and H. Shareef, ''State-of-the-art review on power system resilience and assessment techniques,'' IET Gener., Transmiss. Distribution , vol. 14, no. 25, pp. 6107-6121, Dec. 2020. [Online]. Available: https://ietresearch. onlinelibrary.wiley.com/doi/abs/10.1049/iet-gtd.2020.0531\n- [2] A. Umunnakwe, H. Huang, K. Oikonomou, and K. Davis, ''Quantitative analysis of power systems resilience: Standardization, categorizations, and challenges,'' Renew. Sustain. Energy Rev. , vol. 149, Oct. 2021, Art. no. 111252. [Online]. Available: https://www.sciencedirect.com/ science/article/pii/S1364032121005396\n- [3] A. Younesi, H. Shayeghi, Z. Wang, P. Siano, A. Mehrizi-Sani, and A. Safari, ''Trends in modern power systems resilience: State-ofthe-art review,'' Renew. Sustain. Energy Rev. , vol. 162, Jul. 2022, Art. no. 112397. [Online]. Available: https://www.sciencedirect.com/ science/article/pii/S1364032122003070\n- [4] D. Wei and K. Ji, ''Resilient industrial control system (RICS): Concepts, formulation, metrics, and insights,'' in Proc. 3rd Int. Symp. Resilient Control Syst. , Aug. 2010, pp. 15-22.\n- [5] A. Clark and S. Zonouz, ''Cyber-physical resilience: Definition and assessment metric,'' IEEE Trans. Smart Grid , vol. 10, no. 2, pp. 1671-1684, Mar. 2018.\n- [6] J. D. Taft, ''Electric grid resilience and reliability for grid architecture,'' Pacific Northwest Nat. Lab., Richland, WA, USA, Tech. Rep. PNNL26623, 2017.\n- [7] T. R. McJunkin and C. G. Rieger, ''Electricity distribution system resilient control system metrics,'' in Proc. Resilience Week (RWS) , Sep. 2017, pp. 103-112.\n- [8] C. G. Rieger, ''Resilient control systems practical metrics basis for defining mission impact,'' in Proc. 7th Int. Symp. Resilient Control Syst. (ISRCS) , Aug. 2014, pp. 1-10.\n- [9] T. Phillips, T. McJunkin, C. Rieger, J. Gardner, and H. Mehrpouyan, ''An operational resilience metric for modern power distribution systems,'' in Proc. IEEE 20th Int. Conf. Softw. Qual., Rel. Secur. Companion (QRS-C) , Dec. 2020, pp. 334-342.\n- [10] T. Phillips, T. McJunkin, C. Rieger, J. Gardner, and H. Mehrpouyan, ''A framework for evaluating the resilience contribution of solar PV and battery storage on the grid,'' in Proc. Resilience Week (RWS) , Oct. 2020, pp. 133-139.\n- [11] T. Phillips, V. Chalishazar, T. McJunkin, M. Maharjan, S. M. Shafiul Alam, T. Mosier, and A. Somani, ''A metric framework for evaluating the resilience contribution of hydropower to the grid,'' in Proc. Resilience Week (RWS) , Oct. 2020, pp. 78-85.\n- [12] M. Jackson and J. S. Fitzgerald, ''Towards resilience-explicit modelling and co-simulation of cyber-physical systems,'' in Software Engineering and Formal Methods , A. Cerone and M. Roveri, Eds. Berlin, Germany: Springer, 2018, pp. 361-376.\n- [13] P. T. Mana, K. P. Schneider, W. Du, M. Mukherjee, T. Hardy, and F. K. Tuffner, ''Study of microgrid resilience through co-simulation of power system dynamics and communication systems,'' IEEE Trans. Ind. Informat. , vol. 17, no. 3, pp. 1905-1915, Mar. 2021.\n- [14] K. Hoth, T. Steffen, B. Wiegel, A. Youssfi, D. Babazadeh, M. Venzke, C. Becker, K. Fischer, and V. Turau, ''Holistic simulation approach for optimal operation of smart integrated energy systems under consideration of resilience, economics and sustainability,'' Infrastructures , vol. 6, no. 11, p. 150, Oct. 2021. [Online]. Available: https://www.mdpi.com/24123811/6/11/150\n- [15] B. Bhattarai, L. Marinovici, P. S. Sarker, and A. Orrell, ''MIRACL co-simulation platform for control and operation of distributed wind in microgrid,'' IET Smart Grid , vol. 5, no. 2, pp. 90-100, Apr. 2022. [Online]. Available: https://ietresearch.onlinelibrary.wiley.com/ doi/abs/10.1049/stg2.12054\n- [16] PNNL. (2022). GridLAB-D . [Online]. Available: https://www.gridlabd. org/\n- [17] LNNL. (2022). Hierarchical Engine for Large-Scale Infrastructure CoSimulation . [Online]. Available: https://www.helics.org/\n- [18] B. Palmintier, D. Krishnamurthy, P. Top, S. Smith, J. Daily, and J. Fuller, ''Design of the HELICS high-performance transmissiondistribution-communication-market co-simulation framework,'' in Proc. Workshop Model. Simul. Cyber-Phys. Energy Syst. (MSCPES) , Apr. 2017, pp. 1-6.\n- [19] D. P. Chassin, K. Schneider, and C. Gerkensmeyer, ''GridLAB-D: An open-source power systems modeling and simulation environment,'' in Proc. IEEE/PES Transmiss. Distrib. Conf. Expo. , Apr. 2008, pp. 1-5.\n- [20] (2022). Python . [Online]. Available: https://www.python.org/\n- [21] INL. (2019). PowDDeR . [Online]. Available: https://github.com/ IdahoLabUnsupported/PowDDeR",
    "context": "Provides a collection of research on power system resilience assessment techniques and simulation tools.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      9
    ],
    "id": "f42b5f0168169a1d5f62d038143ec06f50e9ecb2cfec229d3cb0b8280093bdd0"
  },
  {
    "text": "- [22] T. Phillips, T. McJunkin, S. M. S. Alam, B. Poudel, and T. Mosier, ''An operational resilience metric to evaluate inertia and inverter-based generation on the grid,'' in Proc. IEEE Power Energy Soc. Gen. Meeting (PESGM) , Jul. 2022, pp. 1-5.\n- [23] Utilities Local. (2022). Residential Electricity Rates in Saint Mary's . [Online]. Available: https://utilitieslocal.com/states/alaska/saint-marys/ #electricity\n- [24] US EIA. (Nov. 2021). State Electricity Profiles . [Online]. Available: https://www.eia.gov/electricity/state/\n- [25] D. Vaught, ''Saint Mary's, Alaska REF 8 wind-diesel project analysis,'' V3 Energy LLC, Eagle River, AK, USA, Tech. Rep., 2014.\n- [26] J. Flicker, J. Hernandez-Alvidrez, M. Shirazi, J. Vandermeer, and W. Thomson, ''Grid forming inverters for spinning reserve in hybrid diesel microgrids,'' in Proc. IEEE Power Energy Soc. Gen. Meeting (PESGM) , Aug. 2020, pp. 1-5.\n- [27] J. D. Flicker. (Dec. 2019). Grid-Bridging Inverter Application at St. Mary's/Mountain Village Microgrid Systems . [Online]. Available: https://www.osti.gov/biblio/1646326\n- [28] LNNL. (2022). Hierarchical Engine for Large-scale Infrastructure CoSimulation-C API Reference . [Online]. Available: https://docs.helics.org/ en/latest/references/api-reference/C_API.html\nTYLER PHILLIPS (Member, IEEE) received the B.S. and M.S. degrees in mechanical engineering and the Ph.D. degree in computing, computational mathematics, science, and engineering from Boise State University, in 2009, 2014, and 2020, respectively. He is currently a Postdoctoral Researcher with the Idaho National Laboratory, Energy and Environmental Science and Technology Group. His research interests include power and energy system simulations, numerical comput- ing, and data analysis in the areas of resilient control, resilience metrics, integration of renewable generation, microgrids, and dynamic line ratings. He is also involved with the interaction of human factors in power systems. Since 2015, he has been with the IEEE Power and Energy Society.\nLAURENTIU D. MARINOVICI (Member, IEEE) received the B.Eng. degree in computer engineering and the M.Sc. degree in automatic control from the Gheorghe Asachi Technical University of Iaşi, Iaşi, Romania, in 2000 and 2001, respectively, and the M.Sc. and Ph.D. degrees in electrical and computer engineering from Louisiana State University, in 2007 and 2011, respectively. He is currently a Research Engineer with the Pacific Northwest National Laboratory, Optimization and Control\nGroup. His research interests lie at the confluence of software engineering and development, and model-based control, simulation and data analysis. He has been involved with designing, developing, and testing co-simulation platforms to implement and validate control algorithms for applications at different levels of the electrical power grid. Since 2011, he has been with the IEEE Power and Energy Society and IEEE Control Systems Society.\nCRAIG RIEGER (Senior Member, IEEE) received the B.S. and M.S. degrees in chemical engineering from Montana State University, in 1983 and 1985, respectively, and the Ph.D. degree in engineering and applied science from Idaho State University, in 2008. His Ph.D. coursework and dissertation focused on measurements and control, with specific application to intelligent, supervisory ventilation controls for critical infrastructure. He is currently a Professional Engineer. He is the Chief\nControl Systems Research Engineer and a Directorate Fellow with the Idaho National Laboratory (INL), pioneering interdisciplinary research in next generation resilient control systems. The grand challenge provided an integrated research strategy to address the cognitive, cyber-physical challenges of complex control systems into self-aware, trust-confirming, and threatresilient architectures. In addition, he has organized and chaired 14 cosponsored symposia and one National Science Foundation workshop in this new research area and authored more than 75 peer-reviewed publications. He has 20 years of software and hardware design experience for process control system upgrades and new installations. He has also been a supervisor and the technical lead for control systems engineering groups having design, configuration management, and security responsibilities for several INL nuclear facilities and various control system architectures.\nALICE ORRELL received the B.S. degree in mechanical engineering from the University of Vermont and the M.B.A. degree from the University of Washington. She is currently a Professional Engineer and manages the Pacific Northwest National Laboratory's distributed wind research portfolio, which included the MIRACL Project. She is also the Lead Author of the U.S. Department of Energy's annual ''Distributed Wind Market Report.''\n\nProvides context on researchers and their affiliations involved in power grid resilience and control systems research, including specific projects and publications.",
    "original_text": "- [22] T. Phillips, T. McJunkin, S. M. S. Alam, B. Poudel, and T. Mosier, ''An operational resilience metric to evaluate inertia and inverter-based generation on the grid,'' in Proc. IEEE Power Energy Soc. Gen. Meeting (PESGM) , Jul. 2022, pp. 1-5.\n- [23] Utilities Local. (2022). Residential Electricity Rates in Saint Mary's . [Online]. Available: https://utilitieslocal.com/states/alaska/saint-marys/ #electricity\n- [24] US EIA. (Nov. 2021). State Electricity Profiles . [Online]. Available: https://www.eia.gov/electricity/state/\n- [25] D. Vaught, ''Saint Mary's, Alaska REF 8 wind-diesel project analysis,'' V3 Energy LLC, Eagle River, AK, USA, Tech. Rep., 2014.\n- [26] J. Flicker, J. Hernandez-Alvidrez, M. Shirazi, J. Vandermeer, and W. Thomson, ''Grid forming inverters for spinning reserve in hybrid diesel microgrids,'' in Proc. IEEE Power Energy Soc. Gen. Meeting (PESGM) , Aug. 2020, pp. 1-5.\n- [27] J. D. Flicker. (Dec. 2019). Grid-Bridging Inverter Application at St. Mary's/Mountain Village Microgrid Systems . [Online]. Available: https://www.osti.gov/biblio/1646326\n- [28] LNNL. (2022). Hierarchical Engine for Large-scale Infrastructure CoSimulation-C API Reference . [Online]. Available: https://docs.helics.org/ en/latest/references/api-reference/C_API.html\nTYLER PHILLIPS (Member, IEEE) received the B.S. and M.S. degrees in mechanical engineering and the Ph.D. degree in computing, computational mathematics, science, and engineering from Boise State University, in 2009, 2014, and 2020, respectively. He is currently a Postdoctoral Researcher with the Idaho National Laboratory, Energy and Environmental Science and Technology Group. His research interests include power and energy system simulations, numerical comput- ing, and data analysis in the areas of resilient control, resilience metrics, integration of renewable generation, microgrids, and dynamic line ratings. He is also involved with the interaction of human factors in power systems. Since 2015, he has been with the IEEE Power and Energy Society.\nLAURENTIU D. MARINOVICI (Member, IEEE) received the B.Eng. degree in computer engineering and the M.Sc. degree in automatic control from the Gheorghe Asachi Technical University of Iaşi, Iaşi, Romania, in 2000 and 2001, respectively, and the M.Sc. and Ph.D. degrees in electrical and computer engineering from Louisiana State University, in 2007 and 2011, respectively. He is currently a Research Engineer with the Pacific Northwest National Laboratory, Optimization and Control\nGroup. His research interests lie at the confluence of software engineering and development, and model-based control, simulation and data analysis. He has been involved with designing, developing, and testing co-simulation platforms to implement and validate control algorithms for applications at different levels of the electrical power grid. Since 2011, he has been with the IEEE Power and Energy Society and IEEE Control Systems Society.\nCRAIG RIEGER (Senior Member, IEEE) received the B.S. and M.S. degrees in chemical engineering from Montana State University, in 1983 and 1985, respectively, and the Ph.D. degree in engineering and applied science from Idaho State University, in 2008. His Ph.D. coursework and dissertation focused on measurements and control, with specific application to intelligent, supervisory ventilation controls for critical infrastructure. He is currently a Professional Engineer. He is the Chief\nControl Systems Research Engineer and a Directorate Fellow with the Idaho National Laboratory (INL), pioneering interdisciplinary research in next generation resilient control systems. The grand challenge provided an integrated research strategy to address the cognitive, cyber-physical challenges of complex control systems into self-aware, trust-confirming, and threatresilient architectures. In addition, he has organized and chaired 14 cosponsored symposia and one National Science Foundation workshop in this new research area and authored more than 75 peer-reviewed publications. He has 20 years of software and hardware design experience for process control system upgrades and new installations. He has also been a supervisor and the technical lead for control systems engineering groups having design, configuration management, and security responsibilities for several INL nuclear facilities and various control system architectures.\nALICE ORRELL received the B.S. degree in mechanical engineering from the University of Vermont and the M.B.A. degree from the University of Washington. She is currently a Professional Engineer and manages the Pacific Northwest National Laboratory's distributed wind research portfolio, which included the MIRACL Project. She is also the Lead Author of the U.S. Department of Energy's annual ''Distributed Wind Market Report.''",
    "context": "Provides context on researchers and their affiliations involved in power grid resilience and control systems research, including specific projects and publications.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      10
    ],
    "id": "1b896171821ce33182bc990497b020c0d231e722080ba98e3d0471c448bddf3c"
  },
  {
    "text": "Tomasz Ryczkowski, Agata Froncz ak & Piotr Fronczak\nIn this paper, we analyse the gravity model in the global passenger air-transport network. We show that in the standard form, the model is inadequate for correctly describing the relationship between passenger flows and typical geo-economic variables that characterize connected countries. We propose a model for transfer flights that allows exploitation of these discrepancies in order to discover hidden subflows in the network. We illustrate its usefulness by retrieving the distance coefficient in the gravity model, which is one of the determinants of the globalization process. Finally, we discuss the correctness of the presented approach by comparing the distance coefficient to several well-known economic events.\nFor many decades, gravity models have been successfully applied in many different contexts for analysing socio-economic flows of varying types. The well-known examples include migration 1-3 , consumer spatial behaviour 4 , inter-city telephone communication flows 5 , hospital-patient flow systems 6 , and international trade 7-12 .\nAll these models predict or describe certain behaviours that mimic gravitational interaction, as described in Isaac Newton's law of gravity. They assume that a flow between two places is directly proportional to their importance (expressed in, e.g., population size, gross domestic product (GDP), or some attractiveness index) and is inversely proportional to the physical distance between them. Thus, the simplest form of the gravity equation, written, for example, for the bilateral trade volume, is given by\n<!-- formula-not-decoded -->\nwhere f ij is the trade volume between country i and country j ; xi xj is the product of their GDPs; r ij is the geographic distance between them; and G is a constant. Gravity models (GM) work particularly well in systems where all the places are directly connected (i.e., where the underlying structure is a complete graph). International trade network is a typical example of such a system. The value f ij of products or services exported from country i to country j does not affect (at least not directly) the other flows in the network.\nUnlike in the above example, most transport networks involve a series of intermediate stops, which are, themselves, generators of originating and terminating traffic (see e.g. Chapter 7 in ref. 13). In such networks, especially for large distances, no direct connection may be present from location i to location j . In these cases, the potential flow, f ij ( g ) , which might be described by Eq. (1), is realized by the increase in subsequent flows ... -f f f f , , , , ib b b b b b j n n n 1 1 2 1 .\nObviously, this scenario will lead to an observed flow, that differs from the expected one:\n<!-- formula-not-decoded -->\nIt means that, in the case of airline networks, the standard gravity model cannot be directly used to estimate weights of the existing connection flights.\nContrary to appearances, the divergence of the gravity model with actual data may prove useful for obtaining deeper insight into the details of the traffic patterns in transportation networks. In this paper, we demonstrate how one can exploit these discrepancies to discover statistical paths i -b 1 -… -bn -j underlying the observed flows, f ij , in the network.\nUsually, traffic data are collected in two ways. First, the data are obtained by counting objects (e.g., people, vehicles, or information packets) that pass any available link in the network. Such a counting provides information about local traffic intensity, but it says nothing about the places or the objects that started the travel or where they plan to finish. Second, the data are obtained by gathering information about the origin and destination of\nFaculty of Physics, Warsaw University of Technology, Koszykowa 75, PL-00-662, Warsaw, Poland. Correspondence and requests for materials should be addressed to P.F. (email: fronczak@if.pw.edu.pl )\nReceived: 30 March 2017\nAccepted: 7 June 2017\nPublished: xx xx xxxx\neach object (e.g., from survey data or from travel tickets) without knowledge about the detailed path each object follows.\nFor this study, we had at our disposal the data of the first type relating to international flights. We have checked that regardless of the choice of xi (GDP, population, size etc.) in the standard gravity model, the flows f ij are not correctly described by Eq. (1). Careful data analysis shows that the observed inconsistency is due to transfer flights, which allow passengers to travel from (or to) less developed regions even though the network is rare. The so-called'transfer passengers' contribute to reducing flight costs and enhance the frequency of flights, which is profitable especially for large airports. They also have a positive impact on the development of small airports. Thus, the understanding of how people choose between different intermediate airports has great practical potential. In this paper, we make a small contribution toward this goal.\nWe propose a simple model of connecting flights, which is confirmed by real data. The main assumption of the model is that the potential flows between two countries, f ij g ( ) , which includes all the passengers who start the journey in country i and end it in country j , regardless of the transfer flights, is given by the gravity law, Eq. (1), with xi xj referring to the product of GDPs (the case of population size is discussed in Supplementary Material). Although the mentioned assumption cannot be directly verified, it is well supported by the common observation that the gravity relationship arises from almost any microscopic economic model that includes costs that increase with distance 7 . The last condition is certainly true in most types of transportation networks.\nThe final subject of this paper is the discussion of the distance coefficient α in Eq. (1). Its behaviour over time is strictly related to the globalization process, which can be conceptualized as a continuous reduction of the effective distance in the world. Unexpectedly, most studies about gravity models in econometrics clearly show that, since the distance coefficient increases in time, the role of the distance grows simultaneously 14-17 . This counter-intuitive result is currently known as the missing globalization puzzle. Here, by recovering the gravity relationship in the flight network, we are able to analyse the time dependence of the distance coefficient in a typical transportation network.\nThe outline of the paper is as follows. First, we provide a version of the gravity model adapted to the flight network. Then, we introducthe model of connecting flights. Finally, we present the obtained results and discuss the behaviour of the distance coefficient. The data used in this study are described in the Methods section.\n\nIntroduces a revised gravity model for airline networks accounting for transfer flights and their impact on the distance coefficient.",
    "original_text": "Tomasz Ryczkowski, Agata Froncz ak & Piotr Fronczak\nIn this paper, we analyse the gravity model in the global passenger air-transport network. We show that in the standard form, the model is inadequate for correctly describing the relationship between passenger flows and typical geo-economic variables that characterize connected countries. We propose a model for transfer flights that allows exploitation of these discrepancies in order to discover hidden subflows in the network. We illustrate its usefulness by retrieving the distance coefficient in the gravity model, which is one of the determinants of the globalization process. Finally, we discuss the correctness of the presented approach by comparing the distance coefficient to several well-known economic events.\nFor many decades, gravity models have been successfully applied in many different contexts for analysing socio-economic flows of varying types. The well-known examples include migration 1-3 , consumer spatial behaviour 4 , inter-city telephone communication flows 5 , hospital-patient flow systems 6 , and international trade 7-12 .\nAll these models predict or describe certain behaviours that mimic gravitational interaction, as described in Isaac Newton's law of gravity. They assume that a flow between two places is directly proportional to their importance (expressed in, e.g., population size, gross domestic product (GDP), or some attractiveness index) and is inversely proportional to the physical distance between them. Thus, the simplest form of the gravity equation, written, for example, for the bilateral trade volume, is given by\n<!-- formula-not-decoded -->\nwhere f ij is the trade volume between country i and country j ; xi xj is the product of their GDPs; r ij is the geographic distance between them; and G is a constant. Gravity models (GM) work particularly well in systems where all the places are directly connected (i.e., where the underlying structure is a complete graph). International trade network is a typical example of such a system. The value f ij of products or services exported from country i to country j does not affect (at least not directly) the other flows in the network.\nUnlike in the above example, most transport networks involve a series of intermediate stops, which are, themselves, generators of originating and terminating traffic (see e.g. Chapter 7 in ref. 13). In such networks, especially for large distances, no direct connection may be present from location i to location j . In these cases, the potential flow, f ij ( g ) , which might be described by Eq. (1), is realized by the increase in subsequent flows ... -f f f f , , , , ib b b b b b j n n n 1 1 2 1 .\nObviously, this scenario will lead to an observed flow, that differs from the expected one:\n<!-- formula-not-decoded -->\nIt means that, in the case of airline networks, the standard gravity model cannot be directly used to estimate weights of the existing connection flights.\nContrary to appearances, the divergence of the gravity model with actual data may prove useful for obtaining deeper insight into the details of the traffic patterns in transportation networks. In this paper, we demonstrate how one can exploit these discrepancies to discover statistical paths i -b 1 -… -bn -j underlying the observed flows, f ij , in the network.\nUsually, traffic data are collected in two ways. First, the data are obtained by counting objects (e.g., people, vehicles, or information packets) that pass any available link in the network. Such a counting provides information about local traffic intensity, but it says nothing about the places or the objects that started the travel or where they plan to finish. Second, the data are obtained by gathering information about the origin and destination of\nFaculty of Physics, Warsaw University of Technology, Koszykowa 75, PL-00-662, Warsaw, Poland. Correspondence and requests for materials should be addressed to P.F. (email: fronczak@if.pw.edu.pl )\nReceived: 30 March 2017\nAccepted: 7 June 2017\nPublished: xx xx xxxx\neach object (e.g., from survey data or from travel tickets) without knowledge about the detailed path each object follows.\nFor this study, we had at our disposal the data of the first type relating to international flights. We have checked that regardless of the choice of xi (GDP, population, size etc.) in the standard gravity model, the flows f ij are not correctly described by Eq. (1). Careful data analysis shows that the observed inconsistency is due to transfer flights, which allow passengers to travel from (or to) less developed regions even though the network is rare. The so-called'transfer passengers' contribute to reducing flight costs and enhance the frequency of flights, which is profitable especially for large airports. They also have a positive impact on the development of small airports. Thus, the understanding of how people choose between different intermediate airports has great practical potential. In this paper, we make a small contribution toward this goal.\nWe propose a simple model of connecting flights, which is confirmed by real data. The main assumption of the model is that the potential flows between two countries, f ij g ( ) , which includes all the passengers who start the journey in country i and end it in country j , regardless of the transfer flights, is given by the gravity law, Eq. (1), with xi xj referring to the product of GDPs (the case of population size is discussed in Supplementary Material). Although the mentioned assumption cannot be directly verified, it is well supported by the common observation that the gravity relationship arises from almost any microscopic economic model that includes costs that increase with distance 7 . The last condition is certainly true in most types of transportation networks.\nThe final subject of this paper is the discussion of the distance coefficient α in Eq. (1). Its behaviour over time is strictly related to the globalization process, which can be conceptualized as a continuous reduction of the effective distance in the world. Unexpectedly, most studies about gravity models in econometrics clearly show that, since the distance coefficient increases in time, the role of the distance grows simultaneously 14-17 . This counter-intuitive result is currently known as the missing globalization puzzle. Here, by recovering the gravity relationship in the flight network, we are able to analyse the time dependence of the distance coefficient in a typical transportation network.\nThe outline of the paper is as follows. First, we provide a version of the gravity model adapted to the flight network. Then, we introducthe model of connecting flights. Finally, we present the obtained results and discuss the behaviour of the distance coefficient. The data used in this study are described in the Methods section.",
    "context": "Introduces a revised gravity model for airline networks accounting for transfer flights and their impact on the distance coefficient.",
    "document": "s41598-017-06108-z.pdf",
    "pages": [
      1,
      2
    ],
    "id": "06de918a758aef03483e0d83eb79e32712720e4741f6164a8c14aa405cd0774b"
  },
  {
    "text": "Simple gravity model. Before we can verify if the gravity model can reproduce the weights of flight connections, we need to determine the value of the constant G in Eq. (1). To do this, one has to keep in mind that, in Eq. (1), in addition to G , there is another free parameter, namely the distance coefficient α . This coefficient is usually found from the slope of the linear relation (see, e.g., Fig. 1 in ref. 17)\n<!-- formula-not-decoded -->\nWe will discuss the distance coefficient in the next subsection. At the moment, let us assume, that it its value is known.\nIn the systems, such as the international trade network, where the flow between i and j only depends on the importance of trading countries, the constant G can be simply obtained from Eq. (1),\n<!-- formula-not-decoded -->\nafter summing over all pairs of countries, i.e.\n<!-- formula-not-decoded -->\nwhere X is the total world GDP, and the left side of Eq. (5) is related to a distance-averaged value of a typical trade channel. This shows that for a fixed value of α , the parameter G can be calculated directly from real data. Unfortunately, this is not the case of the airline network.\nIn the air-transport network, besides the main contribution to the flow f ij coming from the'direct passengers' traveling from i to j , the value f ij also contains those travellers, for which the flight i -j is only an intermediate link in a longer chain of flights. In other words, the total number of occupied seats, i.e., the sum of all the elements f ij of the matrix F ( t ),\n<!-- formula-not-decoded -->\nis larger than the total number of traveling people. In particular, people traveling from i to j with one change occur in this sum twice. Correspondingly, those who travel with two changes (i.e., with three connecting flights) are taken three times. Therefore, the global traffic T can be estimated as follows:\n<!-- formula-not-decoded -->\nwhere the summation runs over all pairs of countries ( i , j ), such that the shortest path between them, in terms of the number of links, is dij , and the expected flow f ij g ( ) is given by the gravity equation (1),\nFigure 1. The observed weights of connections in the airline network, f ij , vs. their expected values, f ij g ( ) . Plots in the same row correspond to the same year: 1996 (top row) and 2004 (bottom row). Values of the distance coefficient α are indicated in the plots. All data are logarithmically binned (black squares). In panel (b), we have also shown raw data for comparison (grey squares).\n<!-- formula-not-decoded -->\nwith xi xj standing for the product of GDPs of the connected countries. This means that the constant G can be estimated from the following relation\n<!-- formula-not-decoded -->\nHaving the constant G estimated, one can plot the observed flows, f ij , versus these expected, f ij q ( ) . In Fig. 1, we present the data for two different years, 1996 and 2004, and for three different values of the distance parameter, α = 0, 1, and 3. The straight line demonstrating the expected flows f ij g ( ) , resulting from Eq. (8), is also drawn for better comparison. Let us note that the noise, which is inherent to the raw data, makes it difficult to clearly estimate the plotted relation (see Fig. 1b). To overcome this problem, in all the figures, we present logarithmically binned data only.\nIt is obvious that the direct applicability of the gravity model to the flight network is at least questionable. The best fit is obtained for α ≈ 1 (panels b) and e) in Fig. 1), which coincides with the results obtained by other studies of the distance coefficient in econometric data 17 . However, even if one agrees with such a choice of the distance coefficient, the fit is correct only for the right part of each plot. Over a span of at least three decades, the expected, f ij g ( ) , and the observed flows, f ij , differ even by several decades. It seems that there are important factors at play other than economic ones that increase the passenger flow between some countries. In the next section, we will show that the connecting flights from country i to j , which do not depend of the economic conditions, xi xj , of these two countries, can radically change the total flow f ij , and we explain the discrepancies between the gravity model and real data presented above.\nModel of connecting flights. We claim that the passenger flow, f ij , from country i to country j , that is observed in the data, is composed of two components:\nFigure 2. Graphical presentation of the summations in Eq. (10).\n- f ij g ( ) - the number of passengers traveling directly from the origin of a trip in the country i to the final destination in country j , which, we assume, is given by Eq. (8),\n- and the number of passengers, f ij transit ( ) , who use the connection i → j as a part of their longer journey.\nFor simplicity, we assume that these longer journeys consist of two direct flights only, i.e., we neglect travels with two or more intermediate stops. This assumption seems to be quite strong. For example, for 2004, we have flight data for 151 countries and 22650 possible connections between them. Only 2308 (10%) of them are direct. There are also 12749 (56%) shortest paths with length equal to 2. It means that we take into consideration only 66% of all possible connections between the countries. However, it is reasonable to expect that the number of passengers traveling with two or more stops is much lesser than the lacking 34% of the global traffic. One of the possible reasons for this is that too many transfers complicate the chance for a convenient schedule, which costs valuable time. Then, it is usually better to choose other kind of transportation to reach a destination. We will come back to this issue later when we discuss the obtained results.\nThe number of passengers f ij transit ( ) can be estimated as follows:\n<!-- formula-not-decoded -->\nwhere the first (second) summation is over such nodes k (respectively l ), that there is no direct connection from i to k (from l to j ). The term p ( i → j → k ) describes the probability that one takes a direct flight from i to j during indirect travel from i to k . Contributions of both summations to the total transit passenger flow f ij transit ( ) are graphically depicted in Fig. 2.\nThe choice of a particular connecting flight from i through j to k (which is expressed by the probability p ( i → j → k )) should depend, in the first approximation, on the distance r ij between i and j , and the distance r jk between j and k . Thus, we omit the other factors such as the convenient flight schedules and type or level of airline service or airport quality, that could influence actual passenger behaviour 18 . Therefore,\n<!-- formula-not-decoded -->\nwhere C is a normalization constant, which is given by\n<!-- formula-not-decoded -->\nand the function f ( r ij , r jk ) should reflect the tendency of the passengers to choose the shortest, and therefore, the cheapest or the fastest connections. Among many possible choices, we have chosen the following form for this function\n<!-- formula-not-decoded -->\nalthough the other possible forms, e.g.\n<!-- formula-not-decoded -->\nlead to similar quantitative results (see Supplementary Material for details).\nNow, having the model defined, one can estimate the total passenger flow between any two countries as follows:\n<!-- formula-not-decoded -->\nwhose components are correspondingly given by Eqs (8) and (10)-(13).\nFigure 3. Performance of the model of connected flights (black squares) against real data (open circles) for two years: 1996 and 2004. Straight lines correspond to the standard gravity model.\n\nThe text establishes the challenges of applying the gravity model to airline networks due to the inclusion of indirect flights and highlights the need to estimate a distance coefficient (α) and the constant G, noting discrepancies between predicted and observed passenger flows.",
    "original_text": "Simple gravity model. Before we can verify if the gravity model can reproduce the weights of flight connections, we need to determine the value of the constant G in Eq. (1). To do this, one has to keep in mind that, in Eq. (1), in addition to G , there is another free parameter, namely the distance coefficient α . This coefficient is usually found from the slope of the linear relation (see, e.g., Fig. 1 in ref. 17)\n<!-- formula-not-decoded -->\nWe will discuss the distance coefficient in the next subsection. At the moment, let us assume, that it its value is known.\nIn the systems, such as the international trade network, where the flow between i and j only depends on the importance of trading countries, the constant G can be simply obtained from Eq. (1),\n<!-- formula-not-decoded -->\nafter summing over all pairs of countries, i.e.\n<!-- formula-not-decoded -->\nwhere X is the total world GDP, and the left side of Eq. (5) is related to a distance-averaged value of a typical trade channel. This shows that for a fixed value of α , the parameter G can be calculated directly from real data. Unfortunately, this is not the case of the airline network.\nIn the air-transport network, besides the main contribution to the flow f ij coming from the'direct passengers' traveling from i to j , the value f ij also contains those travellers, for which the flight i -j is only an intermediate link in a longer chain of flights. In other words, the total number of occupied seats, i.e., the sum of all the elements f ij of the matrix F ( t ),\n<!-- formula-not-decoded -->\nis larger than the total number of traveling people. In particular, people traveling from i to j with one change occur in this sum twice. Correspondingly, those who travel with two changes (i.e., with three connecting flights) are taken three times. Therefore, the global traffic T can be estimated as follows:\n<!-- formula-not-decoded -->\nwhere the summation runs over all pairs of countries ( i , j ), such that the shortest path between them, in terms of the number of links, is dij , and the expected flow f ij g ( ) is given by the gravity equation (1),\nFigure 1. The observed weights of connections in the airline network, f ij , vs. their expected values, f ij g ( ) . Plots in the same row correspond to the same year: 1996 (top row) and 2004 (bottom row). Values of the distance coefficient α are indicated in the plots. All data are logarithmically binned (black squares). In panel (b), we have also shown raw data for comparison (grey squares).\n<!-- formula-not-decoded -->\nwith xi xj standing for the product of GDPs of the connected countries. This means that the constant G can be estimated from the following relation\n<!-- formula-not-decoded -->\nHaving the constant G estimated, one can plot the observed flows, f ij , versus these expected, f ij q ( ) . In Fig. 1, we present the data for two different years, 1996 and 2004, and for three different values of the distance parameter, α = 0, 1, and 3. The straight line demonstrating the expected flows f ij g ( ) , resulting from Eq. (8), is also drawn for better comparison. Let us note that the noise, which is inherent to the raw data, makes it difficult to clearly estimate the plotted relation (see Fig. 1b). To overcome this problem, in all the figures, we present logarithmically binned data only.\nIt is obvious that the direct applicability of the gravity model to the flight network is at least questionable. The best fit is obtained for α ≈ 1 (panels b) and e) in Fig. 1), which coincides with the results obtained by other studies of the distance coefficient in econometric data 17 . However, even if one agrees with such a choice of the distance coefficient, the fit is correct only for the right part of each plot. Over a span of at least three decades, the expected, f ij g ( ) , and the observed flows, f ij , differ even by several decades. It seems that there are important factors at play other than economic ones that increase the passenger flow between some countries. In the next section, we will show that the connecting flights from country i to j , which do not depend of the economic conditions, xi xj , of these two countries, can radically change the total flow f ij , and we explain the discrepancies between the gravity model and real data presented above.\nModel of connecting flights. We claim that the passenger flow, f ij , from country i to country j , that is observed in the data, is composed of two components:\nFigure 2. Graphical presentation of the summations in Eq. (10).\n- f ij g ( ) - the number of passengers traveling directly from the origin of a trip in the country i to the final destination in country j , which, we assume, is given by Eq. (8),\n- and the number of passengers, f ij transit ( ) , who use the connection i → j as a part of their longer journey.\nFor simplicity, we assume that these longer journeys consist of two direct flights only, i.e., we neglect travels with two or more intermediate stops. This assumption seems to be quite strong. For example, for 2004, we have flight data for 151 countries and 22650 possible connections between them. Only 2308 (10%) of them are direct. There are also 12749 (56%) shortest paths with length equal to 2. It means that we take into consideration only 66% of all possible connections between the countries. However, it is reasonable to expect that the number of passengers traveling with two or more stops is much lesser than the lacking 34% of the global traffic. One of the possible reasons for this is that too many transfers complicate the chance for a convenient schedule, which costs valuable time. Then, it is usually better to choose other kind of transportation to reach a destination. We will come back to this issue later when we discuss the obtained results.\nThe number of passengers f ij transit ( ) can be estimated as follows:\n<!-- formula-not-decoded -->\nwhere the first (second) summation is over such nodes k (respectively l ), that there is no direct connection from i to k (from l to j ). The term p ( i → j → k ) describes the probability that one takes a direct flight from i to j during indirect travel from i to k . Contributions of both summations to the total transit passenger flow f ij transit ( ) are graphically depicted in Fig. 2.\nThe choice of a particular connecting flight from i through j to k (which is expressed by the probability p ( i → j → k )) should depend, in the first approximation, on the distance r ij between i and j , and the distance r jk between j and k . Thus, we omit the other factors such as the convenient flight schedules and type or level of airline service or airport quality, that could influence actual passenger behaviour 18 . Therefore,\n<!-- formula-not-decoded -->\nwhere C is a normalization constant, which is given by\n<!-- formula-not-decoded -->\nand the function f ( r ij , r jk ) should reflect the tendency of the passengers to choose the shortest, and therefore, the cheapest or the fastest connections. Among many possible choices, we have chosen the following form for this function\n<!-- formula-not-decoded -->\nalthough the other possible forms, e.g.\n<!-- formula-not-decoded -->\nlead to similar quantitative results (see Supplementary Material for details).\nNow, having the model defined, one can estimate the total passenger flow between any two countries as follows:\n<!-- formula-not-decoded -->\nwhose components are correspondingly given by Eqs (8) and (10)-(13).\nFigure 3. Performance of the model of connected flights (black squares) against real data (open circles) for two years: 1996 and 2004. Straight lines correspond to the standard gravity model.",
    "context": "The text establishes the challenges of applying the gravity model to airline networks due to the inclusion of indirect flights and highlights the need to estimate a distance coefficient (α) and the constant G, noting discrepancies between predicted and observed passenger flows.",
    "document": "s41598-017-06108-z.pdf",
    "pages": [
      2,
      3,
      4,
      5
    ],
    "id": "034ccb16d7cd3e5313fdd2315eb5796c83c491ecd405e9c35b580f7aaabcfd38"
  },
  {
    "text": "In Fig. 3, we compare results obtained from our model of connected flights with real data for two different years, 1996 and 2004. We also plot there the straight lines corresponding to the classical GM, Eq. (8), to demonstrate a significant improvement in performance of the expanded model over GM alone. The largest discrepancies visible in the left part of the plots occur for the long-distance countries with low GDPs, i.e., for large (small) values of the denominator (nominator) in the horizontal axis in Fig. 3. We have checked that these countries are usually island-based (African, Caribbean and Pacific states), and therefore, the travel between them requires multiple transfers - the feature that is not included in our one-stop model. Moreover, a lack of transport alternatives in these countries makes air travel channels more preferred than in the typical continental states. Although it is possible to extend the model to include two-stop connections, we think it is not worth the price, i.e., the significantly increased complexity of the model, especially since its present form correctly predicts more than 98% of the total passenger flow in the world.\nThe numerical results for f ij mcf shown in Fig. 3 have been obtained for the particular values of the distance coefficient α (the reason why we have chosen α = 1.5 and α = 1.6 for years 1996 and 2004, respectively, will become clear shortly). One has to keep in mind that the other values of this quantity can lead to the different results and to better or worse agreement between the model and real data. We can use this observation to select the most probable value of α and to analyse the behaviour of the distance coefficient in time. As mentioned in the introduction, this behaviour can be strictly related with the progress of the globalization process in the context of transportation networks. Thus, analysing changes of the distance coefficient would provide another indicator of the rate of the global integration.\nFor every year in the analysed period 1990-2011, we have created the histograms of empirical and modelled flows, P ( f ij ) and P ( f ij mcf )( α ), respectively, in m = 15 logarithmically spaced bins. The examples of such normalized histograms for year 1996 are presented in Fig. 4b. As one can see, the histograms P ( f ij mcf )( α ) created for different values of the α parameter differ in agreement with the histogram of empirical flows (marked by the shaded grey area). To measure this agreement, ∆ ( α ), we use a simple RMS formula\n<!-- formula-not-decoded -->\nIn Fig. 4a, we show how this quality measure, ∆ ( α ), depends on the parameter α in the year 1996. The clearly visible minimum at α = 1.5 indicates the correct value of the distance coefficient in this year.\nFigure 5 demonstrates the behaviour of the distance coefficient for the years 1990-2011 retrieved by this method. The general conclusion that follows from the figure is that the distance effect in air transportation network is constant over time and the globalization process, which is reflected in the distance coefficient has been stabilized in the XXI century. This conclusion confirms the other results (presented by the grey circles in Fig. 5) obtained in ref. 17, where the authors estimated the distance coefficient for the international trade network.\nNow, let us shortly analyse the major fluctuations around this constant distance coefficient. In Fig. 5, we have marked three historical events that could influence the behaviour of the distance coefficient in the same way as they had an impact on the whole aviation industry. Attacks in New York and Washington D.C. in September of 2001 started a chain of events such as SARS epidemic, additional terrorist attempts, wars, and rising oil prices, that cost the airline industry three years of growth. Airline revenues and traffic surpassed 2000 levels only in 2004 19 . The 2008 global financial crisis costed another several years of growth. The effect was further enhanced by the eruption of the Eyjafjallajökull volcano in Iceland in 2010 that caused the closure of airspace over many countries. The correlation between the distance coefficient and all these events, shown in Fig. 5, confirms that they have a negative impact not only on airline revenues or air traffic but on the whole globalization process.\nFigure 4. ( a ) Example of the agreement measure ∆ ( α ) calculated for different values of parameter α in the year 1996. The arrows show the values for which three histograms P ( f ij mcf )( α ) are shown in panel ( b ). Grey shaded area represents the histogram P ( f ij ) characterizing real data.\nFigure 5. The year-by-year values of the distance coefficient α for the air transportation network resulting from the minimalization of the measure ∆ ( α ) (black squares) and for the world trade network taken from a previous paper 17 (grey circles).\nIt should be noted that the globalization process is sometimes conceptualized as a continuous reduction in the effective distance in the world 20 , which means that the distance coefficient should vanish in time. However, the observed temporary decrease in the distance coefficient is evidently negatively correlated with the progress of globalization. It confirms the recent observations that the distance coefficient is rather associated with the fractal dimension of the considered system and a decrease in this coefficient is the effect of decreasing number and weight of air transport connections, which reduce dimensionality of the system 17 .\nConcluding remarks. The presented model of connecting flights allowed us to retrieve, from the observed flow between any two countries, the terms corresponding to direct and transfer passengers utilizing this connection. Although we neglected many aspects that influence the choice of intermediate airports by travellers, the model allows to correctly predict more than 98% of the total passenger flow in the world. The only assumption we had to take into account was that the gravity model is applicable to the case of air transport network. The\ncorrectness of the above assumption was confirmed by the time behaviour of the retrieved distance coefficient that reflects several historical events with known strong economic impact.\nThere are still many possible research directions that may be worth exploring in this area. First, the most promising of these seems to be derivation of the so-called fluctuation-response relations 21 that would allow predictions of the changes in the flows f ij on the basis of changes in GDPs of the connected countries. Now, when we can determine direct and indirect contributions to the particular flow, this should be possible by the analogy to the similar approach done for international trade network 12 . Next, it would be challenging but also rewarding to extend the model taking into account, e.g., time schedules that strongly determine the passenger preference to select a particular intermediate airport. This would generally allow modelling of the microscopic time-dependent flows in the network. Analysing more detailed level of the air transportation network, in which the nodes represent rather single cities or even airports 22 than the whole countries, can also be interesting for strategic planning in the airport industry.\n\nDetails the time-dependent behavior of the distance coefficient and its correlation with historical events impacting the aviation industry.",
    "original_text": "In Fig. 3, we compare results obtained from our model of connected flights with real data for two different years, 1996 and 2004. We also plot there the straight lines corresponding to the classical GM, Eq. (8), to demonstrate a significant improvement in performance of the expanded model over GM alone. The largest discrepancies visible in the left part of the plots occur for the long-distance countries with low GDPs, i.e., for large (small) values of the denominator (nominator) in the horizontal axis in Fig. 3. We have checked that these countries are usually island-based (African, Caribbean and Pacific states), and therefore, the travel between them requires multiple transfers - the feature that is not included in our one-stop model. Moreover, a lack of transport alternatives in these countries makes air travel channels more preferred than in the typical continental states. Although it is possible to extend the model to include two-stop connections, we think it is not worth the price, i.e., the significantly increased complexity of the model, especially since its present form correctly predicts more than 98% of the total passenger flow in the world.\nThe numerical results for f ij mcf shown in Fig. 3 have been obtained for the particular values of the distance coefficient α (the reason why we have chosen α = 1.5 and α = 1.6 for years 1996 and 2004, respectively, will become clear shortly). One has to keep in mind that the other values of this quantity can lead to the different results and to better or worse agreement between the model and real data. We can use this observation to select the most probable value of α and to analyse the behaviour of the distance coefficient in time. As mentioned in the introduction, this behaviour can be strictly related with the progress of the globalization process in the context of transportation networks. Thus, analysing changes of the distance coefficient would provide another indicator of the rate of the global integration.\nFor every year in the analysed period 1990-2011, we have created the histograms of empirical and modelled flows, P ( f ij ) and P ( f ij mcf )( α ), respectively, in m = 15 logarithmically spaced bins. The examples of such normalized histograms for year 1996 are presented in Fig. 4b. As one can see, the histograms P ( f ij mcf )( α ) created for different values of the α parameter differ in agreement with the histogram of empirical flows (marked by the shaded grey area). To measure this agreement, ∆ ( α ), we use a simple RMS formula\n<!-- formula-not-decoded -->\nIn Fig. 4a, we show how this quality measure, ∆ ( α ), depends on the parameter α in the year 1996. The clearly visible minimum at α = 1.5 indicates the correct value of the distance coefficient in this year.\nFigure 5 demonstrates the behaviour of the distance coefficient for the years 1990-2011 retrieved by this method. The general conclusion that follows from the figure is that the distance effect in air transportation network is constant over time and the globalization process, which is reflected in the distance coefficient has been stabilized in the XXI century. This conclusion confirms the other results (presented by the grey circles in Fig. 5) obtained in ref. 17, where the authors estimated the distance coefficient for the international trade network.\nNow, let us shortly analyse the major fluctuations around this constant distance coefficient. In Fig. 5, we have marked three historical events that could influence the behaviour of the distance coefficient in the same way as they had an impact on the whole aviation industry. Attacks in New York and Washington D.C. in September of 2001 started a chain of events such as SARS epidemic, additional terrorist attempts, wars, and rising oil prices, that cost the airline industry three years of growth. Airline revenues and traffic surpassed 2000 levels only in 2004 19 . The 2008 global financial crisis costed another several years of growth. The effect was further enhanced by the eruption of the Eyjafjallajökull volcano in Iceland in 2010 that caused the closure of airspace over many countries. The correlation between the distance coefficient and all these events, shown in Fig. 5, confirms that they have a negative impact not only on airline revenues or air traffic but on the whole globalization process.\nFigure 4. ( a ) Example of the agreement measure ∆ ( α ) calculated for different values of parameter α in the year 1996. The arrows show the values for which three histograms P ( f ij mcf )( α ) are shown in panel ( b ). Grey shaded area represents the histogram P ( f ij ) characterizing real data.\nFigure 5. The year-by-year values of the distance coefficient α for the air transportation network resulting from the minimalization of the measure ∆ ( α ) (black squares) and for the world trade network taken from a previous paper 17 (grey circles).\nIt should be noted that the globalization process is sometimes conceptualized as a continuous reduction in the effective distance in the world 20 , which means that the distance coefficient should vanish in time. However, the observed temporary decrease in the distance coefficient is evidently negatively correlated with the progress of globalization. It confirms the recent observations that the distance coefficient is rather associated with the fractal dimension of the considered system and a decrease in this coefficient is the effect of decreasing number and weight of air transport connections, which reduce dimensionality of the system 17 .\nConcluding remarks. The presented model of connecting flights allowed us to retrieve, from the observed flow between any two countries, the terms corresponding to direct and transfer passengers utilizing this connection. Although we neglected many aspects that influence the choice of intermediate airports by travellers, the model allows to correctly predict more than 98% of the total passenger flow in the world. The only assumption we had to take into account was that the gravity model is applicable to the case of air transport network. The\ncorrectness of the above assumption was confirmed by the time behaviour of the retrieved distance coefficient that reflects several historical events with known strong economic impact.\nThere are still many possible research directions that may be worth exploring in this area. First, the most promising of these seems to be derivation of the so-called fluctuation-response relations 21 that would allow predictions of the changes in the flows f ij on the basis of changes in GDPs of the connected countries. Now, when we can determine direct and indirect contributions to the particular flow, this should be possible by the analogy to the similar approach done for international trade network 12 . Next, it would be challenging but also rewarding to extend the model taking into account, e.g., time schedules that strongly determine the passenger preference to select a particular intermediate airport. This would generally allow modelling of the microscopic time-dependent flows in the network. Analysing more detailed level of the air transportation network, in which the nodes represent rather single cities or even airports 22 than the whole countries, can also be interesting for strategic planning in the airport industry.",
    "context": "Details the time-dependent behavior of the distance coefficient and its correlation with historical events impacting the aviation industry.",
    "document": "s41598-017-06108-z.pdf",
    "pages": [
      5,
      6,
      7
    ],
    "id": "eab3c6bf7e74fde417c54b6cadd3491fbeb105ca5c6c6ed118561b3f31eb74de"
  },
  {
    "text": "Results reported in this paper are based on data provided by International Civil Aviation Organization (ICAO). They contain 'annual traffic on-board aircraft on individual flight stages of international scheduled services' 23 . As a flight stage or a direct flight, we understand 'the operation of an aircraft from take-off to landing' 24 . It means that if a particular flight consists of two (or more) flight stages, we consider it as two (or more) separated direct flights.\nAmong the many attributes the data contain, such as aircraft type used, the number of flights operated, the aircraft capacity offered, and the traffic (passengers, freight and mail) carried, in our analyses, we use only the number of passengers traveling between countries. The data are employed to build a sequence of weighted directed networks, F ( t ), in the consecutive years t = 1990, … , 2011. In each network, each country is represented by a node and the weight of a link f ij ( t ) refers to the number of passengers traveling from i to j in year t . The flows f ij ( t ) may vary from a few persons (e.g., 6 people travelled for Togo to Uruguay in 2004) to several millions of passengers (e.g., 9532303 people travelled from Great Britain to USA in 2000).\nApart from traffic data, we also use econometric data from Penn World Table 8.1 25 . To characterize the economic performance of a country we use real GDP at constant 2005 national prices value xi ( t ) (in mil. 2005US$). The distance between countries is based on CEPII data 26 . Geodesic distances therein are calculated following the great circle formula, which uses latitudes and longitudes of the most important cities/agglomerations (in terms of population).\n\nDefines the data sources and methodology for constructing weighted directed networks representing international passenger flows and incorporating economic performance data.",
    "original_text": "Results reported in this paper are based on data provided by International Civil Aviation Organization (ICAO). They contain 'annual traffic on-board aircraft on individual flight stages of international scheduled services' 23 . As a flight stage or a direct flight, we understand 'the operation of an aircraft from take-off to landing' 24 . It means that if a particular flight consists of two (or more) flight stages, we consider it as two (or more) separated direct flights.\nAmong the many attributes the data contain, such as aircraft type used, the number of flights operated, the aircraft capacity offered, and the traffic (passengers, freight and mail) carried, in our analyses, we use only the number of passengers traveling between countries. The data are employed to build a sequence of weighted directed networks, F ( t ), in the consecutive years t = 1990, … , 2011. In each network, each country is represented by a node and the weight of a link f ij ( t ) refers to the number of passengers traveling from i to j in year t . The flows f ij ( t ) may vary from a few persons (e.g., 6 people travelled for Togo to Uruguay in 2004) to several millions of passengers (e.g., 9532303 people travelled from Great Britain to USA in 2000).\nApart from traffic data, we also use econometric data from Penn World Table 8.1 25 . To characterize the economic performance of a country we use real GDP at constant 2005 national prices value xi ( t ) (in mil. 2005US$). The distance between countries is based on CEPII data 26 . Geodesic distances therein are calculated following the great circle formula, which uses latitudes and longitudes of the most important cities/agglomerations (in terms of population).",
    "context": "Defines the data sources and methodology for constructing weighted directed networks representing international passenger flows and incorporating economic performance data.",
    "document": "s41598-017-06108-z.pdf",
    "pages": [
      7
    ],
    "id": "ff85f0095d1028eb9297c9344bebab762a13a9d171b812b8af6ef85d31185b1d"
  },
  {
    "text": "1.  Lewer, J. J. & den Berg, H. V . A gravity model of immigration. Economics Letter 99 , 164-167 (2008).\n2.  Karemera, D., Oguledo, V . I. & Davis, B. A gravity model analysis of international migration to north america. Applied Economics 32 , 1745-1755 (2000).\n3.  Ravenstein, E. The laws of migration. J. Royal. Stat. Soc. 52 , 241-310 (1885).\n4.  Huff, D. L. A probabilistic analysis of shopping center trade areas. Land Economics 39 , 81-90 (1963).\n5.  Krings, G., Calabrese, F., Ratti, C. & Blondel, V . D. A gravity model for inter-city telephone communication networks. J. Stat. Mech. 7 , L7003 (2009).\n6.  Lowe, J. M. & Sen, A. Gravity model applications in health planning: analysis of an urban hospital market. J. Regional Sci 36 , 437-461 (1996).\n7.  Deardorff, A. V . vol. 69, chap. Determinants of Bilateral Trade: Does Gravity Work in a Neoclassical World ? 106 (University of Chicago Press, Chicago, 1998).\n8.  Anderson, J. E. A theoretical foundation for the gravity equation. Amer. Econ. Rev. 69 , 106-116 (1979).\n9.  Bergstrand, J. H. The gravity equation in international trade: some microscopic foundations and empirial evidence. Rev. Econ. Stat. 67 , 474-481 (1985).\n10.  Bhattacharya, K., Mukherjee, G., Saramaki, J., Kaski, K. & Manna, S. S. The international trade network: weighted network analysis and modelling. J. Stat. Mech. 02 , P0 (2002).\n11.  Duenas, M. & Fagiolo, G. Modeling the international-trade network: a gravity approach. J. Econ. Inter. Coord. 8 , 155-178 (2013).\n12.  Fronczak, A. & Fronczak, P . Statistical mechanics of the international trade network. Phys. Rev. E 85 , 056113 (2012).\n13.  T affee, E. J., Gauthier, H. L. & O'Kelly, M. E. Geography of Transportation (Upper Saddle River, NJ: Prentice Hall, 1996), second edn.\n14.  Coe, D. T., Subramanian, A. & Tamirisa, N. T. Missing globalization puzzle. IMF Staff Papers 54 , 34 (2007).\n15.  Brun, J.-F ., Carrere, C., Guillaumont, P . & de Melo, J. Has distance died? evidence from a panel gravity model. World Bark Econ. Rev. 19 , 1 (2005).\n16.  Disdier, A.-C. & Head, K. The puzzling persistence of the distance effect on bilateral trade. Rev. Econ. Stat. 90 , 37 (2008).\n17.  Karpiarz, M., Fronczak, A. & Fronczak, P . International trade network: fractal properties and globalization puzzle. Phys. Rev. Lett. 113 , 248701 (2014).\n18.  Johnson, D., Hess, S. & Matthews, B. Understanding air travellers' trade-offs between connecting flights and surface access characteristics. J. Air Transp. Manag. 34 , 70-77 (2014).\n19.  International Air Transport Association (2012). The impact of september 11 2001 on aviation. Geneva, Switzerland. http://www.iata. org/pressroom/Documents/impact-9-11-aviation.pdf (Date of access: 01/06/2017).\n20.  Cairncross, F. The Death of Distance: How the Communications Revolution Is Changing Our Lives (Harvard Business School Press, Cambridge, 1997), 1 edn.\n21.  Fronczak, A., Fronczak, P . & Holyst, J. Fluctuation-dissipation relations in complex networks. Phys. Rev. E 73 , 016108 (2016).\n22.  Grosche, T., Rothlauf, F. & Heinzl, A. Gravity models for airline passenger volume estimation. J. Air Transp. Manag. 13 , 175-183 (2007).\n23.  I nternational Civil Aviation Organization (2016). ICAO Data Plus, Montréal, Canada. https://www4.icao.int/NewDataPlus/Tools (Date of access: 01/06/2017).\n24.  US Government Electronic Code of Federal Regulations. Government Printing Office, 14 C.F.R§ 241.03 (2017).\n25.  Heston A., Summers R. & Aten B. Penn World Table Version 8.1, C enter for International Comparisons of Production , Income and Prices at the University of Pennsylvania (2011).\n26.  Mayer, T. & Zignago, S. Notes on CEPII's distances measures. MPRA Paper 26469, University Library of Munich, Germany (2006).\n\nSummarizes research utilizing and discussing gravity models for analyzing international migration, trade, and transportation patterns.",
    "original_text": "1.  Lewer, J. J. & den Berg, H. V . A gravity model of immigration. Economics Letter 99 , 164-167 (2008).\n2.  Karemera, D., Oguledo, V . I. & Davis, B. A gravity model analysis of international migration to north america. Applied Economics 32 , 1745-1755 (2000).\n3.  Ravenstein, E. The laws of migration. J. Royal. Stat. Soc. 52 , 241-310 (1885).\n4.  Huff, D. L. A probabilistic analysis of shopping center trade areas. Land Economics 39 , 81-90 (1963).\n5.  Krings, G., Calabrese, F., Ratti, C. & Blondel, V . D. A gravity model for inter-city telephone communication networks. J. Stat. Mech. 7 , L7003 (2009).\n6.  Lowe, J. M. & Sen, A. Gravity model applications in health planning: analysis of an urban hospital market. J. Regional Sci 36 , 437-461 (1996).\n7.  Deardorff, A. V . vol. 69, chap. Determinants of Bilateral Trade: Does Gravity Work in a Neoclassical World ? 106 (University of Chicago Press, Chicago, 1998).\n8.  Anderson, J. E. A theoretical foundation for the gravity equation. Amer. Econ. Rev. 69 , 106-116 (1979).\n9.  Bergstrand, J. H. The gravity equation in international trade: some microscopic foundations and empirial evidence. Rev. Econ. Stat. 67 , 474-481 (1985).\n10.  Bhattacharya, K., Mukherjee, G., Saramaki, J., Kaski, K. & Manna, S. S. The international trade network: weighted network analysis and modelling. J. Stat. Mech. 02 , P0 (2002).\n11.  Duenas, M. & Fagiolo, G. Modeling the international-trade network: a gravity approach. J. Econ. Inter. Coord. 8 , 155-178 (2013).\n12.  Fronczak, A. & Fronczak, P . Statistical mechanics of the international trade network. Phys. Rev. E 85 , 056113 (2012).\n13.  T affee, E. J., Gauthier, H. L. & O'Kelly, M. E. Geography of Transportation (Upper Saddle River, NJ: Prentice Hall, 1996), second edn.\n14.  Coe, D. T., Subramanian, A. & Tamirisa, N. T. Missing globalization puzzle. IMF Staff Papers 54 , 34 (2007).\n15.  Brun, J.-F ., Carrere, C., Guillaumont, P . & de Melo, J. Has distance died? evidence from a panel gravity model. World Bark Econ. Rev. 19 , 1 (2005).\n16.  Disdier, A.-C. & Head, K. The puzzling persistence of the distance effect on bilateral trade. Rev. Econ. Stat. 90 , 37 (2008).\n17.  Karpiarz, M., Fronczak, A. & Fronczak, P . International trade network: fractal properties and globalization puzzle. Phys. Rev. Lett. 113 , 248701 (2014).\n18.  Johnson, D., Hess, S. & Matthews, B. Understanding air travellers' trade-offs between connecting flights and surface access characteristics. J. Air Transp. Manag. 34 , 70-77 (2014).\n19.  International Air Transport Association (2012). The impact of september 11 2001 on aviation. Geneva, Switzerland. http://www.iata. org/pressroom/Documents/impact-9-11-aviation.pdf (Date of access: 01/06/2017).\n20.  Cairncross, F. The Death of Distance: How the Communications Revolution Is Changing Our Lives (Harvard Business School Press, Cambridge, 1997), 1 edn.\n21.  Fronczak, A., Fronczak, P . & Holyst, J. Fluctuation-dissipation relations in complex networks. Phys. Rev. E 73 , 016108 (2016).\n22.  Grosche, T., Rothlauf, F. & Heinzl, A. Gravity models for airline passenger volume estimation. J. Air Transp. Manag. 13 , 175-183 (2007).\n23.  I nternational Civil Aviation Organization (2016). ICAO Data Plus, Montréal, Canada. https://www4.icao.int/NewDataPlus/Tools (Date of access: 01/06/2017).\n24.  US Government Electronic Code of Federal Regulations. Government Printing Office, 14 C.F.R§ 241.03 (2017).\n25.  Heston A., Summers R. & Aten B. Penn World Table Version 8.1, C enter for International Comparisons of Production , Income and Prices at the University of Pennsylvania (2011).\n26.  Mayer, T. & Zignago, S. Notes on CEPII's distances measures. MPRA Paper 26469, University Library of Munich, Germany (2006).",
    "context": "Summarizes research utilizing and discussing gravity models for analyzing international migration, trade, and transportation patterns.",
    "document": "s41598-017-06108-z.pdf",
    "pages": [
      7
    ],
    "id": "0df78c5e6669bb2364e1e7dbb5bca087b2161154b62f6d5539d991c0e11a4be1"
  },
  {
    "text": "The work has been supported from the National Science Centre in Poland (grant no. 2012/05/E/ST2/02300).\n\nAcknowledges funding source and grant number.",
    "original_text": "The work has been supported from the National Science Centre in Poland (grant no. 2012/05/E/ST2/02300).",
    "context": "Acknowledges funding source and grant number.",
    "document": "s41598-017-06108-z.pdf",
    "pages": [
      8
    ],
    "id": "07b81941e0df6e3491dd893be6c0b6be6276e91fe1fe64b66e0d91930eba8aac"
  },
  {
    "text": "T.R., P .F . and A.F. wrote the main text, analyzed the data, and discussed the results.\n\nIdentifies the authors and their roles in the research process.",
    "original_text": "T.R., P .F . and A.F. wrote the main text, analyzed the data, and discussed the results.",
    "context": "Identifies the authors and their roles in the research process.",
    "document": "s41598-017-06108-z.pdf",
    "pages": [
      8
    ],
    "id": "6f8a980f08ae5af32a69d7d1f76eee3685cb382cfdc7216b1b9f49cffef28178"
  },
  {
    "text": "Supplementary information accompanies this paper at doi:10.1038/s41598-017-06108-z\nCompeting Interests: The authors declare that they have no competing interests.\nPublisher's note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2017\n\nDetails legal and copyright information regarding the article's publication and usage.",
    "original_text": "Supplementary information accompanies this paper at doi:10.1038/s41598-017-06108-z\nCompeting Interests: The authors declare that they have no competing interests.\nPublisher's note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2017",
    "context": "Details legal and copyright information regarding the article's publication and usage.",
    "document": "s41598-017-06108-z.pdf",
    "pages": [
      8
    ],
    "id": "d294dfaa3d71f881260de3a54ba2582fc01ebeb675a5e1a9c846322d6f5d9285"
  },
  {
    "text": "1234567890():,;\n\nN/A\n",
    "original_text": "1234567890():,;",
    "context": "N/A\n",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      1
    ],
    "id": "46a2bc46175cdc109a96d97b08f8146e1c0c18c8da9462c0a5f6c0851427f11c"
  },
  {
    "text": "https://doi.org/10.1038/s41467-020-15356-z\nOPEN\n\nDetails the methodology for analyzing the gut microbiome in a cohort of patients with Parkinson’s disease.",
    "original_text": "https://doi.org/10.1038/s41467-020-15356-z\nOPEN",
    "context": "Details the methodology for analyzing the gut microbiome in a cohort of patients with Parkinson’s disease.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      1
    ],
    "id": "107af56134d4c13f9c550aa2b5bb14ba6adc75253e20e6a3b89fbf7654aad9d2"
  },
  {
    "text": "Alessandro Spelta 1,2 ✉ , Andrea Flori 3 , Nicolò Pecora 4 , Sergey Buldyrev 5 & Fabio Pammolli 2,3\nWe introduce an indicator that aims to detect the emergence of market instabilities by quantifying the intensity of self-organizing processes arising from stock returns ' comovements. In /uniFB01 nancial markets, phenomena like imitation, herding and positive feedbacks characterize the emergence of endogenous instabilities, which can modify the qualitative and quantitative behavior of the underlying system. The impossibility to formalize ex-ante the dynamic laws that rule the evolution of /uniFB01 nancial systems motivates the use of a parsimonious synthetic indicator to detect the disruption of an existing equilibrium con /uniFB01 guration. Here we show that the emergence of an interconnected sub-graph of stock returns co-movements from a broader market index is a signal of an out-of-equilibrium transition of the underlying system. To test the validity of our approach, we propose a model-free application that builds on the identi /uniFB01 cation of up and down market phases.\n1 Department of Economics and Management, University of Pavia, Via San Felice 7, 27100 Pavia, Italy. 2 CADS, Joint Center for Analysis, Decisions and Society, Human Technopole, Milan, Italy. 3 Department of Management, Economics and Industrial Engineering, Politecnico di Milano, Via Lambruschini, 4/B, 20156 Milan, Italy. 4 Department of Economics and Social Sciences, Catholic University, Via Emilia Parmense 84, 29122 Piacenza, Italy. 5 Department of Physics, Yeshiva University, 500 West 185th Street, Belfer Hall, New York City, NY, USA. ✉ email: alessandro.spelta@unipv.it\nT his paper tackles the issue of detecting long-range memory and co-movements across /uniFB01 nancial time series as informative signals of market instability and of upcoming changes in the dynamic laws governing the evolution of the system. A precise mathematical description of the underlying system through dynamic equations is, in fact, not feasible during the transition between equilibria. An in-depth inspection of the system is thus instrumental to uncover how the evolving relationships between market participants induce distinguishable variations in the set of /uniFB01 nancial variables, which may lead to instability 1 -5 .\nImitation, herding behaviors, and positive feedbacks among market participants have been recognized as phenomena leading to endogenous instabilities 6,7 . Herding behaviors spread when the knowledge about other investors ' allocation decisions in /uniFB02 uences personal strategies, meaning that investors tend to use similar investment practices to those applied by other market participants even when this is not justi /uniFB01 ed by their own information set 8 -10 , while positive feedbacks can induce the underlying system to accumulate instabilities that lead to new con /uniFB01 gurations as a selfful /uniFB01 lling mechanism 11 -14 . Hence, a strengthening of interactions among asset prices may emerge due to market euphoria, which drives prices to a sharp rise or, by contrast, to phenomena of /uniFB01 nancial turmoil, which induce /uniFB01 re sales and stock market crashes.\nSeveral techniques have been applied in the literature to study how cross-market linkages, co-movements, and interdependencies between stocks contribute to in /uniFB02 uence the sustainability conditions of /uniFB01 nancial markets and, possibly, the mechanisms behind shock transmission 15 -19 . Here, against this background, we focus on the intensity of self-organizing processes arising from stock returns ' co-movements and self-similarities.\nInspired by H.A. Simon ' s near decomposability condition 20 to represent a stable system con /uniFB01 guration 21 -23 , we hypothesize that, during instability phases, a sub-graph of stocks displays increasing co-movements and self-similarity patterns, which we propose to quantify by means of the Pearson ' s correlation coef /uniFB01 cient (PCC) and the autocovariance (AC) of stock returns (see Supplementary information, Section 3.1). We refer to this sub-graph of stocks as the leading temporal module (LTM) of the system, whose dynamics is anticipatory for the whole underlying system. In particular, when the system is approaching a change in its equilibrium con /uniFB01 guration, we observe that the absolute value of the average PCC is increasing within the set of stocks composing the LTM sub-graph, but decreasing between stocks belonging to the LTM and stocks outside the LTM group, while the average AC of stocks within the LTM is increasing.\nA rigorous investigation of the properties of the LTM, based on its temporal and spatial dimensions, allows us to build a synthetic and /uniFB02 exible indicator, which we use to detect the emergence of signi /uniFB01 cant changes in the underlying /uniFB01 nancial market. We propose a parsimonious aggregate indicator based on the mean absolute value of the AC of the stocks belonging to the LTM ( < j AC LTM t j > ) and on the ratio between the correlations of stocks within the LTM ( < j PCC LTM t j > ) and the correlations of stocks outside the leading module ( < j PCC f LTM t j > ). We relate the /uniFB01 rst component to the existence of positive feedbacks in the market 24,25 , while the second component reveals the presence of herding behaviors among investors 26,27 . The corresponding synthetic indicator is de /uniFB01 ned accordingly as:\n<!-- formula-not-decoded -->\nTo identify those stocks that have a higher potential triggering, before applying the LTM procedure, we use the detrended\n/uniFB02 uctuation analysis (DFA) 28 -30 on the time series of the original returns and on data obtained by independent time permutation. We focus on stocks that show DFA exponents signi /uniFB01 cantly different from 1/2, which is the expected value for a memoryless signal. Hence, the LTM identi /uniFB01 cation is performed within the set of stocks for which the DFA indicates the presence of long-range memory. For comparison purposes, we also verify the predictive properties of both the set of stocks that have a statistically signi /uniFB01 cant DFA, but are not in the LTM sub-graph (namely, DFA -) and the ones not selected by neither the DFA nor the LTM procedures (indicated as Rest).\nTo mimic the possible system dynamics far and near a transition point, we also employ a Lotka -Volterra model of stocks dynamics (see Supplementary information, Section 3.3). Speci /uniFB01 -cally, we simulate the system with different values of the bifurcation parameter and then we compute the statistical components of our proposed indicator. We note that while far from transition the time series exhibit small correlations and relatively low ACs, close to the bifurcation point the series exhibit both higher ACs and stronger correlation values.\nFinally, we implement an illustrative investment strategy that builds on the identi /uniFB01 cation of the emergence of up and down market phases 31,32 to show the functioning of our approach. Our analysis thus contributes to the understanding of /uniFB01 nancial markets by studying how the effects of linkages at the micro-level turn out to be relevant at the macro-level in the corresponding aggregate system. In fact, at the micro-level investors interact through heterogeneous allocation strategies, adapting their behavior in response to the performance of their investments, the arrival of new information, and the interplay of social interactions and observations, which generate, at the macro-level, non-trivial aggregate patterns of the corresponding /uniFB01 nancial system 13,33 -40 . Related to our work is, therefore, the approach of employing community detection methodologies 41 -43 to understand the properties of the dynamic processes taking place in a correlation network, from which the detection of the LTM is inspired.\nMoreover, we can establish a link between our approach and what is observed in natural sciences, since variations in asset prices can be seen as the social equivalent of nucleation phenomena near the limit of stability in a thermodynamic system, such as a superheated liquid or supercooled gas 44 . In our approach, the LTM can be viewed as analogous to the nucleus of the new phase for /uniFB01 nancial markets. We can say the indicator I LTM t plays a role similar to compressibility in thermodynamic systems, that is, the macroscopic thermodynamic quantity referring to the increasing instability near the spinodal lines.\n\nThe paper introduces a synthetic indicator to detect financial market instability by quantifying self-organizing processes within stock return co-movements, specifically identifying a leading temporal module (LTM) of stocks exhibiting increasing co-movement and self-similarity patterns.",
    "original_text": "Alessandro Spelta 1,2 ✉ , Andrea Flori 3 , Nicolò Pecora 4 , Sergey Buldyrev 5 & Fabio Pammolli 2,3\nWe introduce an indicator that aims to detect the emergence of market instabilities by quantifying the intensity of self-organizing processes arising from stock returns ' comovements. In /uniFB01 nancial markets, phenomena like imitation, herding and positive feedbacks characterize the emergence of endogenous instabilities, which can modify the qualitative and quantitative behavior of the underlying system. The impossibility to formalize ex-ante the dynamic laws that rule the evolution of /uniFB01 nancial systems motivates the use of a parsimonious synthetic indicator to detect the disruption of an existing equilibrium con /uniFB01 guration. Here we show that the emergence of an interconnected sub-graph of stock returns co-movements from a broader market index is a signal of an out-of-equilibrium transition of the underlying system. To test the validity of our approach, we propose a model-free application that builds on the identi /uniFB01 cation of up and down market phases.\n1 Department of Economics and Management, University of Pavia, Via San Felice 7, 27100 Pavia, Italy. 2 CADS, Joint Center for Analysis, Decisions and Society, Human Technopole, Milan, Italy. 3 Department of Management, Economics and Industrial Engineering, Politecnico di Milano, Via Lambruschini, 4/B, 20156 Milan, Italy. 4 Department of Economics and Social Sciences, Catholic University, Via Emilia Parmense 84, 29122 Piacenza, Italy. 5 Department of Physics, Yeshiva University, 500 West 185th Street, Belfer Hall, New York City, NY, USA. ✉ email: alessandro.spelta@unipv.it\nT his paper tackles the issue of detecting long-range memory and co-movements across /uniFB01 nancial time series as informative signals of market instability and of upcoming changes in the dynamic laws governing the evolution of the system. A precise mathematical description of the underlying system through dynamic equations is, in fact, not feasible during the transition between equilibria. An in-depth inspection of the system is thus instrumental to uncover how the evolving relationships between market participants induce distinguishable variations in the set of /uniFB01 nancial variables, which may lead to instability 1 -5 .\nImitation, herding behaviors, and positive feedbacks among market participants have been recognized as phenomena leading to endogenous instabilities 6,7 . Herding behaviors spread when the knowledge about other investors ' allocation decisions in /uniFB02 uences personal strategies, meaning that investors tend to use similar investment practices to those applied by other market participants even when this is not justi /uniFB01 ed by their own information set 8 -10 , while positive feedbacks can induce the underlying system to accumulate instabilities that lead to new con /uniFB01 gurations as a selfful /uniFB01 lling mechanism 11 -14 . Hence, a strengthening of interactions among asset prices may emerge due to market euphoria, which drives prices to a sharp rise or, by contrast, to phenomena of /uniFB01 nancial turmoil, which induce /uniFB01 re sales and stock market crashes.\nSeveral techniques have been applied in the literature to study how cross-market linkages, co-movements, and interdependencies between stocks contribute to in /uniFB02 uence the sustainability conditions of /uniFB01 nancial markets and, possibly, the mechanisms behind shock transmission 15 -19 . Here, against this background, we focus on the intensity of self-organizing processes arising from stock returns ' co-movements and self-similarities.\nInspired by H.A. Simon ' s near decomposability condition 20 to represent a stable system con /uniFB01 guration 21 -23 , we hypothesize that, during instability phases, a sub-graph of stocks displays increasing co-movements and self-similarity patterns, which we propose to quantify by means of the Pearson ' s correlation coef /uniFB01 cient (PCC) and the autocovariance (AC) of stock returns (see Supplementary information, Section 3.1). We refer to this sub-graph of stocks as the leading temporal module (LTM) of the system, whose dynamics is anticipatory for the whole underlying system. In particular, when the system is approaching a change in its equilibrium con /uniFB01 guration, we observe that the absolute value of the average PCC is increasing within the set of stocks composing the LTM sub-graph, but decreasing between stocks belonging to the LTM and stocks outside the LTM group, while the average AC of stocks within the LTM is increasing.\nA rigorous investigation of the properties of the LTM, based on its temporal and spatial dimensions, allows us to build a synthetic and /uniFB02 exible indicator, which we use to detect the emergence of signi /uniFB01 cant changes in the underlying /uniFB01 nancial market. We propose a parsimonious aggregate indicator based on the mean absolute value of the AC of the stocks belonging to the LTM ( < j AC LTM t j > ) and on the ratio between the correlations of stocks within the LTM ( < j PCC LTM t j > ) and the correlations of stocks outside the leading module ( < j PCC f LTM t j > ). We relate the /uniFB01 rst component to the existence of positive feedbacks in the market 24,25 , while the second component reveals the presence of herding behaviors among investors 26,27 . The corresponding synthetic indicator is de /uniFB01 ned accordingly as:\n<!-- formula-not-decoded -->\nTo identify those stocks that have a higher potential triggering, before applying the LTM procedure, we use the detrended\n/uniFB02 uctuation analysis (DFA) 28 -30 on the time series of the original returns and on data obtained by independent time permutation. We focus on stocks that show DFA exponents signi /uniFB01 cantly different from 1/2, which is the expected value for a memoryless signal. Hence, the LTM identi /uniFB01 cation is performed within the set of stocks for which the DFA indicates the presence of long-range memory. For comparison purposes, we also verify the predictive properties of both the set of stocks that have a statistically signi /uniFB01 cant DFA, but are not in the LTM sub-graph (namely, DFA -) and the ones not selected by neither the DFA nor the LTM procedures (indicated as Rest).\nTo mimic the possible system dynamics far and near a transition point, we also employ a Lotka -Volterra model of stocks dynamics (see Supplementary information, Section 3.3). Speci /uniFB01 -cally, we simulate the system with different values of the bifurcation parameter and then we compute the statistical components of our proposed indicator. We note that while far from transition the time series exhibit small correlations and relatively low ACs, close to the bifurcation point the series exhibit both higher ACs and stronger correlation values.\nFinally, we implement an illustrative investment strategy that builds on the identi /uniFB01 cation of the emergence of up and down market phases 31,32 to show the functioning of our approach. Our analysis thus contributes to the understanding of /uniFB01 nancial markets by studying how the effects of linkages at the micro-level turn out to be relevant at the macro-level in the corresponding aggregate system. In fact, at the micro-level investors interact through heterogeneous allocation strategies, adapting their behavior in response to the performance of their investments, the arrival of new information, and the interplay of social interactions and observations, which generate, at the macro-level, non-trivial aggregate patterns of the corresponding /uniFB01 nancial system 13,33 -40 . Related to our work is, therefore, the approach of employing community detection methodologies 41 -43 to understand the properties of the dynamic processes taking place in a correlation network, from which the detection of the LTM is inspired.\nMoreover, we can establish a link between our approach and what is observed in natural sciences, since variations in asset prices can be seen as the social equivalent of nucleation phenomena near the limit of stability in a thermodynamic system, such as a superheated liquid or supercooled gas 44 . In our approach, the LTM can be viewed as analogous to the nucleus of the new phase for /uniFB01 nancial markets. We can say the indicator I LTM t plays a role similar to compressibility in thermodynamic systems, that is, the macroscopic thermodynamic quantity referring to the increasing instability near the spinodal lines.",
    "context": "The paper introduces a synthetic indicator to detect financial market instability by quantifying self-organizing processes within stock return co-movements, specifically identifying a leading temporal module (LTM) of stocks exhibiting increasing co-movement and self-similarity patterns.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      1,
      2
    ],
    "id": "086f5b9ed15ae21446b3e3fe64a197abad0e67dae102e2e64dace7d9b95c5282"
  },
  {
    "text": "The LTM indicator . We analyze the stocks referring to the STOXX Asia/Paci /uniFB01 c 600 Index, which is a broad and liquid subset of the STOXX Global 1800 Index. We investigate the dynamics of the aggregate index starting from the micro-level represented by the stocks that approximately constitute it. We employ daily closure prices along the period 2006 -2017 to compute the corresponding returns at the ground of the analysis. During the period of our analysis, the Asian stock market experienced unstable dynamics, with large booms and bursts. These up and down swings re /uniFB02 ect the 2008 global /uniFB01 nancial crisis /uniFB01 rstly, and, more recently, the real estate bubble and the /uniFB02 ood of debt by municipal governments and local enterprises designed to fund infrastructure investments 45 -48 . We also provide additional evidence on stocks constituting the STOXX North America 600 Index (see Supplementary information, Section 3.7).\nThe dynamics of the LTM sub-graph identi /uniFB01 es market phases characterized by the strengthening of price co-movements\nFig. 1 The leading temporal module (LTM) sub-graph in different market phases together with the indicator I LTM t reported against the market\ndynamics. a , b show the absolute value of the correlation matrices (PCC) derived from stocks returns, emphasizing the LTM sub-graph with a red square, together with the absolute average auto-covariance (AAC) computed on both its members and on the rest of the system. Correlation matrices and autocovariance are displayed for two different market phases, centered around 08-06-2009 in a and around 21-04-2010 in b , which stand for an unstable and a business as usual phase, respectively. c illustrates the sets of stocks within the system: stocks composing the LTM, stocks that have a statistically signi /uniFB01 cant DFA but are not in the LTM sub-graph (DFA -), and the rest of the stocks not selected by neither the LTM algorithm nor the DFA (Rest). d reports the leading indicator (right axis) computed on the LTM members (blue line), on DFA -members (red line), and on the Rest of the stocks (yellow line). These indicators have been smoothed based on a Lowess (locally weighted scatter-plot smoothing) /uniFB01 lter and compared with the dynamics of the underlying reference index displayed in gray (left axis). Vertical bars correspond to crisis events affecting the /uniFB01 nancial market such as the banking sector ratings downgrades of 2007, the failure of Lehman Brothers in September 2008, the American Recovery and Reinvestment Act of 2009, the European Debt crisis of 2011, and the Chinese stock market crisis of 2015 -2016. Error bounds are computed by performing 500 bootstrapping re-sampling of stocks' returns from the empirical distribution of the observed data and computing for each run the LTM indicator. Shaded areas represent the 5 -95% con /uniFB01 dence intervals. Two-sample Kolmogorov -Smirnov (KS) test provides evidence about the statistical difference between I LTM and the indicators computed on DFA -and Rest. The pairwise KS statistics of I LTM vs. the indicators for DFA -and Rest are 0.95 and 0.99 at 1% signi /uniFB01 cance level, respectively, thus suggesting that they are from different continuous distributions. e -g show the dynamics of the components of the leading indicator; from the left to the right: the absolute auto-covariance of stocks' returns ( e ), the within cluster absolute Pearson ' s correlation ( f ) and the between clusters absolute Pearson ' s correlation ( g ). All computations are made using a moving window of 200 days.\nresponsible for the transition of the whole underlying market away from its current con /uniFB01 guration. Upper panels of Fig. 1 show the absolute values of both the correlation and the AC for the LTM members and for the other stocks not included in the LTM. Figure 1a refers to an unstable period (centered around 08-06-2009), while Fig. 1b refers to a stable phase (centered around 21-04-2010). Figure 1c shows the schematic diagram of the sets of stocks composing the system: the LTM group (labeled as LTM), the stocks that have a statistically signi /uniFB01 cant DFA, but that are not in the LTM sub-graph (labeled as DFA -), and the rest of the stocks not selected by neither the LTM algorithm nor the DFA (labeled as Rest). Figure 1d shows the dynamics of the underlying index (in gray), the pattern of I LTM t referring to the LTM members (blue line) and the analogous indicators computed on stocks belonging to the DFA -group (red line) and on the Rest group (yellow line). Figure 1 shows how, during unstable phases, the LTM emerges in the correlation matrix, displaying also relatively high values of the ACs (Fig. 1a). On the\ncontrary, the module is indistinguishable from the remaining part of the system during ' business as usual ' phases (Fig. 1b). In a nutshell, I LTM t increases and assumes higher values around periods of market instability than during a tranquil period. For instance, during the 2008 global /uniFB01 nancial crisis, I LTM t starts to increase prior to the outbreak of the market and it reaches a local maximum approximately in correspondence of the onset of the crisis. Figure 1d also points out that I LTM t shows an increasing dynamics in correspondence of major events affecting the market, such as the banking sector ratings downgrades of 2007, the failure of Lehman Brothers in September 2008, the American Recovery and Reinvestment Act of 2009, the European Debt crisis of 2011, and the Chinese stock market crisis of 2015 -2016. Moreover, I LTM t shows a sharp increase also for transitions occurring during positive market trends, as for instance in the recovery period after the global /uniFB01 nancial crisis and at the end of the sample period. By contrast, the dynamics of both DFA -and the Rest groups seem less informative in distinguishing phases of instability in the market as reported in Fig. 1d. Error bounds are computed by performing 500 bootstraps re-sampling by randomly permuting stocks ' returns. Every bootstrap sample allows acquiring an estimate of I LTM t , which is used to compute the distribution of the indicator and to estimate the error bound as the 5 -95th percentiles of such distribution.\nOur analysis of the system at different points in time is able to identify stages of accumulation of market instability by detecting qualitative changes in the structure of the interactions among market participants.\nThe dynamics of the LTM mimics some behavioral attitudes of market participants, such as positive feedbacks and herding behaviors that reverberate in the path of stock prices. The former empirically translates into an increased AC of stock returns, while the latter empirically drives an increase of the correlation of such returns. The lower panels of Fig. 1 (panels e, f, and g) show the time dynamics of the components of I LTM t as described by Eq. (1). From the left to the right: the absolute AC of stocks ' returns (panel e), the within-group absolute Pearson ' s correlation (panel f), and the between groups absolute Pearson ' s correlation (panel g). These components jointly contribute to detect the emergence of phases of cumulative market instabilities. In particular, the AC signals the presence of positive feedbacks around the outbreak of the global /uniFB01 nancial crisis and of its rebound, while high correlation values between the LTM members indicate the presence of a bunch of stocks having strong synchronized patterns, which deviate from the behavior of the rest of the system. Notice that when these components are evaluated separately, they do not provide a clear interpretation of market conditions, while only once jointly considered they convey a meaningful signal.\n\nIdentifies market phases and their associated dynamics, including correlations and autocovariances, highlighting the emergence of the LTM sub-graph during unstable periods and its diminished presence during stable periods.",
    "original_text": "The LTM indicator . We analyze the stocks referring to the STOXX Asia/Paci /uniFB01 c 600 Index, which is a broad and liquid subset of the STOXX Global 1800 Index. We investigate the dynamics of the aggregate index starting from the micro-level represented by the stocks that approximately constitute it. We employ daily closure prices along the period 2006 -2017 to compute the corresponding returns at the ground of the analysis. During the period of our analysis, the Asian stock market experienced unstable dynamics, with large booms and bursts. These up and down swings re /uniFB02 ect the 2008 global /uniFB01 nancial crisis /uniFB01 rstly, and, more recently, the real estate bubble and the /uniFB02 ood of debt by municipal governments and local enterprises designed to fund infrastructure investments 45 -48 . We also provide additional evidence on stocks constituting the STOXX North America 600 Index (see Supplementary information, Section 3.7).\nThe dynamics of the LTM sub-graph identi /uniFB01 es market phases characterized by the strengthening of price co-movements\nFig. 1 The leading temporal module (LTM) sub-graph in different market phases together with the indicator I LTM t reported against the market\ndynamics. a , b show the absolute value of the correlation matrices (PCC) derived from stocks returns, emphasizing the LTM sub-graph with a red square, together with the absolute average auto-covariance (AAC) computed on both its members and on the rest of the system. Correlation matrices and autocovariance are displayed for two different market phases, centered around 08-06-2009 in a and around 21-04-2010 in b , which stand for an unstable and a business as usual phase, respectively. c illustrates the sets of stocks within the system: stocks composing the LTM, stocks that have a statistically signi /uniFB01 cant DFA but are not in the LTM sub-graph (DFA -), and the rest of the stocks not selected by neither the LTM algorithm nor the DFA (Rest). d reports the leading indicator (right axis) computed on the LTM members (blue line), on DFA -members (red line), and on the Rest of the stocks (yellow line). These indicators have been smoothed based on a Lowess (locally weighted scatter-plot smoothing) /uniFB01 lter and compared with the dynamics of the underlying reference index displayed in gray (left axis). Vertical bars correspond to crisis events affecting the /uniFB01 nancial market such as the banking sector ratings downgrades of 2007, the failure of Lehman Brothers in September 2008, the American Recovery and Reinvestment Act of 2009, the European Debt crisis of 2011, and the Chinese stock market crisis of 2015 -2016. Error bounds are computed by performing 500 bootstrapping re-sampling of stocks' returns from the empirical distribution of the observed data and computing for each run the LTM indicator. Shaded areas represent the 5 -95% con /uniFB01 dence intervals. Two-sample Kolmogorov -Smirnov (KS) test provides evidence about the statistical difference between I LTM and the indicators computed on DFA -and Rest. The pairwise KS statistics of I LTM vs. the indicators for DFA -and Rest are 0.95 and 0.99 at 1% signi /uniFB01 cance level, respectively, thus suggesting that they are from different continuous distributions. e -g show the dynamics of the components of the leading indicator; from the left to the right: the absolute auto-covariance of stocks' returns ( e ), the within cluster absolute Pearson ' s correlation ( f ) and the between clusters absolute Pearson ' s correlation ( g ). All computations are made using a moving window of 200 days.\nresponsible for the transition of the whole underlying market away from its current con /uniFB01 guration. Upper panels of Fig. 1 show the absolute values of both the correlation and the AC for the LTM members and for the other stocks not included in the LTM. Figure 1a refers to an unstable period (centered around 08-06-2009), while Fig. 1b refers to a stable phase (centered around 21-04-2010). Figure 1c shows the schematic diagram of the sets of stocks composing the system: the LTM group (labeled as LTM), the stocks that have a statistically signi /uniFB01 cant DFA, but that are not in the LTM sub-graph (labeled as DFA -), and the rest of the stocks not selected by neither the LTM algorithm nor the DFA (labeled as Rest). Figure 1d shows the dynamics of the underlying index (in gray), the pattern of I LTM t referring to the LTM members (blue line) and the analogous indicators computed on stocks belonging to the DFA -group (red line) and on the Rest group (yellow line). Figure 1 shows how, during unstable phases, the LTM emerges in the correlation matrix, displaying also relatively high values of the ACs (Fig. 1a). On the\ncontrary, the module is indistinguishable from the remaining part of the system during ' business as usual ' phases (Fig. 1b). In a nutshell, I LTM t increases and assumes higher values around periods of market instability than during a tranquil period. For instance, during the 2008 global /uniFB01 nancial crisis, I LTM t starts to increase prior to the outbreak of the market and it reaches a local maximum approximately in correspondence of the onset of the crisis. Figure 1d also points out that I LTM t shows an increasing dynamics in correspondence of major events affecting the market, such as the banking sector ratings downgrades of 2007, the failure of Lehman Brothers in September 2008, the American Recovery and Reinvestment Act of 2009, the European Debt crisis of 2011, and the Chinese stock market crisis of 2015 -2016. Moreover, I LTM t shows a sharp increase also for transitions occurring during positive market trends, as for instance in the recovery period after the global /uniFB01 nancial crisis and at the end of the sample period. By contrast, the dynamics of both DFA -and the Rest groups seem less informative in distinguishing phases of instability in the market as reported in Fig. 1d. Error bounds are computed by performing 500 bootstraps re-sampling by randomly permuting stocks ' returns. Every bootstrap sample allows acquiring an estimate of I LTM t , which is used to compute the distribution of the indicator and to estimate the error bound as the 5 -95th percentiles of such distribution.\nOur analysis of the system at different points in time is able to identify stages of accumulation of market instability by detecting qualitative changes in the structure of the interactions among market participants.\nThe dynamics of the LTM mimics some behavioral attitudes of market participants, such as positive feedbacks and herding behaviors that reverberate in the path of stock prices. The former empirically translates into an increased AC of stock returns, while the latter empirically drives an increase of the correlation of such returns. The lower panels of Fig. 1 (panels e, f, and g) show the time dynamics of the components of I LTM t as described by Eq. (1). From the left to the right: the absolute AC of stocks ' returns (panel e), the within-group absolute Pearson ' s correlation (panel f), and the between groups absolute Pearson ' s correlation (panel g). These components jointly contribute to detect the emergence of phases of cumulative market instabilities. In particular, the AC signals the presence of positive feedbacks around the outbreak of the global /uniFB01 nancial crisis and of its rebound, while high correlation values between the LTM members indicate the presence of a bunch of stocks having strong synchronized patterns, which deviate from the behavior of the rest of the system. Notice that when these components are evaluated separately, they do not provide a clear interpretation of market conditions, while only once jointly considered they convey a meaningful signal.",
    "context": "Identifies market phases and their associated dynamics, including correlations and autocovariances, highlighting the emergence of the LTM sub-graph during unstable periods and its diminished presence during stable periods.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      2,
      3,
      4
    ],
    "id": "90e893fea07e47b45edd23db4bf43f1c8da47db3d97c7dd12228369b697f7e10"
  },
  {
    "text": "I LTM t is a dynamic indicator whose members may vary in time. Changes in the LTM composition are important to identify the drivers of the upcoming period of instability. In order to investigate the composition of the LTM sub-graph, its size and the entry -exit dynamics of the stocks in the module, we report, in Fig. 2a, the stability coef /uniFB01 cient of the LTM computed as the portion of stocks that belong to the module during two consecutive days (green line). We also report the size of the LTM (in red) and that of the group DFA -(in blue), expressed as percentages to the total number of stocks composing the reference index. In the lower part of Fig. 2a, we also report the correlation between the number of stocks selected by the DFA procedure and the average correlation of these stocks ' returns. This helps us to verify whether a rise in the number of stocks with a signi /uniFB01 cant DFA exponent is related to the growth of the average correlation of the returns associated with these stocks, and thus to a higher likelihood of being LTM members. When most of the stocks with a signi /uniFB01 cant DFA exponent belong to the LTM (see red line), we observe a stable dynamics of the leading module (see green line) or, to put it differently, a low turnover of the stocks inside the LTM. On the contrary, the LTM stability drastically decreases when there exists a considerable amount of stocks with a signi /uniFB01 cant DFA exponent that are not part of the leading module (see blue line). Indeed, we observe positive and high values of the correlation when most of the stocks selected by the DFA also belong to the LTM as, for instance, during the 2008 global /uniFB01 nancial crises and during the last semester of 2015, after the Renmimbi devaluation, while low values of the correlation are associated with periods of substantial changes of the LTM members. The negative Pearson ' s correlation ( -0.19) between the LTM stability coef /uniFB01 cient and the size of the subset of stocks composing the DFA -group (i.e., not included in the leading module) indicates that new leading modules are likely to emerge in those periods in which there are stocks with signi /uniFB01 cant DFA exponents but with poorly correlated returns. In the Section 3.4 of\nFig. 2 LTM membership stability. a Shows in green the percentage of stocks that belong to the LTM for two consecutive trading days (LTM stability), while the red line stands for the percentage of stocks belonging to the LTM. The blue line emphasizes the percentage of DFA -stocks. The black line shows the value of the correlation (Corr) between the number of stocks selected by the DFA and the average correlation of these stocks' returns, for those having a DFA exponent signi /uniFB01 cantly larger than 1/2 (AC DFA up-tail), while the gray line refers to stocks with a DFA exponent signi /uniFB01 cantly lower than 1/2 (AC DFA low-tail). Finally, the dynamics of the series are compared with the market index (reported in orange). b Shows the distributions of pairs of stocks such that both stocks have Hurst exponent outside the interval 0.2 -0.8 (in black), of pairs such that none of them is outside the interval 0.2 -0.8 (in green) and those with only one stock belonging to such interval (in red).\nFig. 3 The buy/sell signals provided by the dynamics of I LTM t and of its members ' returns together with the obtained Pro /uniFB01 ts and Losses (P&L). The /uniFB01 gure reports, in a , the forecast dynamics of the benchmark index (green-red colors stand for buy and sell signals, respectively) together with the P&L of the investment strategy based on I LTM t (blue line). The P&L of an investment strategy based on the value at risk (VaR), that is, the maximum potential loss computed on a daily time horizon with an interval of con /uniFB01 dence of 0.975 is also reported (black line) as a comparative measure. The true positives, false positives, false negatives, and true negatives obtained by investing following I LTM t are 53%, 47%, 49%, and 51%, respectively, while for the strategy based on the VaR, we obtain 49%, 51%, 52%, and 48%. Finally, the brown line refers to the P&L evolution obtained by considering an investment strategy based on the indicator proposed by ref. 58 , while the cyan line shows the P&L obtained when considering only the average correlation among stocks' returns. In b , we report the accuracy and precision measures of the proposed investment strategy conditionally on the forecast of the market index returns larger than a certain percentile of their distribution in absolute terms.\nthe Supplementary information, we also report that, on average, stocks stay continuously in the leading module for about 1.5 months. Figure 2b shows the distributions of the correlations between pairs of stocks such that both stocks have Hurst exponent outside the interval 0.2 -0.8 (in black) in comparison with the pairs such that none of them is outside that interval (in green) or such that only one of the stock in the pair belongs to that interval (in red). The shifting to the right of the distribution for the stocks with Hurst exponent outside the interval 0.2 -0.8 suggests that the DFA selects assets with high correlated returns. In other words, an increase of the number of stocks with a signi /uniFB01 cant DFA exponent implies an increase of the average correlation of the returns associated with these stocks and subsequently to a higher probability of entering the LTM.\nmarket dynamics. In other words, whenever I LTM t falls in the right tail of its empirical distribution, a switch between a long or short (or vice versa) investment position is possible depending on the average returns of the LTM members. More speci /uniFB01 cally, we refer to values of the I LTM t larger than the 95th percentile of its empirical distribution as the trigger for switching the investment exposure: if the average return of the stocks composing the LTM sub-graph is positive at time t , then we opt for a buy signal in that day; otherwise, the strategy goes short.\nThe predictive performance . We assess the predictive performance of I LTM t by testing an investment strategy that consists of two steps: /uniFB01 rst, the detection of a cumulative process leading to a phase of instability and, second, the identi /uniFB01 cation of the market direction. As for the /uniFB01 rst point, a thresholding approach for extracting signals from I LTM t appears not suitable, since it would require the prior knowledge of the out-of-sample distribution. Therefore, the most recent value of the indicator is compared against its empirical distribution computed over the previous three trading weeks (15 working days). If this value belongs to the right tail of the distribution, then the LTM is interpreted as signaling a cumulative process leading to market instability. Instead, to detect the direction of the market trend, we exploit the most recent returns of the LTM members, averaging among such values at the day corresponding to the investment decision. If the average value of the returns is positive, then the signal conveyed by the LTM indicates the arrival of a shift towards a bullish equilibrium; otherwise, it stands for a declining and bearish\n\nIdentifies LTM membership stability and its correlation with market volatility, highlighting a potential trigger for investment strategy shifts.",
    "original_text": "I LTM t is a dynamic indicator whose members may vary in time. Changes in the LTM composition are important to identify the drivers of the upcoming period of instability. In order to investigate the composition of the LTM sub-graph, its size and the entry -exit dynamics of the stocks in the module, we report, in Fig. 2a, the stability coef /uniFB01 cient of the LTM computed as the portion of stocks that belong to the module during two consecutive days (green line). We also report the size of the LTM (in red) and that of the group DFA -(in blue), expressed as percentages to the total number of stocks composing the reference index. In the lower part of Fig. 2a, we also report the correlation between the number of stocks selected by the DFA procedure and the average correlation of these stocks ' returns. This helps us to verify whether a rise in the number of stocks with a signi /uniFB01 cant DFA exponent is related to the growth of the average correlation of the returns associated with these stocks, and thus to a higher likelihood of being LTM members. When most of the stocks with a signi /uniFB01 cant DFA exponent belong to the LTM (see red line), we observe a stable dynamics of the leading module (see green line) or, to put it differently, a low turnover of the stocks inside the LTM. On the contrary, the LTM stability drastically decreases when there exists a considerable amount of stocks with a signi /uniFB01 cant DFA exponent that are not part of the leading module (see blue line). Indeed, we observe positive and high values of the correlation when most of the stocks selected by the DFA also belong to the LTM as, for instance, during the 2008 global /uniFB01 nancial crises and during the last semester of 2015, after the Renmimbi devaluation, while low values of the correlation are associated with periods of substantial changes of the LTM members. The negative Pearson ' s correlation ( -0.19) between the LTM stability coef /uniFB01 cient and the size of the subset of stocks composing the DFA -group (i.e., not included in the leading module) indicates that new leading modules are likely to emerge in those periods in which there are stocks with signi /uniFB01 cant DFA exponents but with poorly correlated returns. In the Section 3.4 of\nFig. 2 LTM membership stability. a Shows in green the percentage of stocks that belong to the LTM for two consecutive trading days (LTM stability), while the red line stands for the percentage of stocks belonging to the LTM. The blue line emphasizes the percentage of DFA -stocks. The black line shows the value of the correlation (Corr) between the number of stocks selected by the DFA and the average correlation of these stocks' returns, for those having a DFA exponent signi /uniFB01 cantly larger than 1/2 (AC DFA up-tail), while the gray line refers to stocks with a DFA exponent signi /uniFB01 cantly lower than 1/2 (AC DFA low-tail). Finally, the dynamics of the series are compared with the market index (reported in orange). b Shows the distributions of pairs of stocks such that both stocks have Hurst exponent outside the interval 0.2 -0.8 (in black), of pairs such that none of them is outside the interval 0.2 -0.8 (in green) and those with only one stock belonging to such interval (in red).\nFig. 3 The buy/sell signals provided by the dynamics of I LTM t and of its members ' returns together with the obtained Pro /uniFB01 ts and Losses (P&L). The /uniFB01 gure reports, in a , the forecast dynamics of the benchmark index (green-red colors stand for buy and sell signals, respectively) together with the P&L of the investment strategy based on I LTM t (blue line). The P&L of an investment strategy based on the value at risk (VaR), that is, the maximum potential loss computed on a daily time horizon with an interval of con /uniFB01 dence of 0.975 is also reported (black line) as a comparative measure. The true positives, false positives, false negatives, and true negatives obtained by investing following I LTM t are 53%, 47%, 49%, and 51%, respectively, while for the strategy based on the VaR, we obtain 49%, 51%, 52%, and 48%. Finally, the brown line refers to the P&L evolution obtained by considering an investment strategy based on the indicator proposed by ref. 58 , while the cyan line shows the P&L obtained when considering only the average correlation among stocks' returns. In b , we report the accuracy and precision measures of the proposed investment strategy conditionally on the forecast of the market index returns larger than a certain percentile of their distribution in absolute terms.\nthe Supplementary information, we also report that, on average, stocks stay continuously in the leading module for about 1.5 months. Figure 2b shows the distributions of the correlations between pairs of stocks such that both stocks have Hurst exponent outside the interval 0.2 -0.8 (in black) in comparison with the pairs such that none of them is outside that interval (in green) or such that only one of the stock in the pair belongs to that interval (in red). The shifting to the right of the distribution for the stocks with Hurst exponent outside the interval 0.2 -0.8 suggests that the DFA selects assets with high correlated returns. In other words, an increase of the number of stocks with a signi /uniFB01 cant DFA exponent implies an increase of the average correlation of the returns associated with these stocks and subsequently to a higher probability of entering the LTM.\nmarket dynamics. In other words, whenever I LTM t falls in the right tail of its empirical distribution, a switch between a long or short (or vice versa) investment position is possible depending on the average returns of the LTM members. More speci /uniFB01 cally, we refer to values of the I LTM t larger than the 95th percentile of its empirical distribution as the trigger for switching the investment exposure: if the average return of the stocks composing the LTM sub-graph is positive at time t , then we opt for a buy signal in that day; otherwise, the strategy goes short.\nThe predictive performance . We assess the predictive performance of I LTM t by testing an investment strategy that consists of two steps: /uniFB01 rst, the detection of a cumulative process leading to a phase of instability and, second, the identi /uniFB01 cation of the market direction. As for the /uniFB01 rst point, a thresholding approach for extracting signals from I LTM t appears not suitable, since it would require the prior knowledge of the out-of-sample distribution. Therefore, the most recent value of the indicator is compared against its empirical distribution computed over the previous three trading weeks (15 working days). If this value belongs to the right tail of the distribution, then the LTM is interpreted as signaling a cumulative process leading to market instability. Instead, to detect the direction of the market trend, we exploit the most recent returns of the LTM members, averaging among such values at the day corresponding to the investment decision. If the average value of the returns is positive, then the signal conveyed by the LTM indicates the arrival of a shift towards a bullish equilibrium; otherwise, it stands for a declining and bearish",
    "context": "Identifies LTM membership stability and its correlation with market volatility, highlighting a potential trigger for investment strategy shifts.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      4,
      5
    ],
    "id": "cc2ee39298dbe0071528f0606346397cae6f8d901adc4aea8329d5872ac53053"
  },
  {
    "text": "Figure 3a reports the behavior of the underlying market index in which price forecasts are emphasized by green (i.e., buy) and red (i.e., sell) colors. Notice how, when there is a declining dynamics of the market index, our strategy mostly signals a short position, while in ascending price phases the green color prevails, indicating a long portfolio exposure. In particular, prior to the global /uniFB01 nancial of 2008, the indicator is able to correctly anticipate the price downturn, while at the onset of the crisis the wave of /uniFB01 nancial turbulence prevents a clear market trend detection. However, the subsequent rebound is timely identi /uniFB01 ed. Figure 3a also shows the Pro /uniFB01 t and Loss (P&L) of the strategy based on I LTM t (in blue) to disentangle the phases in which updown movements allow to obtain positive portfolio performances. In fact, the proposed strategy is able to generate a positive cumulative performance along the sample period. In Fig. 3a, we also compare the P&L of an investment strategy based on a wellknown measure of risk such as the value at risk (VaR) 49,50 , which estimates the maximum amount of expected loss over a speci /uniFB01 ed time horizon at a given con /uniFB01 dence level (see Supplementary information, Section 3.5). High values of VaR, that is, values higher than the 95th percentile of its empirical distribution, suggest a phase of instability, and, accordingly, the strategy takes a short position; otherwise, it goes long. Notice how, while I LTM recognizes changes in market trajectories in a timely way, the investment strategy based on VaR (in black), on the other hand, is\n\nTable 1 Pro /uniFB01 ts and Losses (P&L) performances.\n\nCompares the performance of a forecasting strategy to a VaR-based strategy, highlighting the forecasting strategy's ability to anticipate market trends and generate positive cumulative performance.",
    "original_text": "Figure 3a reports the behavior of the underlying market index in which price forecasts are emphasized by green (i.e., buy) and red (i.e., sell) colors. Notice how, when there is a declining dynamics of the market index, our strategy mostly signals a short position, while in ascending price phases the green color prevails, indicating a long portfolio exposure. In particular, prior to the global /uniFB01 nancial of 2008, the indicator is able to correctly anticipate the price downturn, while at the onset of the crisis the wave of /uniFB01 nancial turbulence prevents a clear market trend detection. However, the subsequent rebound is timely identi /uniFB01 ed. Figure 3a also shows the Pro /uniFB01 t and Loss (P&L) of the strategy based on I LTM t (in blue) to disentangle the phases in which updown movements allow to obtain positive portfolio performances. In fact, the proposed strategy is able to generate a positive cumulative performance along the sample period. In Fig. 3a, we also compare the P&L of an investment strategy based on a wellknown measure of risk such as the value at risk (VaR) 49,50 , which estimates the maximum amount of expected loss over a speci /uniFB01 ed time horizon at a given con /uniFB01 dence level (see Supplementary information, Section 3.5). High values of VaR, that is, values higher than the 95th percentile of its empirical distribution, suggest a phase of instability, and, accordingly, the strategy takes a short position; otherwise, it goes long. Notice how, while I LTM recognizes changes in market trajectories in a timely way, the investment strategy based on VaR (in black), on the other hand, is\n\nTable 1 Pro /uniFB01 ts and Losses (P&L) performances.",
    "context": "Compares the performance of a forecasting strategy to a VaR-based strategy, highlighting the forecasting strategy's ability to anticipate market trends and generate positive cumulative performance.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      5,
      6
    ],
    "id": "b2c5e4fa343cb150c160cf5b480ebbfa5ec35ec75f8caa00b487917ffeae8813"
  },
  {
    "text": "MV = 10; PRCTILE = 90, 2006 = 3.15. MV = 10; PRCTILE = 90, 2007 = - 12.80. MV = 10; PRCTILE = 90, 2008 = 7.90. MV = 10; PRCTILE = 90, 2009 = 11.96. MV = 10; PRCTILE = 90, 2010 = 11.64. MV = 10; PRCTILE = 90, 2011 = - 2.04. MV = 10; PRCTILE = 90, 2012 = 2.48. MV = 10; PRCTILE = 90, 2013 = 2.96. MV = 10; PRCTILE = 90, 2014 = 3.78. MV = 10; PRCTILE = 90, 2015 = 19.78. MV = 10; PRCTILE = 90, 2016 = 2.50. MV = 10; PRCTILE = 90, 2017 = 4.76. MV = 10; PRCTILE = 90, 2006 - 17 = 56.05. MV = 10; PRCTILE = 95, 2006 = 2.27. MV = 10; PRCTILE = 95, 2007 = 6.42. MV = 10; PRCTILE = 95, 2008 = - 8.48. MV = 10; PRCTILE = 95, 2009 = 20.26. MV = 10; PRCTILE = 95, 2010 = 3.76. MV = 10; PRCTILE = 95, 2011 = - 7.51. MV = 10; PRCTILE = 95, 2012 = 19.82. MV = 10; PRCTILE = 95, 2013 = 15.40. MV = 10; PRCTILE = 95, 2014 = 1.32. MV = 10; PRCTILE = 95, 2015 = 29.54. MV = 10; PRCTILE = 95, 2016 = 25.48. MV = 10; PRCTILE = 95, 2017 = 6.16. MV = 10; PRCTILE = 95, 2006 - 17 = 114.44. MV = 10; PRCTILE = 98, 2006 = 2.27. MV = 10; PRCTILE = 98, 2007 = 6.42. MV = 10; PRCTILE = 98, 2008 = - 8.48. MV = 10; PRCTILE = 98, 2009 = 20.26. MV = 10; PRCTILE = 98, 2010 = 3.76. MV = 10; PRCTILE = 98, 2011 = - 7.51. MV = 10; PRCTILE = 98, 2012 = 19.82. MV = 10; PRCTILE = 98, 2013 = 15.40. MV = 10; PRCTILE = 98, 2014 = 1.32. MV = 10; PRCTILE = 98, 2015 = 29.54. MV = 10; PRCTILE = 98, 2016 = 25.48. MV = 10; PRCTILE = 98, 2017 = 6.16. MV = 10; PRCTILE = 98, 2006 - 17 = 114.44. MV = 15; PRCTILE = 90, 2006 = 6.86. MV = 15; PRCTILE = 90, 2007 = - 16.37. MV = 15; PRCTILE = 90, 2008 = 24.59. MV = 15; PRCTILE = 90, 2009 = 14.00. MV = 15; PRCTILE = 90, 2010 = 25.96. MV = 15; PRCTILE = 90, 2011 = - 3.23. MV = 15; PRCTILE = 90, 2012 = 11.21. MV = 15; PRCTILE = 90, 2013 = - 17.76. MV = 15; PRCTILE = 90, 2014 = - 6.19. MV = 15; PRCTILE = 90, 2015 = 19.22. MV = 15; PRCTILE = 90, 2016 = 25.85. MV = 15; PRCTILE = 90, 2017 = - 0.63. MV = 15; PRCTILE = 90, 2006 - 17 = 83.51. MV = 15; PRCTILE = 95, 2006 = - 6.18. MV = 15; PRCTILE = 95, 2007 = 10.18. MV = 15; PRCTILE = 95, 2008 = 4.04. MV = 15; PRCTILE = 95, 2009 = 17.42. MV = 15; PRCTILE = 95, 2010 = 23.18. MV = 15; PRCTILE = 95, 2011 = 3.08. MV = 15; PRCTILE = 95, 2012 = 12.40. MV = 15; PRCTILE = 95, 2013 = - 9.82. MV = 15; PRCTILE = 95, 2014 = 0.78. MV = 15; PRCTILE = 95, 2015 = 30.75. MV = 15; PRCTILE = 95, 2016 = 31.05. MV = 15; PRCTILE = 95, 2017 = 6.16. MV = 15; PRCTILE = 95, 2006 - 17 = 123.04. MV = 15; =, 2006 = - 1.94. MV = 15; =, 2007 = 10.18. MV = 15; =, 2008 = 4.04. MV = 15; =, 2009 = 17.42. MV = 15; =, 2010 = 0.00. MV = 15; =, 2011 = - 0.92. MV = 15; =, 2012 = 18.13. MV = 15; =, 2013 = - 6.58. MV = 15; =, 2014 = 0.27. MV = 15; =, 2015 = 30.75. MV = 15; =, 2016 = 25.67. MV = 15; =, 2017 = 6.16. MV = 15; =, 2006 - 17 = 103.18. MV = 20; PRCTILE = 90, 2006 = 8.62. MV = 20; PRCTILE = 90, 2007 = - 0.27. MV = 20; PRCTILE = 90, 2008 = - 0.39. MV = 20; PRCTILE = 90, 2009 = 19.01. MV = 20; PRCTILE = 90, 2010 = 25.34. MV = 20; PRCTILE = 90, 2011 = - 3.99. MV = 20; PRCTILE = 90, 2012 = - 4.03. MV = 20; PRCTILE = 90, 2013 = - 3.76. MV = 20; PRCTILE = 90, 2014 = - 11.80. MV = 20; PRCTILE = 90, 2015 = 19.35. MV = 20; PRCTILE = 90, 2016 = 16.89. MV = 20; PRCTILE = 90, 2017 = 0.43. MV = 20; PRCTILE = 90, 2006 - 17 = 65.39. MV = 20; PRCTILE = 95, 2006 = 3.54. MV = 20; PRCTILE = 95, 2007 = 7.26. MV = 20; PRCTILE = 95, 2008 = 18.99. MV = 20; PRCTILE = 95, 2009 = 41.03. MV = 20; PRCTILE = 95, 2010 = 19.21. MV = 20; PRCTILE = 95, 2011 = - 0.92. MV = 20; PRCTILE = 95, 2012 = 10.60. MV = 20; PRCTILE = 95, 2013 = - 16.14. MV = 20; PRCTILE = 95, 2014 = - 21.74. MV = 20; PRCTILE = 95, 2015 = 18.56. MV = 20; PRCTILE = 95, 2016 = 25.10. MV = 20; PRCTILE = 95, 2017 = 4.76. MV = 20; PRCTILE = 95, 2006 - 17 = 110.25. MV = 20; PRCTILE = 98, 2006 = 2.69. MV = 20; PRCTILE = 98, 2007 = 13.57. MV = 20; PRCTILE = 98, 2008 = 2.61. MV = 20; PRCTILE = 98, 2009 = 43.15. MV = 20; PRCTILE = 98, 2010 = - 1.83. MV = 20; PRCTILE = 98, 2011 = - 4.60. MV = 20; PRCTILE = 98, 2012 = 20.25. MV = 20; PRCTILE = 98, 2013 = - 7.21. MV = 20; PRCTILE = 98, 2014 = - 22.25. MV = 20; PRCTILE = 98, 2015 = 28.73. MV = 20; PRCTILE = 98, 2016 = 25.10. MV = 20; PRCTILE = 98, 2017 = 6.16. MV = 20; PRCTILE = 98, 2006 - 17 = 106.38. MV = 25; PRCTILE = 90, 2006 = 12.79. MV = 25; PRCTILE = 90, 2007 = - 3.50. MV = 25; PRCTILE = 90, 2008 = 14.23. MV = 25; PRCTILE = 90, 2009 = 13.44. MV = 25; PRCTILE = 90, 2010 = 21.30. MV = 25; PRCTILE = 90, 2011 = 6.74. MV = 25; PRCTILE = 90, 2012 = - 20.85. MV = 25; PRCTILE = 90, 2013 = - 3.52. MV = 25; PRCTILE = 90, 2014 = - 15.07. MV = 25; PRCTILE = 90, 2015 = 19.81. MV = 25; PRCTILE = 90, 2016 = 12.38. MV = 25; PRCTILE = 90, 2017 = - 2.12. MV = 25; PRCTILE = 90, 2006 - 17 = 55.63. MV = 25; PRCTILE = 95, 2006 = 8.94. MV = 25; PRCTILE = 95, 2007 = 9.23. MV = 25;\n\nProvides a series of percentile values (90, 95, and 98) for various years, indicating shifts in the distribution of a variable (likely income or a similar metric) over time.",
    "original_text": "MV = 10; PRCTILE = 90, 2006 = 3.15. MV = 10; PRCTILE = 90, 2007 = - 12.80. MV = 10; PRCTILE = 90, 2008 = 7.90. MV = 10; PRCTILE = 90, 2009 = 11.96. MV = 10; PRCTILE = 90, 2010 = 11.64. MV = 10; PRCTILE = 90, 2011 = - 2.04. MV = 10; PRCTILE = 90, 2012 = 2.48. MV = 10; PRCTILE = 90, 2013 = 2.96. MV = 10; PRCTILE = 90, 2014 = 3.78. MV = 10; PRCTILE = 90, 2015 = 19.78. MV = 10; PRCTILE = 90, 2016 = 2.50. MV = 10; PRCTILE = 90, 2017 = 4.76. MV = 10; PRCTILE = 90, 2006 - 17 = 56.05. MV = 10; PRCTILE = 95, 2006 = 2.27. MV = 10; PRCTILE = 95, 2007 = 6.42. MV = 10; PRCTILE = 95, 2008 = - 8.48. MV = 10; PRCTILE = 95, 2009 = 20.26. MV = 10; PRCTILE = 95, 2010 = 3.76. MV = 10; PRCTILE = 95, 2011 = - 7.51. MV = 10; PRCTILE = 95, 2012 = 19.82. MV = 10; PRCTILE = 95, 2013 = 15.40. MV = 10; PRCTILE = 95, 2014 = 1.32. MV = 10; PRCTILE = 95, 2015 = 29.54. MV = 10; PRCTILE = 95, 2016 = 25.48. MV = 10; PRCTILE = 95, 2017 = 6.16. MV = 10; PRCTILE = 95, 2006 - 17 = 114.44. MV = 10; PRCTILE = 98, 2006 = 2.27. MV = 10; PRCTILE = 98, 2007 = 6.42. MV = 10; PRCTILE = 98, 2008 = - 8.48. MV = 10; PRCTILE = 98, 2009 = 20.26. MV = 10; PRCTILE = 98, 2010 = 3.76. MV = 10; PRCTILE = 98, 2011 = - 7.51. MV = 10; PRCTILE = 98, 2012 = 19.82. MV = 10; PRCTILE = 98, 2013 = 15.40. MV = 10; PRCTILE = 98, 2014 = 1.32. MV = 10; PRCTILE = 98, 2015 = 29.54. MV = 10; PRCTILE = 98, 2016 = 25.48. MV = 10; PRCTILE = 98, 2017 = 6.16. MV = 10; PRCTILE = 98, 2006 - 17 = 114.44. MV = 15; PRCTILE = 90, 2006 = 6.86. MV = 15; PRCTILE = 90, 2007 = - 16.37. MV = 15; PRCTILE = 90, 2008 = 24.59. MV = 15; PRCTILE = 90, 2009 = 14.00. MV = 15; PRCTILE = 90, 2010 = 25.96. MV = 15; PRCTILE = 90, 2011 = - 3.23. MV = 15; PRCTILE = 90, 2012 = 11.21. MV = 15; PRCTILE = 90, 2013 = - 17.76. MV = 15; PRCTILE = 90, 2014 = - 6.19. MV = 15; PRCTILE = 90, 2015 = 19.22. MV = 15; PRCTILE = 90, 2016 = 25.85. MV = 15; PRCTILE = 90, 2017 = - 0.63. MV = 15; PRCTILE = 90, 2006 - 17 = 83.51. MV = 15; PRCTILE = 95, 2006 = - 6.18. MV = 15; PRCTILE = 95, 2007 = 10.18. MV = 15; PRCTILE = 95, 2008 = 4.04. MV = 15; PRCTILE = 95, 2009 = 17.42. MV = 15; PRCTILE = 95, 2010 = 23.18. MV = 15; PRCTILE = 95, 2011 = 3.08. MV = 15; PRCTILE = 95, 2012 = 12.40. MV = 15; PRCTILE = 95, 2013 = - 9.82. MV = 15; PRCTILE = 95, 2014 = 0.78. MV = 15; PRCTILE = 95, 2015 = 30.75. MV = 15; PRCTILE = 95, 2016 = 31.05. MV = 15; PRCTILE = 95, 2017 = 6.16. MV = 15; PRCTILE = 95, 2006 - 17 = 123.04. MV = 15; =, 2006 = - 1.94. MV = 15; =, 2007 = 10.18. MV = 15; =, 2008 = 4.04. MV = 15; =, 2009 = 17.42. MV = 15; =, 2010 = 0.00. MV = 15; =, 2011 = - 0.92. MV = 15; =, 2012 = 18.13. MV = 15; =, 2013 = - 6.58. MV = 15; =, 2014 = 0.27. MV = 15; =, 2015 = 30.75. MV = 15; =, 2016 = 25.67. MV = 15; =, 2017 = 6.16. MV = 15; =, 2006 - 17 = 103.18. MV = 20; PRCTILE = 90, 2006 = 8.62. MV = 20; PRCTILE = 90, 2007 = - 0.27. MV = 20; PRCTILE = 90, 2008 = - 0.39. MV = 20; PRCTILE = 90, 2009 = 19.01. MV = 20; PRCTILE = 90, 2010 = 25.34. MV = 20; PRCTILE = 90, 2011 = - 3.99. MV = 20; PRCTILE = 90, 2012 = - 4.03. MV = 20; PRCTILE = 90, 2013 = - 3.76. MV = 20; PRCTILE = 90, 2014 = - 11.80. MV = 20; PRCTILE = 90, 2015 = 19.35. MV = 20; PRCTILE = 90, 2016 = 16.89. MV = 20; PRCTILE = 90, 2017 = 0.43. MV = 20; PRCTILE = 90, 2006 - 17 = 65.39. MV = 20; PRCTILE = 95, 2006 = 3.54. MV = 20; PRCTILE = 95, 2007 = 7.26. MV = 20; PRCTILE = 95, 2008 = 18.99. MV = 20; PRCTILE = 95, 2009 = 41.03. MV = 20; PRCTILE = 95, 2010 = 19.21. MV = 20; PRCTILE = 95, 2011 = - 0.92. MV = 20; PRCTILE = 95, 2012 = 10.60. MV = 20; PRCTILE = 95, 2013 = - 16.14. MV = 20; PRCTILE = 95, 2014 = - 21.74. MV = 20; PRCTILE = 95, 2015 = 18.56. MV = 20; PRCTILE = 95, 2016 = 25.10. MV = 20; PRCTILE = 95, 2017 = 4.76. MV = 20; PRCTILE = 95, 2006 - 17 = 110.25. MV = 20; PRCTILE = 98, 2006 = 2.69. MV = 20; PRCTILE = 98, 2007 = 13.57. MV = 20; PRCTILE = 98, 2008 = 2.61. MV = 20; PRCTILE = 98, 2009 = 43.15. MV = 20; PRCTILE = 98, 2010 = - 1.83. MV = 20; PRCTILE = 98, 2011 = - 4.60. MV = 20; PRCTILE = 98, 2012 = 20.25. MV = 20; PRCTILE = 98, 2013 = - 7.21. MV = 20; PRCTILE = 98, 2014 = - 22.25. MV = 20; PRCTILE = 98, 2015 = 28.73. MV = 20; PRCTILE = 98, 2016 = 25.10. MV = 20; PRCTILE = 98, 2017 = 6.16. MV = 20; PRCTILE = 98, 2006 - 17 = 106.38. MV = 25; PRCTILE = 90, 2006 = 12.79. MV = 25; PRCTILE = 90, 2007 = - 3.50. MV = 25; PRCTILE = 90, 2008 = 14.23. MV = 25; PRCTILE = 90, 2009 = 13.44. MV = 25; PRCTILE = 90, 2010 = 21.30. MV = 25; PRCTILE = 90, 2011 = 6.74. MV = 25; PRCTILE = 90, 2012 = - 20.85. MV = 25; PRCTILE = 90, 2013 = - 3.52. MV = 25; PRCTILE = 90, 2014 = - 15.07. MV = 25; PRCTILE = 90, 2015 = 19.81. MV = 25; PRCTILE = 90, 2016 = 12.38. MV = 25; PRCTILE = 90, 2017 = - 2.12. MV = 25; PRCTILE = 90, 2006 - 17 = 55.63. MV = 25; PRCTILE = 95, 2006 = 8.94. MV = 25; PRCTILE = 95, 2007 = 9.23. MV = 25;",
    "context": "Provides a series of percentile values (90, 95, and 98) for various years, indicating shifts in the distribution of a variable (likely income or a similar metric) over time.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      6
    ],
    "id": "7cde64e58a49dd6f5a4d1b6abbd0bee2195dccc62f23d5067545f10038260212"
  },
  {
    "text": "PRCTILE = 95, 2008 = 7.07. MV = 25; PRCTILE = 95, 2009 = 30.87. MV = 25; PRCTILE = 95, 2010 = 17.87. MV = 25; PRCTILE = 95, 2011 = - 3.47. MV = 25; PRCTILE = 95, 2012 = - 9.95. MV = 25; PRCTILE = 95, 2013 = - 21.56. MV = 25; PRCTILE = 95, 2014 = - 17.47. MV = 25; PRCTILE = 95, 2015 = 33.01. MV = 25; PRCTILE = 95, 2016 = 20.57. MV = 25; PRCTILE = 95, 2017 = - 0.63. MV = 25; PRCTILE = 95, 2006 - 17 = 74.49. MV = 25; PRCTILE = 98, 2006 = 4.97. MV = 25; PRCTILE = 98, 2007 = 13.57. MV = 25; PRCTILE = 98, 2008 = - 11.09. MV = 25; PRCTILE = 98, 2009 = 38.22. MV = 25; PRCTILE = 98, 2010 = - 4.14. MV = 25; PRCTILE = 98, 2011 = - 2.87. MV = 25; PRCTILE = 98, 2012 = 6.73. MV = 25; PRCTILE = 98, 2013 = - 10.08. MV = 25; PRCTILE = 98, 2014 = - 13.73. MV = 25; PRCTILE = 98, 2015 = 20.21. MV = 25; PRCTILE = 98, 2016 = 23.28. MV = 25; PRCTILE = 98, 2017 = 6.16. MV = 25; PRCTILE = 98, 2006 - 17 = 71.24. Buy&Hold, 2006 = - 5.67. Buy&Hold, 2007 = - 5.95. Buy&Hold, 2008 = - 42.25. Buy&Hold, 2009 = 15.24. Buy&Hold, 2010 = 18.83. Buy&Hold, 2011 = - 13.83. Buy&Hold, 2012 = 9.09. Buy&Hold, 2013 = 9.58. Buy&Hold, 2014 = 8.24. Buy&Hold, 2015 = 12.42. Buy&Hold, 2016 = 4.62. Buy&Hold, 2017 = 3.11. Buy&Hold, 2006 - 17 = 13.42\nThe table shows the P&L per year obtained by following either I LTM t or the Buy and Hold strategy. Rows also indicate the sensitivity of the investment strategy to different values of the moving window (MV) and of the threshold of the empirical distribution (PRCTILE) used to identify phases of unstable market co-movements. Last column represents the P&L over the entire sample.\nmuch less reactive as it can be seen; for instance, after the market rebound of the second half of 2009 and in the /uniFB01 rst part of 2016. Finally, lower performances with respect to our proposed indicator clearly emerge if we observe the P&L obtained by applying a strategy relying on the indicator introduced by ref. 49 (in brown) and the P&L derived from a strategy based only on the average correlation of stocks ' returns (in cyan). All in all, this suggests that the behavioral features of market participants that we propose to capture through the use of the AC and correlation values are together instrumental for anticipating the dynamics of the underlying /uniFB01 nancial system.\naccuracy measures for each percentile (see Supplementary information, Section 3.8). From Fig. 3b, it clearly emerges that the capability of the proposed investment strategy in discriminating between positive and negative market movements increases as long as we select larger absolute values of market returns. This means that the trading strategy based on the LTM indicator correctly anticipates future changes in the aggregate stock price indices, especially around large market movement.\nWe also report in Table 1 the annual P&L achieved by following either our proposed investment strategy or a simply Buy&Hold strategy (last row). We investigate the robustness of our /uniFB01 ndings to changes in the length of the moving window adopted to compute the empirical distribution of I LTM t and in the threshold employed to de /uniFB01 ne the extreme values for this indicator. We observe that our strategy over-performs the simple Buy&Hold strategy over the entire sample period (last column). Beside the fact that the proposed investment strategy does not produce positive P&L for all the periods and for all parameters con /uniFB01 gurations, results in Table 1 still support the predictive performance of our approach. While during market up-trends the signal produces P&L in lines with the Buy&Hold strategy, the timely identi /uniFB01 cation of downward phases limits severe losses that, on the contrary, impact on the naive Buy&Hold strategy (see also Supplementary information, Section 3.6).\nTo quantitatively assess the performance of the proposed investment strategy, we employ a non-parametric approach. We proceed by /uniFB01 rst computing the true-positive, true-negative, falsepositive, and false-negative calls of our investment strategy conditionally on some pre-determined percentiles of the distribution of the absolute values of the market returns. We consider returns larger than an α percentile, with α varying from 10% to 90%. Then, we compute the associated precision and\nFinally, in the Section 3.6 of the Supplementary information, the P&L obtained from this strategy is compared against other investment alternatives such as strategies based on the DFA signals alone. We show that our approach outperforms the other strategies even when we do consider transaction costs. In fact, by assuming transaction costs of 10 basis-points for each portfolio rebalance, we still get a positive P&L of about 5.5% per year.\n\nSummarizes investment strategy performance, highlighting its outperformance compared to a simple Buy&Hold strategy and DFA signals, particularly during market downturns.",
    "original_text": "PRCTILE = 95, 2008 = 7.07. MV = 25; PRCTILE = 95, 2009 = 30.87. MV = 25; PRCTILE = 95, 2010 = 17.87. MV = 25; PRCTILE = 95, 2011 = - 3.47. MV = 25; PRCTILE = 95, 2012 = - 9.95. MV = 25; PRCTILE = 95, 2013 = - 21.56. MV = 25; PRCTILE = 95, 2014 = - 17.47. MV = 25; PRCTILE = 95, 2015 = 33.01. MV = 25; PRCTILE = 95, 2016 = 20.57. MV = 25; PRCTILE = 95, 2017 = - 0.63. MV = 25; PRCTILE = 95, 2006 - 17 = 74.49. MV = 25; PRCTILE = 98, 2006 = 4.97. MV = 25; PRCTILE = 98, 2007 = 13.57. MV = 25; PRCTILE = 98, 2008 = - 11.09. MV = 25; PRCTILE = 98, 2009 = 38.22. MV = 25; PRCTILE = 98, 2010 = - 4.14. MV = 25; PRCTILE = 98, 2011 = - 2.87. MV = 25; PRCTILE = 98, 2012 = 6.73. MV = 25; PRCTILE = 98, 2013 = - 10.08. MV = 25; PRCTILE = 98, 2014 = - 13.73. MV = 25; PRCTILE = 98, 2015 = 20.21. MV = 25; PRCTILE = 98, 2016 = 23.28. MV = 25; PRCTILE = 98, 2017 = 6.16. MV = 25; PRCTILE = 98, 2006 - 17 = 71.24. Buy&Hold, 2006 = - 5.67. Buy&Hold, 2007 = - 5.95. Buy&Hold, 2008 = - 42.25. Buy&Hold, 2009 = 15.24. Buy&Hold, 2010 = 18.83. Buy&Hold, 2011 = - 13.83. Buy&Hold, 2012 = 9.09. Buy&Hold, 2013 = 9.58. Buy&Hold, 2014 = 8.24. Buy&Hold, 2015 = 12.42. Buy&Hold, 2016 = 4.62. Buy&Hold, 2017 = 3.11. Buy&Hold, 2006 - 17 = 13.42\nThe table shows the P&L per year obtained by following either I LTM t or the Buy and Hold strategy. Rows also indicate the sensitivity of the investment strategy to different values of the moving window (MV) and of the threshold of the empirical distribution (PRCTILE) used to identify phases of unstable market co-movements. Last column represents the P&L over the entire sample.\nmuch less reactive as it can be seen; for instance, after the market rebound of the second half of 2009 and in the /uniFB01 rst part of 2016. Finally, lower performances with respect to our proposed indicator clearly emerge if we observe the P&L obtained by applying a strategy relying on the indicator introduced by ref. 49 (in brown) and the P&L derived from a strategy based only on the average correlation of stocks ' returns (in cyan). All in all, this suggests that the behavioral features of market participants that we propose to capture through the use of the AC and correlation values are together instrumental for anticipating the dynamics of the underlying /uniFB01 nancial system.\naccuracy measures for each percentile (see Supplementary information, Section 3.8). From Fig. 3b, it clearly emerges that the capability of the proposed investment strategy in discriminating between positive and negative market movements increases as long as we select larger absolute values of market returns. This means that the trading strategy based on the LTM indicator correctly anticipates future changes in the aggregate stock price indices, especially around large market movement.\nWe also report in Table 1 the annual P&L achieved by following either our proposed investment strategy or a simply Buy&Hold strategy (last row). We investigate the robustness of our /uniFB01 ndings to changes in the length of the moving window adopted to compute the empirical distribution of I LTM t and in the threshold employed to de /uniFB01 ne the extreme values for this indicator. We observe that our strategy over-performs the simple Buy&Hold strategy over the entire sample period (last column). Beside the fact that the proposed investment strategy does not produce positive P&L for all the periods and for all parameters con /uniFB01 gurations, results in Table 1 still support the predictive performance of our approach. While during market up-trends the signal produces P&L in lines with the Buy&Hold strategy, the timely identi /uniFB01 cation of downward phases limits severe losses that, on the contrary, impact on the naive Buy&Hold strategy (see also Supplementary information, Section 3.6).\nTo quantitatively assess the performance of the proposed investment strategy, we employ a non-parametric approach. We proceed by /uniFB01 rst computing the true-positive, true-negative, falsepositive, and false-negative calls of our investment strategy conditionally on some pre-determined percentiles of the distribution of the absolute values of the market returns. We consider returns larger than an α percentile, with α varying from 10% to 90%. Then, we compute the associated precision and\nFinally, in the Section 3.6 of the Supplementary information, the P&L obtained from this strategy is compared against other investment alternatives such as strategies based on the DFA signals alone. We show that our approach outperforms the other strategies even when we do consider transaction costs. In fact, by assuming transaction costs of 10 basis-points for each portfolio rebalance, we still get a positive P&L of about 5.5% per year.",
    "context": "Summarizes investment strategy performance, highlighting its outperformance compared to a simple Buy&Hold strategy and DFA signals, particularly during market downturns.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      6
    ],
    "id": "83e7fa20d1a0fbbbba62ba8daee4cd78d2c826e9e29bc2a5a21f0ade62a67337"
  },
  {
    "text": "In a highly interconnected /uniFB01 nancial system, it is of paramount relevance to detect the emergence of an abrupt transition from a stable con /uniFB01 guration to a state of instability 20,52 -55 . It is against this background that our results are derived in the context of a /uniFB01 nancial market in which we investigate how the effects of linkages at the micro-level may bring changes to the macrosystem 39 . The level of connectivity in /uniFB02 uences the probability of the system to remain stable. However, the role played by connectivity depends also on how the structure of the network interacts with additional factors which are speci /uniFB01 c for /uniFB01 nancial markets, such as investors heterogeneity, incentives to misbehave and price changes.\nIn this work, we have introduced an indicator that aims at detecting the emergence of instabilities in /uniFB01 nancial markets. The absence of a uni /uniFB01 ed quantitative framework to properly formalize the laws of motion of /uniFB01 nancial markets motivates the use of instruments derived from the network theory to detect the emergence of discontinuities and their temporal evolution.\nChanges in the market conditions are inspected through the analysis of the underlying system at different points in time. Phases of market instability are then assessed by the changes in the structure of the interactions among stock returns.\nIn particular, we have identi /uniFB01 ed phases of accumulation of instability by detecting the emergence of a sub-graph of stocks characterized by both high cohesiveness among its members and long-range memory, which we relate to herding behaviors and positive feedbacks. We summarize the dynamic of this sub-graph through a synthetic indicator which we show to be able to detect temporary transitions of the underlying system. To test this approach, we have also proposed illustrative investment strategies to identify the emergence of up and down market phases according to the signals provided by the indicator. Our results show that this methodology can timely recognize phases of increasing instability that are likely to drive the underlying system into a new market con /uniFB01 guration.\n\nIntroduces a methodology for detecting financial instability by analyzing network structure and its relationship to market dynamics.",
    "original_text": "In a highly interconnected /uniFB01 nancial system, it is of paramount relevance to detect the emergence of an abrupt transition from a stable con /uniFB01 guration to a state of instability 20,52 -55 . It is against this background that our results are derived in the context of a /uniFB01 nancial market in which we investigate how the effects of linkages at the micro-level may bring changes to the macrosystem 39 . The level of connectivity in /uniFB02 uences the probability of the system to remain stable. However, the role played by connectivity depends also on how the structure of the network interacts with additional factors which are speci /uniFB01 c for /uniFB01 nancial markets, such as investors heterogeneity, incentives to misbehave and price changes.\nIn this work, we have introduced an indicator that aims at detecting the emergence of instabilities in /uniFB01 nancial markets. The absence of a uni /uniFB01 ed quantitative framework to properly formalize the laws of motion of /uniFB01 nancial markets motivates the use of instruments derived from the network theory to detect the emergence of discontinuities and their temporal evolution.\nChanges in the market conditions are inspected through the analysis of the underlying system at different points in time. Phases of market instability are then assessed by the changes in the structure of the interactions among stock returns.\nIn particular, we have identi /uniFB01 ed phases of accumulation of instability by detecting the emergence of a sub-graph of stocks characterized by both high cohesiveness among its members and long-range memory, which we relate to herding behaviors and positive feedbacks. We summarize the dynamic of this sub-graph through a synthetic indicator which we show to be able to detect temporary transitions of the underlying system. To test this approach, we have also proposed illustrative investment strategies to identify the emergence of up and down market phases according to the signals provided by the indicator. Our results show that this methodology can timely recognize phases of increasing instability that are likely to drive the underlying system into a new market con /uniFB01 guration.",
    "context": "Introduces a methodology for detecting financial instability by analyzing network structure and its relationship to market dynamics.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      6,
      7
    ],
    "id": "3308eef978b0d93b2af3ed8ce2bdf24ad745e39d2fab39fdc03e4d61cb429c53"
  },
  {
    "text": "Detrended /uniFB02 uctuation analysis . DFA 28 -30 is employed in the /uniFB01 rst step of the analysis to /uniFB01 lter stocks presenting long-term memory. The returns of these stocks are then clusterized according to their correlation values in order to identify the module approaching the phase transition towards a new equilibrium.\nThe DFA method comprises the following steps. In the /uniFB01 rst step, the data series y ( k ), consisting in the stocks returns, is shifted by its mean < y > and integrated (i.e., cumulatively summed) as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nIn the second step, the transformed series is segmented in windows of various length Δ l . For each segmentation, and repeatedly for all values of Δ l , the summed data are /uniFB01 t with a polynomial x Δ l ( k ). By this, the mean squared residual is found as:\nwhere L is the total number of data points. In our analysis, we have applied a linear /uniFB01 t with L set to 200 days. It is worth to remark that F ( Δ l ) can be viewed as the average of the summed squares of the residual computed in the windows.\nFinally, a log -log graph of F ( Δ l ) against Δ l is drawn. This relationship is expected to be linear if power law scaling is present. In other words, a straight line on this log -log graph indicates statistical self-af /uniFB01 nity expressed as F ( Δ l ) ∝ ( Δ l ) α . The scaling exponent α is calculated as the slope of a straight line /uniFB01 t to the log -log graph of Δ l against F ( Δ l ) using least squares. This scaling parameter is a measure of the presence of self-similarity and, therefore, of long-term memory in the signal, as it tracks down the scaling of dispersion around a regressor for increasing window sizes. In particular, the value of α can describe the following signal behaviors: if 0 < α < 0.5, then the signal has long-term memory and it is anti-correlated; if 0.5 < α < 1, the signal has long-term memory and it is correlated; if α = 0.5, the signal is uncorrelated (has no memory); /uniFB01 nally, if 1 < α < 2, the signal is nonstationary.\nThe /uniFB02 uctuation function has a relationship with the AC of stationary process 56 . Indeed, the square of the /uniFB02 uctuation function F ( Δ l ) can be written as a function of the autocorrelation as:\n<!-- formula-not-decoded -->\nbeing ACo( b ) the autocorrelation function. Thus, in terms of AC, for a linear detrending, it is straightforward to compute W ( Δ l ) and Lb ( Δ l ), as:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThe /uniFB02 uctuation function of DFA is therefore fully determined by the AC(1) and by the variance V (1) of the process. For a long-range correlated process, these components are dominant on all time windows and hence a single scaling range with the correct exponent exists.\nOur study starts by assessing the signi /uniFB01 cance of the DFA coef /uniFB01 cients considering the results computed from the original data and from surrogate data, namely, data obtained by independent time permutation for each stock returns. In other words, for a given time series we obtain its randomized (shuf /uniFB02 ed) counterpart by randomly rearranging time stamps attributed to each element in the series.\nBy comparing original results to those obtained for randomized data, we are able to wash out stocks that presents DFA coef /uniFB01 cients in line with the one observed from the shuf /uniFB02 ed case. Basically, we identify the 5 -95th percentiles of the DFA coef /uniFB01 cients distribution as reference thresholds for assessing the statistical signi /uniFB01 cance of the DFA value. Stocks with extreme values of the DFA, that is, stocks with DFA coef /uniFB01 cients belonging to the tails of the distribution, will be then clusterized to obtain an indicator of price co-movement whose dynamics will be used to identify the occurrence of market instabilities and, accordingly, to distinguish between upward and downward market phases.\nThe leading temporal module . In what follows, we describe the methodology that allows us to identify a general signal indicating an imminent bifurcation. This signal is associated with the presence of an LTM, whose statistical properties re /uniFB02 ect a transition of the underlying system to another state. In particular, it can be shown that, when a system is undergoing a bifurcation, the following general temporal and spatial properties hold: a group of stocks displays an average within PCC that drastically increases in absolute value; the average between PCC of stocks in this group and other stocks in the rest of the system will greatly decrease in absolute value; the average AC of stocks belonging to this group increases in absolute value. If all the three above-mentioned conditions are simultaneously satis /uniFB01 ed, we call the group of stocks ful /uniFB01 lling these requirements the LTM of the system 51,57,58 .\nWe now sketch the theoretical background at the basis of our indicator of market instability. Assume that the following discrete-time dynamical system describes the law of motion of a /uniFB01 nancial market, for example, in terms of stock prices or returns:\n<!-- formula-not-decoded -->\nwhere Z t ð Þ ¼ z 1 t ð Þ ; :::; z n t ð Þ ð Þ is a n -dimensional state vector representing stocks returns, P ¼ p 1 ; :::; p s /C0 /C1 is an s -dimensional parameter vector representing slowly changing factors (e.g., news on earnings or pro /uniFB01 ts, anticipated takeovers or mergers, etc.) and ε ¼ ε 1 ; :::; ε n ð Þ is a n -dimensional stochastic component with ε i Gaussian white noise with zero means and covariances κ ij ¼ Cov ð ε i ; ε j Þ . In general, we assume f : R n ´ R s ! R n is a nonlinear vector-valued function. In order to apply theoretical results on bifurcations of a general discrete-time dynamical model, we consider only the deterministic skeleton of the system, that is, we set ε ( t ) = 0. Furthermore, let us assume that the following conditions for Eq. (7) hold: /C22 Z is a /uniFB01 xed point of (7), that is /C22 Z ¼ f /C22 Z ; P ð Þ ; there exists a value P c such that one or a complex conjugate pair of the eigenvalues of the Jacobian matrix of Eq. (7) evaluated at the /uniFB01 xed point /C22 Z is equal to 1 in modulus; when P ≠ P c the eigenvalues of the Jacobian matrix of (7) are generally not 1 in modulus.\nThese conditions, along with other transversality conditions, imply that the system undergoes a transition at /C22 Z or a codimension-one bifurcation 59 . The parameter P c , at which the transition for the equilibrium value /C22 Z occurs, is called a bifurcation value (or a critical transition value) where a sudden qualitative or topological change takes place. The bifurcation is generic from a mathematical viewpoint, that is, almost all bifurcations for a general system satisfy these conditions. Around the /uniFB01 xed point /C22 Z , it is possible to linearize the system described by Eq. (7) as:\n<!-- formula-not-decoded -->\nwhere J ¼ J P ð Þ denotes the Jacobian matrix of (7). By de /uniFB01 ning X ¼ Z /C0 /C22 Z , it is possible to shift the /uniFB01 xed point to the origin, and the system characterized by Eq. (8) can be re-written as:\n<!-- formula-not-decoded -->\nwhere J is a full-rank matrix that also depends on the vector P . Since the Jacobian matrix J is of full rank, then there exists a full-rank matrix S satisfying:\n<!-- formula-not-decoded -->\nBy de /uniFB01 ning Y = S -1 X , and reintroducing the stochastic component ε , the linearized version of the original system can be re-written as:\n<!-- formula-not-decoded -->\n\nIntroduces the Detrended Fluctuations Analysis (DFA) method and its application to identify long-term memory and potential market instabilities.",
    "original_text": "Detrended /uniFB02 uctuation analysis . DFA 28 -30 is employed in the /uniFB01 rst step of the analysis to /uniFB01 lter stocks presenting long-term memory. The returns of these stocks are then clusterized according to their correlation values in order to identify the module approaching the phase transition towards a new equilibrium.\nThe DFA method comprises the following steps. In the /uniFB01 rst step, the data series y ( k ), consisting in the stocks returns, is shifted by its mean < y > and integrated (i.e., cumulatively summed) as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nIn the second step, the transformed series is segmented in windows of various length Δ l . For each segmentation, and repeatedly for all values of Δ l , the summed data are /uniFB01 t with a polynomial x Δ l ( k ). By this, the mean squared residual is found as:\nwhere L is the total number of data points. In our analysis, we have applied a linear /uniFB01 t with L set to 200 days. It is worth to remark that F ( Δ l ) can be viewed as the average of the summed squares of the residual computed in the windows.\nFinally, a log -log graph of F ( Δ l ) against Δ l is drawn. This relationship is expected to be linear if power law scaling is present. In other words, a straight line on this log -log graph indicates statistical self-af /uniFB01 nity expressed as F ( Δ l ) ∝ ( Δ l ) α . The scaling exponent α is calculated as the slope of a straight line /uniFB01 t to the log -log graph of Δ l against F ( Δ l ) using least squares. This scaling parameter is a measure of the presence of self-similarity and, therefore, of long-term memory in the signal, as it tracks down the scaling of dispersion around a regressor for increasing window sizes. In particular, the value of α can describe the following signal behaviors: if 0 < α < 0.5, then the signal has long-term memory and it is anti-correlated; if 0.5 < α < 1, the signal has long-term memory and it is correlated; if α = 0.5, the signal is uncorrelated (has no memory); /uniFB01 nally, if 1 < α < 2, the signal is nonstationary.\nThe /uniFB02 uctuation function has a relationship with the AC of stationary process 56 . Indeed, the square of the /uniFB02 uctuation function F ( Δ l ) can be written as a function of the autocorrelation as:\n<!-- formula-not-decoded -->\nbeing ACo( b ) the autocorrelation function. Thus, in terms of AC, for a linear detrending, it is straightforward to compute W ( Δ l ) and Lb ( Δ l ), as:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThe /uniFB02 uctuation function of DFA is therefore fully determined by the AC(1) and by the variance V (1) of the process. For a long-range correlated process, these components are dominant on all time windows and hence a single scaling range with the correct exponent exists.\nOur study starts by assessing the signi /uniFB01 cance of the DFA coef /uniFB01 cients considering the results computed from the original data and from surrogate data, namely, data obtained by independent time permutation for each stock returns. In other words, for a given time series we obtain its randomized (shuf /uniFB02 ed) counterpart by randomly rearranging time stamps attributed to each element in the series.\nBy comparing original results to those obtained for randomized data, we are able to wash out stocks that presents DFA coef /uniFB01 cients in line with the one observed from the shuf /uniFB02 ed case. Basically, we identify the 5 -95th percentiles of the DFA coef /uniFB01 cients distribution as reference thresholds for assessing the statistical signi /uniFB01 cance of the DFA value. Stocks with extreme values of the DFA, that is, stocks with DFA coef /uniFB01 cients belonging to the tails of the distribution, will be then clusterized to obtain an indicator of price co-movement whose dynamics will be used to identify the occurrence of market instabilities and, accordingly, to distinguish between upward and downward market phases.\nThe leading temporal module . In what follows, we describe the methodology that allows us to identify a general signal indicating an imminent bifurcation. This signal is associated with the presence of an LTM, whose statistical properties re /uniFB02 ect a transition of the underlying system to another state. In particular, it can be shown that, when a system is undergoing a bifurcation, the following general temporal and spatial properties hold: a group of stocks displays an average within PCC that drastically increases in absolute value; the average between PCC of stocks in this group and other stocks in the rest of the system will greatly decrease in absolute value; the average AC of stocks belonging to this group increases in absolute value. If all the three above-mentioned conditions are simultaneously satis /uniFB01 ed, we call the group of stocks ful /uniFB01 lling these requirements the LTM of the system 51,57,58 .\nWe now sketch the theoretical background at the basis of our indicator of market instability. Assume that the following discrete-time dynamical system describes the law of motion of a /uniFB01 nancial market, for example, in terms of stock prices or returns:\n<!-- formula-not-decoded -->\nwhere Z t ð Þ ¼ z 1 t ð Þ ; :::; z n t ð Þ ð Þ is a n -dimensional state vector representing stocks returns, P ¼ p 1 ; :::; p s /C0 /C1 is an s -dimensional parameter vector representing slowly changing factors (e.g., news on earnings or pro /uniFB01 ts, anticipated takeovers or mergers, etc.) and ε ¼ ε 1 ; :::; ε n ð Þ is a n -dimensional stochastic component with ε i Gaussian white noise with zero means and covariances κ ij ¼ Cov ð ε i ; ε j Þ . In general, we assume f : R n ´ R s ! R n is a nonlinear vector-valued function. In order to apply theoretical results on bifurcations of a general discrete-time dynamical model, we consider only the deterministic skeleton of the system, that is, we set ε ( t ) = 0. Furthermore, let us assume that the following conditions for Eq. (7) hold: /C22 Z is a /uniFB01 xed point of (7), that is /C22 Z ¼ f /C22 Z ; P ð Þ ; there exists a value P c such that one or a complex conjugate pair of the eigenvalues of the Jacobian matrix of Eq. (7) evaluated at the /uniFB01 xed point /C22 Z is equal to 1 in modulus; when P ≠ P c the eigenvalues of the Jacobian matrix of (7) are generally not 1 in modulus.\nThese conditions, along with other transversality conditions, imply that the system undergoes a transition at /C22 Z or a codimension-one bifurcation 59 . The parameter P c , at which the transition for the equilibrium value /C22 Z occurs, is called a bifurcation value (or a critical transition value) where a sudden qualitative or topological change takes place. The bifurcation is generic from a mathematical viewpoint, that is, almost all bifurcations for a general system satisfy these conditions. Around the /uniFB01 xed point /C22 Z , it is possible to linearize the system described by Eq. (7) as:\n<!-- formula-not-decoded -->\nwhere J ¼ J P ð Þ denotes the Jacobian matrix of (7). By de /uniFB01 ning X ¼ Z /C0 /C22 Z , it is possible to shift the /uniFB01 xed point to the origin, and the system characterized by Eq. (8) can be re-written as:\n<!-- formula-not-decoded -->\nwhere J is a full-rank matrix that also depends on the vector P . Since the Jacobian matrix J is of full rank, then there exists a full-rank matrix S satisfying:\n<!-- formula-not-decoded -->\nBy de /uniFB01 ning Y = S -1 X , and reintroducing the stochastic component ε , the linearized version of the original system can be re-written as:\n<!-- formula-not-decoded -->",
    "context": "Introduces the Detrended Fluctuations Analysis (DFA) method and its application to identify long-term memory and potential market instabilities.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      7
    ],
    "id": "6b8509405bb3caee764579df0fdee5e391cacdfb7a832e0e34cb5d680ee37ed6"
  },
  {
    "text": "By /uniFB01 xing the value of parameter P before reaching P c , either J or Λ is a constant matrix of full rank and we may end up with three cases: real and distinct eigenvalues, real and coincident eigenvalues, and complex eigenvalues.\nIf the sum of the dimensions of the eigenspaces with real eigenvalues is n , then there exists a non-singular matrix S satisfying Λ ¼ S /C0 1 JS ¼ diag λ 1 ; :::; λ n ð Þ being λ i the i th eigenvalue of the system (11). Without loss of generality, we may regard the /uniFB01 rst element λ 1 j j as being the nearest to 1, that is, the dominant eigenvalue, whose change leads to the state shift. If matrix J does not have linearly independent eigenvectors, there exists a non-singular matrix S making Λ block diagonal. We can always move the block with the largest eigenvalue in modulus, which is also the nearest to 1, to the /uniFB01 rst entry of Λ . Finally, in the case of complex eigenvalues there is a non-singular matrix S making Λ block diagonal where each two-dimensional block matrix has a pair of complex conjugated eigenvalues whose moduli are <1. As before we move the block in which the eigenvalues have the largest modulus to the /uniFB01 rst entry of Λ . Therefore, irrespective of which case occurs, the /uniFB01 rst element of Λ is the dominant eigenvalue, that is, the one nearest to 1 in modulus, whose change\nactually leads to the state shift from the /uniFB01 xed point. Furthermore, all the eigenvalues (or their moduli) of matrix Λ are within [0, 1) and there is at least one dominant eigenvalue approaching 1 in modulus when P → P c .\nFor simplicity, we shall show the statistical properties of the original variables Z considering only the case of real and distinct eigenvalues, but the same conclusion applies for the other two cases in a similar manner 58 .\nSince Λ is a full diagonal matrix, we have the variance V ( ⋅ ), the covariance C ( ⋅ ), the auto-covariance AC( ⋅ ), and the Pearson correlation coef /uniFB01 cient PCC( ⋅ ) of the autoregressive process expressed in Eq. (11) read as:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThe dynamics of the original variable can be written as:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThus, the variance and covariance of the original variables are given by:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThe correlation is given by:\n<!-- formula-not-decoded -->\nwhile the auto-covariance reads as:\n<!-- formula-not-decoded -->\nEquations (19) and (20) relate the empirical signals of the original system (7) with the value assumed by the dominant eigenvalue of the latent system (11). It is worth to note that an increase of the variance, covariance, and autocorrelation of the original system could be due to both a proximity of a tipping point or a strong and unexpected exogenous shock in the stochastic component of the autoregressive process in (11).\nThe temporal and spatial statistical properties that signal an imminent bifurcation can thus be summarized as follows: if a variable zi is related to y 1 , that is, s i 1 ≠ 0, then the absolute value of the auto-covariance AC ( zi ( t ), zi ( t -1)) increases greatly as λ 1 → 1; otherwise, it is bounded; if variables zi and zj are related to y 1 , that is, s i 1 ≠ 0, s j 1 ≠ 0, then j PCC ð z i ð t Þ ; z j ð t ÞÞj ! 1 as λ 1 → 1; if variables zi and zj are not related to y 1 , that is, s i 1 = 0, s j 1 = 0, then j PCC ð z i ð t Þ ; z j ð t ÞÞj ! a with a 2 0 ; 1 ð Þ as λ 1 → 1; if only variable zi is related to y 1 but zj is not, that is, s i 1 ≠ 0, s j 1 = 0, then j PCC ð z i ð t Þ ; z j ð t ÞÞj ! 0 as λ 1 → 1.\nLTM identi /uniFB01 cation . Stocks zi in the system are represented as a dynamical temporal graph Gt = ( Nt , Et ) composed by Nt nodes, while edges Et denote the pairwise correlation (PCC( zi ( t ), zj ( t ))) between each pair of stocks ' returns ( zi ( t ), zj ( t )) computed over a given moving window. This approach relies on the identi /uniFB01 cation of two main sets of stocks: (i) the LTM denoted as N LTM t and (ii) the remaining stocks Nt n N LTM t not belonging to the leading module. To detect whether the system is approaching a new equilibrium, we expect that 58,59 : (i) the absolute value of the auto-covariance of the time series of the LTM members in N LTM t increases; (ii) the absolute value of the correlation between stocks in the LTM increases as well; (iii) conversely, the absolute value of the correlation between a stock in N LTM t and another stock outside the LTM decreases to zero.\nMore practically, to identify the LTM we apply a hierarchical clustering procedure that distinguishes different groups or modules of stocks. We characterize each identi /uniFB01 ed module H by summarizing the statistical features reported above through a synthetic indicator. Let us denote the mean of the absolute value of the auto-covariance of the nodes in N H t as < j AC H t j > , the mean of the absolute value of the correlation coef /uniFB01 cients between members of the H-th module as < j PCC H t j > ,\nand let < j PCC e H t j > be the analogous between stocks in N H t and the remaining stocks. The corresponding synthetic indicator for stocks within each module is de /uniFB01 ned accordingly as:\n<!-- formula-not-decoded -->\nThen, the module with the highest value of I H t is assumed as the LTM of the underlying system and the corresponding indicator, labeled I LTM t , is employed for monitoring the reinforcement of market instabilities. This indicator is expected to sharply increase when a new phase is about to be reached by the underlying system, representing therefore an effective marker for the identi /uniFB01 cation of a cumulative process leading to a new system con /uniFB01 guration 51,58,60 . Hence, we expect the LTM to emerge more clearly when the system is experiencing a transition, meaning that its members become more cohesive and distinct from the rest of the network. In Supplementary information we present a pseudo-code that formalizes the procedure (see Supplementary Fig. 6).\nReporting summary . Further information on research design is available in the Nature Research Reporting Summary linked to this article.\n\nDetails the relationship between dominant eigenvalue changes and state shifts in the latent system, including variance, covariance, and correlation dynamics.",
    "original_text": "By /uniFB01 xing the value of parameter P before reaching P c , either J or Λ is a constant matrix of full rank and we may end up with three cases: real and distinct eigenvalues, real and coincident eigenvalues, and complex eigenvalues.\nIf the sum of the dimensions of the eigenspaces with real eigenvalues is n , then there exists a non-singular matrix S satisfying Λ ¼ S /C0 1 JS ¼ diag λ 1 ; :::; λ n ð Þ being λ i the i th eigenvalue of the system (11). Without loss of generality, we may regard the /uniFB01 rst element λ 1 j j as being the nearest to 1, that is, the dominant eigenvalue, whose change leads to the state shift. If matrix J does not have linearly independent eigenvectors, there exists a non-singular matrix S making Λ block diagonal. We can always move the block with the largest eigenvalue in modulus, which is also the nearest to 1, to the /uniFB01 rst entry of Λ . Finally, in the case of complex eigenvalues there is a non-singular matrix S making Λ block diagonal where each two-dimensional block matrix has a pair of complex conjugated eigenvalues whose moduli are <1. As before we move the block in which the eigenvalues have the largest modulus to the /uniFB01 rst entry of Λ . Therefore, irrespective of which case occurs, the /uniFB01 rst element of Λ is the dominant eigenvalue, that is, the one nearest to 1 in modulus, whose change\nactually leads to the state shift from the /uniFB01 xed point. Furthermore, all the eigenvalues (or their moduli) of matrix Λ are within [0, 1) and there is at least one dominant eigenvalue approaching 1 in modulus when P → P c .\nFor simplicity, we shall show the statistical properties of the original variables Z considering only the case of real and distinct eigenvalues, but the same conclusion applies for the other two cases in a similar manner 58 .\nSince Λ is a full diagonal matrix, we have the variance V ( ⋅ ), the covariance C ( ⋅ ), the auto-covariance AC( ⋅ ), and the Pearson correlation coef /uniFB01 cient PCC( ⋅ ) of the autoregressive process expressed in Eq. (11) read as:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThe dynamics of the original variable can be written as:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThus, the variance and covariance of the original variables are given by:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThe correlation is given by:\n<!-- formula-not-decoded -->\nwhile the auto-covariance reads as:\n<!-- formula-not-decoded -->\nEquations (19) and (20) relate the empirical signals of the original system (7) with the value assumed by the dominant eigenvalue of the latent system (11). It is worth to note that an increase of the variance, covariance, and autocorrelation of the original system could be due to both a proximity of a tipping point or a strong and unexpected exogenous shock in the stochastic component of the autoregressive process in (11).\nThe temporal and spatial statistical properties that signal an imminent bifurcation can thus be summarized as follows: if a variable zi is related to y 1 , that is, s i 1 ≠ 0, then the absolute value of the auto-covariance AC ( zi ( t ), zi ( t -1)) increases greatly as λ 1 → 1; otherwise, it is bounded; if variables zi and zj are related to y 1 , that is, s i 1 ≠ 0, s j 1 ≠ 0, then j PCC ð z i ð t Þ ; z j ð t ÞÞj ! 1 as λ 1 → 1; if variables zi and zj are not related to y 1 , that is, s i 1 = 0, s j 1 = 0, then j PCC ð z i ð t Þ ; z j ð t ÞÞj ! a with a 2 0 ; 1 ð Þ as λ 1 → 1; if only variable zi is related to y 1 but zj is not, that is, s i 1 ≠ 0, s j 1 = 0, then j PCC ð z i ð t Þ ; z j ð t ÞÞj ! 0 as λ 1 → 1.\nLTM identi /uniFB01 cation . Stocks zi in the system are represented as a dynamical temporal graph Gt = ( Nt , Et ) composed by Nt nodes, while edges Et denote the pairwise correlation (PCC( zi ( t ), zj ( t ))) between each pair of stocks ' returns ( zi ( t ), zj ( t )) computed over a given moving window. This approach relies on the identi /uniFB01 cation of two main sets of stocks: (i) the LTM denoted as N LTM t and (ii) the remaining stocks Nt n N LTM t not belonging to the leading module. To detect whether the system is approaching a new equilibrium, we expect that 58,59 : (i) the absolute value of the auto-covariance of the time series of the LTM members in N LTM t increases; (ii) the absolute value of the correlation between stocks in the LTM increases as well; (iii) conversely, the absolute value of the correlation between a stock in N LTM t and another stock outside the LTM decreases to zero.\nMore practically, to identify the LTM we apply a hierarchical clustering procedure that distinguishes different groups or modules of stocks. We characterize each identi /uniFB01 ed module H by summarizing the statistical features reported above through a synthetic indicator. Let us denote the mean of the absolute value of the auto-covariance of the nodes in N H t as < j AC H t j > , the mean of the absolute value of the correlation coef /uniFB01 cients between members of the H-th module as < j PCC H t j > ,\nand let < j PCC e H t j > be the analogous between stocks in N H t and the remaining stocks. The corresponding synthetic indicator for stocks within each module is de /uniFB01 ned accordingly as:\n<!-- formula-not-decoded -->\nThen, the module with the highest value of I H t is assumed as the LTM of the underlying system and the corresponding indicator, labeled I LTM t , is employed for monitoring the reinforcement of market instabilities. This indicator is expected to sharply increase when a new phase is about to be reached by the underlying system, representing therefore an effective marker for the identi /uniFB01 cation of a cumulative process leading to a new system con /uniFB01 guration 51,58,60 . Hence, we expect the LTM to emerge more clearly when the system is experiencing a transition, meaning that its members become more cohesive and distinct from the rest of the network. In Supplementary information we present a pseudo-code that formalizes the procedure (see Supplementary Fig. 6).\nReporting summary . Further information on research design is available in the Nature Research Reporting Summary linked to this article.",
    "context": "Details the relationship between dominant eigenvalue changes and state shifts in the latent system, including variance, covariance, and correlation dynamics.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      8,
      7
    ],
    "id": "0d0d622ddba3e3adffc3406dd7b3b1f842ca3827c16bf4c90b61612961610a03"
  },
  {
    "text": "The data that support the /uniFB01 ndings of this study are available on request from the corresponding author A.S. The data are not publicly available due to privacy restrictions.\n\nDetails data availability and reasons for restricted public access.",
    "original_text": "The data that support the /uniFB01 ndings of this study are available on request from the corresponding author A.S. The data are not publicly available due to privacy restrictions.",
    "context": "Details data availability and reasons for restricted public access.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      8
    ],
    "id": "72a02b5e5252b7f2c4c03913d0e3c279573dff53fe875b06760b25f41bd605fe"
  },
  {
    "text": "Codes are available upon request.\nReceived: 22 July 2019; Accepted: 5 March 2020;\n\nDocuments publication details and availability of supplementary materials.",
    "original_text": "Codes are available upon request.\nReceived: 22 July 2019; Accepted: 5 March 2020;",
    "context": "Documents publication details and availability of supplementary materials.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      8
    ],
    "id": "6877e6e87cb0917de060b1d2dd3fe9b5ec9f19a365608d42fbc02270f59fda36"
  },
  {
    "text": "1. Mantegna, R. N. & Stanley, H. E. Introduction to Econophysics: Correlations and Complexity in Finance (Cambridge Univ. Press, 1999).\n2. Orsenigo, L., Pammolli, F. & Riccaboni, M. Technological change and network dynamics. Res. Policy 30 , 485 -508 (2001).\n3. Durlauf, S. N. Complexity and empirical economics. Econ. J. 115 , F225 -F243 (2005).\n4. Scheffer, M. et al. Early-warning signals for critical transitions. Nature 461 , 53 (2009).\n5. Diks, C., Hommes, C. & Wang, J. Critical slowing down as an early warning signal for /uniFB01 nancial crises? Emp. Econ. 57 , 1 -28 (2015).\n6. Zhao, L. et al. Herd behavior in a complex adaptive system. Proc. Natl Acad. Sci. USA 108 , 15058 -15063 (2011).\n7. Hüsler, A., Sornette, D. & Hommes, C. Super-exponential bubbles in lab experiments: evidence for anchoring over-optimistic expectations on price. J. Econ. Behav. Org. 92 , 304 -316 (2013).\n8. Trueman, B. Analyst forecasts and herding behavior. Rev. Financial Stud. 7 , 97 -124 (1994).\n9. Sharma, M. S. & Bikhchandani, S. Herd Behavior in Financial Markets: A Review 0 -48 (International Monetary Fund, 2000).\n10. Banerjee, A. V. A simple model of herd behavior. Q. J. Econ. 107 , 797 -817 (1992).\n11. Sornette, D. Predictability of catastrophic events: material rupture, earthquakes, turbulence, /uniFB01 nancial crashes, and human birth. Proc. Natl Acad. Sci. USA 99 , 2522 -2529 (2002).\n12. Sornette, D. Critical market crashes. Phys. Rep. 378 , 1 -98 (2003).\n13. Sornette, D. Why Stock Markets Crash: Critical Events in Complex Financial Systems (Princeton Univ. Press, 2017).\n14. Schweitzer, F. et al. Economic networks: the new challenges. Science 325 , 422 -425 (2009).\n15. Baillie, R. T. & Bollerslev, T. Cointegration, fractional cointegration, and exchange rate dynamics. J. Finance 49 , 737 -745 (1994).\n16. Brenner, R. J. & Kroner, K. F. Arbitrage, cointegration, and testing the unbiasedness hypothesis in /uniFB01 nancial markets. J. Financial Quant. Anal. 30 , 23 -42 (1995).\n17. Forbes, K. J. & Rigobon, R. No contagion, only interdependence: measuring stock market comovements. J. Finance 57 , 2223 -2261 (2002).\n18. Forbes, K. J. & Chinn, M. D. A decomposition of global linkages in /uniFB01 nancial markets over time. Rev. Econ. Stat. 86 , 705 -722 (2004).\n\nSynthesizes research on complex systems, financial crises, and herd behavior within financial markets.",
    "original_text": "1. Mantegna, R. N. & Stanley, H. E. Introduction to Econophysics: Correlations and Complexity in Finance (Cambridge Univ. Press, 1999).\n2. Orsenigo, L., Pammolli, F. & Riccaboni, M. Technological change and network dynamics. Res. Policy 30 , 485 -508 (2001).\n3. Durlauf, S. N. Complexity and empirical economics. Econ. J. 115 , F225 -F243 (2005).\n4. Scheffer, M. et al. Early-warning signals for critical transitions. Nature 461 , 53 (2009).\n5. Diks, C., Hommes, C. & Wang, J. Critical slowing down as an early warning signal for /uniFB01 nancial crises? Emp. Econ. 57 , 1 -28 (2015).\n6. Zhao, L. et al. Herd behavior in a complex adaptive system. Proc. Natl Acad. Sci. USA 108 , 15058 -15063 (2011).\n7. Hüsler, A., Sornette, D. & Hommes, C. Super-exponential bubbles in lab experiments: evidence for anchoring over-optimistic expectations on price. J. Econ. Behav. Org. 92 , 304 -316 (2013).\n8. Trueman, B. Analyst forecasts and herding behavior. Rev. Financial Stud. 7 , 97 -124 (1994).\n9. Sharma, M. S. & Bikhchandani, S. Herd Behavior in Financial Markets: A Review 0 -48 (International Monetary Fund, 2000).\n10. Banerjee, A. V. A simple model of herd behavior. Q. J. Econ. 107 , 797 -817 (1992).\n11. Sornette, D. Predictability of catastrophic events: material rupture, earthquakes, turbulence, /uniFB01 nancial crashes, and human birth. Proc. Natl Acad. Sci. USA 99 , 2522 -2529 (2002).\n12. Sornette, D. Critical market crashes. Phys. Rep. 378 , 1 -98 (2003).\n13. Sornette, D. Why Stock Markets Crash: Critical Events in Complex Financial Systems (Princeton Univ. Press, 2017).\n14. Schweitzer, F. et al. Economic networks: the new challenges. Science 325 , 422 -425 (2009).\n15. Baillie, R. T. & Bollerslev, T. Cointegration, fractional cointegration, and exchange rate dynamics. J. Finance 49 , 737 -745 (1994).\n16. Brenner, R. J. & Kroner, K. F. Arbitrage, cointegration, and testing the unbiasedness hypothesis in /uniFB01 nancial markets. J. Financial Quant. Anal. 30 , 23 -42 (1995).\n17. Forbes, K. J. & Rigobon, R. No contagion, only interdependence: measuring stock market comovements. J. Finance 57 , 2223 -2261 (2002).\n18. Forbes, K. J. & Chinn, M. D. A decomposition of global linkages in /uniFB01 nancial markets over time. Rev. Econ. Stat. 86 , 705 -722 (2004).",
    "context": "Synthesizes research on complex systems, financial crises, and herd behavior within financial markets.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      8
    ],
    "id": "6defb5fe85666615256a3bfbe8f450e46437136a5aa6c6d8b3faca4e79de684b"
  },
  {
    "text": "19. Barberis, N., Shleifer, A. & Wurgler, J. Comovement. J. Financial Econ. 75 , 283 -317 (2005).\n20. Simon, H. A. & Ando, A. Aggregation of variables in dynamic systems. Econometrica 29 , 111 -138 (1961).\n21. Ando, A. & Fisher, F. M. Near-decomposability, partition and aggregation, and the relevance of stability discussions. Int. Econ. Rev. 4 , 53 -67 (1963).\n22. Simon, H. A. The Architecture of Complexity (MIT Press, Cambridge, 1996).\n23. Courtois, P. J. Decomposability: Queueing and Computer System Applications (Academic Press, 2014).\n24. Dakos, V. et al. Methods for detecting early warnings of critical transitions in time series illustrated using simulated ecological data. PLoS ONE 7 , e41010 (2012).\n25. Lenton, T., Livina, V., Dakos, V., Van Nes, E. & Scheffer, M. Early warning of climate tipping points from critical slowing down: comparing methods to improve robustness. Philos. Trans. R. Soc. Ser. A 370 , 1185 -1204 (2012).\n26. Lux, T. Herd behaviour, bubbles and crashes. Econ. J. 105 , 881 -896 (1995).\n27. Hong, H. & Stein, J. C. Differences of opinion, short-sales constraints, and market crashes. Rev. Financial Stud. 16 , 487 -525 (2003).\n28. Peng, C.-K. et al. Mosaic organization of dna nucleotides. Phys. Rev. E 49 , 1685 (1994).\n29. Viswanathan, G. M. et al. Lévy /uniFB02 ight search patterns of wandering albatrosses. Nature 381 , 413 (1996).\n30. Hardstone, R. et al. Detrended /uniFB02 uctuation analysis: a scale-free view on neuronal oscillations. Front. Physiol. 3 , 450 (2012).\n31. Preis, T., Moat, H. S. & Stanley, H. E. Quantifying trading behavior in /uniFB01 nancial markets using google trends. Scienti /uniFB01 c Rep. 3 , 1684 (2013).\n32. Zhong, X. & Raghib, M. Revisiting the use of web search data for stock market movements. Scienti /uniFB01 c Rep. 9 , 1 -8 (2019).\n33. Hommes, C. Modeling the stylized facts in /uniFB01 nance through simple nonlinear adaptive systems. Proc. Natl Acad. Sci. USA 99 , 7221 -7228 (2002).\n34. Hommes, C. Heterogeneous agent models in economics and /uniFB01 nance. Handb. Comput. Econ. 2 , 1109 -1186 (2006).\n35. Gabaix, X., Gopikrishnan, P., Plerou, V. & Stanley, H. E. A theory of powerlaw distributions in /uniFB01 nancial market /uniFB02 uctuations. Nature 423 , 267 (2003).\n36. Scheffer, M. et al. Anticipating critical transitions. Science 338 , 344 -348 (2012).\n37. Battiston, S., Caldarelli, G., May, R. M., Roukny, T. & Stiglitz, J. E. The price of complexity in /uniFB01 nancial networks. Proc. Natl Acad. Sci. USA 113 , 10031 -10036 (2016).\n38. Battiston, S. et al. Complexity theory and /uniFB01 nancial regulation. Science 351 , 818 -819 (2016).\n39. Corsi, F., Marmi, S. & Lillo, F. When micro prudence increases macro risk: the destabilizing effects of /uniFB01 nancial innovation, leverage, and diversi /uniFB01 cation. Oper. Res. 64 , 1073 -1088 (2016).\n40. Flori, A., Pammolli, F., Buldyrev, S. V., Regis, L. & Stanley, H. E. Communities and regularities in the behavior of investment fund managers. Proc. Natl Acad. Sci. USA 116 , 201802976 (2019).\n41. Fortunato, S. Community detection in graphs. Phys. Rep. 486 , 75 -174 (2010).\n42. MacMahon, M. & Garlaschelli, D. Community detection for correlation matrices. Phys. Rev. X 5 , 021006 (2015).\n43. Newman, M. Networks (Oxford Univ. Press, 2018).\n44. Stanley, H. E. Phase Transitions and Critical Phenomena (Clarendon Press, Oxford, 1971).\n45. Chiang, T. C., Jeon, B. N. & Li, H. Dynamic correlation analysis of /uniFB01 nancial contagion: evidence from Asian markets. J. Int. Money Finance 26 , 1206 -1228 (2007).\n46. Kindleberger, C. P. & Aliber, R. Z. Manias, Panics and Crashes: A History of Financial Crises (Palgrave Macmillan, 2011).\n47. Mera, K. & Renaud, B. Asia ' s Financial Crisis and the Role of Real Estate (Routledge, 2016).\n48. Chowdhury, B., Dungey, M. H., Kangogo, M., Sayeed, M. A. & Volkov, V. The changing network of /uniFB01 nancial market linkages: the asian experience. Int. Rev. Financial Anal. 64 , 71 -92 (2019).\n49. Jorion, P. et al. Financial Risk Manager Handbook , Vol. 406 (Wiley, 2007).\n50. Fleten, S.-E., Maribu, K. M. & Wangensteen, I. Optimal investment strategies in decentralized renewable power generation under uncertainty. Energy 32 , 803 -815 (2007).\n51. Chen, L., Liu, R., Liu, Z.-P., Li, M. & Aihara, K. Detecting early-warning signals for sudden deterioration of complex diseases by dynamical network biomarkers. Scienti /uniFB01 c Rep. 2 , 342 (2012).\n52. Gai, P. & Kapadia, S. Contagion in /uniFB01 nancial networks. Proc. R. Soc. Ser. A 466 , 2401 -2423 (2010).\n53. Gai, P., Haldane, A. & Kapadia, S. Complexity, concentration and contagion. J. Monetary Econ. 58 , 453 -470 (2011).\n54. Amini, H., Cont, R. & Minca, A. Stress testing the resilience of /uniFB01 nancial networks. Int. J. Theor. Appl. Finance 15 , 1250006 (2012).\n55. Acemoglu, D., Ozdaglar, A. & Tahbaz-Salehi, A. The Network Origins of Large Economic Downturns . Technical Report (National Bureau of Economic Research, 2013).\n56. Höll, M. & Kantz, H. The relationship between the detrendend /uniFB02 uctuation analysis and the autocorrelation function of a signal. Eur. Phys. J. B 88 , 327 (2015).\n57. Chen, L., Wang, R., Li, C.& Aihara, K. Modeling Biomolecular Networks in Cells: Structures and Dynamics (Springer Science & Business Media, 2010).\n58. Spelta, A., Pecora, N., Flori, A. & Pammolli, F. Transition Drivers and Crisis Signaling in Stock Markets . MPRA Paper 88127 (Univ. Library of Munich, Germany, 2018).\n59. Spelta, A., Flori, A., Pecora, N. & Pammolli, F. Financial crises: uncovering self-organized patterns and predicting stock markets instability. J. Bus. Res. https://doi.org/10.1016/j.jbusres.2019.10.043 (2019).\n60. Preis, T., Schneider, J. J. & Stanley, H. E. Switching processes in /uniFB01 nancial markets. Proc. Natl Acad. Sci. USA 108 , 7674 -7678 (2011).\n\nProvides a collection of research exploring network analysis, financial crises, and complex systems, including methods for detecting early warnings and quantifying financial risk.",
    "original_text": "19. Barberis, N., Shleifer, A. & Wurgler, J. Comovement. J. Financial Econ. 75 , 283 -317 (2005).\n20. Simon, H. A. & Ando, A. Aggregation of variables in dynamic systems. Econometrica 29 , 111 -138 (1961).\n21. Ando, A. & Fisher, F. M. Near-decomposability, partition and aggregation, and the relevance of stability discussions. Int. Econ. Rev. 4 , 53 -67 (1963).\n22. Simon, H. A. The Architecture of Complexity (MIT Press, Cambridge, 1996).\n23. Courtois, P. J. Decomposability: Queueing and Computer System Applications (Academic Press, 2014).\n24. Dakos, V. et al. Methods for detecting early warnings of critical transitions in time series illustrated using simulated ecological data. PLoS ONE 7 , e41010 (2012).\n25. Lenton, T., Livina, V., Dakos, V., Van Nes, E. & Scheffer, M. Early warning of climate tipping points from critical slowing down: comparing methods to improve robustness. Philos. Trans. R. Soc. Ser. A 370 , 1185 -1204 (2012).\n26. Lux, T. Herd behaviour, bubbles and crashes. Econ. J. 105 , 881 -896 (1995).\n27. Hong, H. & Stein, J. C. Differences of opinion, short-sales constraints, and market crashes. Rev. Financial Stud. 16 , 487 -525 (2003).\n28. Peng, C.-K. et al. Mosaic organization of dna nucleotides. Phys. Rev. E 49 , 1685 (1994).\n29. Viswanathan, G. M. et al. Lévy /uniFB02 ight search patterns of wandering albatrosses. Nature 381 , 413 (1996).\n30. Hardstone, R. et al. Detrended /uniFB02 uctuation analysis: a scale-free view on neuronal oscillations. Front. Physiol. 3 , 450 (2012).\n31. Preis, T., Moat, H. S. & Stanley, H. E. Quantifying trading behavior in /uniFB01 nancial markets using google trends. Scienti /uniFB01 c Rep. 3 , 1684 (2013).\n32. Zhong, X. & Raghib, M. Revisiting the use of web search data for stock market movements. Scienti /uniFB01 c Rep. 9 , 1 -8 (2019).\n33. Hommes, C. Modeling the stylized facts in /uniFB01 nance through simple nonlinear adaptive systems. Proc. Natl Acad. Sci. USA 99 , 7221 -7228 (2002).\n34. Hommes, C. Heterogeneous agent models in economics and /uniFB01 nance. Handb. Comput. Econ. 2 , 1109 -1186 (2006).\n35. Gabaix, X., Gopikrishnan, P., Plerou, V. & Stanley, H. E. A theory of powerlaw distributions in /uniFB01 nancial market /uniFB02 uctuations. Nature 423 , 267 (2003).\n36. Scheffer, M. et al. Anticipating critical transitions. Science 338 , 344 -348 (2012).\n37. Battiston, S., Caldarelli, G., May, R. M., Roukny, T. & Stiglitz, J. E. The price of complexity in /uniFB01 nancial networks. Proc. Natl Acad. Sci. USA 113 , 10031 -10036 (2016).\n38. Battiston, S. et al. Complexity theory and /uniFB01 nancial regulation. Science 351 , 818 -819 (2016).\n39. Corsi, F., Marmi, S. & Lillo, F. When micro prudence increases macro risk: the destabilizing effects of /uniFB01 nancial innovation, leverage, and diversi /uniFB01 cation. Oper. Res. 64 , 1073 -1088 (2016).\n40. Flori, A., Pammolli, F., Buldyrev, S. V., Regis, L. & Stanley, H. E. Communities and regularities in the behavior of investment fund managers. Proc. Natl Acad. Sci. USA 116 , 201802976 (2019).\n41. Fortunato, S. Community detection in graphs. Phys. Rep. 486 , 75 -174 (2010).\n42. MacMahon, M. & Garlaschelli, D. Community detection for correlation matrices. Phys. Rev. X 5 , 021006 (2015).\n43. Newman, M. Networks (Oxford Univ. Press, 2018).\n44. Stanley, H. E. Phase Transitions and Critical Phenomena (Clarendon Press, Oxford, 1971).\n45. Chiang, T. C., Jeon, B. N. & Li, H. Dynamic correlation analysis of /uniFB01 nancial contagion: evidence from Asian markets. J. Int. Money Finance 26 , 1206 -1228 (2007).\n46. Kindleberger, C. P. & Aliber, R. Z. Manias, Panics and Crashes: A History of Financial Crises (Palgrave Macmillan, 2011).\n47. Mera, K. & Renaud, B. Asia ' s Financial Crisis and the Role of Real Estate (Routledge, 2016).\n48. Chowdhury, B., Dungey, M. H., Kangogo, M., Sayeed, M. A. & Volkov, V. The changing network of /uniFB01 nancial market linkages: the asian experience. Int. Rev. Financial Anal. 64 , 71 -92 (2019).\n49. Jorion, P. et al. Financial Risk Manager Handbook , Vol. 406 (Wiley, 2007).\n50. Fleten, S.-E., Maribu, K. M. & Wangensteen, I. Optimal investment strategies in decentralized renewable power generation under uncertainty. Energy 32 , 803 -815 (2007).\n51. Chen, L., Liu, R., Liu, Z.-P., Li, M. & Aihara, K. Detecting early-warning signals for sudden deterioration of complex diseases by dynamical network biomarkers. Scienti /uniFB01 c Rep. 2 , 342 (2012).\n52. Gai, P. & Kapadia, S. Contagion in /uniFB01 nancial networks. Proc. R. Soc. Ser. A 466 , 2401 -2423 (2010).\n53. Gai, P., Haldane, A. & Kapadia, S. Complexity, concentration and contagion. J. Monetary Econ. 58 , 453 -470 (2011).\n54. Amini, H., Cont, R. & Minca, A. Stress testing the resilience of /uniFB01 nancial networks. Int. J. Theor. Appl. Finance 15 , 1250006 (2012).\n55. Acemoglu, D., Ozdaglar, A. & Tahbaz-Salehi, A. The Network Origins of Large Economic Downturns . Technical Report (National Bureau of Economic Research, 2013).\n56. Höll, M. & Kantz, H. The relationship between the detrendend /uniFB02 uctuation analysis and the autocorrelation function of a signal. Eur. Phys. J. B 88 , 327 (2015).\n57. Chen, L., Wang, R., Li, C.& Aihara, K. Modeling Biomolecular Networks in Cells: Structures and Dynamics (Springer Science & Business Media, 2010).\n58. Spelta, A., Pecora, N., Flori, A. & Pammolli, F. Transition Drivers and Crisis Signaling in Stock Markets . MPRA Paper 88127 (Univ. Library of Munich, Germany, 2018).\n59. Spelta, A., Flori, A., Pecora, N. & Pammolli, F. Financial crises: uncovering self-organized patterns and predicting stock markets instability. J. Bus. Res. https://doi.org/10.1016/j.jbusres.2019.10.043 (2019).\n60. Preis, T., Schneider, J. J. & Stanley, H. E. Switching processes in /uniFB01 nancial markets. Proc. Natl Acad. Sci. USA 108 , 7674 -7678 (2011).",
    "context": "Provides a collection of research exploring network analysis, financial crises, and complex systems, including methods for detecting early warnings and quantifying financial risk.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      9
    ],
    "id": "9eab4cf5eac7c01652e67804e047094ba4bc212064dc7e65707b28deff22f210"
  },
  {
    "text": "We acknowledge funding from the National Research Project (PNR) CRISIS Lab. We thank anonymous Reviewers and the Editor for their detailed and constructive feedback on our paper.\n\nAcknowledges funding source and expresses gratitude for reviewer feedback.",
    "original_text": "We acknowledge funding from the National Research Project (PNR) CRISIS Lab. We thank anonymous Reviewers and the Editor for their detailed and constructive feedback on our paper.",
    "context": "Acknowledges funding source and expresses gratitude for reviewer feedback.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      9
    ],
    "id": "c89afa650501d6b6b88b4d04714319fa343cbda7c9a8bb78f89e77ee399b75ed"
  },
  {
    "text": "A.S., A.F., N.P., F.P., and S.B.: designed the research, performed the research, analyzed the data, and wrote the paper.\n\nLists the authors and their contributions to the research.",
    "original_text": "A.S., A.F., N.P., F.P., and S.B.: designed the research, performed the research, analyzed the data, and wrote the paper.",
    "context": "Lists the authors and their contributions to the research.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      9
    ],
    "id": "a3851cb4e25b2d2ea5e6b84684b6499e471a94cbd9d0eeaffe924e3990aca05c"
  },
  {
    "text": "The authors declare no competing interests.\n\nDeclares absence of conflicts of interest.",
    "original_text": "The authors declare no competing interests.",
    "context": "Declares absence of conflicts of interest.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      9
    ],
    "id": "9ba9761df944344679e71f50f98a25479ec835067d14e2f3d453b7d21c5f9a4f"
  },
  {
    "text": "Supplementary information is available for this paper at https://doi.org/10.1038/s41467020-15356-z.\nCorrespondence and requests for materials should be addressed to A.S.\nPeer review information Nature Communications thanks Š tefan Lyócsa and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Peer reviewer reports are available.\nReprints and permission information is available at http://www.nature.com/reprints\nPublisher ' s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional af /uniFB01 liations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article ' s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article ' s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/ licenses/by/4.0/.\n© The Author(s) 2020\n\nDetails about supplementary materials, contact information, and copyright/licensing details.",
    "original_text": "Supplementary information is available for this paper at https://doi.org/10.1038/s41467020-15356-z.\nCorrespondence and requests for materials should be addressed to A.S.\nPeer review information Nature Communications thanks Š tefan Lyócsa and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Peer reviewer reports are available.\nReprints and permission information is available at http://www.nature.com/reprints\nPublisher ' s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional af /uniFB01 liations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article ' s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article ' s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/ licenses/by/4.0/.\n© The Author(s) 2020",
    "context": "Details about supplementary materials, contact information, and copyright/licensing details.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      9
    ],
    "id": "53bc6c4287fe964c11243a6405f8f575a02f73bd143097f7c3a0a932b21d3830"
  }
]