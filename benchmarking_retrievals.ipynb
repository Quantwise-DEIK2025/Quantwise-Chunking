{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d156ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             1_minus_recall_at_4  context_precision_ratio  mrr_at_4  ndcg_at_4\n",
      "question_id                                                                   \n",
      "1                       0.333333                      1.0       1.0   0.703918\n",
      "2                       1.000000                      0.0       0.0   0.000000\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import json\n",
    "import lancedb\n",
    "from lancedb.rerankers import ColbertReranker\n",
    "from tqdm.notebook import tqdm\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from devtools import debug\n",
    "from utils.metrics_calculate import calculate_advanced_retrieval_metrics\n",
    "import instructor\n",
    "\n",
    "METRICS_FIGURE_NAME = \"Contextual retrieval performance metrics (document slice only)\"\n",
    "\n",
    "\n",
    "OWN_TABLE_NAME = \"my_anthropic_sliding_table\"\n",
    "BASELINE_TABLE_NAME = \"anthropic_table\"\n",
    "QNA_FILE_PATH = \"q_and_a/Gemini/scientific_multi_chunk.json\"\n",
    "MODEL_TAG = \"qwen3-vl:8b-instruct-q4_K_M\"\n",
    "\n",
    "client = instructor.from_provider(\n",
    "\tf\"ollama/{MODEL_TAG}\",\n",
    "\tbase_url=\"http://localhost:11434/v1\",\n",
    "\tmode=instructor.Mode.JSON,\n",
    ")\n",
    "\n",
    "class RelevanceEvaluation(BaseModel):\n",
    "\tchain_of_thought: str = Field(\n",
    "\t\t..., \n",
    "\t\tdescription=\"A brief reasoning step explaining why the score was given.\"\n",
    "\t)\n",
    "\tscore: Literal[0,1,2,3] = Field(\n",
    "\t\t..., \n",
    "\t\tdescription=\"The relevance score (0, 1, 2, or 3) based on the grading rubric.\"\n",
    "\t)\n",
    "\n",
    "def grade_chunk_relevance(question: str, chunk_text: str, model_name: str) -> RelevanceEvaluation:\n",
    "\t\"\"\"\n",
    "\tUses Qwen to grade a single chunk against a question.\n",
    "\tReturns the integer score (0-3).\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# Precise rubric for the system prompt\n",
    "\tsystem_prompt = \"\"\"\n",
    "\tYou are an impartial expert judge evaluating retrieval quality for a RAG system.\n",
    "\tEvaluate the relevance of the PASSAGE to the QUESTION using this strict scale:\n",
    "\t\n",
    "\t0: Irrelevant. The passage is on a different topic or does not help.\n",
    "\t1: Tangential. Mentions related entities but does not elaborate or provides explicit answer to the question.\n",
    "\t2: Relevant/Partial. Provides useful context or a partial answer.\n",
    "\t3: Highly Relevant. Contains the direct answer or core evidence required.\n",
    "\t\"\"\"\n",
    "\n",
    "\ttry:\n",
    "\t\tresp = client.create(\n",
    "\t\t\tmodel=model_name,\n",
    "\t\t\tresponse_model=RelevanceEvaluation,\n",
    "\t\t\tmessages=[\n",
    "\t\t\t\t{\"role\": \"system\", \"content\": system_prompt},\n",
    "\t\t\t\t{\"role\": \"user\", \"content\": f\"QUESTION: {question}\\nPASSAGE: {chunk_text}\"}\n",
    "\t\t\t],\n",
    "\t\t\ttemperature=0\n",
    "\t\t)\n",
    "\t\treturn resp\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Error grading chunk: {e}\")\n",
    "\t\treturn 0 # Fail-safe: assume irrelevant if model crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cf4a0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ColBERTRanker model colbert-ir/colbertv2.0 (this message can be suppressed by setting verbose=0)\n",
      "No device set\n",
      "Using device cuda\n",
      "No dtype set\n",
      "Using dtype torch.float32\n",
      "Loading model colbert-ir/colbertv2.0, this might take a while...\n",
      "Linear Dim set to: 128 for downcasting\n"
     ]
    }
   ],
   "source": [
    "db = lancedb.connect(\"./db\")\n",
    "own_table = db.open_table(OWN_TABLE_NAME)\n",
    "baseline_table = db.open_table(BASELINE_TABLE_NAME)\n",
    "reranker = ColbertReranker()\n",
    "\n",
    "with open(QNA_FILE_PATH, \"r\") as f:\n",
    "\tqna = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e35df180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do the fundamental goals of open science conflict with the specific characteristics of empirical software engineering research involving industrial contexts?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna[0]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d9d8697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7227b8740e44004a41a1925f93025f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing questions...:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "own_recalls_per_question = []\n",
    "baseline_recalls_per_question = []\n",
    "\n",
    "own_avg_recall_at_n = {}\n",
    "baseline_avg_recall_at_n = {}\n",
    "\n",
    "for question in tqdm(qna, desc=\"Processing questions...\"):\n",
    "\tquestion_prompt = question[\"question\"]\n",
    "\tsupporting_chunk_ids = set(question[\"supporting_chunks\"])\n",
    "\n",
    "\town_df = own_table.search(question_prompt, query_type=\"hybrid\", vector_column_name=\"vector\", fts_columns=\"text\") \\\n",
    "\t\t\t.rerank(reranker=reranker) \\\n",
    "\t\t\t.limit(20) \\\n",
    "\t\t\t.to_pandas()\n",
    "\t\n",
    "\tbaseline_df = baseline_table.search(question_prompt, query_type=\"hybrid\", vector_column_name=\"vector\", fts_columns=\"text\") \\\n",
    "\t\t\t.rerank(reranker=reranker) \\\n",
    "\t\t\t.limit(20) \\\n",
    "\t\t\t.to_pandas()\n",
    "\n",
    "\town_recalls = {}\n",
    "\tbaseline_recalls = {}\n",
    "\n",
    "\tfor i in range(5,21,5):\n",
    "\t\town_df_cutoff = own_df.head(i)\n",
    "\t\tbaseline_df_cutoff = baseline_df.head(i)\n",
    "\n",
    "\t\town_retreived_chunk_ids = own_df_cutoff['id'].tolist()\n",
    "\t\town_recall_at_i = sum([1 if id in supporting_chunk_ids else 0 for id in own_retreived_chunk_ids]) / len(supporting_chunk_ids)\n",
    "\n",
    "\t\tbaseline_retreived_chunk_ids = baseline_df_cutoff['id'].tolist()\n",
    "\t\tbaseline_recall_at_i = sum([1 if id in supporting_chunk_ids else 0 for id in baseline_retreived_chunk_ids]) / len(supporting_chunk_ids)\n",
    "\n",
    "\t\town_recalls[i] = own_recall_at_i\n",
    "\t\tbaseline_recalls[i] = baseline_recall_at_i\n",
    "\n",
    "\town_recalls_per_question.append(own_recalls)\n",
    "\tbaseline_recalls_per_question.append(baseline_recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d93280d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Own results 0.7743827160493827\n",
      "Baseline: 0.43364197530864196\n",
      "Own results 0.8975308641975309\n",
      "Baseline: 0.4981481481481482\n",
      "Own results 0.9324074074074075\n",
      "Baseline: 0.5160493827160494\n",
      "Own results 0.9561728395061727\n",
      "Baseline: 0.5225308641975309\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i in range(5,21,5):\n",
    "    own_avg_recall_at_n[i] = np.average([recall[i] for recall in own_recalls_per_question])\n",
    "    baseline_avg_recall_at_n[i] = np.average([recall[i] for recall in baseline_recalls_per_question])\n",
    "\n",
    "    print(f\"Own results {own_avg_recall_at_n[i]}\\nBaseline: {baseline_avg_recall_at_n[i]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f975a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pydantic import BaseModel, ValidationError, Field, model_validator \n",
    "from typing import List, Self\n",
    "\n",
    "class BarChartData(BaseModel):\n",
    "    \"\"\"\n",
    "    Schema for validating bar chart data.\n",
    "    Ensures data consistency before visualization.\n",
    "    \"\"\"\n",
    "    labels: List[str] = Field(..., description=\"Names of the bars\")\n",
    "    values: List[float] = Field(..., description=\"Numerical values for the bars\")\n",
    "    title: str = Field(..., description=\"Title of the chart\")\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_length_match(self) -> Self:\n",
    "        if len(self.labels) != len(self.values):\n",
    "            raise ValidationError(f\"There is different number of labels and values!\\n{self.labels=}\\n{self.values=}\")\n",
    "        return self\n",
    "    \n",
    "\n",
    "def create_annotated_bar_chart(data: BarChartData, output_file: str = 'annotated_bar_chart.png'):\n",
    "    \"\"\"\n",
    "    Generates a bar chart with explicit value annotations above each bar.\n",
    "    \"\"\"\n",
    "    # Create the figure and axis explicitly for better control\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Capture the container of bars to access their properties later\n",
    "    bars = ax.bar(data.labels, data.values, color=['#1f77b4', '#ff7f0e'])\n",
    "    \n",
    "    ax.bar_label(bars, padding=3)\n",
    "\n",
    "    ax.set_xlabel('Categories')\n",
    "    ax.set_ylabel('Values')\n",
    "    ax.set_title(data.title)\n",
    "    \n",
    "    # Dynamic Y-Axis Adjustment\n",
    "    # Crucial: Increase the y-axis limit by 10% to prevent the text from being cut off at the top\n",
    "    ax.set_ylim(0, max(data.values) * 1.1)\n",
    "    \n",
    "    plt.savefig(output_file, dpi=300)\n",
    "    plt.close() # Always close the plot to free memory in batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc489378",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = BarChartData(labels=[\"Anthropic Baseline\", \"Document slices\"], values=[round(baseline_avg_recall_at_n[5],5), round(own_avg_recall_at_n[5],5)], title=\"Recall@5\")\n",
    "\n",
    "create_annotated_bar_chart(data=data, output_file=\"benchmarking_results/retrieval/recall_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8639c1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-plot generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def create_multi_bar_chart(data_list: List[BarChartData], output_file: str = 'multi_plot.png'):\n",
    "    \"\"\"\n",
    "    Generates a figure containing subplots for each BarChartData instance.\n",
    "    Dynamically calculates grid dimensions.\n",
    "    \"\"\"\n",
    "    n = len(data_list)\n",
    "    if n == 0:\n",
    "        raise ValueError(\"Input list is empty.\")\n",
    "\n",
    "    # 1. Calculate Grid Dimensions\n",
    "    # We aim for a roughly square grid, prioritizing width (max 3 columns)\n",
    "    cols = min(n, 3)\n",
    "    rows = math.ceil(n / cols)\n",
    "    \n",
    "    # Calculate figure size: 6 inches per col, 5 inches per row\n",
    "    figsize = (6 * cols, 5 * rows)\n",
    "\n",
    "    # 2. Create Subplots\n",
    "    # squeeze=False ensures axes is always a 2D array, simplifying indexing\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize, squeeze=False)\n",
    "    \n",
    "    # Flatten axes array for easy 1D iteration\n",
    "    axes_flat = axes.flatten()\n",
    "\n",
    "    # 3. Plotting Loop\n",
    "    for i, data in enumerate(data_list):\n",
    "        ax = axes_flat[i]\n",
    "        \n",
    "        # Original Plotting Logic applied to specific axis\n",
    "        bars = ax.bar(data.labels, data.values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "        \n",
    "        # Native matplotlib annotation (requires matplotlib >= 3.4.0)\n",
    "        ax.bar_label(bars, padding=3, fmt='%.5f')\n",
    "\n",
    "        ax.set_xlabel('Categories')\n",
    "        ax.set_ylabel('Values')\n",
    "        ax.set_title(data.title)\n",
    "        \n",
    "        # Dynamic Y-Axis Adjustment for headroom\n",
    "        if data.values:\n",
    "            ax.set_ylim(0, max(data.values) * 1.15)\n",
    "\n",
    "    # 4. Cleanup Unused Axes\n",
    "    # If we have a 2x2 grid (4 slots) but only 3 data items, turn off the 4th slot\n",
    "    for j in range(n, len(axes_flat)):\n",
    "        axes_flat[j].axis('off')\n",
    "\n",
    "    # Adjust layout to prevent overlapping titles/labels\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(output_file, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# --- Execution Example ---\n",
    "try:\n",
    "    dataset = []\n",
    "    for i in range(5,21,5):\n",
    "        dataset.append(BarChartData(labels=[\"Anthropic Baseline\", \"Document slices\"], values=[baseline_avg_recall_at_n[i], own_avg_recall_at_n[i]], title=f\"Recall@{i}\"))\n",
    "    \n",
    "    create_multi_bar_chart(dataset, 'dashboard_view.png')\n",
    "    print(\"Multi-plot generated successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb93ae65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.43364)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(baseline_avg_recall_at_n[5],5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da53235",
   "metadata": {},
   "source": [
    "# LLM as a judge (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9add269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c644e0e69a4b6e9262aeb24237b4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grading Chunks:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  llm_grade  \\\n",
      "0  The text outlines a conceptual framework and r...        3.0   \n",
      "1  The text outlines a project to develop methodo...        3.0   \n",
      "2  This chunk outlines the core concepts of open ...        2.0   \n",
      "3  This section outlines the authorsâ€™ work: a con...        3.0   \n",
      "4  Provides a framework for balancing open scienc...        2.0   \n",
      "5  This chunk details the categorization of empir...        2.0   \n",
      "6  Identifies the Gander project as a case study ...        2.0   \n",
      "7  The analysis focuses on a framework for unders...        2.0   \n",
      "8  Provides supporting evidence for the argument ...        0.0   \n",
      "9  This chunk highlights the contrasting approach...        2.0   \n",
      "\n",
      "                                           reasoning  \n",
      "0  The passage directly addresses the conflict be...  \n",
      "1  The passage directly addresses the conflict be...  \n",
      "2  The passage acknowledges the tension between o...  \n",
      "3  The passage directly addresses the conflict be...  \n",
      "4  The passage acknowledges a framework for balan...  \n",
      "5  The passage discusses data protection concerns...  \n",
      "6  The passage discusses the Gander project, whic...  \n",
      "7  The passage discusses a conceptual framework f...  \n",
      "8  The passage does not address the specific conf...  \n",
      "9  The passage discusses challenges in open data ...  \n"
     ]
    }
   ],
   "source": [
    "# for question in tqdm(qna[0], desc=\"Processing questions...\"):\n",
    "\t# question_prompt = qna[0][\"question\"]\n",
    "\n",
    "\t# df = table.search(question_prompt, query_type=\"hybrid\", vector_column_name=\"vector\", fts_columns=\"text\") \\\n",
    "\t#             .rerank(reranker=reranker) \\\n",
    "\t#             .limit(10) \\\n",
    "\t#             .to_pandas()\n",
    "\n",
    "\t# for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Grading Chunks\"):\n",
    "\t#     resp = grade_chunk_relevance(question_prompt, row['text'], MODEL_TAG)\n",
    "\t#     df.at[idx, 'llm_grade'] = resp.score\n",
    "\t#     df.at[idx, 'reasoning'] = resp.chain_of_thought\n",
    "\n",
    "\t# df[['text', 'llm_grade', 'reasoning']]\n",
    "\t\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
