[
  {
    "text": "Received July 29, 2018, accepted August 27, 2018, date of publication September 13, 2018, date of current version October 8, 2018.\nDigital Object Identifier 10.1 109/ACCESS.2018.2869735\n\nThis chunk details the publication information and acknowledges funding sources for the research, outlining the paper's context and supporting details.",
    "original_text": "Received July 29, 2018, accepted August 27, 2018, date of publication September 13, 2018, date of current version October 8, 2018.\nDigital Object Identifier 10.1 109/ACCESS.2018.2869735",
    "context": "This chunk details the publication information and acknowledges funding sources for the research, outlining the paper's context and supporting details.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      1
    ],
    "id": "990434da769b3b976cb9daaf6357417afdaad3ec0ad97db94bbba4a92877bdc2"
  },
  {
    "text": "XI ZHANG 1 , (Member, IEEE), SIYU QU 1 , JIEYUN HUANG 1 , BINXING FANG 1 , AND PHILIP YU 2 , (Fellow, IEEE)\n1 Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, Beijing University of Posts and Telecommunications,\nBeijing 100876, China\n2 Department of Computer Science, The University of Illinois at Chicago, Chicago, IL 60607, USA\nCorresponding author: Xi Zhang (zhangx@bupt.edu.cn)\nThis work was supported in part by the State Key Development Program of Basic Research of China under Grant 2013CB329605, in part by the Natural Science Foundation of China under Grant 61300014, in part by the NSF under Grant IIS-1526499, Grant IIS-1763325, and Grant CNS-1626432, and in part by the DongGuan Innovative Research Team Program under Grant 201636000100038.\nABSTRACT Forecasting the stock market movements is an important and challenging task. As the Web information grows, researchers begin to extract effective indicators (e.g., the events and sentiments) from the Web to facilitate the prediction. However, the indicators obtained in previous studies are usually based on only one data source and thus may not fully cover the factors that can affect the stock market movements. In this paper, to improve the prediction for stock market composite index movements, we exploit the consistencies among different data sources, and develop a multi-source multiple instance model that can effectively combine events, sentiments, as well as the quantitative data into a comprehensive framework. To effectively capture the news events, we successfully apply a novel event extraction and representation method. Evaluations on the data from the year 2015 and 2016 demonstrate the effectiveness of our model. In addition, our approach is able to automatically determine the importance of each data source and identify the crucial input information that is considered to drive the movements, making the predictions interpretable.\nINDEX TERMS Stock prediction, multiple instance, event extraction, sentiment analysis.\n\nThis section introduces the authors and funding sources, and outlines the paper's focus on improving stock market prediction by integrating data from multiple sources, including event extraction and sentiment analysis, with an emphasis on interpretability.",
    "original_text": "XI ZHANG 1 , (Member, IEEE), SIYU QU 1 , JIEYUN HUANG 1 , BINXING FANG 1 , AND PHILIP YU 2 , (Fellow, IEEE)\n1 Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, Beijing University of Posts and Telecommunications,\nBeijing 100876, China\n2 Department of Computer Science, The University of Illinois at Chicago, Chicago, IL 60607, USA\nCorresponding author: Xi Zhang (zhangx@bupt.edu.cn)\nThis work was supported in part by the State Key Development Program of Basic Research of China under Grant 2013CB329605, in part by the Natural Science Foundation of China under Grant 61300014, in part by the NSF under Grant IIS-1526499, Grant IIS-1763325, and Grant CNS-1626432, and in part by the DongGuan Innovative Research Team Program under Grant 201636000100038.\nABSTRACT Forecasting the stock market movements is an important and challenging task. As the Web information grows, researchers begin to extract effective indicators (e.g., the events and sentiments) from the Web to facilitate the prediction. However, the indicators obtained in previous studies are usually based on only one data source and thus may not fully cover the factors that can affect the stock market movements. In this paper, to improve the prediction for stock market composite index movements, we exploit the consistencies among different data sources, and develop a multi-source multiple instance model that can effectively combine events, sentiments, as well as the quantitative data into a comprehensive framework. To effectively capture the news events, we successfully apply a novel event extraction and representation method. Evaluations on the data from the year 2015 and 2016 demonstrate the effectiveness of our model. In addition, our approach is able to automatically determine the importance of each data source and identify the crucial input information that is considered to drive the movements, making the predictions interpretable.\nINDEX TERMS Stock prediction, multiple instance, event extraction, sentiment analysis.",
    "context": "This section introduces the authors and funding sources, and outlines the paper's focus on improving stock market prediction by integrating data from multiple sources, including event extraction and sentiment analysis, with an emphasis on interpretability.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      1
    ],
    "id": "9ed6907515207f4a5e5cd48f826d4d5e8cc95ce11d0377e9b4f08ae801c7d000"
  },
  {
    "text": "Stock markets play important roles in the economic operations of modern society. The estimation of the stock market index is of clear interest to various stakeholders in the market. According to the EfGLYPH<28>cient Market Hypothesis (EMH) [1], the stock market prices reGLYPH<29>ect all available information, and thus the prediction naturally relies on information from multiple sources, which can be roughly categorized into (1) quantitative data, e.g., historical prices, turnover rate, and (2) qualitative descriptions, such as the annual reports, announcements, news and social media posts. It is challenging to deal with qualitative data as they are usually unstructured and thus extracting useful signals from them is not trivial.\nAlong with the growing Web information and the advance of Natural Language Processing (NLP) techniques, recent works begin to explore Web news for market prediction. A number of existing studies have shown that the events reported in news are important signals that can drive market\nGLYPH<29>uctuations [2]GLYPH<21>[4]. However, most of the previous works represent news documents using simple features (e.g., bagwords, noun phrases, named entities) [5], [6], which may discard syntax information. Due to the large volume and diverse expressions of the events, how to represent them as useful features, and how to identify the crucial events that have signiGLYPH<28>cant impacts on the stock market are not trivial problems. In addition to events, a line of studies has shown that the investors' opinions can also largely inGLYPH<29>uence the market volatility [7], [8]. With the prosperity of Web 2.0, the sentiments extracted from social media can be beneGLYPH<28>cial to predictions. Since both events and sentiments can drive the GLYPH<29>uctuations of the market, it is natural to investigate how to effectively fuse them together to make a better prediction. The improvement may come from the correlations among different sources, and the consensus prediction with multisource information can potentially outperform each prediction relying on a single source. This problem is analogous to the multi-labeler learning problem in crowdsourcing [9], [10],\nVOLUME 6, 2018\nbut different from those studies that usually assume a labeler conducts classiGLYPH<28>cation with full information, each ''labeler'' (i.e., classiGLYPH<28>er) in this study is source-speciGLYPH<28>c and only provided with limited information from its own source, making the consensus among labelers even more challenging.\nIn this work, we aim to learn a predictive model for describing the GLYPH<29>uctuations in the stock market index by utilizing various sources of data, involving the historical quantitative data, the social media and Web news. The essential features we extract include the event representations from news articles and the sentiments from social media. Firstly, we propose a novel method to capture the event information. SpeciGLYPH<28>cally, structured events are extracted from news texts and then used as the inputs for Restricted Boltzmann Machines (RBMs) to do the pre-training. After that, the output vectors from RBMs are used as the inputs to a recently proposed sentence2vec framework [11], in order to achieve effective event embeddings. Secondly, we exploit the latent relationships among different data sources with carefully designed loss terms, and propose an extension of the Multiple Instance Learning (MIL) model that can effectively integrate the features from multiple sources to make more accurate predictions. One beneGLYPH<28>t of our method is that we can determine source-speciGLYPH<28>c weights and identify the speciGLYPH<28>c factors that incur the changes in the composite index. Figure 1 shows an example of the news precursors identiGLYPH<28>ed by our model, and the dots with numbers denote the probabilistic estimates for the events leading to the index change on Jan. 26, 2016.\nFIGURE 1. An example of the news events that are responsible for the Shanghai Composite Index change on Jan. 26, 2016. The x-axis is the timeline. The left y-axis is the probability of each event leading to the index change. The right y-axis is the composite index in Shanghai Stock Exchange.\nThe summary of the contributions is as follows:\n- 1) To provide robust and accurate predictions for stock market movements, we extend the Multiple Instance Learning model to integrate the heterogeneous information including Web news, social media posts, and quantitative data.\n- 2) The latent consistencies among different data sources are modeled in our framework by sharing the common estimated true label among the hinge losses of different data sources at the instance level.\n- 3) A novel event representation model is proposed by GLYPH<28>rst extracting structured events from news text, and then training them with deep learning methods involving RBM and sentence2vec to obtain dense vectors.\n- 4) Evaluation results on two-year datasets show that our proposal can outperform the state-of-art baselines. Moreover, the impacts of different sources and the key factors that drive the movements can be obtained.\n\nIntroduces the central thesis about stock prediction by integrating multiple data sources and proposing a novel event representation model.",
    "original_text": "Stock markets play important roles in the economic operations of modern society. The estimation of the stock market index is of clear interest to various stakeholders in the market. According to the EfGLYPH<28>cient Market Hypothesis (EMH) [1], the stock market prices reGLYPH<29>ect all available information, and thus the prediction naturally relies on information from multiple sources, which can be roughly categorized into (1) quantitative data, e.g., historical prices, turnover rate, and (2) qualitative descriptions, such as the annual reports, announcements, news and social media posts. It is challenging to deal with qualitative data as they are usually unstructured and thus extracting useful signals from them is not trivial.\nAlong with the growing Web information and the advance of Natural Language Processing (NLP) techniques, recent works begin to explore Web news for market prediction. A number of existing studies have shown that the events reported in news are important signals that can drive market\nGLYPH<29>uctuations [2]GLYPH<21>[4]. However, most of the previous works represent news documents using simple features (e.g., bagwords, noun phrases, named entities) [5], [6], which may discard syntax information. Due to the large volume and diverse expressions of the events, how to represent them as useful features, and how to identify the crucial events that have signiGLYPH<28>cant impacts on the stock market are not trivial problems. In addition to events, a line of studies has shown that the investors' opinions can also largely inGLYPH<29>uence the market volatility [7], [8]. With the prosperity of Web 2.0, the sentiments extracted from social media can be beneGLYPH<28>cial to predictions. Since both events and sentiments can drive the GLYPH<29>uctuations of the market, it is natural to investigate how to effectively fuse them together to make a better prediction. The improvement may come from the correlations among different sources, and the consensus prediction with multisource information can potentially outperform each prediction relying on a single source. This problem is analogous to the multi-labeler learning problem in crowdsourcing [9], [10],\nVOLUME 6, 2018\nbut different from those studies that usually assume a labeler conducts classiGLYPH<28>cation with full information, each ''labeler'' (i.e., classiGLYPH<28>er) in this study is source-speciGLYPH<28>c and only provided with limited information from its own source, making the consensus among labelers even more challenging.\nIn this work, we aim to learn a predictive model for describing the GLYPH<29>uctuations in the stock market index by utilizing various sources of data, involving the historical quantitative data, the social media and Web news. The essential features we extract include the event representations from news articles and the sentiments from social media. Firstly, we propose a novel method to capture the event information. SpeciGLYPH<28>cally, structured events are extracted from news texts and then used as the inputs for Restricted Boltzmann Machines (RBMs) to do the pre-training. After that, the output vectors from RBMs are used as the inputs to a recently proposed sentence2vec framework [11], in order to achieve effective event embeddings. Secondly, we exploit the latent relationships among different data sources with carefully designed loss terms, and propose an extension of the Multiple Instance Learning (MIL) model that can effectively integrate the features from multiple sources to make more accurate predictions. One beneGLYPH<28>t of our method is that we can determine source-speciGLYPH<28>c weights and identify the speciGLYPH<28>c factors that incur the changes in the composite index. Figure 1 shows an example of the news precursors identiGLYPH<28>ed by our model, and the dots with numbers denote the probabilistic estimates for the events leading to the index change on Jan. 26, 2016.\nFIGURE 1. An example of the news events that are responsible for the Shanghai Composite Index change on Jan. 26, 2016. The x-axis is the timeline. The left y-axis is the probability of each event leading to the index change. The right y-axis is the composite index in Shanghai Stock Exchange.\nThe summary of the contributions is as follows:\n- 1) To provide robust and accurate predictions for stock market movements, we extend the Multiple Instance Learning model to integrate the heterogeneous information including Web news, social media posts, and quantitative data.\n- 2) The latent consistencies among different data sources are modeled in our framework by sharing the common estimated true label among the hinge losses of different data sources at the instance level.\n- 3) A novel event representation model is proposed by GLYPH<28>rst extracting structured events from news text, and then training them with deep learning methods involving RBM and sentence2vec to obtain dense vectors.\n- 4) Evaluation results on two-year datasets show that our proposal can outperform the state-of-art baselines. Moreover, the impacts of different sources and the key factors that drive the movements can be obtained.",
    "context": "Introduces the central thesis about stock prediction by integrating multiple data sources and proposing a novel event representation model.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      1,
      2
    ],
    "id": "7ad78330122f8822c806a60689a342ee4a4dc34abf7a776d73dcd0f131fad1ed"
  },
  {
    "text": "There is a line of research works using event-driven stock prediction models. Hogenboom et al . [12] give an overview of event extraction methods. Akita et al . [13] convert newspaper articles into distributed representations via Paragraph Vector and model the temporal effects of past events with LSTM on opening prices of stocks in Tokyo Stock Exchange. Nguyen et al . [14] formulated a temporal sentiment index function, which is used to extract signiGLYPH<28>cant events. Then the corresponding blog posts are analyzed using topic modeling to understand the contents. Ding et al . [15] applied the Open IE tool to extract structured events from texts, and this event extraction method is also implemented as a baseline and compared with our proposal. Ding et al . [16] then trained event embeddings with a neural tensor network and then used the deep convolutional neural network to model inGLYPH<29>uences of events.\nIn addition to events, investors' emotions also have great impacts on the stock market index. Bollen et al . [17] revealed that the public moods derived from Twitter have impacts on stock indicators. Si et al . [3] proposed a technique to leverage topic based sentiments from Twitter to predict the stock market. Makrehchi et al . [18] assigned a positive or negative label for each tweet according to stock movements. The aggregate sentiment per day shows predictive power for stock market prediction. Topic-speciGLYPH<28>c sentiments are learned in [19] to facilitate the stock prediction. However, this method is not suitable to short texts in social media.\nThe common limitation of the aforementioned methods is that they rely only on a single data source and thus may limit the predictive power. In [20], events and sentiments are integrated into a tensor framework together with GLYPH<28>rm-speciGLYPH<28>c features (e.g., P/B, P/E), to model the joint impacts on the stock volatility. We also implement it as a baseline. However, it uses a simple event extraction method which may not fully capture sufGLYPH<28>cient event information.\n\nSummarizes existing event-driven stock prediction models and highlights a common limitation: reliance on a single data source.",
    "original_text": "There is a line of research works using event-driven stock prediction models. Hogenboom et al . [12] give an overview of event extraction methods. Akita et al . [13] convert newspaper articles into distributed representations via Paragraph Vector and model the temporal effects of past events with LSTM on opening prices of stocks in Tokyo Stock Exchange. Nguyen et al . [14] formulated a temporal sentiment index function, which is used to extract signiGLYPH<28>cant events. Then the corresponding blog posts are analyzed using topic modeling to understand the contents. Ding et al . [15] applied the Open IE tool to extract structured events from texts, and this event extraction method is also implemented as a baseline and compared with our proposal. Ding et al . [16] then trained event embeddings with a neural tensor network and then used the deep convolutional neural network to model inGLYPH<29>uences of events.\nIn addition to events, investors' emotions also have great impacts on the stock market index. Bollen et al . [17] revealed that the public moods derived from Twitter have impacts on stock indicators. Si et al . [3] proposed a technique to leverage topic based sentiments from Twitter to predict the stock market. Makrehchi et al . [18] assigned a positive or negative label for each tweet according to stock movements. The aggregate sentiment per day shows predictive power for stock market prediction. Topic-speciGLYPH<28>c sentiments are learned in [19] to facilitate the stock prediction. However, this method is not suitable to short texts in social media.\nThe common limitation of the aforementioned methods is that they rely only on a single data source and thus may limit the predictive power. In [20], events and sentiments are integrated into a tensor framework together with GLYPH<28>rm-speciGLYPH<28>c features (e.g., P/B, P/E), to model the joint impacts on the stock volatility. We also implement it as a baseline. However, it uses a simple event extraction method which may not fully capture sufGLYPH<28>cient event information.",
    "context": "Summarizes existing event-driven stock prediction models and highlights a common limitation: reliance on a single data source.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      2
    ],
    "id": "63466cd1c3215b2b14348f9a75e130ab740dfbf5849f70b156789da93e4a08bc"
  },
  {
    "text": "The multiple instance learning (MIL) paradigm is a form of weakly supervised learning. Training instances arranged in sets are called bags or groups. A label is provided for entire groups instead of individual instances. Negative groups don't contain any positive instances, while positive groups contain at least one positive instance [21]. Various applications and the comparisons of different methods in MIL\nwere given in [22]. The common MIL approach is used to predict the group-level label, Liu et al . [23], however, proposed an approach to identify the instance-level labels, especially the labels of key instances in groups based on K nearest neighbors ( K -NN). Kotzias et al . [24] predicted the labels for sentences given labels for reviews, which can be used to detect sentiments. A multiple-instance multiple-label learning framework with deep neutral network formation is proposed in [25]. An event forecasting framework via the nested multiple instance learning is proposed in [26]. However, it only uses one data source and simple event features, which may not be sufGLYPH<28>cient in the stock market application. We have implemented this algorithm as a baseline for comparison.\n\nProvides a detailed explanation of the multiple instance learning (MIL) paradigm and its applications, highlighting its limitations in the context of stock market prediction due to reliance on a single data source and simple event features.",
    "original_text": "The multiple instance learning (MIL) paradigm is a form of weakly supervised learning. Training instances arranged in sets are called bags or groups. A label is provided for entire groups instead of individual instances. Negative groups don't contain any positive instances, while positive groups contain at least one positive instance [21]. Various applications and the comparisons of different methods in MIL\nwere given in [22]. The common MIL approach is used to predict the group-level label, Liu et al . [23], however, proposed an approach to identify the instance-level labels, especially the labels of key instances in groups based on K nearest neighbors ( K -NN). Kotzias et al . [24] predicted the labels for sentences given labels for reviews, which can be used to detect sentiments. A multiple-instance multiple-label learning framework with deep neutral network formation is proposed in [25]. An event forecasting framework via the nested multiple instance learning is proposed in [26]. However, it only uses one data source and simple event features, which may not be sufGLYPH<28>cient in the stock market application. We have implemented this algorithm as a baseline for comparison.",
    "context": "Provides a detailed explanation of the multiple instance learning (MIL) paradigm and its applications, highlighting its limitations in the context of stock market prediction due to reliance on a single data source and simple event features.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      2,
      3
    ],
    "id": "0428f89847fd011bab3d275aa32980711a132649d362258d11276dd5b09751f7"
  },
  {
    "text": "In this section, we GLYPH<28>rst state and formulate the problem, and then propose the multi-source multiple instance (M-MI) framework. Before going into details of our framework, we deGLYPH<28>ne some important notations as shown in Table 1.\nTABLE 1. Notations in our model.\n\nIntroduces the multi-source multiple instance (M-MI) framework and defines key notations for the model.",
    "original_text": "In this section, we GLYPH<28>rst state and formulate the problem, and then propose the multi-source multiple instance (M-MI) framework. Before going into details of our framework, we deGLYPH<28>ne some important notations as shown in Table 1.\nTABLE 1. Notations in our model.",
    "context": "Introduces the multi-source multiple instance (M-MI) framework and defines key notations for the model.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      3
    ],
    "id": "af3c341f1413d2c6e010922625c3ede6cb8e26fd77851bd0b8e952a0d8c9cf5c"
  },
  {
    "text": "Stock markets are impacted by various factors, such as the trading volume, news events and the investors' emotions. Thus, relying on a single data source may not be sufGLYPH<28>cient to make accurate predictions. The object of our study is to develop a multi-source data integration approach to predict the stock market trends. SpeciGLYPH<28>cally, given a collection of economic news, social network posts and historical trading data, we aim to forecast the stock market index movements. Moreover, we also try to obtain the impacts of each data source and identify the key factors that have decisive inGLYPH<29>uences, which may be inGLYPH<29>uential news, collective sentiments or some important quantitative index in the trading data. These key factors are supporting evidence for further analysis and can make our prediction interpretable.\nFormally, according to Table 1, a news article j on day i is denoted as a V -dimensional vector x ij 2 R V GLYPH<2> 1 (please note that the process of representing a news article as a vector will be illustrated in the next section). In order to predict the stock market movement on day t C k , we assume that there are a group of news articles for each day i ( i < t ), which is denoted as X i , and thus X i D f x ij g ; j 2 f 1 ; GLYPH<1> GLYPH<1> GLYPH<1> ; ni g . In addition to the news articles, the sentiment and quantitative indices on day i (denoted as si and di respectively) are also taken into account. Then the temporal ordered collection of news articles, sentiments and quantitative indices across t days can be represented as a multi-source super group, that is, G D f Ci g ; i 2 f 1 ; GLYPH<1> GLYPH<1> GLYPH<1> ; t g , where Ci D f X i ; di ; si g . The change in the stock market movement on day t C k can be denoted as Yt C k 2 fC 1 ; GLYPH<0> 1 g , where C 1 denotes the index rise and -1 denotes the index decline. Then the forecasting problem can be modeled as a mathematical function f ( G ) ! Yt C k , indicating that we map the multi-source information to an indicator (i.e., label) k days in the future from the day t , where k is number of the lead days that we aim to forecast.\nFIGURE 2. The system framework of our proposed model.\n\nThis chunk introduces the multi-source data integration approach for stock market prediction, specifically outlining the framework for mapping multi-source information (news, social media, trading data) to forecast stock index movements, and identifying key factors influencing those movements.",
    "original_text": "Stock markets are impacted by various factors, such as the trading volume, news events and the investors' emotions. Thus, relying on a single data source may not be sufGLYPH<28>cient to make accurate predictions. The object of our study is to develop a multi-source data integration approach to predict the stock market trends. SpeciGLYPH<28>cally, given a collection of economic news, social network posts and historical trading data, we aim to forecast the stock market index movements. Moreover, we also try to obtain the impacts of each data source and identify the key factors that have decisive inGLYPH<29>uences, which may be inGLYPH<29>uential news, collective sentiments or some important quantitative index in the trading data. These key factors are supporting evidence for further analysis and can make our prediction interpretable.\nFormally, according to Table 1, a news article j on day i is denoted as a V -dimensional vector x ij 2 R V GLYPH<2> 1 (please note that the process of representing a news article as a vector will be illustrated in the next section). In order to predict the stock market movement on day t C k , we assume that there are a group of news articles for each day i ( i < t ), which is denoted as X i , and thus X i D f x ij g ; j 2 f 1 ; GLYPH<1> GLYPH<1> GLYPH<1> ; ni g . In addition to the news articles, the sentiment and quantitative indices on day i (denoted as si and di respectively) are also taken into account. Then the temporal ordered collection of news articles, sentiments and quantitative indices across t days can be represented as a multi-source super group, that is, G D f Ci g ; i 2 f 1 ; GLYPH<1> GLYPH<1> GLYPH<1> ; t g , where Ci D f X i ; di ; si g . The change in the stock market movement on day t C k can be denoted as Yt C k 2 fC 1 ; GLYPH<0> 1 g , where C 1 denotes the index rise and -1 denotes the index decline. Then the forecasting problem can be modeled as a mathematical function f ( G ) ! Yt C k , indicating that we map the multi-source information to an indicator (i.e., label) k days in the future from the day t , where k is number of the lead days that we aim to forecast.\nFIGURE 2. The system framework of our proposed model.",
    "context": "This chunk introduces the multi-source data integration approach for stock market prediction, specifically outlining the framework for mapping multi-source information (news, social media, trading data) to forecast stock index movements, and identifying key factors influencing those movements.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      3
    ],
    "id": "39c825d2635eebb324ba7686fde1bb602ed93eea99b7c365141b1c4f4dc38a2e"
  },
  {
    "text": "The framework of our proposal is shown in Fig. 2. The inputs of the framework are the stock quantitative data, the social media and Web news. We GLYPH<28>rst use the sentiment analyzer to obtain the collective sentiments from social media, and extract effective event representations from the Web news. Then the extracted sentiments, events as well as the stock quantitative data are fed into the M-MI model. The M-MI model is proposed based on the Multiple Instance Learning algorithm, that is, a group of instances are given group labels, which are assumed to be an association function (e.g., OR, average) of the instance-level labels. Our work further distinguishes the instance-level labels, multi-source group-level labels, and multi-source super group-level labels. The primary goal is to predict the label for the multi-source super group that indicates the rise or decline of the stock market index. In addition, we also try to estimate the instancelevel probabilities indicating how related a speciGLYPH<28>c instance is to the index movement (i.e., target label), as well as the\nsource-speciGLYPH<28>c weights that reveal how related a speciGLYPH<28>c source is to the index movement.\nTo this end, for a given day, we GLYPH<28>rst model the instancelevel probability pij for a news article j on day i to the target label with a logistic function, that is\n<!-- formula-not-decoded -->\nwhere w m denotes the weight vector of the news articles. The higher the probability pij , the more related the article j is to the target label. The probability of all the news articles for a given day i can be computed as the average of probabilities of each news article, that is\n<!-- formula-not-decoded -->\nIn addition to news articles, we also model the probability p d GLYPH<0> i for stock quantitative data and p s GLYPH<0> i for sentiments on day i , that is\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere w d and w s denote the weight vector of d i and s i respectively. We then model the probability Pi for multisource information on day i as\n<!-- formula-not-decoded -->\nwhere GLYPH<18> 0, GLYPH<18> 1 and GLYPH<18> 2 denote the source-speciGLYPH<28>c weights of p m GLYPH<0> i , p d GLYPH<0> i and p s GLYPH<0> i respectively, and GLYPH<18> 0 C GLYPH<18> 1 C GLYPH<18> 2 D 1. It is obvious that Pi 2 [0 ; 1]. We use GLYPH<18> D ( GLYPH<18> 0 ; GLYPH<18> 1 ; GLYPH<18> 2) to denote the source weight vector, and then model the probability of the multi-source super group as the average of the probabilities in t days, that is\n<!-- formula-not-decoded -->\nThen, we start with a log-likelihood loss function:\n<!-- formula-not-decoded -->\n■ As the inGLYPH<29>uences of the multi-source information usually last for a number of days, we assume the probabilities on two consecutive days are essentially similar, which can be represented by minimizing the cost where G is a multi-source super group, n is the number of multi-source super groups, and Y denotes the set of true labels. ( GLYPH<1> ) is the indicator function.\n<!-- formula-not-decoded -->\nwhere Ci denotes the multi-source group for the day i . By introducing this loss term, Eq. 7 can be rewritten as:\n<!-- formula-not-decoded -->\nwhere GLYPH<12> is a constant to control the contribution of the GLYPH<28>rst term. Eq. 9 aggregates the costs at the super group level and the group level. However, the instance-level loss has not been considered yet, which is challenging to be designed due to two reasons: (1) it lacks of true labels at the instance level; (2) the instances from different sources are heterogeneous but intrinsically correlated. The instances can be categorized into three types according to their sources, and each type leads to a distinct loss term. Inspired by the hinge loss used in Support Vector Machines (SVMs), the classiGLYPH<28>cation loss term for the instances of news article instance xij is\n<!-- formula-not-decoded -->\nHere, sgn ( GLYPH<1> ) is the sign function, m 0 is a margin parameter used to separate the positive and negative instances from the hyperplane in the feature space. w T m x ij denotes the prediction with article xij . As the true label for each instance is unknown during the classiGLYPH<28>er training, we replace it with the estimated true label sgn ( Pi -P 0 ), where P 0 is a threshold parameter to determine the positiveness of the prediction. If ( Pi -P 0 ) > 0, the prediction with multiple-source information on day i would be positive. Otherwise, it would be negative. Similarly, we can derive the instance-level loss terms for quantitative data and sentiments respectively,\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nBased on Eq. 10, 11 and 12, the classiGLYPH<28>cation loss at the instance level for each data source has been obtained. We then explain why they share a common estimated true label (i.e., sgn ( Pi -P 0 )). As predictions from different sources are commonly correlated with each other, instead of treating the loss of each source independently, we need to consider their intrinsic consistencies. The intuition behind is that according to EfGLYPH<28>cient Market Hypothesis, different data sources would keep up to date with the latest stock market information, and they commonly indicate the same sign (index rise or fall). Thus, through sharing the same estimated true label, we are able to combine the indications from different data sources to learn a consensus label. This can potentially provide more robust and conGLYPH<28>dent predictions.\nWe then give several cases to illustrate the consensus among sources. The three source-speciGLYPH<28>c predictions are denoted as l 0 , l 1 and l 2 respectively. Firstly, if l 0 , l 1 and l 2 all make very positive predictions, i.e., large values of pm -i , pd -i and ps -i , it would be conGLYPH<28>dent to make a positive group-level prediction due to Pi > P 0 and l 0 , l 1 and l 2\nall agree with the label without costs. Secondly, if only l 0 disagrees with the estimated true label, l 0 will be penalized as l 1 and l 2 agree with this label and make Pi approach their predictions. Thirdly, if l 0 disagrees with l 1 and l 2 , but l 0 is very conGLYPH<28>dent (and thus far from hyperplane) while l 1 and l 2 are not conGLYPH<28>dent enough (and thus close to hyperplane), this may make Pi approach l 0 , resulting in that l 0 agrees with the estimated true label while l 1 and l 2 disagree with it and thus are penalized. Our proposed instance-level loss terms are consistent with these cases and thus make sense.\nThen we try to minimize the overall instance-level loss, that is, h 1( x ij ; w m ) C h 2( d i ; w d ) C h 3( s i ; w s ). By introducing this summation and other regularization terms, the objective function Eq. 9 can be reformulated as\n<!-- formula-not-decoded -->\nEq. 13 is the ultimate objective function to optimize. To summarize, it consists of losses at three levels: the super group level, the group level and the instance level. In addition, it includes the regularization terms, that is, R ( w m ), R ( w d ), R ( w s ) and R ( GLYPH<18> ), and GLYPH<12> , GLYPH<21> m , GLYPH<21> d , GLYPH<21> s and GLYPH<21> GLYPH<18> are constants to control the trade-offs among multiple terms. The model learning goal is to estimate the parameters w m , w d , w s and GLYPH<18> to minimize L ( w m ; w d ; w s ; GLYPH<18> ). We randomly choose a set ( G ; Y ) from S , and the online stochastic gradient descent optimization is adopted to GLYPH<28>t the model.\n\nDescribes the framework for predicting stock market trends, utilizing multiple data sources like news, social media, and quantitative data. It employs multiple instance learning, distinguishing instance-level, group-level, and super group-level labels. The model estimates instance-level probabilities and source-specific weights to combine information from various data sources, aiming for a consensus prediction of market index movement.",
    "original_text": "The framework of our proposal is shown in Fig. 2. The inputs of the framework are the stock quantitative data, the social media and Web news. We GLYPH<28>rst use the sentiment analyzer to obtain the collective sentiments from social media, and extract effective event representations from the Web news. Then the extracted sentiments, events as well as the stock quantitative data are fed into the M-MI model. The M-MI model is proposed based on the Multiple Instance Learning algorithm, that is, a group of instances are given group labels, which are assumed to be an association function (e.g., OR, average) of the instance-level labels. Our work further distinguishes the instance-level labels, multi-source group-level labels, and multi-source super group-level labels. The primary goal is to predict the label for the multi-source super group that indicates the rise or decline of the stock market index. In addition, we also try to estimate the instancelevel probabilities indicating how related a speciGLYPH<28>c instance is to the index movement (i.e., target label), as well as the\nsource-speciGLYPH<28>c weights that reveal how related a speciGLYPH<28>c source is to the index movement.\nTo this end, for a given day, we GLYPH<28>rst model the instancelevel probability pij for a news article j on day i to the target label with a logistic function, that is\n<!-- formula-not-decoded -->\nwhere w m denotes the weight vector of the news articles. The higher the probability pij , the more related the article j is to the target label. The probability of all the news articles for a given day i can be computed as the average of probabilities of each news article, that is\n<!-- formula-not-decoded -->\nIn addition to news articles, we also model the probability p d GLYPH<0> i for stock quantitative data and p s GLYPH<0> i for sentiments on day i , that is\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere w d and w s denote the weight vector of d i and s i respectively. We then model the probability Pi for multisource information on day i as\n<!-- formula-not-decoded -->\nwhere GLYPH<18> 0, GLYPH<18> 1 and GLYPH<18> 2 denote the source-speciGLYPH<28>c weights of p m GLYPH<0> i , p d GLYPH<0> i and p s GLYPH<0> i respectively, and GLYPH<18> 0 C GLYPH<18> 1 C GLYPH<18> 2 D 1. It is obvious that Pi 2 [0 ; 1]. We use GLYPH<18> D ( GLYPH<18> 0 ; GLYPH<18> 1 ; GLYPH<18> 2) to denote the source weight vector, and then model the probability of the multi-source super group as the average of the probabilities in t days, that is\n<!-- formula-not-decoded -->\nThen, we start with a log-likelihood loss function:\n<!-- formula-not-decoded -->\n■ As the inGLYPH<29>uences of the multi-source information usually last for a number of days, we assume the probabilities on two consecutive days are essentially similar, which can be represented by minimizing the cost where G is a multi-source super group, n is the number of multi-source super groups, and Y denotes the set of true labels. ( GLYPH<1> ) is the indicator function.\n<!-- formula-not-decoded -->\nwhere Ci denotes the multi-source group for the day i . By introducing this loss term, Eq. 7 can be rewritten as:\n<!-- formula-not-decoded -->\nwhere GLYPH<12> is a constant to control the contribution of the GLYPH<28>rst term. Eq. 9 aggregates the costs at the super group level and the group level. However, the instance-level loss has not been considered yet, which is challenging to be designed due to two reasons: (1) it lacks of true labels at the instance level; (2) the instances from different sources are heterogeneous but intrinsically correlated. The instances can be categorized into three types according to their sources, and each type leads to a distinct loss term. Inspired by the hinge loss used in Support Vector Machines (SVMs), the classiGLYPH<28>cation loss term for the instances of news article instance xij is\n<!-- formula-not-decoded -->\nHere, sgn ( GLYPH<1> ) is the sign function, m 0 is a margin parameter used to separate the positive and negative instances from the hyperplane in the feature space. w T m x ij denotes the prediction with article xij . As the true label for each instance is unknown during the classiGLYPH<28>er training, we replace it with the estimated true label sgn ( Pi -P 0 ), where P 0 is a threshold parameter to determine the positiveness of the prediction. If ( Pi -P 0 ) > 0, the prediction with multiple-source information on day i would be positive. Otherwise, it would be negative. Similarly, we can derive the instance-level loss terms for quantitative data and sentiments respectively,\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nBased on Eq. 10, 11 and 12, the classiGLYPH<28>cation loss at the instance level for each data source has been obtained. We then explain why they share a common estimated true label (i.e., sgn ( Pi -P 0 )). As predictions from different sources are commonly correlated with each other, instead of treating the loss of each source independently, we need to consider their intrinsic consistencies. The intuition behind is that according to EfGLYPH<28>cient Market Hypothesis, different data sources would keep up to date with the latest stock market information, and they commonly indicate the same sign (index rise or fall). Thus, through sharing the same estimated true label, we are able to combine the indications from different data sources to learn a consensus label. This can potentially provide more robust and conGLYPH<28>dent predictions.\nWe then give several cases to illustrate the consensus among sources. The three source-speciGLYPH<28>c predictions are denoted as l 0 , l 1 and l 2 respectively. Firstly, if l 0 , l 1 and l 2 all make very positive predictions, i.e., large values of pm -i , pd -i and ps -i , it would be conGLYPH<28>dent to make a positive group-level prediction due to Pi > P 0 and l 0 , l 1 and l 2\nall agree with the label without costs. Secondly, if only l 0 disagrees with the estimated true label, l 0 will be penalized as l 1 and l 2 agree with this label and make Pi approach their predictions. Thirdly, if l 0 disagrees with l 1 and l 2 , but l 0 is very conGLYPH<28>dent (and thus far from hyperplane) while l 1 and l 2 are not conGLYPH<28>dent enough (and thus close to hyperplane), this may make Pi approach l 0 , resulting in that l 0 agrees with the estimated true label while l 1 and l 2 disagree with it and thus are penalized. Our proposed instance-level loss terms are consistent with these cases and thus make sense.\nThen we try to minimize the overall instance-level loss, that is, h 1( x ij ; w m ) C h 2( d i ; w d ) C h 3( s i ; w s ). By introducing this summation and other regularization terms, the objective function Eq. 9 can be reformulated as\n<!-- formula-not-decoded -->\nEq. 13 is the ultimate objective function to optimize. To summarize, it consists of losses at three levels: the super group level, the group level and the instance level. In addition, it includes the regularization terms, that is, R ( w m ), R ( w d ), R ( w s ) and R ( GLYPH<18> ), and GLYPH<12> , GLYPH<21> m , GLYPH<21> d , GLYPH<21> s and GLYPH<21> GLYPH<18> are constants to control the trade-offs among multiple terms. The model learning goal is to estimate the parameters w m , w d , w s and GLYPH<18> to minimize L ( w m ; w d ; w s ; GLYPH<18> ). We randomly choose a set ( G ; Y ) from S , and the online stochastic gradient descent optimization is adopted to GLYPH<28>t the model.",
    "context": "Describes the framework for predicting stock market trends, utilizing multiple data sources like news, social media, and quantitative data. It employs multiple instance learning, distinguishing instance-level, group-level, and super group-level labels. The model estimates instance-level probabilities and source-specific weights to combine information from various data sources, aiming for a consensus prediction of market index movement.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      3,
      4,
      5
    ],
    "id": "d3db3e341b9b3a661356b26b5a5314024e4eaf4f7ea86141844454c45c6ac721"
  },
  {
    "text": "After the learning process, the source weight vector GLYPH<18> is obtained, representing the impacts of different data sources on the market movements. In addition, a probability for each piece of input information can also be obtained through Eq. 1, 3 or 4, which reveals the probability of that information signifying the rise of the market index on the target day. Note that if the probability signifying the index rise is pr , the probability indicating index decline would be 1 GLYPH<0> pr . We can identify the key input information that triggers the market index movement, if the product of its probability value and its source-speciGLYPH<28>c weight is above a given threshold GLYPH<28> .\n\nIdentifies the relative importance of different data sources (stock data, social media, news) in predicting market movements and highlights the ability to pinpoint key information driving those movements, using probability scores and source weights.",
    "original_text": "After the learning process, the source weight vector GLYPH<18> is obtained, representing the impacts of different data sources on the market movements. In addition, a probability for each piece of input information can also be obtained through Eq. 1, 3 or 4, which reveals the probability of that information signifying the rise of the market index on the target day. Note that if the probability signifying the index rise is pr , the probability indicating index decline would be 1 GLYPH<0> pr . We can identify the key input information that triggers the market index movement, if the product of its probability value and its source-speciGLYPH<28>c weight is above a given threshold GLYPH<28> .",
    "context": "Identifies the relative importance of different data sources (stock data, social media, news) in predicting market movements and highlights the ability to pinpoint key information driving those movements, using probability scores and source weights.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      5
    ],
    "id": "25f44ed84cbffde363f0517d1dc3d35384ab567270a351db961722e8f117c30c"
  },
  {
    "text": "The quantitative features are quite simple to extract, we just collect three indices and normalize each index to form d i 2 R 3 GLYPH<2> 1 . Here we introduce how to extract event representations from news articles and extract the sentiments from posts in social media, which are used as the inputs to M-MI framework.\nFIGURE 3. Structured event extraction from texts.\n\nDescribes the process of extracting structured events from news articles and sentiment from social media posts, highlighting their role as inputs to the M-MI framework.",
    "original_text": "The quantitative features are quite simple to extract, we just collect three indices and normalize each index to form d i 2 R 3 GLYPH<2> 1 . Here we introduce how to extract event representations from news articles and extract the sentiments from posts in social media, which are used as the inputs to M-MI framework.\nFIGURE 3. Structured event extraction from texts.",
    "context": "Describes the process of extracting structured events from news articles and sentiment from social media posts, highlighting their role as inputs to the M-MI framework.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      5
    ],
    "id": "9b7a7361b3ab2b0a7fc7db3c3365d0c49669f69970f8755dd2003bdbc78273b3"
  },
  {
    "text": "Conventional methods commonly represent events using simple features such as TF-IDF, noun phrases or named entities. Recent advances in NLP techniques enable more accurate event models with structures. In this study, we GLYPH<28>rst use the syntactic analysis method to extract the main structure information of the sentences, and then use it as the input to an RBM. The output of an RBM would be a pre-trained vector used as the input to sentence2vec, and then the event representations are obtained. The process is shown in Figure 3 and described in detail as follows. Note that though we use the Chinese dataset in this study, this process can also be applied to other languages.\n- 1) Structured event extraction. With a commonly used text parser HanLP, 1 we can capture the syntactic structure of a sentence, which is depicted as a three-level tree at the top of Fig. 3. The root node denotes the core verb, and the nodes of the second layer are the subject of the verb and the object of the verb respectively. The child of the subject is the modiGLYPH<28>er who is the nearest to the subject in the sentence, and so is the child of the object. Then we connect these core words together as the structure information to represent the event information.\n- 2) Training with RBM. Wethen map the structured event into a vector. To make the vectors better reconstruct the original events, we use RBM as a pre-training module. The Restricted Boltzmann Machine (RBM) is a generative stochastic artiGLYPH<28>cial neural network, and has been applied in various applications such as dimensionality reduction [27]. RBM contains two-layer neural nets, one is the visible layer or input layer, and the\n1 https://github.com/hankcs/HanLP\nother is the hidden layer. In our model, each event is represented as an m -dimensional vector with one-hot encoding, which is the visible layer. Our target is to estimate the n -dimensional hidden layer to approximate the input layer as much as possible. Then the hidden layer will be set as the initial vector in sentence2vec. The reason is that directly training the representations using sentence2vec without RBM may fall into the local minimum.\n- 3) Training with sentence2vec. Finally, we use sentence2vec, the neural probabilistic language model to obtain the event representations. Different from the word2vec with CBOW model, the sentence id will be added during the training process of sentence2vec, and will also be mapped into a vector, called sentence vector, which would be the GLYPH<28>nal vector that we want. In the training process, the sentence vector and the word vectors of context will be concatenated as the input to softmax. After training, the sentence vector will be obtained and used as the features for the proposed model.\nHere is an example of extracting structured events from the news. The news text is that it is expected that the Renminbi speculators will face huge losses. After the dependency parsing analysis, the core words (Renminbi, speculators, face, huge, losses) are obtained, and after one hot coding, each word is encoded into zero or one vector. Then the vector preprocesses by RBM into a 100-dimensional vector, and GLYPH<28>nally processes by the sentence2vec became the news event feature vector. Through the above steps, the news event is obtained as a feature of the M-MI model, a 100-dimensional vector.\n\nIntroduces the method for extracting structured events from news text, utilizing syntactic analysis and an RBM to generate event representations for the M-MI model.",
    "original_text": "Conventional methods commonly represent events using simple features such as TF-IDF, noun phrases or named entities. Recent advances in NLP techniques enable more accurate event models with structures. In this study, we GLYPH<28>rst use the syntactic analysis method to extract the main structure information of the sentences, and then use it as the input to an RBM. The output of an RBM would be a pre-trained vector used as the input to sentence2vec, and then the event representations are obtained. The process is shown in Figure 3 and described in detail as follows. Note that though we use the Chinese dataset in this study, this process can also be applied to other languages.\n- 1) Structured event extraction. With a commonly used text parser HanLP, 1 we can capture the syntactic structure of a sentence, which is depicted as a three-level tree at the top of Fig. 3. The root node denotes the core verb, and the nodes of the second layer are the subject of the verb and the object of the verb respectively. The child of the subject is the modiGLYPH<28>er who is the nearest to the subject in the sentence, and so is the child of the object. Then we connect these core words together as the structure information to represent the event information.\n- 2) Training with RBM. Wethen map the structured event into a vector. To make the vectors better reconstruct the original events, we use RBM as a pre-training module. The Restricted Boltzmann Machine (RBM) is a generative stochastic artiGLYPH<28>cial neural network, and has been applied in various applications such as dimensionality reduction [27]. RBM contains two-layer neural nets, one is the visible layer or input layer, and the\n1 https://github.com/hankcs/HanLP\nother is the hidden layer. In our model, each event is represented as an m -dimensional vector with one-hot encoding, which is the visible layer. Our target is to estimate the n -dimensional hidden layer to approximate the input layer as much as possible. Then the hidden layer will be set as the initial vector in sentence2vec. The reason is that directly training the representations using sentence2vec without RBM may fall into the local minimum.\n- 3) Training with sentence2vec. Finally, we use sentence2vec, the neural probabilistic language model to obtain the event representations. Different from the word2vec with CBOW model, the sentence id will be added during the training process of sentence2vec, and will also be mapped into a vector, called sentence vector, which would be the GLYPH<28>nal vector that we want. In the training process, the sentence vector and the word vectors of context will be concatenated as the input to softmax. After training, the sentence vector will be obtained and used as the features for the proposed model.\nHere is an example of extracting structured events from the news. The news text is that it is expected that the Renminbi speculators will face huge losses. After the dependency parsing analysis, the core words (Renminbi, speculators, face, huge, losses) are obtained, and after one hot coding, each word is encoded into zero or one vector. Then the vector preprocesses by RBM into a 100-dimensional vector, and GLYPH<28>nally processes by the sentence2vec became the news event feature vector. Through the above steps, the news event is obtained as a feature of the M-MI model, a 100-dimensional vector.",
    "context": "Introduces the method for extracting structured events from news text, utilizing syntactic analysis and an RBM to generate event representations for the M-MI model.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      5,
      6
    ],
    "id": "af43596b4685514849c49cd2a5747e81ce26aab614736401713f519efbaa891f"
  },
  {
    "text": "To extract the sentiments from the posts in the social network, we use the LDA-S method [28], an extension of Latent Dirichlet Allocation (LDA) model that proposed to obtain the topic-speciGLYPH<28>c sentiments for short texts. The intuition behind is that extracting sentiments discarding topics may be not sufGLYPH<28>cient as sentiment polarities usually depend on topics or domains [29]. In other words, the exact same word mayexpress different sentiment polarities for different topics, e.g., the opinion word ''low'' in the phrase ''low speed'' in a trafGLYPH<28>c-related topic and ''low fat'' in a food-related topic. Therefore, extracting the sentiments corresponding to different topics can potentially improve the sentiment classiGLYPH<28>cation accuracy. The LDA-S model can infer sentiment distribution and topic distribution simultaneously for short texts. It consists of two steps. The GLYPH<28>rst step aims to obtain the topic distribution of each post, and then set the topic as the one with the largest probability. The second step gets the sentiment distribution of each post.\nIn this work, a sentiment word list called NTUSD [30] is adopted, which contains 4370 negative words and 4566 positive words. If a word is an adjective but not in the sentiment\nword list, the sentiment label of this word is set as neutral. If a word is a noun, it is considered as a topic word. Otherwise, it is considered as a background word. For each topic, opinion word distributions are distinguished from two polarities, that is, positive or negative.\n\nIntroduces the LDA-S method for sentiment extraction from social media posts, highlighting its advantage over simple sentiment extraction by considering topic dependencies and utilizing a sentiment word list.",
    "original_text": "To extract the sentiments from the posts in the social network, we use the LDA-S method [28], an extension of Latent Dirichlet Allocation (LDA) model that proposed to obtain the topic-speciGLYPH<28>c sentiments for short texts. The intuition behind is that extracting sentiments discarding topics may be not sufGLYPH<28>cient as sentiment polarities usually depend on topics or domains [29]. In other words, the exact same word mayexpress different sentiment polarities for different topics, e.g., the opinion word ''low'' in the phrase ''low speed'' in a trafGLYPH<28>c-related topic and ''low fat'' in a food-related topic. Therefore, extracting the sentiments corresponding to different topics can potentially improve the sentiment classiGLYPH<28>cation accuracy. The LDA-S model can infer sentiment distribution and topic distribution simultaneously for short texts. It consists of two steps. The GLYPH<28>rst step aims to obtain the topic distribution of each post, and then set the topic as the one with the largest probability. The second step gets the sentiment distribution of each post.\nIn this work, a sentiment word list called NTUSD [30] is adopted, which contains 4370 negative words and 4566 positive words. If a word is an adjective but not in the sentiment\nword list, the sentiment label of this word is set as neutral. If a word is a noun, it is considered as a topic word. Otherwise, it is considered as a background word. For each topic, opinion word distributions are distinguished from two polarities, that is, positive or negative.",
    "context": "Introduces the LDA-S method for sentiment extraction from social media posts, highlighting its advantage over simple sentiment extraction by considering topic dependencies and utilizing a sentiment word list.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      6
    ],
    "id": "d4e215c5fadc5fac4d170615b4265dea2c1c5e1e8211ce725c91926dd7e520fa"
  },
  {
    "text": "We collected stock market-related information from Jan. 1, 2015 to Dec. 31, 2016, and separate the information into two data sets, one for the year 2015 and the other for 2016. The data consist of three parts, the historical quantitative data, the news articles and the posts on the social network, which are introduced in detail as follows.\n- GLYPH<15> Quantitative data : the source of quantitative data is Wind, 2 a widely used GLYPH<28>nancial information service provider in China. The data we collect are the average prices, market index change and turnover rate of the Shanghai Composite Index in each trading day.\n- GLYPH<15> News data : we collect the news articles on the macro economy through Wind, and get 38,727 and 39,465 news articles in 2015 and 2016 respectively. The news articles are aggregated by Wind from major GLYPH<28>nancial news websites in China, such as http://GLYPH<28>nance.sina.com.cn and http://www.hexun.com. We process the news titles rather than the whole articles to extract the events, as the main topic of a news article is often summed up in the title.\n- GLYPH<15> Social media data : the sentiments are extracted from the posts crawled from a popular investor social network in China named Xueqiu. 3 Totally 6,163,056 postings are collected for 2015 and 2016. For each post, we get the posting time stamp and the content.\nFor each trading day, if the stock market index rises, it would be a positive instance, otherwise it is a negative instance. For each year, we use the data from the GLYPH<28>rst 10 months as the training set and the last 2 months as the testing set. We evaluate the performance of our model with varying lead days and varying historical days. Lead days refers to the number of days in advance the model makes predictions and the historical days indicates the number of days over which the multi-source information is utilized. The evaluation metrics we use are F1-score and accuracy (ACC).\n\nDescribes the data collection process for the study, including the sources (Wind, Xueqiu), time period (2015-2016), and the types of data gathered (quantitative indices, news articles, social media posts) used for training and testing the model.",
    "original_text": "We collected stock market-related information from Jan. 1, 2015 to Dec. 31, 2016, and separate the information into two data sets, one for the year 2015 and the other for 2016. The data consist of three parts, the historical quantitative data, the news articles and the posts on the social network, which are introduced in detail as follows.\n- GLYPH<15> Quantitative data : the source of quantitative data is Wind, 2 a widely used GLYPH<28>nancial information service provider in China. The data we collect are the average prices, market index change and turnover rate of the Shanghai Composite Index in each trading day.\n- GLYPH<15> News data : we collect the news articles on the macro economy through Wind, and get 38,727 and 39,465 news articles in 2015 and 2016 respectively. The news articles are aggregated by Wind from major GLYPH<28>nancial news websites in China, such as http://GLYPH<28>nance.sina.com.cn and http://www.hexun.com. We process the news titles rather than the whole articles to extract the events, as the main topic of a news article is often summed up in the title.\n- GLYPH<15> Social media data : the sentiments are extracted from the posts crawled from a popular investor social network in China named Xueqiu. 3 Totally 6,163,056 postings are collected for 2015 and 2016. For each post, we get the posting time stamp and the content.\nFor each trading day, if the stock market index rises, it would be a positive instance, otherwise it is a negative instance. For each year, we use the data from the GLYPH<28>rst 10 months as the training set and the last 2 months as the testing set. We evaluate the performance of our model with varying lead days and varying historical days. Lead days refers to the number of days in advance the model makes predictions and the historical days indicates the number of days over which the multi-source information is utilized. The evaluation metrics we use are F1-score and accuracy (ACC).",
    "context": "Describes the data collection process for the study, including the sources (Wind, Xueqiu), time period (2015-2016), and the types of data gathered (quantitative indices, news articles, social media posts) used for training and testing the model.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      6
    ],
    "id": "08aa975b95d5c9c782b71fcb335e2ab61751bb4c4f6bb145ecf3952b96836668"
  },
  {
    "text": "The following baselines and variations of our proposed model are implemented for comparisons. The full implementation of our framework is named as Multi-source Multiple Instance (M-MI) model.\n- GLYPH<15> SVM : the standard support vector machine is used as a basic prediction method. During the training process, the label assigned to each instance and each group is the same as its multi-source super group label. During the\n2 http://www.wind.com.cn/\n3 https://xueqiu.com/\nprediction phase, we obtain the predicted label for each of the instance, and then average the labels as the GLYPH<28>nal label of the super group.\n- GLYPH<15> TeSIA: the tensor-based learning approach [20] utilizes multi-source information for stock prediction. SpeciGLYPH<28>cally, it uses a third-order tensor to model the GLYPH<28>rm-mode, event-mode, and sentiment-mode data.\n- GLYPH<15> nMIL : nested Multi-Instance Learning (nMIL) model [26] is the state-of-art baseline. In this model, only one data source, i.e., the news, is used to extract simple event features. It ignores the impacts of the sentiments and the historical quantitative indices.\n- GLYPH<15> O-MI : Open IE Multiple Instance (O-MI) Learning model differs from M-MI in the event extraction module. It adopts a previously proposed event extraction method [15], and uses Open IE [31] to extract event tuples from sentences. The structured event tuples are then processed by sentence2vec to obtain event representations. Please note that the sentiment data and quantitative data are also used in this model.\n- GLYPH<15> WoR-MI : Without RBM Multiple Instance (WoR-MI) Learning model is also a part of the M-MI framework. It differs M-MI in that it works without the RBM module, and therefore the sentence2vec module is fed with original structured events instead of pre-trained vectors.\n- GLYPH<15> WoH-MI : Compare to M-MI, Without Hinge loss Multiple Instance (WoH-MI) Learning model lacks the instance-level hinge loss terms (i.e., Eq. 10, 11 and 12).\nTo make a fair comparison, we use the same set of instances and the same setting of parameters to evaluate different methods. In our proposal and the baselines, we set the predicted label to GLYPH<0> 1 if the estimated probability for a multisource super group is less than 0.5; otherwise, we set the predicted label to C 1.\nTABLE 2. Prediction Results (history day=1, lead day=1).\n\nCompares various baseline models for stock prediction, including SVM, TeSIA, nMIL, O-MI, WoR-MI, and WoH-MI, all evaluated using the same dataset and parameters to ensure a fair comparison.",
    "original_text": "The following baselines and variations of our proposed model are implemented for comparisons. The full implementation of our framework is named as Multi-source Multiple Instance (M-MI) model.\n- GLYPH<15> SVM : the standard support vector machine is used as a basic prediction method. During the training process, the label assigned to each instance and each group is the same as its multi-source super group label. During the\n2 http://www.wind.com.cn/\n3 https://xueqiu.com/\nprediction phase, we obtain the predicted label for each of the instance, and then average the labels as the GLYPH<28>nal label of the super group.\n- GLYPH<15> TeSIA: the tensor-based learning approach [20] utilizes multi-source information for stock prediction. SpeciGLYPH<28>cally, it uses a third-order tensor to model the GLYPH<28>rm-mode, event-mode, and sentiment-mode data.\n- GLYPH<15> nMIL : nested Multi-Instance Learning (nMIL) model [26] is the state-of-art baseline. In this model, only one data source, i.e., the news, is used to extract simple event features. It ignores the impacts of the sentiments and the historical quantitative indices.\n- GLYPH<15> O-MI : Open IE Multiple Instance (O-MI) Learning model differs from M-MI in the event extraction module. It adopts a previously proposed event extraction method [15], and uses Open IE [31] to extract event tuples from sentences. The structured event tuples are then processed by sentence2vec to obtain event representations. Please note that the sentiment data and quantitative data are also used in this model.\n- GLYPH<15> WoR-MI : Without RBM Multiple Instance (WoR-MI) Learning model is also a part of the M-MI framework. It differs M-MI in that it works without the RBM module, and therefore the sentence2vec module is fed with original structured events instead of pre-trained vectors.\n- GLYPH<15> WoH-MI : Compare to M-MI, Without Hinge loss Multiple Instance (WoH-MI) Learning model lacks the instance-level hinge loss terms (i.e., Eq. 10, 11 and 12).\nTo make a fair comparison, we use the same set of instances and the same setting of parameters to evaluate different methods. In our proposal and the baselines, we set the predicted label to GLYPH<0> 1 if the estimated probability for a multisource super group is less than 0.5; otherwise, we set the predicted label to C 1.\nTABLE 2. Prediction Results (history day=1, lead day=1).",
    "context": "Compares various baseline models for stock prediction, including SVM, TeSIA, nMIL, O-MI, WoR-MI, and WoH-MI, all evaluated using the same dataset and parameters to ensure a fair comparison.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      6,
      7
    ],
    "id": "9ebe8b983f46dfb8659a23d9eca0404c35dc829d846d4b847fece42f59732aa8"
  },
  {
    "text": "We set both the number of history days and the number of lead days to 1. We empirically set m 0 D 0 : 6, and set m 1 ; m 2 and P 0 all as 0.5, i.e., the default setting in hinge loss. GLYPH<12> is set as 3.0, and GLYPH<21> m , GLYPH<21> d , GLYPH<21> s and GLYPH<21> GLYPH<18> are set as 0.05 by sensitivity analysis. The dimension of event representations is set as 100. Table 2 shows the performance of M-MI and the baselines. We can observe that M-MI outperforms all the baselines in both of the metrics, while SVM method shows the worst performance, indicating that simply tagging each news article with the label of its super-group is not effective. It can also be observed that M-MI and its variations (i.e., O-MI, WoH-MI and WoR-MI) all outperform nMIL. Compared to nMIL, M-MI improves F-1 by 6.9% in 2015 and 9.2% in 2016, while it improves accuracy by 6.7% and 9.4% in 2015 and 2016 respectively. Such gains mainly come from (1) utilizing multi-source information instead of only news articles, and (2) the advanced event representations rather than simple event features. Though all using multi-source information, TeSIA performs worse than M-MI and its variations, showing the effectiveness of our proposed models and feature extraction methods. Both M-MI and WoR-MI perform better than O-MI, indicating that both the structured event extraction module and the RBM pre-training module in our framework are effective. WoH-MI performs worse than M-MI, showing the proposed instance-level hinge losses across multiple data sources are useful for accurate predictions.\nFIGURE 4. F-1 scores with varying history days. (a) 2015. (b) 2016.\nFigure 4 (a) and (b) show the F-1 scores of all the comparative models with varying history days in training for 2015 and 2016 respectively (where lead day remains 1). The number of history days (i.e., t in Eq. 13) is varied from 1 to 5 and the results show that M-MI consistently performs better than the others. We can also observe that as the number of history days keeps increasing, the F-1 scores generally GLYPH<28>rst go up and then go down. The possible reason is that the impacts of the news, sentiments and quantitative indices released on some day will quickly decay after a period of time (2 or 3 days). Thus, out-of-date information should be assigned with small weights or even discarded. Fortunately, our learning process can automatically assign small weights for information with weak impacts, alleviating the impact decaying problem.\nIn order to know how early our model can predict the index movement, we show the F-1 scores of WoR-MI and M-MI with varied lead days from 1 to 3 and history days from 1 to 5 in Table 3. We observe that as the number of lead days increases, the predictive capabilities of our models decrease. This makes sense since the stock market commonly reGLYPH<29>ects the available information in a timely manner. In other words, the up-to-date information will immediately be reGLYPH<29>ected in the index change and the impacts will decay as time goes, making it difGLYPH<28>cult for long-term predictions.\nFigure 5 shows the weights of different data sources, that is, GLYPH<18> 1, GLYPH<18> 2 and GLYPH<18> 3. It can be observed that among the\nFIGURE 5. The weights of different data sources.\nTABLE 3. F-1 scores for M-MI and WoR-MI in 2015 and 2016 with varying lead days.\nthree sources, news events contribute most to the overall prediction, while the quantitative data takes the second place. It indicates that both news events and quantitative data have larger impacts to drive stock GLYPH<29>uctuations than sentiments.\n\nSpecifies the experimental settings for evaluating the model’s performance, including the number of history days, lead days, and default parameter settings.",
    "original_text": "We set both the number of history days and the number of lead days to 1. We empirically set m 0 D 0 : 6, and set m 1 ; m 2 and P 0 all as 0.5, i.e., the default setting in hinge loss. GLYPH<12> is set as 3.0, and GLYPH<21> m , GLYPH<21> d , GLYPH<21> s and GLYPH<21> GLYPH<18> are set as 0.05 by sensitivity analysis. The dimension of event representations is set as 100. Table 2 shows the performance of M-MI and the baselines. We can observe that M-MI outperforms all the baselines in both of the metrics, while SVM method shows the worst performance, indicating that simply tagging each news article with the label of its super-group is not effective. It can also be observed that M-MI and its variations (i.e., O-MI, WoH-MI and WoR-MI) all outperform nMIL. Compared to nMIL, M-MI improves F-1 by 6.9% in 2015 and 9.2% in 2016, while it improves accuracy by 6.7% and 9.4% in 2015 and 2016 respectively. Such gains mainly come from (1) utilizing multi-source information instead of only news articles, and (2) the advanced event representations rather than simple event features. Though all using multi-source information, TeSIA performs worse than M-MI and its variations, showing the effectiveness of our proposed models and feature extraction methods. Both M-MI and WoR-MI perform better than O-MI, indicating that both the structured event extraction module and the RBM pre-training module in our framework are effective. WoH-MI performs worse than M-MI, showing the proposed instance-level hinge losses across multiple data sources are useful for accurate predictions.\nFIGURE 4. F-1 scores with varying history days. (a) 2015. (b) 2016.\nFigure 4 (a) and (b) show the F-1 scores of all the comparative models with varying history days in training for 2015 and 2016 respectively (where lead day remains 1). The number of history days (i.e., t in Eq. 13) is varied from 1 to 5 and the results show that M-MI consistently performs better than the others. We can also observe that as the number of history days keeps increasing, the F-1 scores generally GLYPH<28>rst go up and then go down. The possible reason is that the impacts of the news, sentiments and quantitative indices released on some day will quickly decay after a period of time (2 or 3 days). Thus, out-of-date information should be assigned with small weights or even discarded. Fortunately, our learning process can automatically assign small weights for information with weak impacts, alleviating the impact decaying problem.\nIn order to know how early our model can predict the index movement, we show the F-1 scores of WoR-MI and M-MI with varied lead days from 1 to 3 and history days from 1 to 5 in Table 3. We observe that as the number of lead days increases, the predictive capabilities of our models decrease. This makes sense since the stock market commonly reGLYPH<29>ects the available information in a timely manner. In other words, the up-to-date information will immediately be reGLYPH<29>ected in the index change and the impacts will decay as time goes, making it difGLYPH<28>cult for long-term predictions.\nFigure 5 shows the weights of different data sources, that is, GLYPH<18> 1, GLYPH<18> 2 and GLYPH<18> 3. It can be observed that among the\nFIGURE 5. The weights of different data sources.\nTABLE 3. F-1 scores for M-MI and WoR-MI in 2015 and 2016 with varying lead days.\nthree sources, news events contribute most to the overall prediction, while the quantitative data takes the second place. It indicates that both news events and quantitative data have larger impacts to drive stock GLYPH<29>uctuations than sentiments.",
    "context": "Specifies the experimental settings for evaluating the model’s performance, including the number of history days, lead days, and default parameter settings.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      8,
      7
    ],
    "id": "7aac9256c0c8e83e2244347075e7e8b17eba7168f24a797bc72d92f03944e653"
  },
  {
    "text": "In this paper, a Multi-source Multiple Instance model is proposed which can predict the stock market movement and identify the importance of the information simultaneously. Different from previous studies that commonly exploit only one data source, our model effectively integrates heterogeneous information, that is, the events, sentiments and historical quantitative features into a comprehensive framework, and considers the consistencies among different data sources to make a better prediction. We also propose a novel event representation learning process that can effectively capture the event information. Extensive evaluations on the two-year data conGLYPH<28>rm the effectiveness of our model.\n\nSummarizes the paper's core contribution: a multi-source model for stock prediction that integrates diverse data sources and proposes a new event representation learning process, validated through two-year data evaluation.",
    "original_text": "In this paper, a Multi-source Multiple Instance model is proposed which can predict the stock market movement and identify the importance of the information simultaneously. Different from previous studies that commonly exploit only one data source, our model effectively integrates heterogeneous information, that is, the events, sentiments and historical quantitative features into a comprehensive framework, and considers the consistencies among different data sources to make a better prediction. We also propose a novel event representation learning process that can effectively capture the event information. Extensive evaluations on the two-year data conGLYPH<28>rm the effectiveness of our model.",
    "context": "Summarizes the paper's core contribution: a multi-source model for stock prediction that integrates diverse data sources and proposes a new event representation learning process, validated through two-year data evaluation.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      8
    ],
    "id": "583c07887826aa08be6742cde96fdbbb6535a18f014a75598ef8db6dc09ab916"
  },
  {
    "text": "- [1] E. F. Fama, ''The behavior of stock-market prices,'' J. Bus. , vol. 38, no. 1, pp. 34GLYPH<21>105, 1965.\n- [2] S. R. Das and M. Y. Chen, ''Yahoo! for Amazon: Sentiment extraction from small talk on the Web,'' Manage. Sci. , vol. 53, no. 9, pp. 1375GLYPH<21>1388, 2007.\n- [3] J. Si, A. Mukherjee, B. Liu, Q. Li, H. Li, and X. Deng, ''Exploiting topic based twitter sentiment for stock prediction,'' in Proc. 51st Annu. Meeting Assoc. Comput. Linguistics (ACL) , 2013, pp. 24GLYPH<21>29.\n- [4] W. Y. Wang and Z. Hua, ''A semiparametric Gaussian copula regression model for predicting GLYPH<28>nancial risks from earnings calls,'' in Proc. 52nd Annu. Meeting Assoc. Comput. Linguistics (ACL) , Jun. 2014, pp. 1155GLYPH<21>1165.\n- [5] S. Kogan, D. Levin, B. R. Routledge, J. S. Sagi, and N. A. Smith, ''Predicting risk from GLYPH<28>nancial reports with regression,'' in Proc. Annu. Conf. North Amer. Chapter Assoc. Comput. Linguistics Hum. Lang. Technol. , 2009, pp. 272GLYPH<21>280.\n- [6] R. Luss and A. D'Aspremont, ''Predicting abnormal returns from news using text classiGLYPH<28>cation,'' Quant. Finance , vol. 15, no. 6, pp. 999GLYPH<21>1012, 2015.\n- [7] R. R. Prechter, The Wave Principle of Human Social Behavior and the New Science of Socionomics , vol. 1. Gainesville, GA, USA: New Classics Library, 1999.\n- [8] J. R. Nofsinger, ''Social mood and GLYPH<28>nancial economics,'' J. Behav. Finance , vol. 6, no. 3, pp. 144GLYPH<21>160, 2005.\n- [9] J. Bi and X. Wang, ''Learning classiGLYPH<28>ers from dual annotation ambiguity via a minGLYPH<21>max framework,'' Neurocomputing , vol. 151, pp. 891GLYPH<21>904, Mar. 2015.\n- [10] S. Xie, W. Fan, and P. S. Yu, ''An iterative and re-weighting framework for rejection and uncertainty resolution in crowdsourcing,'' in Proc. SIAM Int. Conf. Data Mining , 2012, pp. 1107GLYPH<21>1118.\n- [11] Q. Le and T. Mikolov, ''Distributed representations of sentences and documents,'' in Proc. 31st Int. Conf. Mach. Learn. (ICML) , 2014, pp. 1188GLYPH<21>1196.\n- [12] F. Hogenboom, F. Frasincar, U. Kaymak, and F. De Jong, ''An overview of event extraction from text,'' in Proc. Workshop Detection, Represent., Exploitation Events Semantic Web (DeRiVE), 10th Int. Semantic Web Conf. (ISWC) , vol. 779, 2011, pp. 48GLYPH<21>57.\n- [13] R. Akita, A. Yoshihara, T. Matsubara, and K. Uehara, ''Deep learning for stock prediction using numerical and textual information,'' in Proc. IEEE/ACIS 15th Int. Conf. Comput. Inf. Sci. (ICIS) , Jun. 2016, pp. 1GLYPH<21>6.\n- [14] T. Nguyen, D. Phung, B. Adams, and S. Venkatesh, ''Event extraction using behaviors of sentiment signals and burst structure in social media,'' Knowl. Inf. Syst. , vol. 37, no. 2, pp. 279GLYPH<21>304, 2013.\n- [15] X. Ding, Y. Zhang, T. Liu, and J. Duan, ''Using structured events to predict stock price movement: An empirical investigation,'' in Proc. Conf. Empirical Methods Natural Lang. Process. (EMNLP) , 2014, pp. 1415GLYPH<21>1425.\n- [16] X. Ding, Y. Zhang, T. Liu, and J. Duan, ''Deep learning for event-driven stock prediction,'' in Proc. 24th Int. Joint Conf. Artif. Intell. (IJCAI) , 2015, pp. 2327GLYPH<21>2333.\n- [17] J. Bollen, H. Mao, and X. Zeng, ''Twitter mood predicts the stock market,'' J. Comput. Sci. , vol. 2, no. 1, pp. 1GLYPH<21>8, Mar. 2011.\n- [18] M. Makrehchi, S. Shah, and W. Liao, ''Stock prediction using eventbased sentiment analysis,'' in Proc. IEEE/WIC/ACM Int. Joint Conf. Web Intell. (WI) Intell. Agent Technol. (IAT) , vol. 1, Nov. 2013, pp. 337GLYPH<21>342.\n- [19] T. H. Nguyen and K. Shirai, ''Topic modeling based sentiment analysis on social media for stock market prediction,'' in Proc. 53rd Annu. Meeting Assoc. Comput. Linguistics (ACL) , 2015, pp. 1354GLYPH<21>1364.\n- [20] Q. Li, L. Jiang, P. Li, and H. Chen, ''Tensor-based learning for predicting stock movements,'' in Proc. 29th AAAI Conf. Artif. Intell. (AAAI) , 2015, pp. 1784GLYPH<21>1790.\n- [21] T. G. Dietterich, R. H. Lathrop, and T. Lozano-PØrez, ''Solving the multiple instance problem with axis-parallel rectangles,'' Artif. Intell. , vol. 89, nos. 1GLYPH<21>2, pp. 31GLYPH<21>71, 1997.\n- [22] J. Amores, ''Multiple instance classiGLYPH<28>cation: Review, taxonomy and comparative study,'' Artif. Intell. , vol. 201, pp. 81GLYPH<21>105, Aug. 2013.\n- [23] G. Liu, J. Wu, and Z.-H. Zhou, ''Key instance detection in multiinstance learning,'' in Proc. Asian Conf. Mach. Learn. , 2012, pp. 253GLYPH<21>268.\n- [24] D. Kotzias, M. Denil, N. de Freitas, and P. Smyth, ''From group to individual labels using deep features,'' in Proc. 21st ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD) , 2015, pp. 597GLYPH<21>606.\n- [25] J. Feng and Z.-H. Zhou, ''Deep MIML network,'' in Proc. 21st AAAI Conf. Artif. Intell. (AAAI) , 2017, pp. 1884GLYPH<21>1890.\n- [26] Y. Ning, S. Muthiah, H. Rangwala, and N. Ramakrishnan, ''Modeling precursors for event forecasting via nested multi-instance learning,'' in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD) , 2016, pp. 1095GLYPH<21>1104.\n- [27] G. E. Hinton and R. R. Salakhutdinov, ''Reducing the dimensionality of data with neural networks,'' Science , vol. 313, no. 5786, pp. 504GLYPH<21>507, 2006.\n- [28] X. Zhang et al. , ''IAD: Interaction-aware diffusion framework in social networks,'' IEEE Trans. Knowl. Data Eng. , to be published.\n- [29] W. X. Zhao, J. Jiang, H. Yan, and X. Li, ''Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid,'' in Proc. Conf. Empirical Methods Natural Lang. Process. , 2010, pp. 56GLYPH<21>65.\n\nIntroduces various methods for predicting stock market movements, including sentiment analysis from social media, event extraction, and deep learning techniques.",
    "original_text": "- [1] E. F. Fama, ''The behavior of stock-market prices,'' J. Bus. , vol. 38, no. 1, pp. 34GLYPH<21>105, 1965.\n- [2] S. R. Das and M. Y. Chen, ''Yahoo! for Amazon: Sentiment extraction from small talk on the Web,'' Manage. Sci. , vol. 53, no. 9, pp. 1375GLYPH<21>1388, 2007.\n- [3] J. Si, A. Mukherjee, B. Liu, Q. Li, H. Li, and X. Deng, ''Exploiting topic based twitter sentiment for stock prediction,'' in Proc. 51st Annu. Meeting Assoc. Comput. Linguistics (ACL) , 2013, pp. 24GLYPH<21>29.\n- [4] W. Y. Wang and Z. Hua, ''A semiparametric Gaussian copula regression model for predicting GLYPH<28>nancial risks from earnings calls,'' in Proc. 52nd Annu. Meeting Assoc. Comput. Linguistics (ACL) , Jun. 2014, pp. 1155GLYPH<21>1165.\n- [5] S. Kogan, D. Levin, B. R. Routledge, J. S. Sagi, and N. A. Smith, ''Predicting risk from GLYPH<28>nancial reports with regression,'' in Proc. Annu. Conf. North Amer. Chapter Assoc. Comput. Linguistics Hum. Lang. Technol. , 2009, pp. 272GLYPH<21>280.\n- [6] R. Luss and A. D'Aspremont, ''Predicting abnormal returns from news using text classiGLYPH<28>cation,'' Quant. Finance , vol. 15, no. 6, pp. 999GLYPH<21>1012, 2015.\n- [7] R. R. Prechter, The Wave Principle of Human Social Behavior and the New Science of Socionomics , vol. 1. Gainesville, GA, USA: New Classics Library, 1999.\n- [8] J. R. Nofsinger, ''Social mood and GLYPH<28>nancial economics,'' J. Behav. Finance , vol. 6, no. 3, pp. 144GLYPH<21>160, 2005.\n- [9] J. Bi and X. Wang, ''Learning classiGLYPH<28>ers from dual annotation ambiguity via a minGLYPH<21>max framework,'' Neurocomputing , vol. 151, pp. 891GLYPH<21>904, Mar. 2015.\n- [10] S. Xie, W. Fan, and P. S. Yu, ''An iterative and re-weighting framework for rejection and uncertainty resolution in crowdsourcing,'' in Proc. SIAM Int. Conf. Data Mining , 2012, pp. 1107GLYPH<21>1118.\n- [11] Q. Le and T. Mikolov, ''Distributed representations of sentences and documents,'' in Proc. 31st Int. Conf. Mach. Learn. (ICML) , 2014, pp. 1188GLYPH<21>1196.\n- [12] F. Hogenboom, F. Frasincar, U. Kaymak, and F. De Jong, ''An overview of event extraction from text,'' in Proc. Workshop Detection, Represent., Exploitation Events Semantic Web (DeRiVE), 10th Int. Semantic Web Conf. (ISWC) , vol. 779, 2011, pp. 48GLYPH<21>57.\n- [13] R. Akita, A. Yoshihara, T. Matsubara, and K. Uehara, ''Deep learning for stock prediction using numerical and textual information,'' in Proc. IEEE/ACIS 15th Int. Conf. Comput. Inf. Sci. (ICIS) , Jun. 2016, pp. 1GLYPH<21>6.\n- [14] T. Nguyen, D. Phung, B. Adams, and S. Venkatesh, ''Event extraction using behaviors of sentiment signals and burst structure in social media,'' Knowl. Inf. Syst. , vol. 37, no. 2, pp. 279GLYPH<21>304, 2013.\n- [15] X. Ding, Y. Zhang, T. Liu, and J. Duan, ''Using structured events to predict stock price movement: An empirical investigation,'' in Proc. Conf. Empirical Methods Natural Lang. Process. (EMNLP) , 2014, pp. 1415GLYPH<21>1425.\n- [16] X. Ding, Y. Zhang, T. Liu, and J. Duan, ''Deep learning for event-driven stock prediction,'' in Proc. 24th Int. Joint Conf. Artif. Intell. (IJCAI) , 2015, pp. 2327GLYPH<21>2333.\n- [17] J. Bollen, H. Mao, and X. Zeng, ''Twitter mood predicts the stock market,'' J. Comput. Sci. , vol. 2, no. 1, pp. 1GLYPH<21>8, Mar. 2011.\n- [18] M. Makrehchi, S. Shah, and W. Liao, ''Stock prediction using eventbased sentiment analysis,'' in Proc. IEEE/WIC/ACM Int. Joint Conf. Web Intell. (WI) Intell. Agent Technol. (IAT) , vol. 1, Nov. 2013, pp. 337GLYPH<21>342.\n- [19] T. H. Nguyen and K. Shirai, ''Topic modeling based sentiment analysis on social media for stock market prediction,'' in Proc. 53rd Annu. Meeting Assoc. Comput. Linguistics (ACL) , 2015, pp. 1354GLYPH<21>1364.\n- [20] Q. Li, L. Jiang, P. Li, and H. Chen, ''Tensor-based learning for predicting stock movements,'' in Proc. 29th AAAI Conf. Artif. Intell. (AAAI) , 2015, pp. 1784GLYPH<21>1790.\n- [21] T. G. Dietterich, R. H. Lathrop, and T. Lozano-PØrez, ''Solving the multiple instance problem with axis-parallel rectangles,'' Artif. Intell. , vol. 89, nos. 1GLYPH<21>2, pp. 31GLYPH<21>71, 1997.\n- [22] J. Amores, ''Multiple instance classiGLYPH<28>cation: Review, taxonomy and comparative study,'' Artif. Intell. , vol. 201, pp. 81GLYPH<21>105, Aug. 2013.\n- [23] G. Liu, J. Wu, and Z.-H. Zhou, ''Key instance detection in multiinstance learning,'' in Proc. Asian Conf. Mach. Learn. , 2012, pp. 253GLYPH<21>268.\n- [24] D. Kotzias, M. Denil, N. de Freitas, and P. Smyth, ''From group to individual labels using deep features,'' in Proc. 21st ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD) , 2015, pp. 597GLYPH<21>606.\n- [25] J. Feng and Z.-H. Zhou, ''Deep MIML network,'' in Proc. 21st AAAI Conf. Artif. Intell. (AAAI) , 2017, pp. 1884GLYPH<21>1890.\n- [26] Y. Ning, S. Muthiah, H. Rangwala, and N. Ramakrishnan, ''Modeling precursors for event forecasting via nested multi-instance learning,'' in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD) , 2016, pp. 1095GLYPH<21>1104.\n- [27] G. E. Hinton and R. R. Salakhutdinov, ''Reducing the dimensionality of data with neural networks,'' Science , vol. 313, no. 5786, pp. 504GLYPH<21>507, 2006.\n- [28] X. Zhang et al. , ''IAD: Interaction-aware diffusion framework in social networks,'' IEEE Trans. Knowl. Data Eng. , to be published.\n- [29] W. X. Zhao, J. Jiang, H. Yan, and X. Li, ''Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid,'' in Proc. Conf. Empirical Methods Natural Lang. Process. , 2010, pp. 56GLYPH<21>65.",
    "context": "Introduces various methods for predicting stock market movements, including sentiment analysis from social media, event extraction, and deep learning techniques.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      8
    ],
    "id": "b8924c479fd4638dd76359ff91183307ed7f1d5947938673a5c3298a89152683"
  },
  {
    "text": "- [30] L.-W. Ku and H.-H. Chen, ''Mining opinions from the Web: Beyond relevance retrieval,'' J. Amer. Soc. Inf. Sci. Technol. , vol. 58, no. 12, pp. 1838GLYPH<21>1850, 2007.\n- [31] A. Fader, S. Soderland, and O. Etzioni, ''Identifying relations for open information extraction,'' in Proc. Conf. Empirical Methods Natural Lang. Process. , 2011, pp. 1535GLYPH<21>1545.\nXI ZHANG (M'17) received the Ph.D. degree in computer science from Tsinghua University. He was a Visiting Scholar at The University of Illinois at Chicago. He is currently an Associate Professor with the Beijing University of Posts and Telecommunications and is also the Vice Director of the Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, China. His research interests include data mining and computer architecture.\nSIYU QU received the bachelor's degree in computer science from Xidian University in 2012. She is currently pursuing the master's degree with the Key Laboratory of Trustworthy Distributed Computing and Service, Beijing University of Posts and Telecommunications, Ministry of Education, China. Her research interests include data mining and machine learning.\nJIEYUN HUANG received the bachelor's degree in information security from the Beijing University of Posts and Telecommunications in 2017, where she is currently pursuing the master's degree with the Key Laboratory of Trustworthy Distributed Computing and Service. Her research interests are in data mining and machine learning.\nBINXING FANG received the Ph.D. degree from the Harbin Institute of Technology, China, in 1989. He was the Chief Scientist of the State Key Development Program of Basic Research of China. He is currently a member of the Chinese Academy of Engineering and is also a Professor with the School of Cyberspace Security, Beijing University of Posts and Telecommunications. His current research interests include big data and cybersecurity.\nPHILIP YU (F'93) received the Ph.D. degree in electrical engineering from Stanford University. He is currently a Distinguished Professor in computer science at The University of Illinois at Chicago and is also the Wexler Chair in information technology. His research interests include big data, data mining, data stream, database, and privacy. He is a fellow of ACM. He received the Research Contributions Award from the IEEE International Conference on Data Mining in 2003, the Technical Achievement Award from the IEEE Computer Society in 2013, and the ACM SIGKDD 2016 Innovation Award. He was the Editor-in-Chief of the IEEE TRANSACTIONSON KNOWLEDGEAND DATA ENGINEERING and the ACM Transactions on Knowledge Discovery from Data .\n\nProvides citations to relevant research on opinion mining and information extraction techniques.",
    "original_text": "- [30] L.-W. Ku and H.-H. Chen, ''Mining opinions from the Web: Beyond relevance retrieval,'' J. Amer. Soc. Inf. Sci. Technol. , vol. 58, no. 12, pp. 1838GLYPH<21>1850, 2007.\n- [31] A. Fader, S. Soderland, and O. Etzioni, ''Identifying relations for open information extraction,'' in Proc. Conf. Empirical Methods Natural Lang. Process. , 2011, pp. 1535GLYPH<21>1545.\nXI ZHANG (M'17) received the Ph.D. degree in computer science from Tsinghua University. He was a Visiting Scholar at The University of Illinois at Chicago. He is currently an Associate Professor with the Beijing University of Posts and Telecommunications and is also the Vice Director of the Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, China. His research interests include data mining and computer architecture.\nSIYU QU received the bachelor's degree in computer science from Xidian University in 2012. She is currently pursuing the master's degree with the Key Laboratory of Trustworthy Distributed Computing and Service, Beijing University of Posts and Telecommunications, Ministry of Education, China. Her research interests include data mining and machine learning.\nJIEYUN HUANG received the bachelor's degree in information security from the Beijing University of Posts and Telecommunications in 2017, where she is currently pursuing the master's degree with the Key Laboratory of Trustworthy Distributed Computing and Service. Her research interests are in data mining and machine learning.\nBINXING FANG received the Ph.D. degree from the Harbin Institute of Technology, China, in 1989. He was the Chief Scientist of the State Key Development Program of Basic Research of China. He is currently a member of the Chinese Academy of Engineering and is also a Professor with the School of Cyberspace Security, Beijing University of Posts and Telecommunications. His current research interests include big data and cybersecurity.\nPHILIP YU (F'93) received the Ph.D. degree in electrical engineering from Stanford University. He is currently a Distinguished Professor in computer science at The University of Illinois at Chicago and is also the Wexler Chair in information technology. His research interests include big data, data mining, data stream, database, and privacy. He is a fellow of ACM. He received the Research Contributions Award from the IEEE International Conference on Data Mining in 2003, the Technical Achievement Award from the IEEE Computer Society in 2013, and the ACM SIGKDD 2016 Innovation Award. He was the Editor-in-Chief of the IEEE TRANSACTIONSON KNOWLEDGEAND DATA ENGINEERING and the ACM Transactions on Knowledge Discovery from Data .",
    "context": "Provides citations to relevant research on opinion mining and information extraction techniques.",
    "document": "Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf",
    "pages": [
      8,
      9
    ],
    "id": "eef4cdb3ae7db3a8433e04cb2678050d9bc1d66ad48ee7d7d1387f909c3127ee"
  },
  {
    "text": "Per Runeson per.runeson@cs.lth.se Lund University Lund, Sweden\nEmma Söderberg emma.soderberg@cs.lth.se Lund University Lund, Sweden\nMartin Höst martin.host@mau.se Malmö University Malmö, Sweden\n\nThe chunk outlines the research project's aim to derive advice for open science practices in empirical software engineering, specifically addressing the balance between data openness and concerns regarding privacy, company secrets, and commercial interests. It details a conceptual framework based on the Gander case, incorporating literature on open data, and proposes recommendations for SE researchers.",
    "original_text": "Per Runeson per.runeson@cs.lth.se Lund University Lund, Sweden\nEmma Söderberg emma.soderberg@cs.lth.se Lund University Lund, Sweden\nMartin Höst martin.host@mau.se Malmö University Malmö, Sweden",
    "context": "The chunk outlines the research project's aim to derive advice for open science practices in empirical software engineering, specifically addressing the balance between data openness and concerns regarding privacy, company secrets, and commercial interests. It details a conceptual framework based on the Gander case, incorporating literature on open data, and proposes recommendations for SE researchers.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      1
    ],
    "id": "43b625cdc355da9e2b6bdab7a35dadfb97b8627dfe54afb97110db5b01b7de8d"
  },
  {
    "text": "Background. Open science aims to improve research accessibility, replicability, and consequently its quality. Empirical software engineering entails both data and artifacts, which may be shared more or less openly, to support transparency. However, the trade-offs involved in balancing the openness against integrity and secrecy concerns need methodological guidance. Aim. We aim to derive such advice, based on our own experiences from a research project, in the field of gaze-assisted code reviews - the Gander case. Method. We draw on literature about open data and artifacts in socio-technical research. Next, we describe our case project and derive a conceptual framework of steps in research data analysis and artifact development, using our data and artifacts as illustrating examples. Results. The conceptual framework contains 1) a categorization of humans involved as participants and their concerns, 2) four steps for data analysis, each resulting in corresponding data and meta-data, and 3) three steps of artifact distribution, matching different levels of openness. We derive a preliminary set of recommendations for open science practices for data and artifacts. Conclusion. The conceptual framework has proven useful in summarizing and discussing data and artifacts in the studied case project. We envision that the framework and recommendations will provide a foundation for further advancement of open science research practices in empirical, socio-technical software engineering.\n\nThe chunk outlines a research project aiming to develop guidance for open science practices in empirical software engineering, specifically addressing the balance between openness and concerns regarding data integrity and secrecy, based on experiences with a gaze-assisted code review project.",
    "original_text": "Background. Open science aims to improve research accessibility, replicability, and consequently its quality. Empirical software engineering entails both data and artifacts, which may be shared more or less openly, to support transparency. However, the trade-offs involved in balancing the openness against integrity and secrecy concerns need methodological guidance. Aim. We aim to derive such advice, based on our own experiences from a research project, in the field of gaze-assisted code reviews - the Gander case. Method. We draw on literature about open data and artifacts in socio-technical research. Next, we describe our case project and derive a conceptual framework of steps in research data analysis and artifact development, using our data and artifacts as illustrating examples. Results. The conceptual framework contains 1) a categorization of humans involved as participants and their concerns, 2) four steps for data analysis, each resulting in corresponding data and meta-data, and 3) three steps of artifact distribution, matching different levels of openness. We derive a preliminary set of recommendations for open science practices for data and artifacts. Conclusion. The conceptual framework has proven useful in summarizing and discussing data and artifacts in the studied case project. We envision that the framework and recommendations will provide a foundation for further advancement of open science research practices in empirical, socio-technical software engineering.",
    "context": "The chunk outlines a research project aiming to develop guidance for open science practices in empirical software engineering, specifically addressing the balance between openness and concerns regarding data integrity and secrecy, based on experiences with a gaze-assisted code review project.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      1
    ],
    "id": "18d60b82daee2894a31097cbcee6654ca0973b4449e75e73305e314c9953b373"
  },
  {
    "text": "- Software and its engineering → Integrated and visual development environments ;\n\nThis section introduces the context of integrated development environments and visual development tools, highlighting the relevance of open science practices within the field of software engineering.",
    "original_text": "- Software and its engineering → Integrated and visual development environments ;",
    "context": "This section introduces the context of integrated development environments and visual development tools, highlighting the relevance of open science practices within the field of software engineering.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      1
    ],
    "id": "4a1cc0d3184c87ff15b84d60b5228bc49a37db3117f584e5707369bd2bc915a0"
  },
  {
    "text": "Open science, Open research, Open data, FAIR data, Open artifacts, Socio-technical software engineering\n\nThis chunk outlines the context of open science in software engineering, highlighting the need for balancing openness with concerns about data privacy, company secrets, and ethical considerations, particularly within socio-technical research. It establishes the framework for the paper's exploration of these challenges and proposes a conceptual framework to guide open data practices in the field.",
    "original_text": "Open science, Open research, Open data, FAIR data, Open artifacts, Socio-technical software engineering",
    "context": "This chunk outlines the context of open science in software engineering, highlighting the need for balancing openness with concerns about data privacy, company secrets, and ethical considerations, particularly within socio-technical research. It establishes the framework for the paper's exploration of these challenges and proposes a conceptual framework to guide open data practices in the field.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      1
    ],
    "id": "5f452aa54b19fc9c92f1fc32916612cf02eabcc69215d57af43f66942d8ccaa7"
  },
  {
    "text": "Per Runeson, Emma Söderberg, and Martin Höst. 2024. A Conceptual Framework and Recommendations for Open Data and Artifacts in Empirical Software Engineering. In International Workshop on Methodological Issues with Empirical Studies in Software Engineering (WSESE '24 ), April 16, 2024, Lisbon, Portugal. ACM, New York, NY, USA, 8 pages. https: //doi.org/10.1145/3643664.3648206\nThis work licensed under Creative Commons Attribution International 4.0 License.\nWSESE '24 , April 16, 2024, Lisbon, Portugal © 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0567-0/24/04 https://doi.org/10.1145/3643664.3648206\nOpen science is brought forward as a means to increase research quality and efficiency, e.g., by making it easier to reproduce and replicate studies, and to democratize research. Open science or open scientific knowledge , as defined by UNESCO 1 , includes open scientific publications, open research data, open educational resources, open source software, and open hardware. In open science, the transparency is expected to increase reviewability, reproducibility and replicability, and thereby the quality of research. The open access and data is expected to contribute to democratization and to increase efficiency by avoiding double work. This development is, for example, manifested in the Empirical Software Engineering journal open science initiative [21] and the ACM artifact evaluation policy 2 . In empirical software engineering, experimental material have been made available as replication or laboratory packages [30] to some extent.\nHowever, open does not mean out of control. According to the European Horison 2020 Program Guidelines on FAIR Data 3 , data should be 'as open as possible and as closed as necessary' - open in order to foster the reusability and to accelerate research, but at the same time they should be as closed as needed to safeguard the privacy of the subjects as well as legitimate secrecy concerns for commercial entities. Recently sharpened legislation on personal data protection - e.g. the GDPR in Europe 4 - and ethical approval - e.g. changed interpretation of the ethical approval act in Sweden - clearly illustrate these conflicting interests.\nSoftware engineering (SE) is a research and practice domain, which is fundamentally socio-technical [31]. This implies that research and practice involves humans, and data generated by and about humans. As many SE research challenges come with scaling, it is ideally being conducted in real-world industrial contexts [2], involving real-world products and business. These characteristics lead to a series of ethical and legal concerns for SE research, which conflict with the aims of open science, at first sight. Not only has research to protect personal data and integrity , company data and secrets have also to be safeguarded. Further, since researchers and companies may have commercial interests in software tools, they may be reluctant to sharing their artifacts.\nMendez et al. discuss open science in SE as a means to improve repeatability, replicability, and reproducibility of research [19]. They\n1 https://doi.org/10.54677/UTCD9302\n3 FAIR - Findable, Accessible, Interoperable, Reusable Data https://ec.europa.eu/ research/participants/data/ref/h2020/grants_manual/hi/oa_pilot/h2020-hi-oa-data-mgt_en.pdf\n2 https://www.acm.org/publications/policies/artifact-review-and-badging-current\n4 The General Data Protection Regulation (EU 2016/679, GDPR) is a European Union regulation on information privacy in the European Union (EU) and the European Economic Area (EEA). https://en.wikipedia.org/wiki/General_Data_Protection_Regulation\nacknowledge the above mentioned concerns, namely personal data and company data protection, and 'the conflict between anonymity and confidentiality on one side, and openness on the other' [19, p.493]. However, they offer little practical guidance for research projects in this balancing act. Further, based on observation of the citation of method guidelines for empirical software engineering (e.g., [14, 24, 33]), authors seem to prefer guidelines tailored to SE, rather than using their general counterparts.\nWefirst discuss relevant literature on open data in socio-technical research in Section 2. Then, we introduce the case project in Section 3 and analyze open science aspects that appeared in the project, rendering our conceptual framework in Section 4. In Section 5 we map our project to the conceptual framework and present recommendations for SE researchers. Section 6 concludes the paper.\nWe therefore share our experiences and considerations on open science in socio-technical SE from a research project on gaze-driven assistance in code review as a case study - the Gander case. The project covers interview and survey data from industry practitioners, eye-tracking data from human subjects, and open source tools for experimentation. From our experiences, as well as literature on data sharing and software reuse, we derive a conceptual framework, which aims to structure analysis and communication about data and artifact openness and thereby guide future open science in SE. We also provide our own project data and artifacts as an illustrating example and derive preliminary recommendations.\n\nThis section introduces a conceptual framework and recommendations for open data and artifacts in empirical software engineering, stemming from a case study on gaze-driven code review. It highlights the inherent conflicts between open science goals and ethical/legal concerns related to data privacy, company secrets, and commercial interests within the socio-technical nature of SE research.",
    "original_text": "Per Runeson, Emma Söderberg, and Martin Höst. 2024. A Conceptual Framework and Recommendations for Open Data and Artifacts in Empirical Software Engineering. In International Workshop on Methodological Issues with Empirical Studies in Software Engineering (WSESE '24 ), April 16, 2024, Lisbon, Portugal. ACM, New York, NY, USA, 8 pages. https: //doi.org/10.1145/3643664.3648206\nThis work licensed under Creative Commons Attribution International 4.0 License.\nWSESE '24 , April 16, 2024, Lisbon, Portugal © 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0567-0/24/04 https://doi.org/10.1145/3643664.3648206\nOpen science is brought forward as a means to increase research quality and efficiency, e.g., by making it easier to reproduce and replicate studies, and to democratize research. Open science or open scientific knowledge , as defined by UNESCO 1 , includes open scientific publications, open research data, open educational resources, open source software, and open hardware. In open science, the transparency is expected to increase reviewability, reproducibility and replicability, and thereby the quality of research. The open access and data is expected to contribute to democratization and to increase efficiency by avoiding double work. This development is, for example, manifested in the Empirical Software Engineering journal open science initiative [21] and the ACM artifact evaluation policy 2 . In empirical software engineering, experimental material have been made available as replication or laboratory packages [30] to some extent.\nHowever, open does not mean out of control. According to the European Horison 2020 Program Guidelines on FAIR Data 3 , data should be 'as open as possible and as closed as necessary' - open in order to foster the reusability and to accelerate research, but at the same time they should be as closed as needed to safeguard the privacy of the subjects as well as legitimate secrecy concerns for commercial entities. Recently sharpened legislation on personal data protection - e.g. the GDPR in Europe 4 - and ethical approval - e.g. changed interpretation of the ethical approval act in Sweden - clearly illustrate these conflicting interests.\nSoftware engineering (SE) is a research and practice domain, which is fundamentally socio-technical [31]. This implies that research and practice involves humans, and data generated by and about humans. As many SE research challenges come with scaling, it is ideally being conducted in real-world industrial contexts [2], involving real-world products and business. These characteristics lead to a series of ethical and legal concerns for SE research, which conflict with the aims of open science, at first sight. Not only has research to protect personal data and integrity , company data and secrets have also to be safeguarded. Further, since researchers and companies may have commercial interests in software tools, they may be reluctant to sharing their artifacts.\nMendez et al. discuss open science in SE as a means to improve repeatability, replicability, and reproducibility of research [19]. They\n1 https://doi.org/10.54677/UTCD9302\n3 FAIR - Findable, Accessible, Interoperable, Reusable Data https://ec.europa.eu/ research/participants/data/ref/h2020/grants_manual/hi/oa_pilot/h2020-hi-oa-data-mgt_en.pdf\n2 https://www.acm.org/publications/policies/artifact-review-and-badging-current\n4 The General Data Protection Regulation (EU 2016/679, GDPR) is a European Union regulation on information privacy in the European Union (EU) and the European Economic Area (EEA). https://en.wikipedia.org/wiki/General_Data_Protection_Regulation\nacknowledge the above mentioned concerns, namely personal data and company data protection, and 'the conflict between anonymity and confidentiality on one side, and openness on the other' [19, p.493]. However, they offer little practical guidance for research projects in this balancing act. Further, based on observation of the citation of method guidelines for empirical software engineering (e.g., [14, 24, 33]), authors seem to prefer guidelines tailored to SE, rather than using their general counterparts.\nWefirst discuss relevant literature on open data in socio-technical research in Section 2. Then, we introduce the case project in Section 3 and analyze open science aspects that appeared in the project, rendering our conceptual framework in Section 4. In Section 5 we map our project to the conceptual framework and present recommendations for SE researchers. Section 6 concludes the paper.\nWe therefore share our experiences and considerations on open science in socio-technical SE from a research project on gaze-driven assistance in code review as a case study - the Gander case. The project covers interview and survey data from industry practitioners, eye-tracking data from human subjects, and open source tools for experimentation. From our experiences, as well as literature on data sharing and software reuse, we derive a conceptual framework, which aims to structure analysis and communication about data and artifact openness and thereby guide future open science in SE. We also provide our own project data and artifacts as an illustrating example and derive preliminary recommendations.",
    "context": "This section introduces a conceptual framework and recommendations for open data and artifacts in empirical software engineering, stemming from a case study on gaze-driven code review. It highlights the inherent conflicts between open science goals and ethical/legal concerns related to data privacy, company secrets, and commercial interests within the socio-technical nature of SE research.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      1,
      2
    ],
    "id": "a7349d306ec369a5ff86cad4a20ef2d7ee6fe2318a481857c9a8a28b30c532df"
  },
  {
    "text": "Empirical data collection techniques in software engineering field studies were categorized by Lethbridge et al. into three degrees: (1) direct involvement of software engineers - human-enacted inquisitive and observational techniques (e.g. interviews), (2) indirect involvement of software engineers - technically enabled observations (e.g. eye-tracking), and (3) study of work artifacts only (e.g. code) [17]. All the three categories have similar concerns with respect to both the personal and company data protection needs. For example, eye-tracking and the code may reveal inefficient work practices by the individual, and interviews and commercial code may reveal company secrets. Generally, companies involved in empirical studies are concerned with protection of their data, preventing researchers from opening the data to the research community.\nA special branch of empirical software engineering is the mining of software repositories (MSR) studies, where data is collected from open development repositories. González-Barahona and Robles proposed a method for assessing the reproducibility of MSR studies [10], which they validated a decade later [11]. Since the raw data in MSR studies come from open sources, their method\nA special type of SE contexts is open source software (OSS) development. Then, software artifacts are open by default, as well as personal data in the form of contributors' names or pseudonyms and contact information. As a consequence of the easy access, SE research on OSS projects is popular, although only some aspects are comparable for corporate SE [23]. Also, there is a risk of revealing personal identities and data if interviewees are selected via open source software, for example, from an OSS community [20].\nprimarily focuses on the transparency of analysis methods and procedures.\nSince the data collected in SE research varies across these degrees and factors, a conceptual framework and recommendations for open data must take them into account.\nIn the research field of Open data ecosystems [25], the concern about sharing data between commercial actors is addressed, as well as open data from governmental and other public sources. Based on a survey of the literature, Enders et al. explored factors to take into account when deciding the degree of openness for data, i.e. selective revealing of data [6]. These factors are (1) Coreness (closeness to the core business), (2) Currentness (how recent is the data), (3) Extent (volume of data), (4) Granularity (level of detail), (5) Interoperability (e.g, standardized formats), and (6) Quality (fit for purpose).\n\nThis chunk details the categorization of empirical data collection methods in software engineering – direct involvement (interviews, eye-tracking), indirect involvement (observational data), and study of artifacts (code). It highlights the shared ethical and legal concerns regarding personal and company data protection across these methods, particularly emphasizing the potential for revealing sensitive information through data like eye-tracking results and commercial code. It also introduces the concept of mining software repositories and the challenges of reproducibility within that field.",
    "original_text": "Empirical data collection techniques in software engineering field studies were categorized by Lethbridge et al. into three degrees: (1) direct involvement of software engineers - human-enacted inquisitive and observational techniques (e.g. interviews), (2) indirect involvement of software engineers - technically enabled observations (e.g. eye-tracking), and (3) study of work artifacts only (e.g. code) [17]. All the three categories have similar concerns with respect to both the personal and company data protection needs. For example, eye-tracking and the code may reveal inefficient work practices by the individual, and interviews and commercial code may reveal company secrets. Generally, companies involved in empirical studies are concerned with protection of their data, preventing researchers from opening the data to the research community.\nA special branch of empirical software engineering is the mining of software repositories (MSR) studies, where data is collected from open development repositories. González-Barahona and Robles proposed a method for assessing the reproducibility of MSR studies [10], which they validated a decade later [11]. Since the raw data in MSR studies come from open sources, their method\nA special type of SE contexts is open source software (OSS) development. Then, software artifacts are open by default, as well as personal data in the form of contributors' names or pseudonyms and contact information. As a consequence of the easy access, SE research on OSS projects is popular, although only some aspects are comparable for corporate SE [23]. Also, there is a risk of revealing personal identities and data if interviewees are selected via open source software, for example, from an OSS community [20].\nprimarily focuses on the transparency of analysis methods and procedures.\nSince the data collected in SE research varies across these degrees and factors, a conceptual framework and recommendations for open data must take them into account.\nIn the research field of Open data ecosystems [25], the concern about sharing data between commercial actors is addressed, as well as open data from governmental and other public sources. Based on a survey of the literature, Enders et al. explored factors to take into account when deciding the degree of openness for data, i.e. selective revealing of data [6]. These factors are (1) Coreness (closeness to the core business), (2) Currentness (how recent is the data), (3) Extent (volume of data), (4) Granularity (level of detail), (5) Interoperability (e.g, standardized formats), and (6) Quality (fit for purpose).",
    "context": "This chunk details the categorization of empirical data collection methods in software engineering – direct involvement (interviews, eye-tracking), indirect involvement (observational data), and study of artifacts (code). It highlights the shared ethical and legal concerns regarding personal and company data protection across these methods, particularly emphasizing the potential for revealing sensitive information through data like eye-tracking results and commercial code. It also introduces the concept of mining software repositories and the challenges of reproducibility within that field.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      2
    ],
    "id": "d9f30ae387268f0d4de08e612fb6b4aaff46b1fd9cde63e6723267e5f9eafd58"
  },
  {
    "text": "Socio-technical research in SE embraces both quantitative and qualitative data. 'Quantitative data is more exact, while qualitative data is 'richer' in what it may express.' [24, p.15]. Quantitative data are easier to summarize, e.g. through means and dispersion measures, or statistical distributions. They may also be easier anonymized, since the identities are mostly connected to the meta data and context descriptions, rather than the data itself. In some cases, removing or changing the scale of data may help addressing secrecy issues. Also quantitative data is more often encoded and does not reveal opinions, work tasks, etc. that make it possible to identify the data source.\nFor example, Chauvette et al. [3] are critical to open data for epistemological, methodological and ethical reasons. Epistemologically , qualitative data are tightly linked with the context, so changing context make them meaningless, they claim. Methodologically , the reflexivity in the qualitative analysis requires the researcher to be part of the data collection to be close enough to the studied phenomenon. Ethically , issues related to confidentiality and anonymity prevent open data.\nThere is, as far as we have found, no discussion on open qualitative research data in the literature specifically for SE, beyond open science policies for journals and conferences. However, the topic is discussed in social science and psychology, including a multitude of perspectives on the feasibility of open qualitative research data.\nOther researchers, e.g. Field et al. [7], are more neutral regarding open, qualitative data. They acknowledge the above mentioned problems, but weigh them against potential benefits. Among this, participants may want to share their data for the greater good, and improved research efficiency may speak for open data. They continue nuancing the issue, by proposing to share codebooks with 'a list of codes with associated definitions, examples and descriptions' that may add to the transparency and replicability of research.\nFinally, Joyce et al. [13], are proponents for openness and argue that 'there are several notable benefits to sharing qualitative research data'. They claim that most concerns can be addressed by good policies and practices of data repositories. This position is supported by DuBois et al. [5] who report that data sharing has been an established practice in the research field of Conversation Analysis for over three decades. They also offer practical guidelines for sharing qualitative data [4].\nThere are conceptual differences, related to open data, between qualitative and quantitative data. However, there are also differences between research domains in their principles and practices for open data. SE consequently has to find a position as a community.\n\nHighlights the contrasting characteristics of qualitative and quantitative data in software engineering research, emphasizing the challenges of open qualitative data due to contextual sensitivity and ethical concerns, while noting a lack of established practices specifically for SE.",
    "original_text": "Socio-technical research in SE embraces both quantitative and qualitative data. 'Quantitative data is more exact, while qualitative data is 'richer' in what it may express.' [24, p.15]. Quantitative data are easier to summarize, e.g. through means and dispersion measures, or statistical distributions. They may also be easier anonymized, since the identities are mostly connected to the meta data and context descriptions, rather than the data itself. In some cases, removing or changing the scale of data may help addressing secrecy issues. Also quantitative data is more often encoded and does not reveal opinions, work tasks, etc. that make it possible to identify the data source.\nFor example, Chauvette et al. [3] are critical to open data for epistemological, methodological and ethical reasons. Epistemologically , qualitative data are tightly linked with the context, so changing context make them meaningless, they claim. Methodologically , the reflexivity in the qualitative analysis requires the researcher to be part of the data collection to be close enough to the studied phenomenon. Ethically , issues related to confidentiality and anonymity prevent open data.\nThere is, as far as we have found, no discussion on open qualitative research data in the literature specifically for SE, beyond open science policies for journals and conferences. However, the topic is discussed in social science and psychology, including a multitude of perspectives on the feasibility of open qualitative research data.\nOther researchers, e.g. Field et al. [7], are more neutral regarding open, qualitative data. They acknowledge the above mentioned problems, but weigh them against potential benefits. Among this, participants may want to share their data for the greater good, and improved research efficiency may speak for open data. They continue nuancing the issue, by proposing to share codebooks with 'a list of codes with associated definitions, examples and descriptions' that may add to the transparency and replicability of research.\nFinally, Joyce et al. [13], are proponents for openness and argue that 'there are several notable benefits to sharing qualitative research data'. They claim that most concerns can be addressed by good policies and practices of data repositories. This position is supported by DuBois et al. [5] who report that data sharing has been an established practice in the research field of Conversation Analysis for over three decades. They also offer practical guidelines for sharing qualitative data [4].\nThere are conceptual differences, related to open data, between qualitative and quantitative data. However, there are also differences between research domains in their principles and practices for open data. SE consequently has to find a position as a community.",
    "context": "Highlights the contrasting characteristics of qualitative and quantitative data in software engineering research, emphasizing the challenges of open qualitative data due to contextual sensitivity and ethical concerns, while noting a lack of established practices specifically for SE.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      2,
      3
    ],
    "id": "84d4d0a0c7dd3ca4e7f45795b8196e3d2026a44ec337dc23a6f60c4027d18137"
  },
  {
    "text": "Wehypothesize that the ethical and legal conditions for open data in socio-technical SE research differs between data collection methods.\nInterview data are also first degree data, but primarily qualitative . Thus, researchers are in direct contact with respondents and can get their consent. However, the richness and strong connection to context in qualitative data makes it harder to anonymize data to enable openness. Similar conditions hold for focus group data .\nSurvey data are first degree data, primarily quantitative or semiquantitative (e.g. Likert scales). This implies that respondents may give consent to openness and that the opportunities for anonymization are good, particularly for large populations and samples.\nHuman-enacted observational data are also first degree data, mostly constituted of qualitative data. In contrast to interviews and focus group data, observed participants are (by design) less aware of the researcher's presence. Consequently, the participants have less control over the data they provide. Such data must be more actively cleared from sensitive and irrelevant information.\nFinally, archival data or work artifacts are third degree data. Since they are derived for other primary purposes, the openness must be considered in relation to the original contributors. Company internal artifacts are rarely possible to open, while OSS artifacts are open by definition. However, open data may still be personal, e.g. defect reports and commits, and ethical and legal concerns in relation to individuals and their data must be properly handled.\nTechnology-based observational data are second degree data and mostly of quantitative character. Thus, the data may be more easily separated from the context compared to qualitative data. However, the interpretation of the quantitative data is highly related to the context, and thus the value is reduced by anonymization. A special case is instrumented data for learning , where the data is collected by technical instruments, used to train machine learning algorithms. There are several ethical and legal concerns raised regarding these technologies, e.g. in relation to Microsoft's CoPilot 5 .\nDepending on the legal and ethical conditions for each data collection method, these must be reflected in how data is handled with respect to openness.\n\nData collection methods significantly impact open data considerations; qualitative data, like interviews and observations, present challenges due to contextual richness and participant awareness, while quantitative data, such as surveys, offer better anonymization potential. Archival data, derived from other sources, require careful attention to original contributors’ rights. Technology-based observational data, particularly instrumented data, raise unique ethical and legal concerns.",
    "original_text": "Wehypothesize that the ethical and legal conditions for open data in socio-technical SE research differs between data collection methods.\nInterview data are also first degree data, but primarily qualitative . Thus, researchers are in direct contact with respondents and can get their consent. However, the richness and strong connection to context in qualitative data makes it harder to anonymize data to enable openness. Similar conditions hold for focus group data .\nSurvey data are first degree data, primarily quantitative or semiquantitative (e.g. Likert scales). This implies that respondents may give consent to openness and that the opportunities for anonymization are good, particularly for large populations and samples.\nHuman-enacted observational data are also first degree data, mostly constituted of qualitative data. In contrast to interviews and focus group data, observed participants are (by design) less aware of the researcher's presence. Consequently, the participants have less control over the data they provide. Such data must be more actively cleared from sensitive and irrelevant information.\nFinally, archival data or work artifacts are third degree data. Since they are derived for other primary purposes, the openness must be considered in relation to the original contributors. Company internal artifacts are rarely possible to open, while OSS artifacts are open by definition. However, open data may still be personal, e.g. defect reports and commits, and ethical and legal concerns in relation to individuals and their data must be properly handled.\nTechnology-based observational data are second degree data and mostly of quantitative character. Thus, the data may be more easily separated from the context compared to qualitative data. However, the interpretation of the quantitative data is highly related to the context, and thus the value is reduced by anonymization. A special case is instrumented data for learning , where the data is collected by technical instruments, used to train machine learning algorithms. There are several ethical and legal concerns raised regarding these technologies, e.g. in relation to Microsoft's CoPilot 5 .\nDepending on the legal and ethical conditions for each data collection method, these must be reflected in how data is handled with respect to openness.",
    "context": "Data collection methods significantly impact open data considerations; qualitative data, like interviews and observations, present challenges due to contextual richness and participant awareness, while quantitative data, such as surveys, offer better anonymization potential. Archival data, derived from other sources, require careful attention to original contributors’ rights. Technology-based observational data, particularly instrumented data, raise unique ethical and legal concerns.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      3
    ],
    "id": "2a0175332c45a13cfa875336eb4996ed01bc6d94292616ac512e4b69e41b2427"
  },
  {
    "text": "Artifact evaluation has emerged as an open research practice in computer science during the last decade 6 . It aims towards improved quality and transparency, by supporting reproducible research. ACM has established a policy with three separate badge categories for papers published with ACM, related to artifact review, namely Artifacts Evaluated (Functional, Reusable), Artifacts Available, and Results Validated (Reproduced, Replicated) 7 .\n5 https://www.sdxcentral.com/articles/news/github-copilot-raises-ownership-\n6 https://artifact-eval.org\nethical-concerns/2022/07/\n7 https://www.acm.org/publications/policies/artifact-review-and-badging-current\nFor software reuse in a broader sense, it is acknowledged that software reuse without any strategy or support is difficult and software must be prepared to make it reusable. For example, Belfadel et al. [1] propose an Enterprise Architecture Capability Framework, with the aim to increase reuse of software by upgrading technical components to match end-user's requirements. However, software can be made available for future reuse in several ways. Publishing software as Open Source is another way of using software available to a broader audience and therefore used in more systems.\nWhile open data includes aspects related to human subjects from which empirical data is collected, artifact openness is primarily an issue on the researcher side. Researchers have to decide what degree of openness they apply with respect to their intellectual property and with respect to the effort it takes to make the artifacts openly available, support their usage, etc.\n\nHighlights the emergence of artifact evaluation as an open research practice within computer science, emphasizing that artifact openness is largely determined by researchers’ decisions regarding intellectual property and the effort involved in making artifacts accessible.",
    "original_text": "Artifact evaluation has emerged as an open research practice in computer science during the last decade 6 . It aims towards improved quality and transparency, by supporting reproducible research. ACM has established a policy with three separate badge categories for papers published with ACM, related to artifact review, namely Artifacts Evaluated (Functional, Reusable), Artifacts Available, and Results Validated (Reproduced, Replicated) 7 .\n5 https://www.sdxcentral.com/articles/news/github-copilot-raises-ownership-\n6 https://artifact-eval.org\nethical-concerns/2022/07/\n7 https://www.acm.org/publications/policies/artifact-review-and-badging-current\nFor software reuse in a broader sense, it is acknowledged that software reuse without any strategy or support is difficult and software must be prepared to make it reusable. For example, Belfadel et al. [1] propose an Enterprise Architecture Capability Framework, with the aim to increase reuse of software by upgrading technical components to match end-user's requirements. However, software can be made available for future reuse in several ways. Publishing software as Open Source is another way of using software available to a broader audience and therefore used in more systems.\nWhile open data includes aspects related to human subjects from which empirical data is collected, artifact openness is primarily an issue on the researcher side. Researchers have to decide what degree of openness they apply with respect to their intellectual property and with respect to the effort it takes to make the artifacts openly available, support their usage, etc.",
    "context": "Highlights the emergence of artifact evaluation as an open research practice within computer science, emphasizing that artifact openness is largely determined by researchers’ decisions regarding intellectual property and the effort involved in making artifacts accessible.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      3
    ],
    "id": "566f556221195b672e1d4c529739f9463e6f05d03ecbef0524b387f0ba5e7a83"
  },
  {
    "text": "We present the Gander research project as a case to identify a multitude of research data and related open data concerns. The objective of the Gander project was to gain an increased understanding of code review in practice and to use this understanding to inform the design of new code review tooling. The project had two main components; 1) an empirical component with focus on problem conceptualization, providing input to the tool design - conducted as a mixed-method study with practitioners in industry [29, 32], and 2) a development component focused on development of an experimental code review platform incorporating eye-tracking [9, 28]. A key aspect in the project was to explore how gaze data from eyetracking can be used to trigger assistance in code review tools. The Gander project was a continuation on a line of research started by the second author in two earlier studies connecting to code review, carried out in an industrial setting [26, 27].\nOne aim with the platform development in the Gander project was to build an experimental code review setup that would allow for more realistic code review experiments closer to practitioners [16], i.e., outside of the lab environment and with realistic data. With this goal in mind, the platform strives to provide a similar look-and-feel as well-used code review environments like GitLab or GitHub (with regard to the textual diff view) and it should be easy to populate the platform with realistic samples from open-source. The latter resulted in a connection to GitHub to facilitate the experimental setup process. Finally, the platform should be easy to run outside the lab, which means it has been developed using portable eye-trackers used with a laptop to increase the mobility of the setup. Figure 1\nThe empirical code review study [29, 32] in the Gander project included a series of 12 semi-structured interviews with practitioners at two companies, about the experience of code review (below referred to as Data and artifact set 1 ). The interviews were recorded after informed consent and transcribed for thematic analysis. They were followed by a survey, based on the interview results, that rendered replies from 78 practitioners ( Data and artifact set 2 ). The survey results were gathered, coded, and summarized for reporting. Neither the qualitative nor the quantitative data gathered in this study have been shared as supplementary material to any publication, but the protocols were shared during the review process.\nFigure 1: A screenshot of the Gander platform showing a textual diff view in replay mode, where the interaction of a session is replayed from logged interaction and eye-tracking data.\nshows a screenshot of the Gander platform where the textual diff is populated with data from the FlappyBird project on GitHub.\nAs a proof-of-concept, the Gander platform was used to develop a simple gaze assistant that triggers visualisation of use-declaration relationships in the code based on gaze fixation point on, for instance, variable names. This assistant was tested in a user study with eight participants. During the study, participants were given a number of tasks to solve on the Gander platform with the gaze assistant enabled and then they were interviewed about their experience ( Data and artifact sets 3a and b ). The study included both quantitative data gathering, in the form of interaction logs and eye-tracking data ( Data and artifacts sets 3c and d ), and qualitative data in the form of interviews recorded after informed consent. For this study, the quantitative data has been shared as a data set, both serving as supplementary material to the publication about the platform [28] and as a test set for how to use the replay function in the platform which has been released as open-source [9].\nThe connection to eye-tracking and processing of gaze data is central to the design of the platform. The architecture is structured around the needs of processing eye-tracking data, for replay of sessions with participants and for exploration of real-time gazebased assistance during a code review session. Gaze data is analyzed in real-time to detect fixation points which can be connected to programming language elements which may correspond to areas of interest. This data processing can be used to trigger assistance in response to certain gaze behavior in relation to the content of the code being reviewed.\nIn releasing the platform as open-source, licenses of any system used in the project, e.g., the JastAdd 8 project and the ExtendJ 9 project, had to be considered. During this review, it became clear that one of the used projects (for gaze data analysis) was available online but did not have a licence. However, after reaching out to the author of that project a licence file was added (the MIT license) and the use of the project remained unchanged. After considering the interaction of licenses in used projects and after discussion within the contributor team, a BSD license was selected for the open-source project.\n8 https://jastadd.org\n9 https://extendj.org\nThe project is conducted at Lund University, Sweden, which like many higher research institutes and funding agencies has an increasing focus on open science. Open science is one of the prioritised issues in the university's Research strategy 2023-26, although there is not yet any mandatory prescriptions. The Swedish Research Council, which is one of the funding agencies of this work, requires a data management plan (DMP) to be created and maintained for all projects funded 2019 or later, while the other funding agencies for the research do not yet have any requirements on open science. A data management plan was created in Lund university's DMP system, but it is not public. Creating a DMP is a first step towards fostering open data sciences, although there are no specified requirements on openess, neither from the university nor the funding agencies.\n\nThe chunk details the Gander research project, focusing on its development of a code review platform incorporating eye-tracking data. It highlights the project’s aim to understand code review practices and its use of open-source components, including a gaze assistant. Notably, the data gathered from the project, including interviews and quantitative data, has not been shared publicly, though protocols were shared during the review process.",
    "original_text": "We present the Gander research project as a case to identify a multitude of research data and related open data concerns. The objective of the Gander project was to gain an increased understanding of code review in practice and to use this understanding to inform the design of new code review tooling. The project had two main components; 1) an empirical component with focus on problem conceptualization, providing input to the tool design - conducted as a mixed-method study with practitioners in industry [29, 32], and 2) a development component focused on development of an experimental code review platform incorporating eye-tracking [9, 28]. A key aspect in the project was to explore how gaze data from eyetracking can be used to trigger assistance in code review tools. The Gander project was a continuation on a line of research started by the second author in two earlier studies connecting to code review, carried out in an industrial setting [26, 27].\nOne aim with the platform development in the Gander project was to build an experimental code review setup that would allow for more realistic code review experiments closer to practitioners [16], i.e., outside of the lab environment and with realistic data. With this goal in mind, the platform strives to provide a similar look-and-feel as well-used code review environments like GitLab or GitHub (with regard to the textual diff view) and it should be easy to populate the platform with realistic samples from open-source. The latter resulted in a connection to GitHub to facilitate the experimental setup process. Finally, the platform should be easy to run outside the lab, which means it has been developed using portable eye-trackers used with a laptop to increase the mobility of the setup. Figure 1\nThe empirical code review study [29, 32] in the Gander project included a series of 12 semi-structured interviews with practitioners at two companies, about the experience of code review (below referred to as Data and artifact set 1 ). The interviews were recorded after informed consent and transcribed for thematic analysis. They were followed by a survey, based on the interview results, that rendered replies from 78 practitioners ( Data and artifact set 2 ). The survey results were gathered, coded, and summarized for reporting. Neither the qualitative nor the quantitative data gathered in this study have been shared as supplementary material to any publication, but the protocols were shared during the review process.\nFigure 1: A screenshot of the Gander platform showing a textual diff view in replay mode, where the interaction of a session is replayed from logged interaction and eye-tracking data.\nshows a screenshot of the Gander platform where the textual diff is populated with data from the FlappyBird project on GitHub.\nAs a proof-of-concept, the Gander platform was used to develop a simple gaze assistant that triggers visualisation of use-declaration relationships in the code based on gaze fixation point on, for instance, variable names. This assistant was tested in a user study with eight participants. During the study, participants were given a number of tasks to solve on the Gander platform with the gaze assistant enabled and then they were interviewed about their experience ( Data and artifact sets 3a and b ). The study included both quantitative data gathering, in the form of interaction logs and eye-tracking data ( Data and artifacts sets 3c and d ), and qualitative data in the form of interviews recorded after informed consent. For this study, the quantitative data has been shared as a data set, both serving as supplementary material to the publication about the platform [28] and as a test set for how to use the replay function in the platform which has been released as open-source [9].\nThe connection to eye-tracking and processing of gaze data is central to the design of the platform. The architecture is structured around the needs of processing eye-tracking data, for replay of sessions with participants and for exploration of real-time gazebased assistance during a code review session. Gaze data is analyzed in real-time to detect fixation points which can be connected to programming language elements which may correspond to areas of interest. This data processing can be used to trigger assistance in response to certain gaze behavior in relation to the content of the code being reviewed.\nIn releasing the platform as open-source, licenses of any system used in the project, e.g., the JastAdd 8 project and the ExtendJ 9 project, had to be considered. During this review, it became clear that one of the used projects (for gaze data analysis) was available online but did not have a licence. However, after reaching out to the author of that project a licence file was added (the MIT license) and the use of the project remained unchanged. After considering the interaction of licenses in used projects and after discussion within the contributor team, a BSD license was selected for the open-source project.\n8 https://jastadd.org\n9 https://extendj.org\nThe project is conducted at Lund University, Sweden, which like many higher research institutes and funding agencies has an increasing focus on open science. Open science is one of the prioritised issues in the university's Research strategy 2023-26, although there is not yet any mandatory prescriptions. The Swedish Research Council, which is one of the funding agencies of this work, requires a data management plan (DMP) to be created and maintained for all projects funded 2019 or later, while the other funding agencies for the research do not yet have any requirements on open science. A data management plan was created in Lund university's DMP system, but it is not public. Creating a DMP is a first step towards fostering open data sciences, although there are no specified requirements on openess, neither from the university nor the funding agencies.",
    "context": "The chunk details the Gander research project, focusing on its development of a code review platform incorporating eye-tracking data. It highlights the project’s aim to understand code review practices and its use of open-source components, including a gaze assistant. Notably, the data gathered from the project, including interviews and quantitative data, has not been shared publicly, though protocols were shared during the review process.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      3,
      4
    ],
    "id": "1782f51bf8d5829dae2db01fdacaa942735963b7b91c953611d8bf2d247ba1db"
  },
  {
    "text": "To guide the analysis and discussion on open science in sociotechnical software engineering, we define a conceptual framework of the execution of a research study, and the analysis and generalization of research artifacts. The framework emerged during our post-mortem analysis of the research projects, its actors, data, and artifacts, by iterating over the following principal steps.\n- (1) Identify participants and other stakeholders\n- (3) Identify artifacts developed for and in the project\n- (2) Identify data collected, and analyze types of data and corresponding analysis processes\nWe identified specific instances of the Gander projects and then abstracted the framework elements towards more general concepts.\nThe framework has three main concepts: participants, data and artifacts, as shown in Figure 2. The participants have different roles and relations to the research endeavour, while the data and artifacts are refined and evolved in separate pipelines.\n\nThe analysis focuses on a framework for understanding research studies, specifically outlining the roles of participants, data, and artifacts within a research process. It highlights the distinct stages of data collection, analysis, and the eventual generalization of research findings, emphasizing the challenges of open data sharing due to privacy and intellectual property concerns.",
    "original_text": "To guide the analysis and discussion on open science in sociotechnical software engineering, we define a conceptual framework of the execution of a research study, and the analysis and generalization of research artifacts. The framework emerged during our post-mortem analysis of the research projects, its actors, data, and artifacts, by iterating over the following principal steps.\n- (1) Identify participants and other stakeholders\n- (3) Identify artifacts developed for and in the project\n- (2) Identify data collected, and analyze types of data and corresponding analysis processes\nWe identified specific instances of the Gander projects and then abstracted the framework elements towards more general concepts.\nThe framework has three main concepts: participants, data and artifacts, as shown in Figure 2. The participants have different roles and relations to the research endeavour, while the data and artifacts are refined and evolved in separate pipelines.",
    "context": "The analysis focuses on a framework for understanding research studies, specifically outlining the roles of participants, data, and artifacts within a research process. It highlights the distinct stages of data collection, analysis, and the eventual generalization of research findings, emphasizing the challenges of open data sharing due to privacy and intellectual property concerns.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      4
    ],
    "id": "e45de937396e2485140d34ced084312e75d0ede0fafb2d0fc2cf5d1abff07577"
  },
  {
    "text": "Weidentify three typical categories of participants in socio-technical software engineering research, namely (1) employees or other stakeholders of software development organizations (marked with round cap in Figure 2), (2) students or other beneficiaries of the university involved (marked with square cap), and (3) independent participants (without cap). The categorization is conducted based on legal and ethical concerns in the relation between researchers and participants, and consequently the openness of data and artifacts emerging from the research. For example, employees or students may feel pressure to participate in a study, even if participation should be fully voluntary. Further, employees may participate under certain secrecy conditions bound by their employment contract.\n\nHighlights the ethical considerations surrounding participant recruitment and data openness due to potential pressures and confidentiality agreements.",
    "original_text": "Weidentify three typical categories of participants in socio-technical software engineering research, namely (1) employees or other stakeholders of software development organizations (marked with round cap in Figure 2), (2) students or other beneficiaries of the university involved (marked with square cap), and (3) independent participants (without cap). The categorization is conducted based on legal and ethical concerns in the relation between researchers and participants, and consequently the openness of data and artifacts emerging from the research. For example, employees or students may feel pressure to participate in a study, even if participation should be fully voluntary. Further, employees may participate under certain secrecy conditions bound by their employment contract.",
    "context": "Highlights the ethical considerations surrounding participant recruitment and data openness due to potential pressures and confidentiality agreements.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      4
    ],
    "id": "9b11cfbcb656a5a45ac5870a26301315dde05db2a397690abbdb12c289f21edb"
  },
  {
    "text": "Regarding our research project data we observe that the analysis process of research data at a general level resembles a data pipeline . In a typical research project the researcher starts with detailed raw data, analyse that data in a sequence of steps and ends up with a set of findings. The data from the last step, i.e., the findings, typically consist of data that is 'open' in the sense of being published, while the results from the steps prior to that are increasingly more difficult to make publicly available, due to privacy and secrecy concerns.\nFigure 2: Graphical representation of the conceptual framework with steps of data analysis and material generalization. Data sources to the left, the data pipeline top right and artifact pipeline bottom right.\nFor example, in a qualitative research project with interview data, it is common to publish the general findings of an interview, but the audio recordings from the interview are rarely openly available nor are the full transcripts. In terms of openness factors, discussed by Enders et al. [6] (see Section 2.1), audio recordings and transcripts are of finer granularity than the abstracted findings, and thus harder to make open than the course grained findings. Consequently, not only the raw data, but the full analysis pipeline contains relevant concepts for open science.\n- In a quantitative study this would mean statistical analysis, e.g., building prediction models, hypothesis testing, etc. In a study this would involve defining a set of\nThe conceptual data pipeline, illustrated in the top right part of Figure 2, is divided into four steps which are inspired by Majeed and Hwang [18] from the field of data science, and illustrated by typical study examples below.\n- (1) Data cleaning:\n2. (2)\nIn a quantitative study this involves transforming the data into a readable form for statistical tools, which may, for example, involve coding of the data based on a pre-defined scheme. It may also include anonymizing the data. In a qualitative study, this typically includes transcribing the data, as well as anonymization.\nIn a qualitative study this would include getting a first understanding of the data and probably a first idea of procedures for coding.\n- Data exploration and visualization: In a quantitative study this includes investigating descriptive statistics and visualising data. This is also a natural step for identifying outliers.\n- (3) Model building and analysis:\n- qualitative codes, coding, obtaining findings, and potentially theory building, in an iterative manner.\n- (4) Findings presentation: Findings are commonly presented in journal/conference publications, technical reports, or similarly, including data and analyses to support the findings.\nGenerally, outcomes from the later stages of the data pipeline are less sensitive to share openly, both with respect to participants' privacy and potential company secrets.\n\nDescribes a research data pipeline, outlining how raw data is analyzed in stages, with findings being the most publicly available due to privacy concerns.",
    "original_text": "Regarding our research project data we observe that the analysis process of research data at a general level resembles a data pipeline . In a typical research project the researcher starts with detailed raw data, analyse that data in a sequence of steps and ends up with a set of findings. The data from the last step, i.e., the findings, typically consist of data that is 'open' in the sense of being published, while the results from the steps prior to that are increasingly more difficult to make publicly available, due to privacy and secrecy concerns.\nFigure 2: Graphical representation of the conceptual framework with steps of data analysis and material generalization. Data sources to the left, the data pipeline top right and artifact pipeline bottom right.\nFor example, in a qualitative research project with interview data, it is common to publish the general findings of an interview, but the audio recordings from the interview are rarely openly available nor are the full transcripts. In terms of openness factors, discussed by Enders et al. [6] (see Section 2.1), audio recordings and transcripts are of finer granularity than the abstracted findings, and thus harder to make open than the course grained findings. Consequently, not only the raw data, but the full analysis pipeline contains relevant concepts for open science.\n- In a quantitative study this would mean statistical analysis, e.g., building prediction models, hypothesis testing, etc. In a study this would involve defining a set of\nThe conceptual data pipeline, illustrated in the top right part of Figure 2, is divided into four steps which are inspired by Majeed and Hwang [18] from the field of data science, and illustrated by typical study examples below.\n- (1) Data cleaning:\n2. (2)\nIn a quantitative study this involves transforming the data into a readable form for statistical tools, which may, for example, involve coding of the data based on a pre-defined scheme. It may also include anonymizing the data. In a qualitative study, this typically includes transcribing the data, as well as anonymization.\nIn a qualitative study this would include getting a first understanding of the data and probably a first idea of procedures for coding.\n- Data exploration and visualization: In a quantitative study this includes investigating descriptive statistics and visualising data. This is also a natural step for identifying outliers.\n- (3) Model building and analysis:\n- qualitative codes, coding, obtaining findings, and potentially theory building, in an iterative manner.\n- (4) Findings presentation: Findings are commonly presented in journal/conference publications, technical reports, or similarly, including data and analyses to support the findings.\nGenerally, outcomes from the later stages of the data pipeline are less sensitive to share openly, both with respect to participants' privacy and potential company secrets.",
    "context": "Describes a research data pipeline, outlining how raw data is analyzed in stages, with findings being the most publicly available due to privacy concerns.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      4,
      5
    ],
    "id": "5fe973cad82c579c4c022c445d1548343ebf66d9d279c73e7f43c56ed1bed0ff"
  },
  {
    "text": "Research artifacts may emerge from a study, like questionnaires and other tools for data collection, software script for analysing qualitative data, software tools for illustration of the research conducted (e.g., a tool for managing code reviews in a code review experiment) or the studied code itself.\nThis type of research artifacts can be used in studies where results are validated by other research groups repeating the studies. In ACM terminology a study may be reproduced (different team, same\nexperiment setup) or replicated (different team, different experiment setup) 10\nInspired by Belfadel et al. [1], we identify three steps of artifact generalization towards increasing reuse:\nIn these situations artifacts can either be input, or serve as background information. For example, in one case exactly the same research is conducted by another group, and in another case studies build on the research, but develop or adapt artifacts and introduce them in a new context. Notice that these terms are used in different ways by different researchers. In Empirical Software Engineering, the term replication can mean conducting new studies similar to previously conducted studies (see, e.g., Gómez et al. [12]).\n- i) Publication for reproduction, resulting in artifacts in original state (non-editable documents, executable code, etc).\n2. iii) Generalization for continued development, resulting in artifacts released with guidance how to adapt it, e.g., by inviting to a community.\n3. ii) Generalization for general use, resulting in artifacts in editable state (editable documents, editable source code, etc).\nEach of these steps require additional investments in making the artifacts openly available. The first step (i) focuses on transparency through accessibility, connecting to the goals of the ACM artifact badges. To advance the research, access to editable artifacts are needed (step ii). To build a community (step iii) around tools or other research artifacts, requires governance effort, like for any open source software.\nThese three dimensions constitute our conceptual framework, and is used next to analyze data and artifacts in the Gander case.\n\nProvides a framework for analyzing research artifacts and their openness, outlining three steps of artifact generalization for increasing reuse – publication for reproduction, generalization for continued development, and generalization for general use.",
    "original_text": "Research artifacts may emerge from a study, like questionnaires and other tools for data collection, software script for analysing qualitative data, software tools for illustration of the research conducted (e.g., a tool for managing code reviews in a code review experiment) or the studied code itself.\nThis type of research artifacts can be used in studies where results are validated by other research groups repeating the studies. In ACM terminology a study may be reproduced (different team, same\nexperiment setup) or replicated (different team, different experiment setup) 10\nInspired by Belfadel et al. [1], we identify three steps of artifact generalization towards increasing reuse:\nIn these situations artifacts can either be input, or serve as background information. For example, in one case exactly the same research is conducted by another group, and in another case studies build on the research, but develop or adapt artifacts and introduce them in a new context. Notice that these terms are used in different ways by different researchers. In Empirical Software Engineering, the term replication can mean conducting new studies similar to previously conducted studies (see, e.g., Gómez et al. [12]).\n- i) Publication for reproduction, resulting in artifacts in original state (non-editable documents, executable code, etc).\n2. iii) Generalization for continued development, resulting in artifacts released with guidance how to adapt it, e.g., by inviting to a community.\n3. ii) Generalization for general use, resulting in artifacts in editable state (editable documents, editable source code, etc).\nEach of these steps require additional investments in making the artifacts openly available. The first step (i) focuses on transparency through accessibility, connecting to the goals of the ACM artifact badges. To advance the research, access to editable artifacts are needed (step ii). To build a community (step iii) around tools or other research artifacts, requires governance effort, like for any open source software.\nThese three dimensions constitute our conceptual framework, and is used next to analyze data and artifacts in the Gander case.",
    "context": "Provides a framework for analyzing research artifacts and their openness, outlining three steps of artifact generalization for increasing reuse – publication for reproduction, generalization for continued development, and generalization for general use.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      5,
      6
    ],
    "id": "f9f243e04366dd30eb008bd9bc18d3ff03fb2269165827731c0cd9ca9ede5ddc"
  },
  {
    "text": "To demonstrate potential use of the conceptual framework from Figure 2, we extract data sets and artifacts shared in the Gander case and list them in Table 1. Further, based on trade-offs in our case with respect to different data sets, we propose recommendations for open data practices, summarized in framed boxes below. When possible, we have also indicated which ACM badge level we believe that the recommendation supports.\nThe data gathered in the empirical part of the Gander project, data set (1a) and (2a), went through the pipeline of Figure 2, i.e., data cleaning, data exploration and visualisation, model building and analysis, and were then shared in anonymized and summarized form in the presentation of the results [29, 32]. The protocols from the empirical part were shared as metadata (Step i) for inspection during the review process.\nThe data sets and artifacts are split into three sections; the semistructured interviews of the empirical part of the project (1), the survey part of the same empirical part (2), and the user study connected to the development part of the project (3).\nSimilarly, for the development part of the Gander project, the interview data gathered during the user study, data set (3a), were shared in anonymized and summarized form in the presentation of the results [28]. The protocols for the interviews were shared\n10 https://www.acm.org/publications/policies/artifact-review-badging Notice that ACM has recently swapped the meaning of the two terms after discussion with the National Information Standards Organization.\nas metadata (Step i) both during the review process and also later. In addition, the session data gathered during the user study, in the form of interaction data and gaze data, data set (3c), was shared as anonymized data (Data step 1) along side the experimental setup used, the Gander platform (Artifact step iii). The purpose of sharing the Gander platform is to contribute to the research community and to enable and facilitate further research into gaze-assisted code review tooling. With this goal in mind, care was taken to select an appropriate license for the project and to review dependencies with regard to licenses. There was one project among the dependencies that was shared without a license (matching artifact step ii) but after discussion with the Gander team the project added a license (matching artifact step iii).\nGiving open access to research artifacts , like interview and survey protocols (artifacts 1b, 2b, and 3b) is non-controversial and mostly a matter of practical procedures for their publication and sustained accessibility. In the Gander case they were provided for peer review in the first two cases, but then not published with the papers, while published with the platform artifact in the third case. Given the space constraints of conference papers, authors are reluctant to use the space by adding such protocols as appendices. However, conferences and journals may offer online publication of supplementary material with the main publication. Alternatively, artifacts may be given persistent digital object identifiers (DOI) on their own right, although this adds to the bureaucracy burden for researchers. Providing access through a university's persistent storage, like in our third case [8], is convenient for the researchers, although not optimal from a traceability point of view.\nR1 . We therefore advice open research artifacts be given persistent DOI to enable traceability, independently of storage solution - as long as it is persistent enough. [ACM Available]\nProviding research platform artifacts as open source software is a highly recommended practice. The Gander platform builds on other open source projects, which helped speed up the development. However, the licensing issues reported in Section 3 demonstrates clearly, that the artifact step ii is not sufficient to build further research on. This is both due to the unclear licensing situation, and lack of community that might respond to questions and improvement proposals. In our case, the issue was sorted out in dialogue with the originating author, but that is not a scalable solution.\nThere might be conflicting interests with opening research artifacts, if the originator aims to commercialise the material or some services or products build thereon. However, we still advocate for open source solutions, which actually may be compatible with business models, such as freemium [22] or servitization [15].\nR2 . We advice research software be made open source with an appropriate license. We further advice research institutes and funding agencies to cover costs related to governing OSS communities for such software. [ACM Artifacts Evaluated - Functional]\nAccess to research data is more sensitive and is in the Gander case published only in synthesized form in the publications. Both data sets (1a) and (2a), i.e. qualitative interview data and quantitative survey data, were collected within the same two multi-national companies.\nTable 1: Data and artifacts made open from the Gander case. * Shared as part of the peer review process but not after.\n\n1a) Semi-structured interview data (12 participants) 1b) Semi-structured interview protocol, Class = First -. 1a) Semi-structured interview data (12 participants) 1b) Semi-structured interview protocol, Kind = Qual Artifact. 1a) Semi-structured interview data (12 participants) 1b) Semi-structured interview protocol, Participants = Industry -. 1a) Semi-structured interview data (12 participants) 1b) Semi-structured interview protocol, Figure 2 step = 4 [29, 32] i*. 1a) Semi-structured interview data (12 participants) 1b) Semi-structured interview protocol, Purpose = Conceptualization. 2a) Survey data (78 practitioners) 2b) Survey protocol, Class = First -. 2a) Survey data (78 practitioners) 2b) Survey protocol, Kind = Quant Artifact. 2a) Survey data (78 practitioners) 2b) Survey protocol, Participants = Industry -. 2a) Survey data (78 practitioners) 2b) Survey protocol, Figure 2 step = 2, 3, 4 [29] i*. 2a) Survey data (78 practitioners) 2b) Survey protocol, Purpose = Conceptualization. 3a) User study interview data (8 participants) 3b) User study protocol 3c) User study gaze and interaction data 3d) User study software (the Gander platform), Class = First - Second -. 3a) User study interview data (8 participants) 3b) User study protocol 3c) User study gaze and interaction data 3d) User study software (the Gander platform), Kind = Qual Artifact Quant Artifact. 3a) User study interview data (8 participants) 3b) User study protocol 3c) User study gaze and interaction data 3d) User study software (the Gander platform), Participants = Students - Students -. 3a) User study interview data (8 participants) 3b) User study protocol 3c) User study gaze and interaction data 3d) User study software (the Gander platform), Figure 2 step = 4 [28] i 1 iii. 3a) User study interview data (8 participants) 3b) User study protocol 3c) User study gaze and interaction data 3d) User study software (the Gander platform), Purpose = Validation\nThe risks related to sharing the qualitative interview data are multifold: firstly, information related to the company that is not relevant for the focus of the study might be mentioned in the interview, e.g. information about the physical design of an embedded product to come. Secondly, the information about the company is relevant, but has to be filtered before publication, e.g. a critical event for the company in relation to security that both interviewer and interviewee knew about, but still was not in the public communication. Thirdly, the interviewee could mention facts or opinions that are sensitive with respect to their own future in the company, i.e. criticising a manager for certain actions, or lack thereof. Any of these factors on their own prevents from publishing the raw interview data, both with respect to the information as such, and that it is impossible to anonymize individuals among a such small set of interviewees. On top of that, fourthly, the general criticism with respect to epistemological concerns, raised by e.g. Chauvette et al. [3], that the lack of connection to the context makes data useless. We share these concerns partially in our case, since we have a long research collaboration track record with the companies in the study, which means that the shared understanding of the software engineering practice is significant. Transcripts of the interviews might be hard to understand without the knowledge of the context, e.g. code review practices of the company.\nthe questions are asked to shame the company. Finally, the statistical analysis methods and tools enable more standardized analyses, which reduce the need for transparency in terms of open data, unless there are suspicions of fake data which should be checked.\nR4 . We recommend quantitative data be shared openly, if and only if, the data is anonymized sufficiently to protect the identity of the individual or company (if requested).\n\nProvides a detailed breakdown of data and artifact sharing practices within a socio-technical software engineering research project.",
    "original_text": "To demonstrate potential use of the conceptual framework from Figure 2, we extract data sets and artifacts shared in the Gander case and list them in Table 1. Further, based on trade-offs in our case with respect to different data sets, we propose recommendations for open data practices, summarized in framed boxes below. When possible, we have also indicated which ACM badge level we believe that the recommendation supports.\nThe data gathered in the empirical part of the Gander project, data set (1a) and (2a), went through the pipeline of Figure 2, i.e., data cleaning, data exploration and visualisation, model building and analysis, and were then shared in anonymized and summarized form in the presentation of the results [29, 32]. The protocols from the empirical part were shared as metadata (Step i) for inspection during the review process.\nThe data sets and artifacts are split into three sections; the semistructured interviews of the empirical part of the project (1), the survey part of the same empirical part (2), and the user study connected to the development part of the project (3).\nSimilarly, for the development part of the Gander project, the interview data gathered during the user study, data set (3a), were shared in anonymized and summarized form in the presentation of the results [28]. The protocols for the interviews were shared\n10 https://www.acm.org/publications/policies/artifact-review-badging Notice that ACM has recently swapped the meaning of the two terms after discussion with the National Information Standards Organization.\nas metadata (Step i) both during the review process and also later. In addition, the session data gathered during the user study, in the form of interaction data and gaze data, data set (3c), was shared as anonymized data (Data step 1) along side the experimental setup used, the Gander platform (Artifact step iii). The purpose of sharing the Gander platform is to contribute to the research community and to enable and facilitate further research into gaze-assisted code review tooling. With this goal in mind, care was taken to select an appropriate license for the project and to review dependencies with regard to licenses. There was one project among the dependencies that was shared without a license (matching artifact step ii) but after discussion with the Gander team the project added a license (matching artifact step iii).\nGiving open access to research artifacts , like interview and survey protocols (artifacts 1b, 2b, and 3b) is non-controversial and mostly a matter of practical procedures for their publication and sustained accessibility. In the Gander case they were provided for peer review in the first two cases, but then not published with the papers, while published with the platform artifact in the third case. Given the space constraints of conference papers, authors are reluctant to use the space by adding such protocols as appendices. However, conferences and journals may offer online publication of supplementary material with the main publication. Alternatively, artifacts may be given persistent digital object identifiers (DOI) on their own right, although this adds to the bureaucracy burden for researchers. Providing access through a university's persistent storage, like in our third case [8], is convenient for the researchers, although not optimal from a traceability point of view.\nR1 . We therefore advice open research artifacts be given persistent DOI to enable traceability, independently of storage solution - as long as it is persistent enough. [ACM Available]\nProviding research platform artifacts as open source software is a highly recommended practice. The Gander platform builds on other open source projects, which helped speed up the development. However, the licensing issues reported in Section 3 demonstrates clearly, that the artifact step ii is not sufficient to build further research on. This is both due to the unclear licensing situation, and lack of community that might respond to questions and improvement proposals. In our case, the issue was sorted out in dialogue with the originating author, but that is not a scalable solution.\nThere might be conflicting interests with opening research artifacts, if the originator aims to commercialise the material or some services or products build thereon. However, we still advocate for open source solutions, which actually may be compatible with business models, such as freemium [22] or servitization [15].\nR2 . We advice research software be made open source with an appropriate license. We further advice research institutes and funding agencies to cover costs related to governing OSS communities for such software. [ACM Artifacts Evaluated - Functional]\nAccess to research data is more sensitive and is in the Gander case published only in synthesized form in the publications. Both data sets (1a) and (2a), i.e. qualitative interview data and quantitative survey data, were collected within the same two multi-national companies.\nTable 1: Data and artifacts made open from the Gander case. * Shared as part of the peer review process but not after.\n\n1a) Semi-structured interview data (12 participants) 1b) Semi-structured interview protocol, Class = First -. 1a) Semi-structured interview data (12 participants) 1b) Semi-structured interview protocol, Kind = Qual Artifact. 1a) Semi-structured interview data (12 participants) 1b) Semi-structured interview protocol, Participants = Industry -. 1a) Semi-structured interview data (12 participants) 1b) Semi-structured interview protocol, Figure 2 step = 4 [29, 32] i*. 1a) Semi-structured interview data (12 participants) 1b) Semi-structured interview protocol, Purpose = Conceptualization. 2a) Survey data (78 practitioners) 2b) Survey protocol, Class = First -. 2a) Survey data (78 practitioners) 2b) Survey protocol, Kind = Quant Artifact. 2a) Survey data (78 practitioners) 2b) Survey protocol, Participants = Industry -. 2a) Survey data (78 practitioners) 2b) Survey protocol, Figure 2 step = 2, 3, 4 [29] i*. 2a) Survey data (78 practitioners) 2b) Survey protocol, Purpose = Conceptualization. 3a) User study interview data (8 participants) 3b) User study protocol 3c) User study gaze and interaction data 3d) User study software (the Gander platform), Class = First - Second -. 3a) User study interview data (8 participants) 3b) User study protocol 3c) User study gaze and interaction data 3d) User study software (the Gander platform), Kind = Qual Artifact Quant Artifact. 3a) User study interview data (8 participants) 3b) User study protocol 3c) User study gaze and interaction data 3d) User study software (the Gander platform), Participants = Students - Students -. 3a) User study interview data (8 participants) 3b) User study protocol 3c) User study gaze and interaction data 3d) User study software (the Gander platform), Figure 2 step = 4 [28] i 1 iii. 3a) User study interview data (8 participants) 3b) User study protocol 3c) User study gaze and interaction data 3d) User study software (the Gander platform), Purpose = Validation\nThe risks related to sharing the qualitative interview data are multifold: firstly, information related to the company that is not relevant for the focus of the study might be mentioned in the interview, e.g. information about the physical design of an embedded product to come. Secondly, the information about the company is relevant, but has to be filtered before publication, e.g. a critical event for the company in relation to security that both interviewer and interviewee knew about, but still was not in the public communication. Thirdly, the interviewee could mention facts or opinions that are sensitive with respect to their own future in the company, i.e. criticising a manager for certain actions, or lack thereof. Any of these factors on their own prevents from publishing the raw interview data, both with respect to the information as such, and that it is impossible to anonymize individuals among a such small set of interviewees. On top of that, fourthly, the general criticism with respect to epistemological concerns, raised by e.g. Chauvette et al. [3], that the lack of connection to the context makes data useless. We share these concerns partially in our case, since we have a long research collaboration track record with the companies in the study, which means that the shared understanding of the software engineering practice is significant. Transcripts of the interviews might be hard to understand without the knowledge of the context, e.g. code review practices of the company.\nthe questions are asked to shame the company. Finally, the statistical analysis methods and tools enable more standardized analyses, which reduce the need for transparency in terms of open data, unless there are suspicions of fake data which should be checked.\nR4 . We recommend quantitative data be shared openly, if and only if, the data is anonymized sufficiently to protect the identity of the individual or company (if requested).",
    "context": "Provides a detailed breakdown of data and artifact sharing practices within a socio-technical software engineering research project.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      6,
      7
    ],
    "id": "4afc9d300975fc1ec80fce9fe6b1847bd4a82ab86b8b60608b70ff8447564c48"
  },
  {
    "text": "Finally, we have a set of data (3c) which is collected through 'human-enacted inquisitive and observational techniques', i.e. eyetracking data. In the case where raw image data is collected, the data is by definition personal and non-anonymizable, since the eye iris is possible to uniquely identifiable to persons. Thus, in such cases anonymization of eye-tracking data must take place to an abstracted level, e.g. eye movement positions. In the case of the Gander project, the eye-tracker used does not collect iris images, but rather details such as left and right gaze positions and pupil diameter.\nR5 . In case of data that can be directly or indirectly traceable to individuals, it cannot be open. Transforming such data into anonymized forms may enable publication.\nWe derived this conceptual framework from one line of research in software engineering, in the context of our experience of many years of empirical software engineering research. The project contains a multitude of data and artifacts, and our collective experience is extensive. Still we do not claim this conceptual framework is generally applicable nor in a final state. We therefore invite the research community to validate and further extend the framework and its recommendations for practice.\n\nFocuses on the challenges of sharing eye-tracking data due to its potential for individual identification, necessitating abstraction techniques like gaze position data rather than raw image data.",
    "original_text": "Finally, we have a set of data (3c) which is collected through 'human-enacted inquisitive and observational techniques', i.e. eyetracking data. In the case where raw image data is collected, the data is by definition personal and non-anonymizable, since the eye iris is possible to uniquely identifiable to persons. Thus, in such cases anonymization of eye-tracking data must take place to an abstracted level, e.g. eye movement positions. In the case of the Gander project, the eye-tracker used does not collect iris images, but rather details such as left and right gaze positions and pupil diameter.\nR5 . In case of data that can be directly or indirectly traceable to individuals, it cannot be open. Transforming such data into anonymized forms may enable publication.\nWe derived this conceptual framework from one line of research in software engineering, in the context of our experience of many years of empirical software engineering research. The project contains a multitude of data and artifacts, and our collective experience is extensive. Still we do not claim this conceptual framework is generally applicable nor in a final state. We therefore invite the research community to validate and further extend the framework and its recommendations for practice.",
    "context": "Focuses on the challenges of sharing eye-tracking data due to its potential for individual identification, necessitating abstraction techniques like gaze position data rather than raw image data.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      7
    ],
    "id": "24819910ba8712ebd113856f6b29de125368d8ea40d3b4935fbad891e5ef788e"
  },
  {
    "text": "To support the transition of SE research towards open science, we have derived a conceptual framework, based on our experiences with a multitude of data and artifacts in a socio-technical software engineering project, that entails participants, data and artifacts. We unfold the variation in these concepts across our project, and discuss openness practices in relation to those.\nBased on the guiding principles for FAIR data - 'as open as possible and as closed as nescessary' - we recommend that research artifacts, such as survey and interview instruments are always made open access. Research platforms should also be made open, but need governance, e.g. licence and community, to reach its full potential. Quantitative data may be more open, due to its standardization and\nThis discussion leads us not to recommend sharing qualitative data from companies openly. However, researchers could consider publishing code books from the qualitative analysis (data step 3), as proposed by Field et al. [7] and DuBois et al. [4], as well as transparently reporting evolution of codes, conceptual model and theory, for example, as done by Runeson et al. [25]. Regarding qualitative data from students, the participants' integrity must be protected although there are no company secrets to protect, which leads to a similar recommendation as for company participants.\nR3 . We advice not to openly publish qualitative research data, but to publish study and analysis artifacts, such as study protocols, interview guides, interviewee descriptions, and code books from thematic analysis. [4, 7]\nThe quantitative survey data is somewhat easier to share more openly. Firstly, there are more participants - finding a person in a large pool is harder than in a small one, although modern data analyses are very powerful in finding a 'nail in a haystack'. Secondly, the opinions shared in Likert scale responses are not as rich and detailed as qualitative survey or interview responses, unless\npure volume, while opening qualitative data comes with several risks and challenges. At the end of the day, participants' integrity and companies secrecy concerns are essential to respect, also while advocating the benefits of open science.\nWe advise that our conceptual framework be used to guide tradeoffs between openness and closeness. We hope that the preliminary recommendations become a starting point for the research community on open science practices for empirical software engineering. We further invite the community to validate and evolve the guidelines to be more comprehensive.\nThe framework and recommendations align with open science and FAIR data principles, as well as artifact evaluation policies. Our contribution is to interweave these with our experiences from a concrete research project and to generalize for a broader range of software engineering projects.\n\nProvides a conceptual framework for open science practices in software engineering research, advocating for open access to artifacts like study protocols and code books while cautioning against sharing sensitive qualitative data from companies.",
    "original_text": "To support the transition of SE research towards open science, we have derived a conceptual framework, based on our experiences with a multitude of data and artifacts in a socio-technical software engineering project, that entails participants, data and artifacts. We unfold the variation in these concepts across our project, and discuss openness practices in relation to those.\nBased on the guiding principles for FAIR data - 'as open as possible and as closed as nescessary' - we recommend that research artifacts, such as survey and interview instruments are always made open access. Research platforms should also be made open, but need governance, e.g. licence and community, to reach its full potential. Quantitative data may be more open, due to its standardization and\nThis discussion leads us not to recommend sharing qualitative data from companies openly. However, researchers could consider publishing code books from the qualitative analysis (data step 3), as proposed by Field et al. [7] and DuBois et al. [4], as well as transparently reporting evolution of codes, conceptual model and theory, for example, as done by Runeson et al. [25]. Regarding qualitative data from students, the participants' integrity must be protected although there are no company secrets to protect, which leads to a similar recommendation as for company participants.\nR3 . We advice not to openly publish qualitative research data, but to publish study and analysis artifacts, such as study protocols, interview guides, interviewee descriptions, and code books from thematic analysis. [4, 7]\nThe quantitative survey data is somewhat easier to share more openly. Firstly, there are more participants - finding a person in a large pool is harder than in a small one, although modern data analyses are very powerful in finding a 'nail in a haystack'. Secondly, the opinions shared in Likert scale responses are not as rich and detailed as qualitative survey or interview responses, unless\npure volume, while opening qualitative data comes with several risks and challenges. At the end of the day, participants' integrity and companies secrecy concerns are essential to respect, also while advocating the benefits of open science.\nWe advise that our conceptual framework be used to guide tradeoffs between openness and closeness. We hope that the preliminary recommendations become a starting point for the research community on open science practices for empirical software engineering. We further invite the community to validate and evolve the guidelines to be more comprehensive.\nThe framework and recommendations align with open science and FAIR data principles, as well as artifact evaluation policies. Our contribution is to interweave these with our experiences from a concrete research project and to generalize for a broader range of software engineering projects.",
    "context": "Provides a conceptual framework for open science practices in software engineering research, advocating for open access to artifacts like study protocols and code books while cautioning against sharing sensitive qualitative data from companies.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      8,
      7
    ],
    "id": "6eeabf194b8c4a2893b27cb725541e05c7f5d165723d351d7feebab586e0bfe0"
  },
  {
    "text": "The authors would like to thanks the co-workers in the Gander project. This work has been partially supported by the Swedish Foundation for Strategic Research (grant no. FFL18-0231), the Swedish Research Council (grant no. 2019-05658), ELLIIT - the Swedish Strategic Research Area in IT and Mobile Communications, and the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.\n\nAcknowledges funding sources and expresses gratitude to collaborators involved in the Gander project.",
    "original_text": "The authors would like to thanks the co-workers in the Gander project. This work has been partially supported by the Swedish Foundation for Strategic Research (grant no. FFL18-0231), the Swedish Research Council (grant no. 2019-05658), ELLIIT - the Swedish Strategic Research Area in IT and Mobile Communications, and the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.",
    "context": "Acknowledges funding sources and expresses gratitude to collaborators involved in the Gander project.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      8
    ],
    "id": "84a0ccb912e41aed931c63ac561570f8f3a7fe5063b6ecc0e5ac837cc4826cd0"
  },
  {
    "text": "The following data and artifacts are openly available for use.\n- The Gander platform [9]\n- Supplementary data to the Gander user study and platform (protocols and session data)[8]\n\nThe document outlines recommendations for open science practices in software engineering research, specifically regarding data sharing. It advises against openly publishing qualitative data from companies but suggests making research artifacts like study protocols and code books available, aligning with FAIR data principles and emphasizing the need to protect participant integrity.",
    "original_text": "The following data and artifacts are openly available for use.\n- The Gander platform [9]\n- Supplementary data to the Gander user study and platform (protocols and session data)[8]",
    "context": "The document outlines recommendations for open science practices in software engineering research, specifically regarding data sharing. It advises against openly publishing qualitative data from companies but suggests making research artifacts like study protocols and code books available, aligning with FAIR data principles and emphasizing the need to protect participant integrity.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      8
    ],
    "id": "68761c7ba0a30e9e55ccfc5f8609cf29c0990b3c31d37c855a593cd5f21e8ad9"
  },
  {
    "text": "- [1] Belfadel, A., Amdouni, E., Laval, J., Cherifi, C.B., Moalla, N., 2022. Towards software reuse through an enterprise architecture-based software capability profile. Enterprise Information Systems 16, 29 - 70. doi: 10.1080/17517575. 2020.1843076 .\n- [3] Chauvette, A., Schick-Makaroff, K., Molzahn, A.E., 2019. Open data in qualitative research. International Journal of Qualitative Methods 18, 160940691882386. doi: 10.1177/1609406918823863 .\n- [2] Briand, L.C., Bianculli, D., Nejati, S., Pastore, F., Sabetzadeh, M., 2017. The case for context-driven software engineering research: Generalizability is overrated. IEEE Software 34, 72-75. doi: 10.1109/MS.2017.3571562 .\n- [4] DuBois, J.M., Mozersky, J., Parsons, M., Walsh, H.A., Friedrich, A., Pienta, A., 2023. Exchanging words: Engaging the challenges of sharing qualitative research data. Proceedings of the National Academy of Sciences 120. doi: 10.1073/pnas. 2206981120 .\n- [6] Enders, T., Wolff, C., Satzger, G., 2020. Knowing what to share: Selective revealing in open data, in: European Conference on Information Systems (ECIS) Researchin-Progress Papers, p. 11. URL: https://aisel.aisnet.org/ecis2020_rip/11.\n- [5] DuBois, J.M., Strait, M., Walsh, H., 2018. Is it time to share qualitative research data? Qualitative Psychology 5, 380-393. doi: 10.1037/qup0000076 .\n- [7] Field, S.M., van Ravenzwaaij, D., Pittelkow, M.M., Hoek, J.M., Derksen, M., 2021. Qualitative open science - pain points and perspectives, in: OSF preprints, Center for Open Science. doi: 10.31219/osf.io/e3cq4 .\n- [9] Gander Contributors, 2023b. The Gander open source platform. https://gitlab. com/lund-university/gander.\n- [8] Gander Contributors, 2023a. Gander: a platform for exploration of gaze-driven assistance in code review - supplementary material. https://doi.org/10.5281/ zenodo.10527122.\n- [10] González-Barahona, J.M., Robles, G., 2012. On the reproducibility of empirical software engineering studies based on data retrieved from development repositories. Empirirical Software Engineering 17, 75-89. doi: 10.1007/S10664-011-9181-9 .\n- [11] González-Barahona, J.M., Robles, G., 2023. Revisiting the reproducibility of empirical software engineering studies based on data retrieved from development\n12. repositories. Information and Software Technology 164, 107318. doi: 10.1016/J. INFSOF.2023.107318 .\n- [13] Joyce, J.B., Douglass, T., Benwell, B., Rhys, C.S., Parry, R., Simmons, R., Kerrison, A., 2022. Should we share qualitative data? Epistemological and practical insights from conversation analysis. International Journal of Social Research Methodology 0, 1-15. doi: 10.1080/13645579.2022.2087851 .\n- [12] Gómez, O.S., Juristo, N., Vegas, S., 2014. Understanding replication of experiments in software engineering: A classification. Information and Software Technology 56, 1033-1048. doi: https://doi.org/10.1016/j.infsof.2014.04.004 .\n- [14] Kitchenham, B.A., Budgen, D., Brereton, P., 2015. Evidence-Based Software Engineering and Systematic Reviews. Routledge.\n- [16] Kuang, P., Söderberg, E., Niehorster, D., Höst, M., 2023. Toward gaze-assisted developer tools, in: Proceedings of the 45th IEEE/ACM International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER). doi: 10. 1109/ICSE-NIER58687.2023.00015 .\n- [15] Kowalkowski, C., Gebauer, H., Kamp, B., Parry, G., 2017. Servitization and deservitization: Overview, concepts, and definitions. Industrial Marketing Management 60, 4-10. doi: 10.1016/j.indmarman.2016.12.007 .\n- [17] Lethbridge, T.C., Sim, S.E., Singer, J., 2005. Studying software engineers: Data collection techniques for software field studies. Empirical Software Engineering 10, 311-341. doi: 10.1007/s10664-005-1290-x .\n- [19] Mendez, D., Graziotin, D., Wagner, S., Seibold, H., 2020. Open science in software engineering, in: Felderer, M., Travassos, G.H. (Eds.), Contemporary Empirical Methods in Software Engineering. Springer International Publishing, Cham, pp. 477-501. doi: 10.1007/978-3-030-32489-6_17 .\n- [18] Majeed, A., Hwang, S.O., 2023. Data-centric artificial intelligence, preprocessing, and the quest for transformative artificial intelligence systems development. Computer 56, 109-115. doi: 10.1109/MC.2023.3240450 .\n- [20] Munir, H., Linåker, J., Wnuk, K., Runeson, P., Regnell, B., 2017. Open innovation using open source tools: A case study at Sony Mobile. Empirical Software Engineering 23, 186-223. doi: 10.1007/s10664-017-9511-7 .\n- [22] Niculescu, M.F., Wu, D.J., 2014. Economics of free under perpetual licensing: Implications for the software industry. Information Systems Research 25, 173-199. doi: 10.2139/ssrn.1853603 .\n- [21] Méndez Fernández, D., Monperrus, M., Feldt, R., Zimmermann, T., 2019. The open science initiative of the empirical software engineering journal. Empirical Software Engineering 24, 1057-1060. doi: 10.1007/s10664-019-09712-x .\n- [23] Robinson, B., Francis, P., 2010. Improving industrial adoption of software engineering research: a comparison of open and closed source software, in: Succi, G., Morisio, M., Nagappan, N. (Eds.), Proceedings of the International Symposium on Empirical Software Engineering and Measurement (ESEM), ACM. doi: 10.1145/1852786.1852814 .\n- [25] Runeson, P., Olsson, T., Linåker, J., 2021. Open data ecosystems - an empirical investigation into an emerging industry collaboration concept. Journal of Systems and Software 182, 111088. doi: 10.1016/j.jss.2021.111088 .\n- [24] Runeson, P., Höst, M., Rainer, A., Regnell, B., 2012. Case Study Research in Software Engineering - Guidelines and Examples. Wiley. doi: 10.1002/ 9781118181034 .\n- [26] Sadowski, C., Söderberg, E., Church, L., Sipko, M., Bacchelli, A., 2018. Modern code review: A case study at google, in: Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice, ACM. pp. 181-190. doi: 10.1145/3183519.3183525 .\n\nIntroduces key research areas in software engineering, including data sharing, reproducibility, and open science practices.",
    "original_text": "- [1] Belfadel, A., Amdouni, E., Laval, J., Cherifi, C.B., Moalla, N., 2022. Towards software reuse through an enterprise architecture-based software capability profile. Enterprise Information Systems 16, 29 - 70. doi: 10.1080/17517575. 2020.1843076 .\n- [3] Chauvette, A., Schick-Makaroff, K., Molzahn, A.E., 2019. Open data in qualitative research. International Journal of Qualitative Methods 18, 160940691882386. doi: 10.1177/1609406918823863 .\n- [2] Briand, L.C., Bianculli, D., Nejati, S., Pastore, F., Sabetzadeh, M., 2017. The case for context-driven software engineering research: Generalizability is overrated. IEEE Software 34, 72-75. doi: 10.1109/MS.2017.3571562 .\n- [4] DuBois, J.M., Mozersky, J., Parsons, M., Walsh, H.A., Friedrich, A., Pienta, A., 2023. Exchanging words: Engaging the challenges of sharing qualitative research data. Proceedings of the National Academy of Sciences 120. doi: 10.1073/pnas. 2206981120 .\n- [6] Enders, T., Wolff, C., Satzger, G., 2020. Knowing what to share: Selective revealing in open data, in: European Conference on Information Systems (ECIS) Researchin-Progress Papers, p. 11. URL: https://aisel.aisnet.org/ecis2020_rip/11.\n- [5] DuBois, J.M., Strait, M., Walsh, H., 2018. Is it time to share qualitative research data? Qualitative Psychology 5, 380-393. doi: 10.1037/qup0000076 .\n- [7] Field, S.M., van Ravenzwaaij, D., Pittelkow, M.M., Hoek, J.M., Derksen, M., 2021. Qualitative open science - pain points and perspectives, in: OSF preprints, Center for Open Science. doi: 10.31219/osf.io/e3cq4 .\n- [9] Gander Contributors, 2023b. The Gander open source platform. https://gitlab. com/lund-university/gander.\n- [8] Gander Contributors, 2023a. Gander: a platform for exploration of gaze-driven assistance in code review - supplementary material. https://doi.org/10.5281/ zenodo.10527122.\n- [10] González-Barahona, J.M., Robles, G., 2012. On the reproducibility of empirical software engineering studies based on data retrieved from development repositories. Empirirical Software Engineering 17, 75-89. doi: 10.1007/S10664-011-9181-9 .\n- [11] González-Barahona, J.M., Robles, G., 2023. Revisiting the reproducibility of empirical software engineering studies based on data retrieved from development\n12. repositories. Information and Software Technology 164, 107318. doi: 10.1016/J. INFSOF.2023.107318 .\n- [13] Joyce, J.B., Douglass, T., Benwell, B., Rhys, C.S., Parry, R., Simmons, R., Kerrison, A., 2022. Should we share qualitative data? Epistemological and practical insights from conversation analysis. International Journal of Social Research Methodology 0, 1-15. doi: 10.1080/13645579.2022.2087851 .\n- [12] Gómez, O.S., Juristo, N., Vegas, S., 2014. Understanding replication of experiments in software engineering: A classification. Information and Software Technology 56, 1033-1048. doi: https://doi.org/10.1016/j.infsof.2014.04.004 .\n- [14] Kitchenham, B.A., Budgen, D., Brereton, P., 2015. Evidence-Based Software Engineering and Systematic Reviews. Routledge.\n- [16] Kuang, P., Söderberg, E., Niehorster, D., Höst, M., 2023. Toward gaze-assisted developer tools, in: Proceedings of the 45th IEEE/ACM International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER). doi: 10. 1109/ICSE-NIER58687.2023.00015 .\n- [15] Kowalkowski, C., Gebauer, H., Kamp, B., Parry, G., 2017. Servitization and deservitization: Overview, concepts, and definitions. Industrial Marketing Management 60, 4-10. doi: 10.1016/j.indmarman.2016.12.007 .\n- [17] Lethbridge, T.C., Sim, S.E., Singer, J., 2005. Studying software engineers: Data collection techniques for software field studies. Empirical Software Engineering 10, 311-341. doi: 10.1007/s10664-005-1290-x .\n- [19] Mendez, D., Graziotin, D., Wagner, S., Seibold, H., 2020. Open science in software engineering, in: Felderer, M., Travassos, G.H. (Eds.), Contemporary Empirical Methods in Software Engineering. Springer International Publishing, Cham, pp. 477-501. doi: 10.1007/978-3-030-32489-6_17 .\n- [18] Majeed, A., Hwang, S.O., 2023. Data-centric artificial intelligence, preprocessing, and the quest for transformative artificial intelligence systems development. Computer 56, 109-115. doi: 10.1109/MC.2023.3240450 .\n- [20] Munir, H., Linåker, J., Wnuk, K., Runeson, P., Regnell, B., 2017. Open innovation using open source tools: A case study at Sony Mobile. Empirical Software Engineering 23, 186-223. doi: 10.1007/s10664-017-9511-7 .\n- [22] Niculescu, M.F., Wu, D.J., 2014. Economics of free under perpetual licensing: Implications for the software industry. Information Systems Research 25, 173-199. doi: 10.2139/ssrn.1853603 .\n- [21] Méndez Fernández, D., Monperrus, M., Feldt, R., Zimmermann, T., 2019. The open science initiative of the empirical software engineering journal. Empirical Software Engineering 24, 1057-1060. doi: 10.1007/s10664-019-09712-x .\n- [23] Robinson, B., Francis, P., 2010. Improving industrial adoption of software engineering research: a comparison of open and closed source software, in: Succi, G., Morisio, M., Nagappan, N. (Eds.), Proceedings of the International Symposium on Empirical Software Engineering and Measurement (ESEM), ACM. doi: 10.1145/1852786.1852814 .\n- [25] Runeson, P., Olsson, T., Linåker, J., 2021. Open data ecosystems - an empirical investigation into an emerging industry collaboration concept. Journal of Systems and Software 182, 111088. doi: 10.1016/j.jss.2021.111088 .\n- [24] Runeson, P., Höst, M., Rainer, A., Regnell, B., 2012. Case Study Research in Software Engineering - Guidelines and Examples. Wiley. doi: 10.1002/ 9781118181034 .\n- [26] Sadowski, C., Söderberg, E., Church, L., Sipko, M., Bacchelli, A., 2018. Modern code review: A case study at google, in: Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice, ACM. pp. 181-190. doi: 10.1145/3183519.3183525 .",
    "context": "Introduces key research areas in software engineering, including data sharing, reproducibility, and open science practices.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      8
    ],
    "id": "4e914b4d0caa8c8003e13998109c56cb47cbeab7db342ddf764d4371a7b15013"
  },
  {
    "text": "- [28] Saranpää, W., Apell Skjutar, F., Heander, J., Söderberg, E., Niehorster, D.C., Mattsson, O., Klintskog, H., Church, L., 2023. Gander: A platform for exploration of gaze-driven assistance in code review, in: Proceedings of the 2023 Symposium on Eye Tracking Research and Applications, ACM. doi: 10.1145/3588015.3589191 .\n- [27] Sadowski, C., Van Gogh, J., Jaspan, C., Soderberg, E., Winter, C., 2015. Tricorder: Building a program analysis ecosystem, in: 37th IEEE International Conference on Software Engineering, IEEE. pp. 598-608. doi: 10.1109/ICSE.2015.76 .\n- [29] Söderberg, E., Church, L., Börstler, J., Niehorster, D., Rydenfält, C., 2022. Understanding the experience of code review: Misalignments, attention, and units of analysis, in: Proceedings of the International Conference on Evaluation and Assessment in Software Engineering (EASE), ACM. doi: 10.1145/3530019. 3530037 .\n- [31] Storey, M.A., Ernst, N.A., Williams, C., Kalliamvakou, E., 2020. The who, what, how of software engineering research: a socio-technical framework. Empirical Software Engineering 25, 4097-4129. doi: 10.1007/s10664-020-09858-z .\n- [30] Solari, M., Vegas, S., Juristo, N., 2018. Content and structure of laboratory packages for software engineering experiments. Information and Software Technology 97, 64-79. doi: 10.1016/j.infsof.2017.12.016 .\n- [32] Söderberg, E., Church, L., Börstler, J., Niehorster, D.C., Rydenfält, C., 2022. What's bothering developers in code review?, in: IEEE/ACM 44th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), pp. 341-342. doi: 10.1145/3510457.3513083 .\n- [33] Wohlin, C., Runeson, P., Höst, M., Ohlsson, M.C., Regnell, B., Wesslén, A., 2012. Experimentation in Software Engineering. Springer. doi: 10.1007/978-3-64229044-2 .\n\nDetails a new platform for gaze-driven assistance in code review, building upon previous research into code review processes and developer experience.",
    "original_text": "- [28] Saranpää, W., Apell Skjutar, F., Heander, J., Söderberg, E., Niehorster, D.C., Mattsson, O., Klintskog, H., Church, L., 2023. Gander: A platform for exploration of gaze-driven assistance in code review, in: Proceedings of the 2023 Symposium on Eye Tracking Research and Applications, ACM. doi: 10.1145/3588015.3589191 .\n- [27] Sadowski, C., Van Gogh, J., Jaspan, C., Soderberg, E., Winter, C., 2015. Tricorder: Building a program analysis ecosystem, in: 37th IEEE International Conference on Software Engineering, IEEE. pp. 598-608. doi: 10.1109/ICSE.2015.76 .\n- [29] Söderberg, E., Church, L., Börstler, J., Niehorster, D., Rydenfält, C., 2022. Understanding the experience of code review: Misalignments, attention, and units of analysis, in: Proceedings of the International Conference on Evaluation and Assessment in Software Engineering (EASE), ACM. doi: 10.1145/3530019. 3530037 .\n- [31] Storey, M.A., Ernst, N.A., Williams, C., Kalliamvakou, E., 2020. The who, what, how of software engineering research: a socio-technical framework. Empirical Software Engineering 25, 4097-4129. doi: 10.1007/s10664-020-09858-z .\n- [30] Solari, M., Vegas, S., Juristo, N., 2018. Content and structure of laboratory packages for software engineering experiments. Information and Software Technology 97, 64-79. doi: 10.1016/j.infsof.2017.12.016 .\n- [32] Söderberg, E., Church, L., Börstler, J., Niehorster, D.C., Rydenfält, C., 2022. What's bothering developers in code review?, in: IEEE/ACM 44th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), pp. 341-342. doi: 10.1145/3510457.3513083 .\n- [33] Wohlin, C., Runeson, P., Höst, M., Ohlsson, M.C., Regnell, B., Wesslén, A., 2012. Experimentation in Software Engineering. Springer. doi: 10.1007/978-3-64229044-2 .",
    "context": "Details a new platform for gaze-driven assistance in code review, building upon previous research into code review processes and developer experience.",
    "document": "A_Conceptual_Framework_and_Recommendations_for_Open_Data_and_Artifacts_in_Empirical_Software_Engineering.pdf",
    "pages": [
      8
    ],
    "id": "b01310556ab2cc4367d6bb3cb538a4ab63e6aa14fc3b88556fca1155d7c62d54"
  },
  {
    "text": "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000\n\nThe article introduces a hybrid algorithm for more accurate gaze distance estimation, combining vergence angle data with depth information from a depth camera to mitigate errors from human factors and environmental conditions.",
    "original_text": "This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000",
    "context": "The article introduces a hybrid algorithm for more accurate gaze distance estimation, combining vergence angle data with depth information from a depth camera to mitigate errors from human factors and environmental conditions.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      1
    ],
    "id": "95c9c5cc4a08bf7cbfb06ae644547a5ee4d953c0729eb4b4bcfc79a2e96622c0"
  },
  {
    "text": "1 Korea Institute of Science and Technology, Seoul, South Korea\n2 KHU-KIST Department of Converging Science and Technology, Kyung Hee University, Seoul, South Korea\nCorresponding author: Min-Koo Kang (e-mail: minkoo@kist.re.kr).\nThis work was financially supported by the Institute of Civil-Military Technology Cooperation Program funded by the Defense Acquisition Program Administration and Ministry of Trade, Industry and Energy of Korean government (grant No. 17-CM-DP-29).\nABSTRACT To deliver an optimal Mixed Reality (MR) experience, wherein virtual elements and real-world objects are seamlessly merged, it is vital to ensure a consistent vergence-accommodation distance. This necessitates the advancement of technology to precisely estimate the user's gaze distance. Presently, various MR devices employ small eye-tracking cameras to capture both eyes and infer the gaze distance based on vergence angle data. However, this technique faces significant challenges, as it is highly sensitive to several human errors, such as strabismus, blinking, and fatigue of the eyes due to prolonged use. To address these issues, this paper introduces an innovative hybrid algorithm for estimating gaze distances. The proposed approach concurrently utilizes an eye camera and a depth camera to conduct parallel estimations: one based on the conventional vergence angle and the other on gaze-mapped depth information. The confidence of each method is then assessed and cross-referenced, and an adaptive weighted average is computed to derive a more precise and stable gaze distance estimation. In the experiment, three challenging test scenarios designed to induce human and environmental errors were administered to 12 subjects under uniform conditions to evaluate the accuracy and stability of the proposed method. The experimental results were validated through both qualitative and quantitative analysis. The findings showed that the proposed method significantly outperformed current methods with a visual angle error of 0.132 degrees under ideal conditions. Furthermore, it consistently maintained robustness against human and environmental errors, achieving an error range of 0.14 to 0.21 degrees even in demanding environments.\nINDEX TERMS Augmented reality (AR), extended reality (XR), eye tracking, gaze distance estimation, mixed reality (MR), varifocal, vergence-accommodation conflict, virtual reality (VR)\n\nIntroduces the paper's focus on improving gaze distance estimation for Mixed Reality (MR) experiences, highlighting the challenges of current eye-tracking methods and presenting a novel hybrid algorithm combining vergence angle and depth camera data for more accurate and stable estimations.",
    "original_text": "1 Korea Institute of Science and Technology, Seoul, South Korea\n2 KHU-KIST Department of Converging Science and Technology, Kyung Hee University, Seoul, South Korea\nCorresponding author: Min-Koo Kang (e-mail: minkoo@kist.re.kr).\nThis work was financially supported by the Institute of Civil-Military Technology Cooperation Program funded by the Defense Acquisition Program Administration and Ministry of Trade, Industry and Energy of Korean government (grant No. 17-CM-DP-29).\nABSTRACT To deliver an optimal Mixed Reality (MR) experience, wherein virtual elements and real-world objects are seamlessly merged, it is vital to ensure a consistent vergence-accommodation distance. This necessitates the advancement of technology to precisely estimate the user's gaze distance. Presently, various MR devices employ small eye-tracking cameras to capture both eyes and infer the gaze distance based on vergence angle data. However, this technique faces significant challenges, as it is highly sensitive to several human errors, such as strabismus, blinking, and fatigue of the eyes due to prolonged use. To address these issues, this paper introduces an innovative hybrid algorithm for estimating gaze distances. The proposed approach concurrently utilizes an eye camera and a depth camera to conduct parallel estimations: one based on the conventional vergence angle and the other on gaze-mapped depth information. The confidence of each method is then assessed and cross-referenced, and an adaptive weighted average is computed to derive a more precise and stable gaze distance estimation. In the experiment, three challenging test scenarios designed to induce human and environmental errors were administered to 12 subjects under uniform conditions to evaluate the accuracy and stability of the proposed method. The experimental results were validated through both qualitative and quantitative analysis. The findings showed that the proposed method significantly outperformed current methods with a visual angle error of 0.132 degrees under ideal conditions. Furthermore, it consistently maintained robustness against human and environmental errors, achieving an error range of 0.14 to 0.21 degrees even in demanding environments.\nINDEX TERMS Augmented reality (AR), extended reality (XR), eye tracking, gaze distance estimation, mixed reality (MR), varifocal, vergence-accommodation conflict, virtual reality (VR)",
    "context": "Introduces the paper's focus on improving gaze distance estimation for Mixed Reality (MR) experiences, highlighting the challenges of current eye-tracking methods and presenting a novel hybrid algorithm combining vergence angle and depth camera data for more accurate and stable estimations.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      1
    ],
    "id": "53d9884230df76f031443d1e69929c1ae3a7108e786e985d56c3a63304fd19ad"
  },
  {
    "text": "Extended Reality (XR) technologies, including Augmented Reality (AR), Virtual Reality (VR), and Mixed Reality (MR), are currently utilized across various industries such as the integrated visual augmentation system for individual soldiers in the defense field and image-guided surgery in the medical field. Additionally, in the everyday lives of general consumers, XR enables convenient, time and space-transcending experiences such as virtual reality gaming, augmented realitybased shopping, virtual classrooms, remote health care and exercise programs, and virtual travel. Despite the rapid market growth driven by strong industry demand and active investments from global companies, XR technologies still faces many technical obstacles that must be swiftly addressed to ensure sustainable growth and commercialization.\nTo achieve an ideal XR where virtual images seamlessly blend with real environments, both AR and VR require common features: lightweight and comfortable wearability, wide field of view with high resolution, high-brightness displays ensuring visibility even outdoors, and long operational hours without battery or heat issues. Above all, overcoming the vergence-accommodation conflict (VAC) is crucial [1]. VAC can cause severe visual fatigue when viewing virtual images for extended periods and prevents users from seeing real objects at various distances and virtual images at a fixed focal distance simultaneously. Therefore, many global companies and research institutions are currently focusing on developing new technologies to resolve VAC issues, such as as varifocal, multi-focal, and all-in-focus technologies [2]-[5]. However, to control the focal distance of virtual images so that vergence and accommodation distances match, the development of gaze distance estimation technology must be preceded.\nIEEE Access\nCurrently, most commercial AR and VR devices incorporate small near-infrared eye cameras to perform eye-tracking, but this pertains to 2D gaze point tracking for human-friendly interaction with XR applications. Although research on accurately performing gaze distance estimation is till lacking, typically, in wearable XR devices, gaze distance is estimated by capturing both eyes with an eye camera, as depicted in Fig.1(a) and Fig.1(b), and using the convergence angle ( θ ) and inter pupillary distance ( β ) parameters to derive the gaze distance [6], [7]. However, this method is highly susceptible to human errors such as blinking, squinting, and eye relaxation [8], limiting the accuracy of estimating the gaze distance that matches the optical system's focal distance.\nAs an alternative to the above method, depth estimation commonly used in the computer vision field can be considered. Depth estimation generally involves analyzing the disparity using infrared (IR) pattern light and stereo vision, and depth cameras that provide distance information between the camera and surrounding environmental elements in the form of 2D images (i.e., depth maps) are already widely used. Thus, as depicted in Fig. 1(c), by estimating the user's 2D gaze point coordinates with an eye-camera and mapping it to the depth map acquired by a depth-camera, the corresponding depth value can be retrieved to estimate the user's gaze distance [9][11]. This method can solidly eliminate human errors arising from the convergence angle-based method. However, it can easily malfunction due to external environmental factors such as disocclusion regions or reflective media.\nThis paper proposes a novel hybrid method that mimics the human visual perception mechanism [12] to complement the limitations of the two conventional gaze distance estimation approaches. Human vergence-accommodation response is generally modeled as two dual-parallel feedback control systems, where these responses interact to enhance gaze distance perception and ensure better visual stability. Similarly, the proposed method combines the vergence angle-based approach with the gaze-mapped depth approach, allowing these two methods to interact and achieve synergy. This enhances the accuracy of each method and applies adaptive weighted averaging to assess the confidence of each method, thereby ensuring the stability of the final derived gaze distance.\n\nThis section introduces the context of XR technologies and highlights the critical challenge of the vergence-accommodation conflict (VAC) as a key obstacle to their widespread adoption. It also outlines the limitations of existing gaze distance estimation methods, particularly those relying solely on eye-tracking and depth cameras.",
    "original_text": "Extended Reality (XR) technologies, including Augmented Reality (AR), Virtual Reality (VR), and Mixed Reality (MR), are currently utilized across various industries such as the integrated visual augmentation system for individual soldiers in the defense field and image-guided surgery in the medical field. Additionally, in the everyday lives of general consumers, XR enables convenient, time and space-transcending experiences such as virtual reality gaming, augmented realitybased shopping, virtual classrooms, remote health care and exercise programs, and virtual travel. Despite the rapid market growth driven by strong industry demand and active investments from global companies, XR technologies still faces many technical obstacles that must be swiftly addressed to ensure sustainable growth and commercialization.\nTo achieve an ideal XR where virtual images seamlessly blend with real environments, both AR and VR require common features: lightweight and comfortable wearability, wide field of view with high resolution, high-brightness displays ensuring visibility even outdoors, and long operational hours without battery or heat issues. Above all, overcoming the vergence-accommodation conflict (VAC) is crucial [1]. VAC can cause severe visual fatigue when viewing virtual images for extended periods and prevents users from seeing real objects at various distances and virtual images at a fixed focal distance simultaneously. Therefore, many global companies and research institutions are currently focusing on developing new technologies to resolve VAC issues, such as as varifocal, multi-focal, and all-in-focus technologies [2]-[5]. However, to control the focal distance of virtual images so that vergence and accommodation distances match, the development of gaze distance estimation technology must be preceded.\nIEEE Access\nCurrently, most commercial AR and VR devices incorporate small near-infrared eye cameras to perform eye-tracking, but this pertains to 2D gaze point tracking for human-friendly interaction with XR applications. Although research on accurately performing gaze distance estimation is till lacking, typically, in wearable XR devices, gaze distance is estimated by capturing both eyes with an eye camera, as depicted in Fig.1(a) and Fig.1(b), and using the convergence angle ( θ ) and inter pupillary distance ( β ) parameters to derive the gaze distance [6], [7]. However, this method is highly susceptible to human errors such as blinking, squinting, and eye relaxation [8], limiting the accuracy of estimating the gaze distance that matches the optical system's focal distance.\nAs an alternative to the above method, depth estimation commonly used in the computer vision field can be considered. Depth estimation generally involves analyzing the disparity using infrared (IR) pattern light and stereo vision, and depth cameras that provide distance information between the camera and surrounding environmental elements in the form of 2D images (i.e., depth maps) are already widely used. Thus, as depicted in Fig. 1(c), by estimating the user's 2D gaze point coordinates with an eye-camera and mapping it to the depth map acquired by a depth-camera, the corresponding depth value can be retrieved to estimate the user's gaze distance [9][11]. This method can solidly eliminate human errors arising from the convergence angle-based method. However, it can easily malfunction due to external environmental factors such as disocclusion regions or reflective media.\nThis paper proposes a novel hybrid method that mimics the human visual perception mechanism [12] to complement the limitations of the two conventional gaze distance estimation approaches. Human vergence-accommodation response is generally modeled as two dual-parallel feedback control systems, where these responses interact to enhance gaze distance perception and ensure better visual stability. Similarly, the proposed method combines the vergence angle-based approach with the gaze-mapped depth approach, allowing these two methods to interact and achieve synergy. This enhances the accuracy of each method and applies adaptive weighted averaging to assess the confidence of each method, thereby ensuring the stability of the final derived gaze distance.",
    "context": "This section introduces the context of XR technologies and highlights the critical challenge of the vergence-accommodation conflict (VAC) as a key obstacle to their widespread adoption. It also outlines the limitations of existing gaze distance estimation methods, particularly those relying solely on eye-tracking and depth cameras.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      1,
      2
    ],
    "id": "c41d64ca686313077df261af3b8668dd771aa3b0392fbe521708d9a6c36e3c78"
  },
  {
    "text": "The vergence has been regarded as one of the most important distance cues of the human visual system for several reasons: simple principles of geometry, natural solution in computer vision field, and effectiveness, etc [13].\nKwon et al. [6] proposed geometry-based gaze depth estimation method. They utilized Prukinje images from two IR light sources with pupil center, then triangulated them to estimate gaze direction. With the idea that the pupil center distance (PCD) changes according to the target depth, of which PCD increases and decreases when the target object appears far and near from the eyes, respectively, they estimated gaze depth using PCD variations. Lee et al. [14] also\nFIGURE 1. (a) Hardware configuration for data acquisition; RGB scene image and corresponding depth image with two eye images. Conventional methods for gaze distance estimation using (b) vergence angle and (c) 2D gaze-mapped depth images.\nproposed 3D gaze estimation method with Prukinje images. Different with previous studies, they applied a multi-layer perceptron (MLP) to estimate gaze distance and designed three features used to train of which are the relative position of first and fourth Prukinje images to the pupil center, interdistance between these two Prukinje images, and pupil size. Note that they utilized the pupil size based on the fact that pupil accommodation happens according to the gaze distance changes. They finally estimated 2D gaze position with the pupil center based on a geometric transform considering the gaze distance obtained by MLP.\nMlot et al. [7] proposed fast and robust method to estimate the 3D gaze position based on the eye vergence. They utilized the ExCuSe algorithm [15] to find a pupil position, then applied a 2 nd order polynomial mapping function that describes the relationship between the pupil position and the target depth to estimate 3D gaze.\nLee et al. [16] also utilized MLP model to estimate the gaze depth. They trained the network using the gaze normal vectors obtained by a commercial eye tracker, Pupil Labs [8], instead Prukinje images, which differs from previous studies.\nThe gaze normal vector represented the three-dimensional direction of the eye's line of sight. They collected the training data by recording gaze normal vectors along with corresponding gaze depths, instructing participants to focus on a target positioned at distances ranging from 1 meter to 5 meters.\n\nThis section details established methods for estimating 3D gaze position, primarily relying on vergence angle and pupil-based techniques like Prukinje images and MLPs. Previous research has utilized pupil center distance (PCD) variations, pupil size changes, and gaze normal vectors to estimate gaze depth, often employing geometric transformations and MLPs to map pupil data to 3D gaze coordinates.",
    "original_text": "The vergence has been regarded as one of the most important distance cues of the human visual system for several reasons: simple principles of geometry, natural solution in computer vision field, and effectiveness, etc [13].\nKwon et al. [6] proposed geometry-based gaze depth estimation method. They utilized Prukinje images from two IR light sources with pupil center, then triangulated them to estimate gaze direction. With the idea that the pupil center distance (PCD) changes according to the target depth, of which PCD increases and decreases when the target object appears far and near from the eyes, respectively, they estimated gaze depth using PCD variations. Lee et al. [14] also\nFIGURE 1. (a) Hardware configuration for data acquisition; RGB scene image and corresponding depth image with two eye images. Conventional methods for gaze distance estimation using (b) vergence angle and (c) 2D gaze-mapped depth images.\nproposed 3D gaze estimation method with Prukinje images. Different with previous studies, they applied a multi-layer perceptron (MLP) to estimate gaze distance and designed three features used to train of which are the relative position of first and fourth Prukinje images to the pupil center, interdistance between these two Prukinje images, and pupil size. Note that they utilized the pupil size based on the fact that pupil accommodation happens according to the gaze distance changes. They finally estimated 2D gaze position with the pupil center based on a geometric transform considering the gaze distance obtained by MLP.\nMlot et al. [7] proposed fast and robust method to estimate the 3D gaze position based on the eye vergence. They utilized the ExCuSe algorithm [15] to find a pupil position, then applied a 2 nd order polynomial mapping function that describes the relationship between the pupil position and the target depth to estimate 3D gaze.\nLee et al. [16] also utilized MLP model to estimate the gaze depth. They trained the network using the gaze normal vectors obtained by a commercial eye tracker, Pupil Labs [8], instead Prukinje images, which differs from previous studies.\nThe gaze normal vector represented the three-dimensional direction of the eye's line of sight. They collected the training data by recording gaze normal vectors along with corresponding gaze depths, instructing participants to focus on a target positioned at distances ranging from 1 meter to 5 meters.",
    "context": "This section details established methods for estimating 3D gaze position, primarily relying on vergence angle and pupil-based techniques like Prukinje images and MLPs. Previous research has utilized pupil center distance (PCD) variations, pupil size changes, and gaze normal vectors to estimate gaze depth, often employing geometric transformations and MLPs to map pupil data to 3D gaze coordinates.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      2
    ],
    "id": "70cd40d43461b6c3628100b26257f9807642a5134874c9779abae1eac409dad3"
  },
  {
    "text": "Though the depth cue from an active sensor has an advantage that is invariant to human factors, there are a few studies exploiting the RGB-D camera for gaze distance estimation.\nElmadjian et al. [9] proposed a calibration procedure to estimate 3D gaze information using an uncalibrated headmounted binocular eye tracker coupled with a RGB-D camera. They compared the accuracy of 3D points of regard (PoR) using both geometric and regressor approaches. Based on\nIEEE Access\nFIGURE 2. (a) Human visual perception mechanism introduced in [12] and (b) overall framework of the proposed method by mimicking the visual perception mechanism.\nthe geometric model, they calculated intersection points of each eye gaze ray to determine the PoRs. For the regressor approach, they employed a Gaussian regressor to estimate both gaze direction and gaze depth. Because the hardware was uncalibrated, they tackled the issue that the inter pupillary distance (IPD) used to estimate a vergence angle did not align with the coordinate system of the scene camera. To address this issue, they designed a regressor pipeline consisting of two separate Gaussian regressors, one for estimating gaze direction and the other for computing the corresponding gaze depth. Despite demonstrating performance improvements, they also noted that there is still room for improvement in the accuracy of gaze distance estimation.\nLiu et al. [10] proposed an automatic calibration method for 3D gaze estimation by integrating gaze vectors with saliency maps. This approach addresses the limitations of traditional calibration methods, which typically require predefined targets and can be time-consuming. To mitigate these issues, they used saliency maps [17] to generate gaze targets based on the premise that human are more likely to fixate on salient features in scene images. To avoid redundant scene images in the calibration data, they applied bag-of-word algorithm [18] to measure image similarity. They then computed the 2D gaze points corresponding to the scene images by calibrating the relationship between the scene images and the gaze vectors from both eyes. Finally, they reconstructed a dense 3D environmental point cloud using an RGB-D camera to accuratelyt determine the 3D points of regards (PoRs).\nThe aforementioned studies utilized RGB-D cameras to estimate 3D gaze, particularly for gaze depths, whereas conventional 3D gaze estimation methods focused on minimizing angular errors in vergence using learning-based approaches. Although the depth information from RGB-D cameras was used to estimate gaze depth, it was not employed to enhance gaze distance accuracy through guided information.\n\nExisting research utilizing RGB-D cameras for gaze distance estimation primarily focuses on 3D gaze, particularly depth, while traditional methods prioritize angular accuracy through learning-based approaches.  RGB-D cameras’ depth information isn’t consistently leveraged to refine gaze distance estimates beyond initial depth measurements.",
    "original_text": "Though the depth cue from an active sensor has an advantage that is invariant to human factors, there are a few studies exploiting the RGB-D camera for gaze distance estimation.\nElmadjian et al. [9] proposed a calibration procedure to estimate 3D gaze information using an uncalibrated headmounted binocular eye tracker coupled with a RGB-D camera. They compared the accuracy of 3D points of regard (PoR) using both geometric and regressor approaches. Based on\nIEEE Access\nFIGURE 2. (a) Human visual perception mechanism introduced in [12] and (b) overall framework of the proposed method by mimicking the visual perception mechanism.\nthe geometric model, they calculated intersection points of each eye gaze ray to determine the PoRs. For the regressor approach, they employed a Gaussian regressor to estimate both gaze direction and gaze depth. Because the hardware was uncalibrated, they tackled the issue that the inter pupillary distance (IPD) used to estimate a vergence angle did not align with the coordinate system of the scene camera. To address this issue, they designed a regressor pipeline consisting of two separate Gaussian regressors, one for estimating gaze direction and the other for computing the corresponding gaze depth. Despite demonstrating performance improvements, they also noted that there is still room for improvement in the accuracy of gaze distance estimation.\nLiu et al. [10] proposed an automatic calibration method for 3D gaze estimation by integrating gaze vectors with saliency maps. This approach addresses the limitations of traditional calibration methods, which typically require predefined targets and can be time-consuming. To mitigate these issues, they used saliency maps [17] to generate gaze targets based on the premise that human are more likely to fixate on salient features in scene images. To avoid redundant scene images in the calibration data, they applied bag-of-word algorithm [18] to measure image similarity. They then computed the 2D gaze points corresponding to the scene images by calibrating the relationship between the scene images and the gaze vectors from both eyes. Finally, they reconstructed a dense 3D environmental point cloud using an RGB-D camera to accuratelyt determine the 3D points of regards (PoRs).\nThe aforementioned studies utilized RGB-D cameras to estimate 3D gaze, particularly for gaze depths, whereas conventional 3D gaze estimation methods focused on minimizing angular errors in vergence using learning-based approaches. Although the depth information from RGB-D cameras was used to estimate gaze depth, it was not employed to enhance gaze distance accuracy through guided information.",
    "context": "Existing research utilizing RGB-D cameras for gaze distance estimation primarily focuses on 3D gaze, particularly depth, while traditional methods prioritize angular accuracy through learning-based approaches.  RGB-D cameras’ depth information isn’t consistently leveraged to refine gaze distance estimates beyond initial depth measurements.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      2,
      3
    ],
    "id": "5eae6f653ef942e6d8350fa7684d1ab047f0ecb16a16ad6f59bf55e097a304cc"
  },
  {
    "text": "In this section, we explain a hybrid method for eye-gaze distance estimation. Note that we do not explain specifics of 2D gaze and a gaze distance from vergence because we only applied one of the commercial eye trackers, Pupil-Labs [8] of which provides 2D gaze with an accuracy of 0 . 60 ◦ and a precision of 0 . 02 ◦ . It is compatible with Intel RealSense RGB-D camera [19] (Fig. 1(a)) as the scene camera, and consists of two near-infrared (NIR) eye cameras, where the 2D gaze is calculated by the intersection of the gaze vectors from each eye in the scene camera. We exploit the human visual perception mechanism [12] (Fig. 2(a)) within the eyegaze distance estimation framework by cross-referencing the gaze distance derived from vergence with that obtained from\nIEEE Access gaze-mapped depth as shown in Fig. 2(b). We utilize the gaze distance from vergence, obtained with Pupil-Labs [8], as the initial estimate for vergence and the depth from the depth image pointed by 2D gaze as the initial estimate for gazemapped depth.\nFIGURE 3. Visualization of gaze distances from vergence captured with Pupil Eye Tracker [8] by gazing at targets placed at different distances, 0.5m, 1.0m, and 1.3m. The gaze distance from vergence has a linear correlation with target depths.\n\nIntroduces a hybrid method for eye-gaze distance estimation, combining vergence-based gaze distance with gaze-mapped depth, and outlines the core principles and weighting strategy.",
    "original_text": "In this section, we explain a hybrid method for eye-gaze distance estimation. Note that we do not explain specifics of 2D gaze and a gaze distance from vergence because we only applied one of the commercial eye trackers, Pupil-Labs [8] of which provides 2D gaze with an accuracy of 0 . 60 ◦ and a precision of 0 . 02 ◦ . It is compatible with Intel RealSense RGB-D camera [19] (Fig. 1(a)) as the scene camera, and consists of two near-infrared (NIR) eye cameras, where the 2D gaze is calculated by the intersection of the gaze vectors from each eye in the scene camera. We exploit the human visual perception mechanism [12] (Fig. 2(a)) within the eyegaze distance estimation framework by cross-referencing the gaze distance derived from vergence with that obtained from\nIEEE Access gaze-mapped depth as shown in Fig. 2(b). We utilize the gaze distance from vergence, obtained with Pupil-Labs [8], as the initial estimate for vergence and the depth from the depth image pointed by 2D gaze as the initial estimate for gazemapped depth.\nFIGURE 3. Visualization of gaze distances from vergence captured with Pupil Eye Tracker [8] by gazing at targets placed at different distances, 0.5m, 1.0m, and 1.3m. The gaze distance from vergence has a linear correlation with target depths.",
    "context": "Introduces a hybrid method for eye-gaze distance estimation, combining vergence-based gaze distance with gaze-mapped depth, and outlines the core principles and weighting strategy.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      3,
      4
    ],
    "id": "f5bc3efc084edd5e7f70cc2f2e5481cb7190ca7d42cacd156c2bc269c75c520b"
  },
  {
    "text": "Different face shapes of users (i.e. eye position, eye ball size, inter pupillary distance, etc.) can cause scale errors of gaze distance from vergence even though the 2D gaze point is well estimated. We exploit the gaze distance from gazemapped depth as a guided information to refine the scale of the initial gaze distance from vergence because the IR active sensor is relatively robust to human factors. Based on the characteristics of which the gaze distance from vergence changes roughly linearly to that of gaze-mapped depth as shown in Fig. 3, we exploit 1 st order polynomial function to refine the initial gaze distance from vergence.\n<!-- formula-not-decoded -->\nwhere ˆ V is the refined gaze distance from vergence for given initial gaze distance from vergence V and α 0 , α 1 are the polynomial coefficients. The optimal coefficients can be estimated by minimizing an objective function defined as\n<!-- formula-not-decoded -->\nwhere N is the number of samples, V n , and D n are n th initial gaze distance from vergence and corresponding that of gazemapped depth, respectively.\nAfter gaze fixation, the relaxation of ocular muscles can lead to drifts in the gaze distance derived from vergence, causing inaccuracies in estimation. Since pupil diameter tends to increase when such drift occurs (Fig. 4), we define a basis gaze distance from vergence to minimize the drift error. This defined basis is continuously updated only when the pupil\nFIGURE 4. Comparison of gaze distance from vergence and normalized pupil diameter. The pupil diameter increases when the gaze distance from vergence drifts (red arrows).\ndiameter constricts, effectively helping to suppress the drift error, maintain estimation accuracy, and stabilize the overall gaze distance measurements.\n<!-- formula-not-decoded -->\nwhere V t is a refined gaze distance from vergence at time t , V basis is the basis gaze distance from vergence, and ▽ P size is the normalized differentiation of the pupil diameter.\nBecause the pupil diameter changes smoothly over time, we apply temporal consistency to it.\n<!-- formula-not-decoded -->\nwhere t pupil is a temporal weight for pupil diameter and P T size is the pupil diameter at time T .\n\nAddresses individual user variations and potential drift errors in vergence-based gaze distance estimation, utilizing gaze-mapped depth as a corrective measure due to its robustness and linear relationship with vergence distance.",
    "original_text": "Different face shapes of users (i.e. eye position, eye ball size, inter pupillary distance, etc.) can cause scale errors of gaze distance from vergence even though the 2D gaze point is well estimated. We exploit the gaze distance from gazemapped depth as a guided information to refine the scale of the initial gaze distance from vergence because the IR active sensor is relatively robust to human factors. Based on the characteristics of which the gaze distance from vergence changes roughly linearly to that of gaze-mapped depth as shown in Fig. 3, we exploit 1 st order polynomial function to refine the initial gaze distance from vergence.\n<!-- formula-not-decoded -->\nwhere ˆ V is the refined gaze distance from vergence for given initial gaze distance from vergence V and α 0 , α 1 are the polynomial coefficients. The optimal coefficients can be estimated by minimizing an objective function defined as\n<!-- formula-not-decoded -->\nwhere N is the number of samples, V n , and D n are n th initial gaze distance from vergence and corresponding that of gazemapped depth, respectively.\nAfter gaze fixation, the relaxation of ocular muscles can lead to drifts in the gaze distance derived from vergence, causing inaccuracies in estimation. Since pupil diameter tends to increase when such drift occurs (Fig. 4), we define a basis gaze distance from vergence to minimize the drift error. This defined basis is continuously updated only when the pupil\nFIGURE 4. Comparison of gaze distance from vergence and normalized pupil diameter. The pupil diameter increases when the gaze distance from vergence drifts (red arrows).\ndiameter constricts, effectively helping to suppress the drift error, maintain estimation accuracy, and stabilize the overall gaze distance measurements.\n<!-- formula-not-decoded -->\nwhere V t is a refined gaze distance from vergence at time t , V basis is the basis gaze distance from vergence, and ▽ P size is the normalized differentiation of the pupil diameter.\nBecause the pupil diameter changes smoothly over time, we apply temporal consistency to it.\n<!-- formula-not-decoded -->\nwhere t pupil is a temporal weight for pupil diameter and P T size is the pupil diameter at time T .",
    "context": "Addresses individual user variations and potential drift errors in vergence-based gaze distance estimation, utilizing gaze-mapped depth as a corrective measure due to its robustness and linear relationship with vergence distance.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      4
    ],
    "id": "2342b78717dbd5003ea60dd2263f9650d17a6a95a6d628f468d6839ea0d2a516"
  },
  {
    "text": "The gaze distances from gaze-mapped depth in disocclusion regions and reflective material are unreliable for eye-gazedistance estimation. To avoid using unreliable gaze distances from gaze-mapped depth in eye-gaze distance estimation, we define activation functions for each environment as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere ▽ D is the normalized differentiation of the gaze distance from gaze-mapped depth, threshdis and errref are the minimum depth value considered as reliable and the maximum error of depth sensor in the reflection, respectively. Because the active depth sensor [19] has a sensing range of 0.3m to 10.0m and returns a near-zero depth value in disoccluded areas, we set the threshold, threshdis , to 0.25. In addition, because the depth sensor typically returns temporally stable depth values but becomes unstable in reflective areas, we empirically set the error threshold, errref , to 0.9925.\n\nThis section details strategies for mitigating errors in gaze distance estimation caused by unreliable depth data from a gaze-mapped depth sensor, specifically addressing issues with disocclusion and reflective surfaces.",
    "original_text": "The gaze distances from gaze-mapped depth in disocclusion regions and reflective material are unreliable for eye-gazedistance estimation. To avoid using unreliable gaze distances from gaze-mapped depth in eye-gaze distance estimation, we define activation functions for each environment as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere ▽ D is the normalized differentiation of the gaze distance from gaze-mapped depth, threshdis and errref are the minimum depth value considered as reliable and the maximum error of depth sensor in the reflection, respectively. Because the active depth sensor [19] has a sensing range of 0.3m to 10.0m and returns a near-zero depth value in disoccluded areas, we set the threshold, threshdis , to 0.25. In addition, because the depth sensor typically returns temporally stable depth values but becomes unstable in reflective areas, we empirically set the error threshold, errref , to 0.9925.",
    "context": "This section details strategies for mitigating errors in gaze distance estimation caused by unreliable depth data from a gaze-mapped depth sensor, specifically addressing issues with disocclusion and reflective surfaces.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      4
    ],
    "id": "a6317ec53c6bcb169e68ed93a4b4b1336beb398a0a242b3a3b371a93dc564cdb"
  },
  {
    "text": "Human errors such as blinking can be transferred to the gaze distance from vergence in the form of peak noises. To avoid utilizing such noisy gaze distances from vergence for eyegaze distance estimation, we design a confidence measure of gaze distance from vergence as\n<!-- formula-not-decoded -->\nwhere ▽ ˆ V is a normalized differentiation of the gaze distance from vergence. In addition, the gaze distance from gazemapped depth may have oscillation according to the environments. To avoid relying on noisy gaze distance from gazemapped depth, we design a confidence measure for the gaze distance from gaze-mapped depth as\n<!-- formula-not-decoded -->\nwhere ▽ D is a normalized differentiation of the gaze distance from gaze-mapped depth. Because the gaze distance from gaze-mapped depth remains temporally consistent when fixating on the same object, we apply temporal consistency to the confidence of gaze distance from gaze-mapped depth as\n<!-- formula-not-decoded -->\nwhere d T conf and t depth denote the gaze-mapped depth confidence at time T and its temporal weight, respectively.\nBased on the gaze distance from vergence confidence and that from the gaze-mapped depth confidence, we estimate the eye-gaze distance by weighting each cue as follows:\n<!-- formula-not-decoded -->\nNote that because temporal consistency is applied to both the pupil diameter used for refining the gaze distance from vergence and the gaze-mapped depth confidence, the estimated eye-gaze distance also remains temporally consistent.\n\nEvaluates gaze distance estimation accuracy under challenging conditions by incorporating confidence measures for both vergence-based and gaze-mapped depth cues, mitigating noise from human errors like blinking and environmental factors such as reflections and occlusion.",
    "original_text": "Human errors such as blinking can be transferred to the gaze distance from vergence in the form of peak noises. To avoid utilizing such noisy gaze distances from vergence for eyegaze distance estimation, we design a confidence measure of gaze distance from vergence as\n<!-- formula-not-decoded -->\nwhere ▽ ˆ V is a normalized differentiation of the gaze distance from vergence. In addition, the gaze distance from gazemapped depth may have oscillation according to the environments. To avoid relying on noisy gaze distance from gazemapped depth, we design a confidence measure for the gaze distance from gaze-mapped depth as\n<!-- formula-not-decoded -->\nwhere ▽ D is a normalized differentiation of the gaze distance from gaze-mapped depth. Because the gaze distance from gaze-mapped depth remains temporally consistent when fixating on the same object, we apply temporal consistency to the confidence of gaze distance from gaze-mapped depth as\n<!-- formula-not-decoded -->\nwhere d T conf and t depth denote the gaze-mapped depth confidence at time T and its temporal weight, respectively.\nBased on the gaze distance from vergence confidence and that from the gaze-mapped depth confidence, we estimate the eye-gaze distance by weighting each cue as follows:\n<!-- formula-not-decoded -->\nNote that because temporal consistency is applied to both the pupil diameter used for refining the gaze distance from vergence and the gaze-mapped depth confidence, the estimated eye-gaze distance also remains temporally consistent.",
    "context": "Evaluates gaze distance estimation accuracy under challenging conditions by incorporating confidence measures for both vergence-based and gaze-mapped depth cues, mitigating noise from human errors like blinking and environmental factors such as reflections and occlusion.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      5
    ],
    "id": "0b5ad2fedd5a2228d299860b6f31034ea3d09ab985b6c157f8b88af79611ae72"
  },
  {
    "text": "In this research, we did not solely focus on comparing the optimal gaze distance estimation accuracy achievable under perfect experimental conditions. Rather, we performed experiments to quantitatively and qualitatively evaluate the accuracy and stability of the proposed hybrid method in more realistic settings, taking into account various potential errors, in comparison to traditional single-method approaches. For this purpose, we arranged the experimental setup as shown in Figure 5. We set up three gaze targets at different distances (0.5m, 1.0m, 1.3m), labeled as near, middle, and far targets, respectively. Specifically, to examine the human error aspect of the existing vergence angle-based gaze distance estimation technique, participants were asked to gaze at each target for several seconds to encourage natural eye blinking and relaxation. To assess errors from external environmental factors\nIEEE Access\nFIGURE 5. Experimental environments setup for the evaluation. Chin holder is used to fixate head. Three targets with different markers are placed in front of the chin holder at 0.5m, 1.0m, and 1.3m. Additional marker is displayed on the laptop with reflective screen.\nin the gaze-mapped depth-based gaze distance estimation approach, a laptop with a reflective screen was placed next to the middle target, and the near and far targets were arranged to overlap, creating non-occluded areas. Furthermore, all experiments were carried out with twelve participants under identical conditions, setting the temporal consistency weights for pupil and depth, t pupil and t depth as 0.95. A chin rest was also utilized to prevent unintended gaze distance variations due to user posture.\nFIGURE 6. Three experimental scenarios; S1: gazing at the near target only, S2: gazing at the middle target and the reflective screen located at the same distance alternately, and S3: gazing at the far target and disocclusion alternately.\n\nEvaluates the proposed hybrid method’s accuracy and stability in realistic settings, considering potential errors and comparing it to single-method approaches. The experiment involved twelve participants, utilizing a chin holder, three gaze targets at varying distances, and a laptop with a reflective screen to simulate environmental factors.",
    "original_text": "In this research, we did not solely focus on comparing the optimal gaze distance estimation accuracy achievable under perfect experimental conditions. Rather, we performed experiments to quantitatively and qualitatively evaluate the accuracy and stability of the proposed hybrid method in more realistic settings, taking into account various potential errors, in comparison to traditional single-method approaches. For this purpose, we arranged the experimental setup as shown in Figure 5. We set up three gaze targets at different distances (0.5m, 1.0m, 1.3m), labeled as near, middle, and far targets, respectively. Specifically, to examine the human error aspect of the existing vergence angle-based gaze distance estimation technique, participants were asked to gaze at each target for several seconds to encourage natural eye blinking and relaxation. To assess errors from external environmental factors\nIEEE Access\nFIGURE 5. Experimental environments setup for the evaluation. Chin holder is used to fixate head. Three targets with different markers are placed in front of the chin holder at 0.5m, 1.0m, and 1.3m. Additional marker is displayed on the laptop with reflective screen.\nin the gaze-mapped depth-based gaze distance estimation approach, a laptop with a reflective screen was placed next to the middle target, and the near and far targets were arranged to overlap, creating non-occluded areas. Furthermore, all experiments were carried out with twelve participants under identical conditions, setting the temporal consistency weights for pupil and depth, t pupil and t depth as 0.95. A chin rest was also utilized to prevent unintended gaze distance variations due to user posture.\nFIGURE 6. Three experimental scenarios; S1: gazing at the near target only, S2: gazing at the middle target and the reflective screen located at the same distance alternately, and S3: gazing at the far target and disocclusion alternately.",
    "context": "Evaluates the proposed hybrid method’s accuracy and stability in realistic settings, considering potential errors and comparing it to single-method approaches. The experiment involved twelve participants, utilizing a chin holder, three gaze targets at varying distances, and a laptop with a reflective screen to simulate environmental factors.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      5
    ],
    "id": "7d87c624c373aac3ae7ec885243685b8017fc8ca21faa6f84e5d22498cf404f4"
  },
  {
    "text": "To verify the stability and accuracy of the proposed method against the aforementioned human and environmental factors, we designed three scenarios of gaze fixation change (S1, S2,\nIEEE Access\nTABLE 1. Average accuracy of the proposed method for each scenario with Euclidean distance and angular difference metrics.\n\nA, Section/Scenario. = S1. A, Vergence [8].mm = 33.678. A, Vergence [8].deg = 0.248. A, Gazed-Depth [19].mm = 26.658. A, Gazed-Depth [19].deg = 0.185. A, Proposed.mm = 23.861. A, Proposed.deg = 0.1777. B, Section/Scenario. = S2. B, Vergence [8].mm = 65.410. B, Vergence [8].deg = 0.205. B, Gazed-Depth [19].mm = 417.046. B, Gazed-Depth [19].deg = 0.987. B, Proposed.mm = 64.807. B, Proposed.deg = 0.199. C, Section/Scenario. = S3. C, Vergence [8].mm = 77.587. C, Vergence [8].deg = 0.108. C, Gazed-Depth [19].mm = 484.946. C, Gazed-Depth [19].deg = 0.280. C, Proposed.mm = 111.133. C, Proposed.deg = 0.141. D, Section/Scenario. = S1. D, Vergence [8].mm = 109.184. D, Vergence [8].deg = 0.628. D, Gazed-Depth [19].mm = 16.583. D, Gazed-Depth [19].deg = 0.114. D, Proposed.mm = 56.801. D, Proposed.deg = 0.210. E, Section/Scenario. = S2. E, Vergence [8].mm = 152.996. E, Vergence [8].deg = 0.531. E, Gazed-Depth [19].mm = 481.143. E, Gazed-Depth [19].deg = 1.267. E, Proposed.mm = 37.421. E, Proposed.deg = 0.132. Overall, Section/Scenario. = Overall. Overall, Vergence [8].mm = 90.867. Overall, Vergence [8].deg = 0.312. Overall, Gazed-Depth [19].mm = 352.98. Overall, Gazed-Depth [19].deg = 0.635. Overall, Proposed.mm = 66.868. Overall, Proposed.deg = 0.167\nS3) as shown in Fig. 6. S1 is a test scenario designed to evaluate the impact of gaze distance estimation errors that may occur due to human factors, such as peak noise from blinking or drift caused by eye relaxation. In this scenario, the user is instructed to continuously gaze at the near target for 8 seconds. S2 and S3 are test scenarios intended to assess the impact of environmental factors on gaze distance estimation errors. In S2, the user is instructed to gaze at the middle target, then shift their gaze to the laptop located at the same distance for 7 seconds, and finally return to the middle target. This scenario is designed to examine the influence of the laptop's reflective surface on gaze estimation. In S3, the user is instructed to gaze at the far target, initially focusing on the center of the target and then shifting their gaze to the lower right corner of the same target for 8 seconds. Most commercial depth cameras use infrared (IR) pattern light, and when multiple objects overlap in the captured scene, the object closer to the camera may block the pattern light, creating disoccluded areas on the more distant object, making depth estimation challenging. The S3 scenario is designed to evaluate the impact of these disoccluded regions. The three test scenarios mentioned were applied in a continuous experimental sequence in the order of S1-S2-S3-S1-S2.\n\nEvaluates the impact of gaze fixation changes (S1, S2, and S3) on gaze distance estimation accuracy, specifically examining the effects of human factors (blinking, relaxation) and environmental factors (reflective surfaces, occlusion).",
    "original_text": "To verify the stability and accuracy of the proposed method against the aforementioned human and environmental factors, we designed three scenarios of gaze fixation change (S1, S2,\nIEEE Access\nTABLE 1. Average accuracy of the proposed method for each scenario with Euclidean distance and angular difference metrics.\n\nA, Section/Scenario. = S1. A, Vergence [8].mm = 33.678. A, Vergence [8].deg = 0.248. A, Gazed-Depth [19].mm = 26.658. A, Gazed-Depth [19].deg = 0.185. A, Proposed.mm = 23.861. A, Proposed.deg = 0.1777. B, Section/Scenario. = S2. B, Vergence [8].mm = 65.410. B, Vergence [8].deg = 0.205. B, Gazed-Depth [19].mm = 417.046. B, Gazed-Depth [19].deg = 0.987. B, Proposed.mm = 64.807. B, Proposed.deg = 0.199. C, Section/Scenario. = S3. C, Vergence [8].mm = 77.587. C, Vergence [8].deg = 0.108. C, Gazed-Depth [19].mm = 484.946. C, Gazed-Depth [19].deg = 0.280. C, Proposed.mm = 111.133. C, Proposed.deg = 0.141. D, Section/Scenario. = S1. D, Vergence [8].mm = 109.184. D, Vergence [8].deg = 0.628. D, Gazed-Depth [19].mm = 16.583. D, Gazed-Depth [19].deg = 0.114. D, Proposed.mm = 56.801. D, Proposed.deg = 0.210. E, Section/Scenario. = S2. E, Vergence [8].mm = 152.996. E, Vergence [8].deg = 0.531. E, Gazed-Depth [19].mm = 481.143. E, Gazed-Depth [19].deg = 1.267. E, Proposed.mm = 37.421. E, Proposed.deg = 0.132. Overall, Section/Scenario. = Overall. Overall, Vergence [8].mm = 90.867. Overall, Vergence [8].deg = 0.312. Overall, Gazed-Depth [19].mm = 352.98. Overall, Gazed-Depth [19].deg = 0.635. Overall, Proposed.mm = 66.868. Overall, Proposed.deg = 0.167\nS3) as shown in Fig. 6. S1 is a test scenario designed to evaluate the impact of gaze distance estimation errors that may occur due to human factors, such as peak noise from blinking or drift caused by eye relaxation. In this scenario, the user is instructed to continuously gaze at the near target for 8 seconds. S2 and S3 are test scenarios intended to assess the impact of environmental factors on gaze distance estimation errors. In S2, the user is instructed to gaze at the middle target, then shift their gaze to the laptop located at the same distance for 7 seconds, and finally return to the middle target. This scenario is designed to examine the influence of the laptop's reflective surface on gaze estimation. In S3, the user is instructed to gaze at the far target, initially focusing on the center of the target and then shifting their gaze to the lower right corner of the same target for 8 seconds. Most commercial depth cameras use infrared (IR) pattern light, and when multiple objects overlap in the captured scene, the object closer to the camera may block the pattern light, creating disoccluded areas on the more distant object, making depth estimation challenging. The S3 scenario is designed to evaluate the impact of these disoccluded regions. The three test scenarios mentioned were applied in a continuous experimental sequence in the order of S1-S2-S3-S1-S2.",
    "context": "Evaluates the impact of gaze fixation changes (S1, S2, and S3) on gaze distance estimation accuracy, specifically examining the effects of human factors (blinking, relaxation) and environmental factors (reflective surfaces, occlusion).",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      5,
      6
    ],
    "id": "5ec191def5ee5f64478896bc1a91b93aa2c0b5156889553208df4c174155d80c"
  },
  {
    "text": "Initially, the experimental results comparing the impact of imposing a pupil diameter constraint on refining gaze distance estimations due to human error using the vergence are depicted in Fig. 7. In the plot, the method without the pupil diameter constraint, detailed in Eq. 1, is shown by a blue line; the method including the pupil diameter constraint, detailed in Eq. 3, by a red line; and the ground truth by a green line. The findings indicate that enforcing the pupil diameter constraint effectively mitigates peak noise caused by eye blinks across all segments and substantially reduces drift noise due to eye relaxation in segments A and D, where a gaze fixation is sustained. However, some deviations between the refined and the true values are noticeable in segments B, C, and E. These discrepancies could be due to variations in initial gaze distance estimates following gaze shifts, indicating inherent limitations in the precision of this method.\nSimilarly, the experimental findings comparing the ef-\ngaze distance from vergence gaze distance from vergence W pupil constraint)\nground truth\nFIGURE 7. Comparison of the gaze distance from vergence with/without the pupil diameter constraint. Gaze distance from vergence with pupil diameter constraint successively suppresses both drift noise caused by eye relaxation and peak noise caused by eye blinks.\nfectiveness of the proposed noise filter in mitigating gaze distance estimation errors due to environmental factors using gaze-mapped depth are depicted in Fig. 8. Particularly, Fig. 8(a) illustrates the efficiency of the proposed noise filter in detecting noise when gazing at disocclusion regions, while Fig. 8 shows its performance in identifying noise when gazing at reflective surfaces. Across the graphs, the gaze distance estimates based on gaze-mapped depth are denoted by red lines, and the estimation errors due to disoccluded regions and reflective surfaces are overlaid with green dotted lines and blue dotted lines, respectively. The findings indicate that the proposed gaze-mapped depth noise filter effectively identifies depth estimation errors caused by environmental factors. This implies that selectively employing gaze-mapped depth-based gaze distance estimation methods based on their confidence can result in highly accurate gaze distance measurements.\nFigure 9 presents a qualitative comparison of the perfor-\nIEEE Access\nFIGURE 8. Results of the proposed gaze-mapped depth noise filters described in Sec. III-B. Shading describes the frames of which the filters detected noise; (a) disocclusion and (b) reflection.\nmance of the proposed hybrid method. To aid understanding, representative data are overlaid in Fig. 9(a) for each of the tests S1, S2, and S3, including the user's gaze coordinates, estimated gaze distance, and the positions of both pupils and the depth map acquired by the depth camera. Figure 9(b) indicates the confidence of vergence-based and gaze-mapped depth-based gaze distance estimates at each timestamp, along with the weights assigned to these estimates in computing the final gaze distance estimation from Eq. 10. Figure 9(c) shows the final ground truth gaze distance values (green solid line), vergence-based gaze distance estimates (blue solid line), gaze-mapped depth-based gaze distance estimates (orange solid line), and the proposed hybrid method's estimates (red solid line). The graphs reveal that the proposed hybrid method remains robust against drift and peak noise in segments corresponding to S1, which commonly suffer from human error, and it also performs reliably in segments corresponding to S2 and S3, which are susceptible to depth estimation errors due to environmental factors like reflective surfaces and occlusion areas. Ultimately, the proposed hybrid method effectively mitigates each type of noise in challenging conditions, yielding estimates close to the ground truth values.\nTo quantitatively evaluate the proposed hybrid approach, we utilized the Euclidean distance and angular difference metrics [7], which are defined as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere Gest , Ggt , and O are the estimated gaze point, ground truth, and origin, represented as ( xest , yest , zest ) , ( xgt , ygt , zgt ) , and (0 , 0 , 0) , respectively.\nCommercial eye trackers typically perform with a visual angle error ranging from 0.5 to 1 degree. Despite the difficulty of fair evaluation due to differing experimental setups, considering the challenging test conditions discussed in this paper, Table 1 illustrates that the proposed method demonstrates superior quantitative performance comparable to existing methods and commercial products. Nevertheless, the primary aim of this study is to validate the adequate performance and stability of the proposed method under practically challenging circumstances. Within this context, the results of the quantitative performance assessment reveal several important insights. First, as observed in Segments A, B, and E, the proposed hybrid method does not solely choose the more reliable value from the two existing methods running in parallel. Instead, it cross-references the values estimated by each method to produce a more precise final estimation. This feature closely mimics the human visual perception mechanism previously discussed. Secondly, as seen in Segments C and D, unless one of the existing methods experiences a severe error, the proposed method remains resilient against errors even in various demanding environments, achieving an accuracy of up to 0.132 degrees. Lastly, even when severe errors occur in one of the two existing methods, as seen in Segments C and D, the proposed method effectively manages these errors, maintaining a moderate accuracy of approximately 0.14 to 0.21 degrees.\n\nEvaluates the hybrid method’s performance under challenging conditions, demonstrating robustness against human error and environmental factors like reflections and occlusion.",
    "original_text": "Initially, the experimental results comparing the impact of imposing a pupil diameter constraint on refining gaze distance estimations due to human error using the vergence are depicted in Fig. 7. In the plot, the method without the pupil diameter constraint, detailed in Eq. 1, is shown by a blue line; the method including the pupil diameter constraint, detailed in Eq. 3, by a red line; and the ground truth by a green line. The findings indicate that enforcing the pupil diameter constraint effectively mitigates peak noise caused by eye blinks across all segments and substantially reduces drift noise due to eye relaxation in segments A and D, where a gaze fixation is sustained. However, some deviations between the refined and the true values are noticeable in segments B, C, and E. These discrepancies could be due to variations in initial gaze distance estimates following gaze shifts, indicating inherent limitations in the precision of this method.\nSimilarly, the experimental findings comparing the ef-\ngaze distance from vergence gaze distance from vergence W pupil constraint)\nground truth\nFIGURE 7. Comparison of the gaze distance from vergence with/without the pupil diameter constraint. Gaze distance from vergence with pupil diameter constraint successively suppresses both drift noise caused by eye relaxation and peak noise caused by eye blinks.\nfectiveness of the proposed noise filter in mitigating gaze distance estimation errors due to environmental factors using gaze-mapped depth are depicted in Fig. 8. Particularly, Fig. 8(a) illustrates the efficiency of the proposed noise filter in detecting noise when gazing at disocclusion regions, while Fig. 8 shows its performance in identifying noise when gazing at reflective surfaces. Across the graphs, the gaze distance estimates based on gaze-mapped depth are denoted by red lines, and the estimation errors due to disoccluded regions and reflective surfaces are overlaid with green dotted lines and blue dotted lines, respectively. The findings indicate that the proposed gaze-mapped depth noise filter effectively identifies depth estimation errors caused by environmental factors. This implies that selectively employing gaze-mapped depth-based gaze distance estimation methods based on their confidence can result in highly accurate gaze distance measurements.\nFigure 9 presents a qualitative comparison of the perfor-\nIEEE Access\nFIGURE 8. Results of the proposed gaze-mapped depth noise filters described in Sec. III-B. Shading describes the frames of which the filters detected noise; (a) disocclusion and (b) reflection.\nmance of the proposed hybrid method. To aid understanding, representative data are overlaid in Fig. 9(a) for each of the tests S1, S2, and S3, including the user's gaze coordinates, estimated gaze distance, and the positions of both pupils and the depth map acquired by the depth camera. Figure 9(b) indicates the confidence of vergence-based and gaze-mapped depth-based gaze distance estimates at each timestamp, along with the weights assigned to these estimates in computing the final gaze distance estimation from Eq. 10. Figure 9(c) shows the final ground truth gaze distance values (green solid line), vergence-based gaze distance estimates (blue solid line), gaze-mapped depth-based gaze distance estimates (orange solid line), and the proposed hybrid method's estimates (red solid line). The graphs reveal that the proposed hybrid method remains robust against drift and peak noise in segments corresponding to S1, which commonly suffer from human error, and it also performs reliably in segments corresponding to S2 and S3, which are susceptible to depth estimation errors due to environmental factors like reflective surfaces and occlusion areas. Ultimately, the proposed hybrid method effectively mitigates each type of noise in challenging conditions, yielding estimates close to the ground truth values.\nTo quantitatively evaluate the proposed hybrid approach, we utilized the Euclidean distance and angular difference metrics [7], which are defined as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere Gest , Ggt , and O are the estimated gaze point, ground truth, and origin, represented as ( xest , yest , zest ) , ( xgt , ygt , zgt ) , and (0 , 0 , 0) , respectively.\nCommercial eye trackers typically perform with a visual angle error ranging from 0.5 to 1 degree. Despite the difficulty of fair evaluation due to differing experimental setups, considering the challenging test conditions discussed in this paper, Table 1 illustrates that the proposed method demonstrates superior quantitative performance comparable to existing methods and commercial products. Nevertheless, the primary aim of this study is to validate the adequate performance and stability of the proposed method under practically challenging circumstances. Within this context, the results of the quantitative performance assessment reveal several important insights. First, as observed in Segments A, B, and E, the proposed hybrid method does not solely choose the more reliable value from the two existing methods running in parallel. Instead, it cross-references the values estimated by each method to produce a more precise final estimation. This feature closely mimics the human visual perception mechanism previously discussed. Secondly, as seen in Segments C and D, unless one of the existing methods experiences a severe error, the proposed method remains resilient against errors even in various demanding environments, achieving an accuracy of up to 0.132 degrees. Lastly, even when severe errors occur in one of the two existing methods, as seen in Segments C and D, the proposed method effectively manages these errors, maintaining a moderate accuracy of approximately 0.14 to 0.21 degrees.",
    "context": "Evaluates the hybrid method’s performance under challenging conditions, demonstrating robustness against human error and environmental factors like reflections and occlusion.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      6,
      7
    ],
    "id": "d5948d204d107a412b6ec54bab23b45770f82d12d2d8473c92c3510d3582fd18"
  },
  {
    "text": "This paper proposed a novel hybrid gaze distance estimation method to overcome the limitations of existing approaches. Typically, gaze distance estimation in wearable XR devices is mainly based on binocular vergence angles. However, this approach is highly prone to errors induced by human factors (e.g., eye blinks, pupil dilation). As an alternative, a\nIEEE Access\nFIGURE 9. Qualitative comparison of the gaze distances estimated by vergence, gaze-mapped depth, and the proposed hybrid method with ground truth. (a) describes gaze transitions for each scenario with pupil detection results. (b) and (c) depict the results of the proposed confidence measures and the estimated gaze distance values, respectively.\nmethod integrating depth estimation techniques from computer vision with 2D gaze point tracking to utilize gazemapped depth information can be considered. However, this method often encounters difficulties in accurately measuring depth due to external environmental factors (e.g. disocclusion regions, reflective surfaces). To tackle these challenges, the introduced method conducts parallel gaze distance estimations: one based on binocular vergence and the other on gaze-mapped depth. It subsequently assesses the confidence of each approach and calculates an adaptive weighted sum through cross-referencing to derive an optimal gaze distance estimate. Qualitative and quantitative evaluations indicate that the proposed method is robust against errors arising from human factors and environmental conditions, which are the inherent drawbacks of existing methods. Additionally, it is evidenced that the proposed technique does not simply select the most reliable value from the two existing methods but crossreferences the estimated values to yield a more precise final estimate. The introduced method can be directly applied to developing user gaze distance estimation modules for varifocal AR/VR devices, aiming for an ideal mixed reality experience without VAC. In addition, it has potential applications in diverse fields, including digital healthcare, human-computer interaction, and analyzing user behavior and preferences.\n\nIntroduces a novel hybrid gaze distance estimation method addressing limitations of vergence-based approaches, specifically highlighting its robustness against human factors and environmental conditions through parallel vergence and gaze-mapped depth estimations.",
    "original_text": "This paper proposed a novel hybrid gaze distance estimation method to overcome the limitations of existing approaches. Typically, gaze distance estimation in wearable XR devices is mainly based on binocular vergence angles. However, this approach is highly prone to errors induced by human factors (e.g., eye blinks, pupil dilation). As an alternative, a\nIEEE Access\nFIGURE 9. Qualitative comparison of the gaze distances estimated by vergence, gaze-mapped depth, and the proposed hybrid method with ground truth. (a) describes gaze transitions for each scenario with pupil detection results. (b) and (c) depict the results of the proposed confidence measures and the estimated gaze distance values, respectively.\nmethod integrating depth estimation techniques from computer vision with 2D gaze point tracking to utilize gazemapped depth information can be considered. However, this method often encounters difficulties in accurately measuring depth due to external environmental factors (e.g. disocclusion regions, reflective surfaces). To tackle these challenges, the introduced method conducts parallel gaze distance estimations: one based on binocular vergence and the other on gaze-mapped depth. It subsequently assesses the confidence of each approach and calculates an adaptive weighted sum through cross-referencing to derive an optimal gaze distance estimate. Qualitative and quantitative evaluations indicate that the proposed method is robust against errors arising from human factors and environmental conditions, which are the inherent drawbacks of existing methods. Additionally, it is evidenced that the proposed technique does not simply select the most reliable value from the two existing methods but crossreferences the estimated values to yield a more precise final estimate. The introduced method can be directly applied to developing user gaze distance estimation modules for varifocal AR/VR devices, aiming for an ideal mixed reality experience without VAC. In addition, it has potential applications in diverse fields, including digital healthcare, human-computer interaction, and analyzing user behavior and preferences.",
    "context": "Introduces a novel hybrid gaze distance estimation method addressing limitations of vergence-based approaches, specifically highlighting its robustness against human factors and environmental conditions through parallel vergence and gaze-mapped depth estimations.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      8,
      7
    ],
    "id": "861372246dc73e55716b93740dff088eed9b6d9349f16e2d7e692ff988f24ef5"
  },
  {
    "text": "- [1] D. M Hoffman, A. R Girshick, K. Akeley, and M. S Banks. ''Vergenceaccommdation conflicts hinder visual performance and cause visual fatigue,'' Journal of vision , vol.8, no.3, pp. 33-33, 2008.\n- [2] A. K. Bhowmick, A. Jamali, D. Bryant, S. Pintz, and P. J. Bos. ''Liquid Crystal Based 5 cm Adaptive Focus Lens to Solve AccommodationConvergence (AC) Mismatch Issue of AR/VR/3D Displays.'' Society of Information Display (SID) , June 28, 2021.\n- [3] A. K. Bhowmick, A. Jamali, D. Bryant, S. Pintz, S. and P. J. Bos. ''Optical performance characterization of 5 cm aperture size continuous focus tunable liquid crystal lens for resolving Accommodation-Convergence mismatch conflict of AR/VR/3D HMDs.'' In Dig. Tech. Pap.-Soc. Inf. Disp. Int. Symp . Vol. 53, no. 1, pp. 166-169, 2022.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\nIEEE Access\n- [4] D. Dunn, C. Tippets, K. Torell, H. Fuchs, P. Kellnhofer, K. Myszkowski, ... and D. Luebke. ''Membrane AR: varifocal, wide field of view augmented reality display from deformable membranes.'' In ACM SIGGRAPH 2017 Emerging Technologies , pp. 1-2, 2017.\n- [5] S. G. Park, K. Kim, and J. Ha, ''T-Glasses: Light-Weight Hight Efficient Augmented Reality Smart Glasses.'' Proceedings of the International Display Workshops , 2022.\n- [6] Y. M. Kwon, K. W. Jeon, J. Ki, Q. M. Shahab, S. Jo, and S. K. Kim. ''3d gaze estimation and interaction to stereo display,'' International Journal of Virtual Reality , vol.5, no.3, pp. 41-45, 2006.\n- [7] E. G. Mlot, H. Bahmani, S. Wahl, and E. Kasneci. ''3d gaze estimation using eye vergence,'' International Conference on Health Informatics , vol.6, pp. 125-131, Scitepress, 2016.\n- [8] M. Kassner, W. Patera, and A. Bulling. ''Pupil: an open source platform for pervasive eye tracking and mobile gaze-based interaction.\", In Proceedings of the 2014 ACM international joint conference on pervasive and ubiquitous computing: Adjunct publication , pp. 1151-1160, 2014.\n- [9] C. Elmadjian, P. Shukla, A. D. Tula, and C. H. Morimoto. ''3D gaze estimation in the scene volume with a head-mounted eye tracker,'' In Proceedings of the Workshop on Communcation by Gaze Interaction , pp. 1-9, 2018.\n- [10] M. Liu, Y. Li, and H. Liu. ''Robust 3-D gaze estimation via data optimization and saliency aggregation for mobile eye-tracking systems.'' IEEE Transactions on Instrumentation and Measurement , 70, pp. 1-10, 2021.\n- [11] M. Liu, Y. Li, and H. Liu. ''3d gaze estimation for head-mounted devices based on visual saliency.'' In IEEE RSJ International Conference on Intelligent Robots and Systems(IROS) , pp. 10611-10616, 2020.\n- [12] M. Lambooij, W. IJsselsteijn, M. Fortuin, and I. Heynderickx. ''Visual discomfort and visual fatigue of stereoscopic displays: a review.'' Journal of imaging science and technology , vol.53, no.3, 30201-1, 2009.\n- [13] P. Linton. ''Does vision extract absolute distance from vergence?'' Attention, Perception, & Psychophysics , vol.82, no.6, pp. 3176-3195, 2020.\n- [14] J. W. Lee, C. W. Cho, K. Y. Shin, E. C. Lee, and K. R. Park. ''3D gaze tracking method using Purkinje images on eye optimal and pupil.'' Optics and Lasers in Engineering , vol.50, no.5, pp. 736-751, 2021.\n- [15] W. Fuhl, T. Kübler, K. Sippel, W. Rosenstiel, and E. Rosenstiel. ''Excuse: Robust pupil detection in real-world scenarios.'' In Computer Analysis of Images and Patterns: 16th International Conference, CAIP 2015, Valletta, Malta, September 2-4, Proceedings, Part I , pp. 39-51, Springer International Publishing, 2015.\n- [16] Y. Lee, C. Shin, A. Plopski, Y. Itoh, T. Piumsomboon, A. Dey, ... , and M. Billinghurst, ''Estimating gaze depth using multi-layer perceptron.'' In IEEE 2017 International Symposium on Ubiquitous Virtual Reality (ISUVR) , pp. 26-29, 2017.\n- [17] J. Harel, C. Koch, and P. Perona. ''Graph-based visual saliency.'' Advances in neural information processing systems , 2006.\n- [18] D. Gálvez-López, and J. D. Tardos. ''Bags of binary words for fast place recognition in image sequences.\" IEEE Transactions on robotics , vol.28, no.5, pp. 1188-1197, 2012.\n- [19] L. Keselman, W. J., Iselin, A. G.- Jepsen, and A. Bhowmik. ''Intel realsense stereoscopic depth cameras.'' In Proceedings of the IEEE conference on computer vision and pattern recognition workshops , pp. 1-10, 2017.\nDAE-YONG CHO received his B.S. degree in Intelligent Systems from the Department of Robotics at Kwangwoon University, South Korea, in 2014. In 2016, he earned a M.S. degree from the School of Information and Communications Engineering at the Gwangju Institute of Science and Technology (GIST). He worked as a Assistant Researcher at ADAS ONE, Inc., Seoul, South Korea, for approximately two years. From 2019, he served as a researcher at the Korea Institute of Science and\nTechnology (KIST) for about thee years. Currently, he is pursuing a Ph.D. in the KHU-KIST Department of Converging Science and Technology at Kyung Hee University, South Korea. His current research interests include smart glasses, focusing on the integration of augmented reality and artificial intelligence technologies for enhanced user interaction and accessibility in various applications, such as real-time information display, health monitoring, and assistive technology for visually impaired users.\nMIN-KOO KANG received his B.S. degree in Electronic Engineering from Inha University, South Korea, in 2008, and his M.S. degree in Electrical Engineering and Ph.D. degree in Information and Communications Engineering from the Gwangju Institute of Science and Technology (GIST) in 2010 and 2015, respectively. He worked as a Postdoctoral Researcher at the Center for Imaging Media Research, Korea Institute of Science and Technology (KIST), Seoul, South\nKorea, for three years, and has been a Principal Researcher at the Intelligent · Interaction Research Center since 2018. In addition, he has been an Adjunct Professor at Korea University since 2020 and at Kyung Hee University since 2023. His current research interests include telepresence, holoporation, remote monitoring and pilot technologies that transcend space and time; integrated visual/perceptual augmentation glass technology to enhance human capabilities; and other areas such as user behavior and intent analysis, digital healthcare, and human-computer interaction technologies.\n\nThis section details research on adaptive focus lenses and optical characterization for augmented and virtual reality displays, building upon previous work on accommodation-convergence mismatch issues.",
    "original_text": "- [1] D. M Hoffman, A. R Girshick, K. Akeley, and M. S Banks. ''Vergenceaccommdation conflicts hinder visual performance and cause visual fatigue,'' Journal of vision , vol.8, no.3, pp. 33-33, 2008.\n- [2] A. K. Bhowmick, A. Jamali, D. Bryant, S. Pintz, and P. J. Bos. ''Liquid Crystal Based 5 cm Adaptive Focus Lens to Solve AccommodationConvergence (AC) Mismatch Issue of AR/VR/3D Displays.'' Society of Information Display (SID) , June 28, 2021.\n- [3] A. K. Bhowmick, A. Jamali, D. Bryant, S. Pintz, S. and P. J. Bos. ''Optical performance characterization of 5 cm aperture size continuous focus tunable liquid crystal lens for resolving Accommodation-Convergence mismatch conflict of AR/VR/3D HMDs.'' In Dig. Tech. Pap.-Soc. Inf. Disp. Int. Symp . Vol. 53, no. 1, pp. 166-169, 2022.\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and\nIEEE Access\n- [4] D. Dunn, C. Tippets, K. Torell, H. Fuchs, P. Kellnhofer, K. Myszkowski, ... and D. Luebke. ''Membrane AR: varifocal, wide field of view augmented reality display from deformable membranes.'' In ACM SIGGRAPH 2017 Emerging Technologies , pp. 1-2, 2017.\n- [5] S. G. Park, K. Kim, and J. Ha, ''T-Glasses: Light-Weight Hight Efficient Augmented Reality Smart Glasses.'' Proceedings of the International Display Workshops , 2022.\n- [6] Y. M. Kwon, K. W. Jeon, J. Ki, Q. M. Shahab, S. Jo, and S. K. Kim. ''3d gaze estimation and interaction to stereo display,'' International Journal of Virtual Reality , vol.5, no.3, pp. 41-45, 2006.\n- [7] E. G. Mlot, H. Bahmani, S. Wahl, and E. Kasneci. ''3d gaze estimation using eye vergence,'' International Conference on Health Informatics , vol.6, pp. 125-131, Scitepress, 2016.\n- [8] M. Kassner, W. Patera, and A. Bulling. ''Pupil: an open source platform for pervasive eye tracking and mobile gaze-based interaction.\", In Proceedings of the 2014 ACM international joint conference on pervasive and ubiquitous computing: Adjunct publication , pp. 1151-1160, 2014.\n- [9] C. Elmadjian, P. Shukla, A. D. Tula, and C. H. Morimoto. ''3D gaze estimation in the scene volume with a head-mounted eye tracker,'' In Proceedings of the Workshop on Communcation by Gaze Interaction , pp. 1-9, 2018.\n- [10] M. Liu, Y. Li, and H. Liu. ''Robust 3-D gaze estimation via data optimization and saliency aggregation for mobile eye-tracking systems.'' IEEE Transactions on Instrumentation and Measurement , 70, pp. 1-10, 2021.\n- [11] M. Liu, Y. Li, and H. Liu. ''3d gaze estimation for head-mounted devices based on visual saliency.'' In IEEE RSJ International Conference on Intelligent Robots and Systems(IROS) , pp. 10611-10616, 2020.\n- [12] M. Lambooij, W. IJsselsteijn, M. Fortuin, and I. Heynderickx. ''Visual discomfort and visual fatigue of stereoscopic displays: a review.'' Journal of imaging science and technology , vol.53, no.3, 30201-1, 2009.\n- [13] P. Linton. ''Does vision extract absolute distance from vergence?'' Attention, Perception, & Psychophysics , vol.82, no.6, pp. 3176-3195, 2020.\n- [14] J. W. Lee, C. W. Cho, K. Y. Shin, E. C. Lee, and K. R. Park. ''3D gaze tracking method using Purkinje images on eye optimal and pupil.'' Optics and Lasers in Engineering , vol.50, no.5, pp. 736-751, 2021.\n- [15] W. Fuhl, T. Kübler, K. Sippel, W. Rosenstiel, and E. Rosenstiel. ''Excuse: Robust pupil detection in real-world scenarios.'' In Computer Analysis of Images and Patterns: 16th International Conference, CAIP 2015, Valletta, Malta, September 2-4, Proceedings, Part I , pp. 39-51, Springer International Publishing, 2015.\n- [16] Y. Lee, C. Shin, A. Plopski, Y. Itoh, T. Piumsomboon, A. Dey, ... , and M. Billinghurst, ''Estimating gaze depth using multi-layer perceptron.'' In IEEE 2017 International Symposium on Ubiquitous Virtual Reality (ISUVR) , pp. 26-29, 2017.\n- [17] J. Harel, C. Koch, and P. Perona. ''Graph-based visual saliency.'' Advances in neural information processing systems , 2006.\n- [18] D. Gálvez-López, and J. D. Tardos. ''Bags of binary words for fast place recognition in image sequences.\" IEEE Transactions on robotics , vol.28, no.5, pp. 1188-1197, 2012.\n- [19] L. Keselman, W. J., Iselin, A. G.- Jepsen, and A. Bhowmik. ''Intel realsense stereoscopic depth cameras.'' In Proceedings of the IEEE conference on computer vision and pattern recognition workshops , pp. 1-10, 2017.\nDAE-YONG CHO received his B.S. degree in Intelligent Systems from the Department of Robotics at Kwangwoon University, South Korea, in 2014. In 2016, he earned a M.S. degree from the School of Information and Communications Engineering at the Gwangju Institute of Science and Technology (GIST). He worked as a Assistant Researcher at ADAS ONE, Inc., Seoul, South Korea, for approximately two years. From 2019, he served as a researcher at the Korea Institute of Science and\nTechnology (KIST) for about thee years. Currently, he is pursuing a Ph.D. in the KHU-KIST Department of Converging Science and Technology at Kyung Hee University, South Korea. His current research interests include smart glasses, focusing on the integration of augmented reality and artificial intelligence technologies for enhanced user interaction and accessibility in various applications, such as real-time information display, health monitoring, and assistive technology for visually impaired users.\nMIN-KOO KANG received his B.S. degree in Electronic Engineering from Inha University, South Korea, in 2008, and his M.S. degree in Electrical Engineering and Ph.D. degree in Information and Communications Engineering from the Gwangju Institute of Science and Technology (GIST) in 2010 and 2015, respectively. He worked as a Postdoctoral Researcher at the Center for Imaging Media Research, Korea Institute of Science and Technology (KIST), Seoul, South\nKorea, for three years, and has been a Principal Researcher at the Intelligent · Interaction Research Center since 2018. In addition, he has been an Adjunct Professor at Korea University since 2020 and at Kyung Hee University since 2023. His current research interests include telepresence, holoporation, remote monitoring and pilot technologies that transcend space and time; integrated visual/perceptual augmentation glass technology to enhance human capabilities; and other areas such as user behavior and intent analysis, digital healthcare, and human-computer interaction technologies.",
    "context": "This section details research on adaptive focus lenses and optical characterization for augmented and virtual reality displays, building upon previous work on accommodation-convergence mismatch issues.",
    "document": "A_Hybrid_Gaze_Distance_Estimation_via_Cross-Reference_of_Vergence_and_Depth.pdf",
    "pages": [
      9
    ],
    "id": "6106058ab963a48f5653068fdc95d69b172640dd522caefcf15f7799fb8d8483"
  },
  {
    "text": "Received August 2, 2021, accepted September 16, 2021, date of publication September 27, 2021, date of current version October 6, 2021.\nDigital Object Identifier 10.1 109/ACCESS.2021.31 1591 1\n\nIntroduces the context of the paper: a study of training-free neural architecture search indicators, building upon previous work and aiming to improve the evaluation of these indicators.",
    "original_text": "Received August 2, 2021, accepted September 16, 2021, date of publication September 27, 2021, date of current version October 6, 2021.\nDigital Object Identifier 10.1 109/ACCESS.2021.31 1591 1",
    "context": "Introduces the context of the paper: a study of training-free neural architecture search indicators, building upon previous work and aiming to improve the evaluation of these indicators.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      1
    ],
    "id": "eef640352621390927f15e0344fc031b0d8eaf10d5d13cafa32dac406b49703c"
  },
  {
    "text": "Department of Computer Science and Engineering, Kyung Hee University, Yongin 17104, Republic of Korea\nCorresponding author: Sung-Ho Bae (shbae@khu.ac.kr)\nThis work was supported by the Technology Innovation Program or the Industrial Strategic Technology Development Program funded by the Ministry of Trade, Industry and Energy (MOTIE), South Korea (Memristor Fault-Aware Neuromorphic System for 3D Memristor Array) under Grant 10085646.\nABSTRACT Neural Architecture Search Without Training (NASWOT) has been proposed recently to replace the conventional Neural Architecture Search (NAS). Pioneer works only deploy one or two indicator(s) to search. Nevertheless, the quantitative assessment for indicators is not fully studied and evaluated. In this paper, we GLYPH<28>rst review several indicators, which are used to evaluate the network in a training-free manner, including the correlation of Jacobian, the output sensitivity, the number of linear regions, and the condition number of the neural tangent kernel. Our observation is that each indicator is responsible for characterizing a network in a speciGLYPH<28>c aspect and there is no single indicator that achieves good performance in all cases, e.g. highly correlated with the test accuracy. This motivated us to develop a novel indicator where all properties of a network are taken into account. To obtain better indicator that can consider various characteristics of networks in a harmonized form, we propose a Fusion Indicator (FI). SpeciGLYPH<28>cally, the proposed FI is formed by combining multiple indicators in a weighted sum manner. We minimize the mean squared error loss between the predicted and actual accuracy of networks to acquire the weights. Moreover, as the conventional training-free NAS researches used limited metrics to evaluate the quality of indicators, we introduce more desirable metrics that can evaluate the quality of training-free NAS indicator in terms of GLYPH<28>delity, correlation and rank-order similarity between the predicted quality value and actual accuracy of networks. That is, we introduce the Pearson Linear CoefGLYPH<28>cient Correlation (PLCC), the Root Mean Square Error (RMSE), the Spearman Rank-Order Correlation CoefGLYPH<28>cient (SROCC), and Kendall Rank-Order Correlation CoefGLYPH<28>cient (KROCC). Extensive experiments on NAS-Bench-101 and NAS-Bench201 demonstrate the effectiveness of our FI, outperforming existing methods by a large margin.\nINDEX TERMS Neural architecture search, training-free neural architecture search, fusion indicator, evaluation metrics.\n\nThis section details the paper's funding and affiliation, and introduces the core research: a novel fusion indicator (FI) for training-free neural architecture search, built upon a review of multiple indicators and incorporating new evaluation metrics.",
    "original_text": "Department of Computer Science and Engineering, Kyung Hee University, Yongin 17104, Republic of Korea\nCorresponding author: Sung-Ho Bae (shbae@khu.ac.kr)\nThis work was supported by the Technology Innovation Program or the Industrial Strategic Technology Development Program funded by the Ministry of Trade, Industry and Energy (MOTIE), South Korea (Memristor Fault-Aware Neuromorphic System for 3D Memristor Array) under Grant 10085646.\nABSTRACT Neural Architecture Search Without Training (NASWOT) has been proposed recently to replace the conventional Neural Architecture Search (NAS). Pioneer works only deploy one or two indicator(s) to search. Nevertheless, the quantitative assessment for indicators is not fully studied and evaluated. In this paper, we GLYPH<28>rst review several indicators, which are used to evaluate the network in a training-free manner, including the correlation of Jacobian, the output sensitivity, the number of linear regions, and the condition number of the neural tangent kernel. Our observation is that each indicator is responsible for characterizing a network in a speciGLYPH<28>c aspect and there is no single indicator that achieves good performance in all cases, e.g. highly correlated with the test accuracy. This motivated us to develop a novel indicator where all properties of a network are taken into account. To obtain better indicator that can consider various characteristics of networks in a harmonized form, we propose a Fusion Indicator (FI). SpeciGLYPH<28>cally, the proposed FI is formed by combining multiple indicators in a weighted sum manner. We minimize the mean squared error loss between the predicted and actual accuracy of networks to acquire the weights. Moreover, as the conventional training-free NAS researches used limited metrics to evaluate the quality of indicators, we introduce more desirable metrics that can evaluate the quality of training-free NAS indicator in terms of GLYPH<28>delity, correlation and rank-order similarity between the predicted quality value and actual accuracy of networks. That is, we introduce the Pearson Linear CoefGLYPH<28>cient Correlation (PLCC), the Root Mean Square Error (RMSE), the Spearman Rank-Order Correlation CoefGLYPH<28>cient (SROCC), and Kendall Rank-Order Correlation CoefGLYPH<28>cient (KROCC). Extensive experiments on NAS-Bench-101 and NAS-Bench201 demonstrate the effectiveness of our FI, outperforming existing methods by a large margin.\nINDEX TERMS Neural architecture search, training-free neural architecture search, fusion indicator, evaluation metrics.",
    "context": "This section details the paper's funding and affiliation, and introduces the core research: a novel fusion indicator (FI) for training-free neural architecture search, built upon a review of multiple indicators and incorporating new evaluation metrics.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      1
    ],
    "id": "3a20bec0f0c0a43137a4edcdf0312d663f5f24b9040f95b11d5c4a76f39dac71"
  },
  {
    "text": "Deep neural networks (DNNs) have shown remarkable performance on various computer vision tasks. Since the success of AlexNet on ImageNet [1] classiGLYPH<28>cation task in 2012 [2], many high-performance networks have been introduced [3]GLYPH<21>[5] where these networks have been designed by experts. However, manual design is not an optimal choice especially when the network goes deeper. Moreover, the process for designing these networks requires immense time and effort. To reduce the cost of designing the network, researchers have studied to automate the process, leading to Neural Architecture Search (NAS). Instead of designing the architecture, experts design the search algorithms that GLYPH<28>nd good candidates (e.g., the number of layers, GLYPH<28>lters and types of activation, etc.) on a given search space.\nThe associate editor coordinating the review of this manuscript and approving it for publication was Danilo Pelusi .\nNAS is able to discover superior architecture on various computer vision tasks such as image classiGLYPH<28>cation [6]GLYPH<21>[8], object detection [9], [10]. However, it suffers from several limitations. Firstly, the search cost is extremely high. It takes years of reinforcement learning (RL) [6] to search for networks which achieve state-of-the-art accuracy on largescale dataset such as ImageNet. The most expensive part in RL approach is the training from scratch of child networks. To alleviate this limitation, subsequent works have suggested to search on smaller conGLYPH<28>guration (e.g., cell) [11], performed with shared weights [12], search on a continuous search space for NAS [7], or incorporate Bi-layer parallel training [13].\nSecondly, there has a barrier to apply NAS to practical applications. That is, we need to search for a new network when the environment (e.g target task or hardware) is changed. For example, the architecture found on this dataset may not work well on others [14], or the latency for a same network is different when being executed on different hardware devices [15]. Moreover, the budget for searching is still high (e.g hours of GPU).\nIn order to search for best architecture in a short period time (e.g., a few minutes), NAS without training (NASWOT) [16] is introduced. SpeciGLYPH<28>cally, NASWOT proposed a trainingfree indicator that predicts the score which is highly correlated with the actual accuracy of a network, guiding the search process. Because the step to calculate this indicator does not require any training of networks, the total cost for NAS reduces signiGLYPH<28>cantly. With its simplicity, they have GLYPH<28>rst demonstrated the possibility of performing NAS without involving any training.\nMeanwhile, there have been growing works on deep learning theory that enables us to understand the behavior of DNNs [17]GLYPH<21>[23]. TE-NAS [24] has made the GLYPH<28>rst attempt to apply the indicators derived from theoretical works for revealing network characteristics, namely trainability and the expressivity, for training-free NAS. Different from [16], the search algorithm of TE-NAS is inspired by pruning-fromscratch.\nTraining-free NAS can replace the conventional NAS algorithms. However, prior works suffer from several limitations. [16] only uses one indicator, namely correlation Jacobian of a batch of images augmented with Cutout [25], by taking one characteristic (robustness to the input perturbation) of the network into account. As [16] uses only one indicator, it cannot represent other aspects of the network such as trainability or expressivity. So, it turns out to have limited performance in correlation between predicted score and actual accuracy. Reference [24] deals with this problem by using two indicators. However, the score (criterion for selecting the best network) is calculated with the sum of two ranks from each indicator, meaning that the two indicators have the same importance. However, this does not guarantee the optimal solution because different indicators may have different behaviors and contributions to the GLYPH<28>nal scores in different importance.\nTo solve this problem, this paper investigate several training-free indicators and harmonizes them in a fusion framework. The FI is capable of measuring the performance of networks under various aspects. Instead of treating all indicators equally, we propose a simple training approach to GLYPH<28>nd the appropriate weights for each indicator. The main contributions of the paper can be summarized as follows V\n- GLYPH<15> We GLYPH<28>rst collect and analyze several indicators that can be used to estimate the network's performance without training. The collected four indicators are the correlation of Jacobian (CJ), output sensitivity (OS), condition number of Neural Tangent Kernel (CNNTK), and the number of Linear Regions (NLR).\n- GLYPH<15> We propose a FI which is a combination of output from multiple indicators with learned weights. The proposed indicator beneGLYPH<28>ts from various properties of a network such as trainability, expressivity, generalibility and robustness against perturbation.\n- GLYPH<15> We introduce new quantitative metrics to measure goodness of indicators for training-free NAS.\nThe rest of the paper is organized as follows. Section II introduces several related works. Section III presents the FI. Section IV demonstrates the experimental results. Section V is the conclusion of the paper.\n\nIntroduces the concept of Neural Architecture Search (NAS) and its limitations, specifically the high search cost and difficulty in applying it to changing environments.",
    "original_text": "Deep neural networks (DNNs) have shown remarkable performance on various computer vision tasks. Since the success of AlexNet on ImageNet [1] classiGLYPH<28>cation task in 2012 [2], many high-performance networks have been introduced [3]GLYPH<21>[5] where these networks have been designed by experts. However, manual design is not an optimal choice especially when the network goes deeper. Moreover, the process for designing these networks requires immense time and effort. To reduce the cost of designing the network, researchers have studied to automate the process, leading to Neural Architecture Search (NAS). Instead of designing the architecture, experts design the search algorithms that GLYPH<28>nd good candidates (e.g., the number of layers, GLYPH<28>lters and types of activation, etc.) on a given search space.\nThe associate editor coordinating the review of this manuscript and approving it for publication was Danilo Pelusi .\nNAS is able to discover superior architecture on various computer vision tasks such as image classiGLYPH<28>cation [6]GLYPH<21>[8], object detection [9], [10]. However, it suffers from several limitations. Firstly, the search cost is extremely high. It takes years of reinforcement learning (RL) [6] to search for networks which achieve state-of-the-art accuracy on largescale dataset such as ImageNet. The most expensive part in RL approach is the training from scratch of child networks. To alleviate this limitation, subsequent works have suggested to search on smaller conGLYPH<28>guration (e.g., cell) [11], performed with shared weights [12], search on a continuous search space for NAS [7], or incorporate Bi-layer parallel training [13].\nSecondly, there has a barrier to apply NAS to practical applications. That is, we need to search for a new network when the environment (e.g target task or hardware) is changed. For example, the architecture found on this dataset may not work well on others [14], or the latency for a same network is different when being executed on different hardware devices [15]. Moreover, the budget for searching is still high (e.g hours of GPU).\nIn order to search for best architecture in a short period time (e.g., a few minutes), NAS without training (NASWOT) [16] is introduced. SpeciGLYPH<28>cally, NASWOT proposed a trainingfree indicator that predicts the score which is highly correlated with the actual accuracy of a network, guiding the search process. Because the step to calculate this indicator does not require any training of networks, the total cost for NAS reduces signiGLYPH<28>cantly. With its simplicity, they have GLYPH<28>rst demonstrated the possibility of performing NAS without involving any training.\nMeanwhile, there have been growing works on deep learning theory that enables us to understand the behavior of DNNs [17]GLYPH<21>[23]. TE-NAS [24] has made the GLYPH<28>rst attempt to apply the indicators derived from theoretical works for revealing network characteristics, namely trainability and the expressivity, for training-free NAS. Different from [16], the search algorithm of TE-NAS is inspired by pruning-fromscratch.\nTraining-free NAS can replace the conventional NAS algorithms. However, prior works suffer from several limitations. [16] only uses one indicator, namely correlation Jacobian of a batch of images augmented with Cutout [25], by taking one characteristic (robustness to the input perturbation) of the network into account. As [16] uses only one indicator, it cannot represent other aspects of the network such as trainability or expressivity. So, it turns out to have limited performance in correlation between predicted score and actual accuracy. Reference [24] deals with this problem by using two indicators. However, the score (criterion for selecting the best network) is calculated with the sum of two ranks from each indicator, meaning that the two indicators have the same importance. However, this does not guarantee the optimal solution because different indicators may have different behaviors and contributions to the GLYPH<28>nal scores in different importance.\nTo solve this problem, this paper investigate several training-free indicators and harmonizes them in a fusion framework. The FI is capable of measuring the performance of networks under various aspects. Instead of treating all indicators equally, we propose a simple training approach to GLYPH<28>nd the appropriate weights for each indicator. The main contributions of the paper can be summarized as follows V\n- GLYPH<15> We GLYPH<28>rst collect and analyze several indicators that can be used to estimate the network's performance without training. The collected four indicators are the correlation of Jacobian (CJ), output sensitivity (OS), condition number of Neural Tangent Kernel (CNNTK), and the number of Linear Regions (NLR).\n- GLYPH<15> We propose a FI which is a combination of output from multiple indicators with learned weights. The proposed indicator beneGLYPH<28>ts from various properties of a network such as trainability, expressivity, generalibility and robustness against perturbation.\n- GLYPH<15> We introduce new quantitative metrics to measure goodness of indicators for training-free NAS.\nThe rest of the paper is organized as follows. Section II introduces several related works. Section III presents the FI. Section IV demonstrates the experimental results. Section V is the conclusion of the paper.",
    "context": "Introduces the concept of Neural Architecture Search (NAS) and its limitations, specifically the high search cost and difficulty in applying it to changing environments.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      1,
      2
    ],
    "id": "57d2720cfd1b80e36eb5ede1aa5f2016b951d05a4c3423cd9ea426e7ed1e6cac"
  },
  {
    "text": "Neural architecture search (NAS) has attracted much attentions nowadays because of the ability to discover superior architectures automatically. However, most NAS algorithms require a huge amount of resources.\nThe earliest works on NAS were based on reinforcement learning [6], [11], [26], [27]. In [6], a controller was trained to generate the network's conGLYPH<28>guration which were used to construct the network. The exhaustive training and evaluation of child-network and the macro search (e.g searching the entire network) made this method unaffordable for practical applications. Particularly, the method in [6] used 800 GPUs and GLYPH<28>nished the searching phase in 28 days. To deal with this limitation, [11] searched for cells (i.e., normal cell and reduction cell). These were stacked to build the complete network. The method achieved 11.2x less search cost than [6]. Another type of NAS algorithm was based on evolutionary algorithms [28]GLYPH<21>[30], reduced the search cost to weeks of GPUs.\nMany attempts have been made to perform NAS just in a few hours [7], [12], [14], [31]. Especially ENAS [12] allowed sharing the weights among candidates. Thus, the most expensive part in the searching phase was eliminated. DARTS [7] proposed to search in a differentiable manner. During searching, a supernet was trained and the network was obtained by removing operators which has a low weight.\n\nSummarizes the introduction to NAS, highlighting the resource-intensive nature of early methods and the subsequent efforts to reduce search costs through techniques like cell-based NAS and differentiable search.",
    "original_text": "Neural architecture search (NAS) has attracted much attentions nowadays because of the ability to discover superior architectures automatically. However, most NAS algorithms require a huge amount of resources.\nThe earliest works on NAS were based on reinforcement learning [6], [11], [26], [27]. In [6], a controller was trained to generate the network's conGLYPH<28>guration which were used to construct the network. The exhaustive training and evaluation of child-network and the macro search (e.g searching the entire network) made this method unaffordable for practical applications. Particularly, the method in [6] used 800 GPUs and GLYPH<28>nished the searching phase in 28 days. To deal with this limitation, [11] searched for cells (i.e., normal cell and reduction cell). These were stacked to build the complete network. The method achieved 11.2x less search cost than [6]. Another type of NAS algorithm was based on evolutionary algorithms [28]GLYPH<21>[30], reduced the search cost to weeks of GPUs.\nMany attempts have been made to perform NAS just in a few hours [7], [12], [14], [31]. Especially ENAS [12] allowed sharing the weights among candidates. Thus, the most expensive part in the searching phase was eliminated. DARTS [7] proposed to search in a differentiable manner. During searching, a supernet was trained and the network was obtained by removing operators which has a low weight.",
    "context": "Summarizes the introduction to NAS, highlighting the resource-intensive nature of early methods and the subsequent efforts to reduce search costs through techniques like cell-based NAS and differentiable search.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      2
    ],
    "id": "fd6a3fa010405c0ef17f8e884eeaa0a7d2225b407673fe0605d22dfbe91e4401"
  },
  {
    "text": "Conventional NAS algorithms required a heavy search cost which is unaffordable to most applications especially for training the candidate networks. Therefore, if the performance of architectures is predicted without any training, the budget for NAS can be reduced signiGLYPH<28>cantly. Reference [16] was the GLYPH<28>rst to demonstrate the feasibility of performing NAS without any training. The authors in [16] empirically found the positive correlation between the correlation of Jacobian matrix among augmented input images and the network's performance. Thus, they suggested using this indicator to score the quality of the networks. Finally, NAS in [16] was performed with a simple search strategy based on Random Search where the indicator is used to replace the training process with simple prediction of the quality in a network. This work opened a new direction\nfor NAS where one can utilize some indicators to estimate the network's performance. However, the major drawback of this method is that the whole framework relies on a single indicator which only captures one characteristic of a network.\nTE-NAS [24] leveraged two training-free indicators, that is, NLR and CNNTK. The NLR is used to measure the expressivity of DNNs [21], [22]. The CNNTK measures the trainability of DNNs [23]. Based on these two indicators, they proposed a pruning-based algorithm to perform the search. Particularly, the criterion for pruning was based on the sum of two ranks measured by NLR and of CNNTK. The process was repeated until a certain stopping criteria was met.\nIn summary, the above works [16], [24] focus on performing NAS without involving any training by leveraging several training-free indicators. The search algorithm is based on random or pruning approach. Although training-free NAS has demonstrated a potential alternative for conventional NAS algorithms, naively summing the ranks in [24] does not reGLYPH<29>ect the importance of each indicator. In order to solve the aforementioned limitation, GLYPH<28>rst, we consider multiple indicators where each indicator can express different characteristic of a network. Overall, there are four indicators in our Fusion method. Second, instead of assigning an equal weight for each indicator in a weighted sum manner, we adopt a training approach to GLYPH<28>nd appropriate weights. This way we can highlight the important of each indicator. Furthermore, we perform extensive evaluation for the indicators in a systematic manner.\n\nHighlights the challenges of conventional NAS and introduces the concept of using training-free indicators to reduce search costs, referencing initial work by [16] and subsequent efforts like TE-NAS that rely on single indicators, ultimately motivating the need for a more comprehensive approach.",
    "original_text": "Conventional NAS algorithms required a heavy search cost which is unaffordable to most applications especially for training the candidate networks. Therefore, if the performance of architectures is predicted without any training, the budget for NAS can be reduced signiGLYPH<28>cantly. Reference [16] was the GLYPH<28>rst to demonstrate the feasibility of performing NAS without any training. The authors in [16] empirically found the positive correlation between the correlation of Jacobian matrix among augmented input images and the network's performance. Thus, they suggested using this indicator to score the quality of the networks. Finally, NAS in [16] was performed with a simple search strategy based on Random Search where the indicator is used to replace the training process with simple prediction of the quality in a network. This work opened a new direction\nfor NAS where one can utilize some indicators to estimate the network's performance. However, the major drawback of this method is that the whole framework relies on a single indicator which only captures one characteristic of a network.\nTE-NAS [24] leveraged two training-free indicators, that is, NLR and CNNTK. The NLR is used to measure the expressivity of DNNs [21], [22]. The CNNTK measures the trainability of DNNs [23]. Based on these two indicators, they proposed a pruning-based algorithm to perform the search. Particularly, the criterion for pruning was based on the sum of two ranks measured by NLR and of CNNTK. The process was repeated until a certain stopping criteria was met.\nIn summary, the above works [16], [24] focus on performing NAS without involving any training by leveraging several training-free indicators. The search algorithm is based on random or pruning approach. Although training-free NAS has demonstrated a potential alternative for conventional NAS algorithms, naively summing the ranks in [24] does not reGLYPH<29>ect the importance of each indicator. In order to solve the aforementioned limitation, GLYPH<28>rst, we consider multiple indicators where each indicator can express different characteristic of a network. Overall, there are four indicators in our Fusion method. Second, instead of assigning an equal weight for each indicator in a weighted sum manner, we adopt a training approach to GLYPH<28>nd appropriate weights. This way we can highlight the important of each indicator. Furthermore, we perform extensive evaluation for the indicators in a systematic manner.",
    "context": "Highlights the challenges of conventional NAS and introduces the concept of using training-free indicators to reduce search costs, referencing initial work by [16] and subsequent efforts like TE-NAS that rely on single indicators, ultimately motivating the need for a more comprehensive approach.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      2,
      3
    ],
    "id": "17c8cdee8474906adcc1fc36d75362f1bf58e89c46de53e5233e1cbe7306af8c"
  },
  {
    "text": "Reproducible and benchmarking NAS datasets are the most important factors for comparing the algorithms. There have been a lot of efforts to develop a benchmark dataset [32], [33]. Particularly, NAS-Bench-101 [32] has made the GLYPH<28>rst effort to build a dataset for benchmarking NAS. There are 423k architectures in this dataset. Each architecture is trained on CIFAR-10 under the same hyper-parameters. NAS-Bench101 provides the test accuracy of all architectures on this search space.\nNAS-Bench-201 [33] extends NAS-Bench-101 [32] by adding more operators (i.e., None and skip-connect), supporting more NAS algorithms (i.e., differentiable NAS), and evaluating on more datasets (i.e., CIFAR-100 and ImageNet16-120 [34]). The network in NAS-Bench-201 is a cell-based structure where the cell is deGLYPH<28>ned as a densely-connected directed acyclic graph with 4 nodes. With this conGLYPH<28>guration, the total architecture in NAS-Bench-201 is 15625. All networks are trained under the same settings. SpeciGLYPH<28>cally, they are trained from scratch with Nesterov momentum SGD for 200 epochs. The initial learning rate is 0.1 and is decayed with cosine annealing. The weight decay is set to 0.0005 and the batch size is 256.\nIn our work, we utilise NAS-Bench-101 and NAS-Bench201 to demonstrate the effectiveness of the proposed FI on all classiGLYPH<28>cation tasks in the dataset.\n\nHighlights the importance of benchmark datasets like NAS-Bench-101 and NAS-Bench-201 for comparing NAS algorithms and details their construction, including the number of architectures and training parameters.",
    "original_text": "Reproducible and benchmarking NAS datasets are the most important factors for comparing the algorithms. There have been a lot of efforts to develop a benchmark dataset [32], [33]. Particularly, NAS-Bench-101 [32] has made the GLYPH<28>rst effort to build a dataset for benchmarking NAS. There are 423k architectures in this dataset. Each architecture is trained on CIFAR-10 under the same hyper-parameters. NAS-Bench101 provides the test accuracy of all architectures on this search space.\nNAS-Bench-201 [33] extends NAS-Bench-101 [32] by adding more operators (i.e., None and skip-connect), supporting more NAS algorithms (i.e., differentiable NAS), and evaluating on more datasets (i.e., CIFAR-100 and ImageNet16-120 [34]). The network in NAS-Bench-201 is a cell-based structure where the cell is deGLYPH<28>ned as a densely-connected directed acyclic graph with 4 nodes. With this conGLYPH<28>guration, the total architecture in NAS-Bench-201 is 15625. All networks are trained under the same settings. SpeciGLYPH<28>cally, they are trained from scratch with Nesterov momentum SGD for 200 epochs. The initial learning rate is 0.1 and is decayed with cosine annealing. The weight decay is set to 0.0005 and the batch size is 256.\nIn our work, we utilise NAS-Bench-101 and NAS-Bench201 to demonstrate the effectiveness of the proposed FI on all classiGLYPH<28>cation tasks in the dataset.",
    "context": "Highlights the importance of benchmark datasets like NAS-Bench-101 and NAS-Bench-201 for comparing NAS algorithms and details their construction, including the number of architectures and training parameters.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      3
    ],
    "id": "e0ccb61d92da0c93645e597bea34c68f02b954cbfff1675c4e336d05db1294ee"
  },
  {
    "text": "The motivation of training-free NAS is to select highperformance potential networks from a search space without any training. In order to design a better indicator, we study several methods that can evaluate some characteristics of the network before training, i.e., CJ, OS, NLR, and CNNTK. In this section, we will GLYPH<28>rst summarize these methods and then explain our proposed FI.\n\nSummarizes methods for evaluating networks before training: CJ, OS, NLR, and CNNTK.",
    "original_text": "The motivation of training-free NAS is to select highperformance potential networks from a search space without any training. In order to design a better indicator, we study several methods that can evaluate some characteristics of the network before training, i.e., CJ, OS, NLR, and CNNTK. In this section, we will GLYPH<28>rst summarize these methods and then explain our proposed FI.",
    "context": "Summarizes methods for evaluating networks before training: CJ, OS, NLR, and CNNTK.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      3
    ],
    "id": "86055ab056b47a33e8badcdb3e3f17a11a774aab744f64316c4277828d244376"
  },
  {
    "text": "To understand the neural network (NN) behavior, we use CJ, OS, NLR, and CNNTK. These methods provide essential knowledge for characterizing NNs.\n\nIntroduces and summarizes four methods (CJ, OS, NLR, and CNNTK) used to characterize neural networks.",
    "original_text": "To understand the neural network (NN) behavior, we use CJ, OS, NLR, and CNNTK. These methods provide essential knowledge for characterizing NNs.",
    "context": "Introduces and summarizes four methods (CJ, OS, NLR, and CNNTK) used to characterize neural networks.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      3
    ],
    "id": "4504d1b07c3b94f06bd5f783370d2f239b2bd8ad7fe98d20fd55297badebb9a8"
  },
  {
    "text": "In order to score a network at an initial state, [16] designs CJ that computes the correlation of activations of a network with a mini-batch of n augmented images. SpeciGLYPH<28>cally, to compute the score, CJ GLYPH<28>rst calculates the derivative of output y with respect to input x of this batch V\n<!-- formula-not-decoded -->\nThen we calculate the correlation for this Jacobian matrix P J and count the number of entries that are smaller than a predeGLYPH<28>ned threshold ( GLYPH<12> ). Thus, the score for Jacobian is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->\n\nIntroduces CJ, a method for scoring a network's initial state by calculating the correlation of activations with augmented images and their Jacobian matrix.",
    "original_text": "In order to score a network at an initial state, [16] designs CJ that computes the correlation of activations of a network with a mini-batch of n augmented images. SpeciGLYPH<28>cally, to compute the score, CJ GLYPH<28>rst calculates the derivative of output y with respect to input x of this batch V\n<!-- formula-not-decoded -->\nThen we calculate the correlation for this Jacobian matrix P J and count the number of entries that are smaller than a predeGLYPH<28>ned threshold ( GLYPH<12> ). Thus, the score for Jacobian is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->",
    "context": "Introduces CJ, a method for scoring a network's initial state by calculating the correlation of activations with augmented images and their Jacobian matrix.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      3
    ],
    "id": "44a1097de93f96d31fbb349e8974f7662d854f3f43a2189b477c4adf329aebb3"
  },
  {
    "text": "For improving the network's generalization ability, authors in [35] proposed an ensemble approach called OS that can estimate the degree of generalization power of a network. Forouzesh et al. [17] further extended the results of [35] where the goal is to study the relation between the sensitivity and generalization in NNs. For determining the sensitivity of the network, external noise is added to the input. Let x be the input vector, GLYPH<18> be the parameters of a NN f GLYPH<18> , and \" be the noise which is sampled from a uniform distribution, then the error err is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->\nThe averaged error is calculated as below V\n<!-- formula-not-decoded -->\nwhere N is the number of classes and err n y indicates the n -th value of erry . The sensitivity of an NN can be measured by computing the variance of the output error. To this end, the score of sensitivity is formulated as V\n<!-- formula-not-decoded -->\nwhere X is the vector of averaged error for M samples and Var is the variance.\n\nIntroduces methods for evaluating network characteristics before training (CJ, OS, NLR, CNNTK) to assess generalization ability and sensitivity.",
    "original_text": "For improving the network's generalization ability, authors in [35] proposed an ensemble approach called OS that can estimate the degree of generalization power of a network. Forouzesh et al. [17] further extended the results of [35] where the goal is to study the relation between the sensitivity and generalization in NNs. For determining the sensitivity of the network, external noise is added to the input. Let x be the input vector, GLYPH<18> be the parameters of a NN f GLYPH<18> , and \" be the noise which is sampled from a uniform distribution, then the error err is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->\nThe averaged error is calculated as below V\n<!-- formula-not-decoded -->\nwhere N is the number of classes and err n y indicates the n -th value of erry . The sensitivity of an NN can be measured by computing the variance of the output error. To this end, the score of sensitivity is formulated as V\n<!-- formula-not-decoded -->\nwhere X is the vector of averaged error for M samples and Var is the variance.",
    "context": "Introduces methods for evaluating network characteristics before training (CJ, OS, NLR, CNNTK) to assess generalization ability and sensitivity.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      3,
      4
    ],
    "id": "33f6e16f0dd01f603c4c35fadab13cb7d34047e8eecba40fab852577b77d9de4"
  },
  {
    "text": "To answer why deep networks outperform a shallow network, [36] analyzes the deep ReLU networks with respect to their complexity. They have shown that, given the same computational resources, a deep network can divide the input space into many regions than a shallow one. Motivated by [21], [36] provided an in-depth analysis on NLR for convolution neural networks (CNNs). Following [24], NLR can be used to measure the expressivity of NNs. Let N be a ReLU CNN and GLYPH<18> be the parameters of N sampled from some distributions. The score for NLR can then be calculated as V\n<!-- formula-not-decoded -->\nwhere R ( GLYPH<1> ) is the region corresponding to P and GLYPH<18> , and is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->\nwhere z ( x 0 I GLYPH<18> ) is the pre-activation of a neuron z and P is an activation pattern such that P ( z ) 2 fGLYPH<0> 1 ; 1 g for each neuron z in N .\n\nAnalyzes deep ReLU networks' complexity, demonstrating their ability to divide the input space into more regions than shallow networks.",
    "original_text": "To answer why deep networks outperform a shallow network, [36] analyzes the deep ReLU networks with respect to their complexity. They have shown that, given the same computational resources, a deep network can divide the input space into many regions than a shallow one. Motivated by [21], [36] provided an in-depth analysis on NLR for convolution neural networks (CNNs). Following [24], NLR can be used to measure the expressivity of NNs. Let N be a ReLU CNN and GLYPH<18> be the parameters of N sampled from some distributions. The score for NLR can then be calculated as V\n<!-- formula-not-decoded -->\nwhere R ( GLYPH<1> ) is the region corresponding to P and GLYPH<18> , and is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->\nwhere z ( x 0 I GLYPH<18> ) is the pre-activation of a neuron z and P is an activation pattern such that P ( z ) 2 fGLYPH<0> 1 ; 1 g for each neuron z in N .",
    "context": "Analyzes deep ReLU networks' complexity, demonstrating their ability to divide the input space into more regions than shallow networks.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      4
    ],
    "id": "46cc18d0fddfa8d1888e76d0442c88e7ce51d14a44d8ceaab34b61c6029a1cc6"
  },
  {
    "text": "In [20] a new tool was proposed to help understand the behavior of DNNs during training, which is called NTK. It has been proved that using NTK, we can obtain the time evolution of linearized NNs at time t without running gradient descent V\n<!-- formula-not-decoded -->\nwhere GLYPH<22> t ( x ) D E [ z L i ( x )] is the expected outputs of inGLYPH<28>nitely wide network, z L i is the output of i -th neuron in the last layer L , GLYPH<17> is the learning rate, and O 2 train,train is the NTK between two training inputs. X train and Y train are the input and target which are drawn from the training set. Id is a constant. The trainability of NNs is studied in [23]. Let GLYPH<21> i be the i -th eigenvalue in the D diagonal matrix and U be the unitary matrix of O 2 train,train , i.e., O 2 train,train D UDU GLYPH<0> 1 . Then Eq. (8) can be rewritten as V\n<!-- formula-not-decoded -->\nwhere Q GLYPH<22> t ( X train ) i D U GLYPH<22> t ( X train) and Q Y train ; i D UY train .\nLet GLYPH<21> 0 and GLYPH<21> m be the minimum and maximum eigenvalues of O 2 train,train. In Eq. (9), the maximum learning rate scales as GLYPH<17> GLYPH<24> 2 =GLYPH<21> 0 in [37]. Thus, the smallest eigenvalue will converge exponentially at a rate given by 1 = k , where k D GLYPH<21> 0 =GLYPH<21> m and is the condition number. If the condition number of the NTK diverges, the network is untrainable. In our work, we inverse the condition number of NTK such that with a higher value,\nFIGURE 1. The illustration of the proposed fusion indicator. CJ, NLR, CNNTK, OS are the four training-free indicators. Each indicator represents a single characteristic of a network. In the fusion indicator, each standalone indicator contributes a certain property for ranking the neural network.\nwe can get better trainability. Thus, the score for trainability can be written as V\n<!-- formula-not-decoded -->\n\nIntroduces NTK and its use for analyzing time evolution of linearized NNs without gradient descent, outlining its connection to eigenvalue analysis and its role in assessing trainability.",
    "original_text": "In [20] a new tool was proposed to help understand the behavior of DNNs during training, which is called NTK. It has been proved that using NTK, we can obtain the time evolution of linearized NNs at time t without running gradient descent V\n<!-- formula-not-decoded -->\nwhere GLYPH<22> t ( x ) D E [ z L i ( x )] is the expected outputs of inGLYPH<28>nitely wide network, z L i is the output of i -th neuron in the last layer L , GLYPH<17> is the learning rate, and O 2 train,train is the NTK between two training inputs. X train and Y train are the input and target which are drawn from the training set. Id is a constant. The trainability of NNs is studied in [23]. Let GLYPH<21> i be the i -th eigenvalue in the D diagonal matrix and U be the unitary matrix of O 2 train,train , i.e., O 2 train,train D UDU GLYPH<0> 1 . Then Eq. (8) can be rewritten as V\n<!-- formula-not-decoded -->\nwhere Q GLYPH<22> t ( X train ) i D U GLYPH<22> t ( X train) and Q Y train ; i D UY train .\nLet GLYPH<21> 0 and GLYPH<21> m be the minimum and maximum eigenvalues of O 2 train,train. In Eq. (9), the maximum learning rate scales as GLYPH<17> GLYPH<24> 2 =GLYPH<21> 0 in [37]. Thus, the smallest eigenvalue will converge exponentially at a rate given by 1 = k , where k D GLYPH<21> 0 =GLYPH<21> m and is the condition number. If the condition number of the NTK diverges, the network is untrainable. In our work, we inverse the condition number of NTK such that with a higher value,\nFIGURE 1. The illustration of the proposed fusion indicator. CJ, NLR, CNNTK, OS are the four training-free indicators. Each indicator represents a single characteristic of a network. In the fusion indicator, each standalone indicator contributes a certain property for ranking the neural network.\nwe can get better trainability. Thus, the score for trainability can be written as V\n<!-- formula-not-decoded -->",
    "context": "Introduces NTK and its use for analyzing time evolution of linearized NNs without gradient descent, outlining its connection to eigenvalue analysis and its role in assessing trainability.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      4
    ],
    "id": "4e1c60ae351aea95d72b48cffe5614b403536c02f2eaff323385a9495bf5e211"
  },
  {
    "text": "In order to evaluate NNs without any training, the process requires a powerful indicator for correctly characterizing the network. Each indicator has its purpose and signiGLYPH<28>cance in specifying our model. Using these indicators separately makes them ineffective and weak since they only characterize the network with a speciGLYPH<28>c factor hence inculcating a certain bias in the network. Thus, this motivates us to design the FI where all indicators are combined. Let S be the set of training free indicators (e.g., S CJ ; S NLR ; S CNNTK ; S OS), our score for FI is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->\nwhere n is the total indicators, b is the bias, wi and Si are the weight and the score using i -th indicator in S . The overview of the proposed Fusion Indicator is demonstrated in Figure 1. We hypothesize that our proposed FI can take all indicators' advantage and make our training-free framework more reliable. We want our FI to reGLYPH<29>ect our network's accuracy closely. We are interested in minimizing the difference between the score and the accuracy. SpeciGLYPH<28>cally, we minimize the following loss function V\n<!-- formula-not-decoded -->\nwhere MSE is the mean squared error. S FI and Acc in Eq. (12) denote the score and the accuracy of a network. Thus, our S FI is the predicted accuracy of a network. The model parameters in Eq. (11) are trained with the stochastic gradient descent (SGD) method. Note that it is prohibitive to use support vector regression or linear regression in this case\nsince there are a huge numbers of samples in NAS-Bench101/202 datasets.\n\nMotivates the design of a fusion indicator combining multiple training-free network indicators to overcome the limitations of individual indicators and improve the reliability of the framework.",
    "original_text": "In order to evaluate NNs without any training, the process requires a powerful indicator for correctly characterizing the network. Each indicator has its purpose and signiGLYPH<28>cance in specifying our model. Using these indicators separately makes them ineffective and weak since they only characterize the network with a speciGLYPH<28>c factor hence inculcating a certain bias in the network. Thus, this motivates us to design the FI where all indicators are combined. Let S be the set of training free indicators (e.g., S CJ ; S NLR ; S CNNTK ; S OS), our score for FI is deGLYPH<28>ned as V\n<!-- formula-not-decoded -->\nwhere n is the total indicators, b is the bias, wi and Si are the weight and the score using i -th indicator in S . The overview of the proposed Fusion Indicator is demonstrated in Figure 1. We hypothesize that our proposed FI can take all indicators' advantage and make our training-free framework more reliable. We want our FI to reGLYPH<29>ect our network's accuracy closely. We are interested in minimizing the difference between the score and the accuracy. SpeciGLYPH<28>cally, we minimize the following loss function V\n<!-- formula-not-decoded -->\nwhere MSE is the mean squared error. S FI and Acc in Eq. (12) denote the score and the accuracy of a network. Thus, our S FI is the predicted accuracy of a network. The model parameters in Eq. (11) are trained with the stochastic gradient descent (SGD) method. Note that it is prohibitive to use support vector regression or linear regression in this case\nsince there are a huge numbers of samples in NAS-Bench101/202 datasets.",
    "context": "Motivates the design of a fusion indicator combining multiple training-free network indicators to overcome the limitations of individual indicators and improve the reliability of the framework.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      4,
      5
    ],
    "id": "c692d2409119c8cfa90f190b170bf4e62cc901ce2edc02432df5992a5aca3ae7"
  },
  {
    "text": "We use four metrics to evaluate the performance of the indicator. The GLYPH<28>rst two metrics are the Spearman Rank-Order Correlation CoefGLYPH<28>cient (SROCC) and the Kendall RankOrder Correlation CoefGLYPH<28>cient (KROCC). The third metric and the fourth metric are the Pearson Linear Correlation CoefGLYPH<28>cient (PLCC) and the Root Mean Square Error (RMSE) between the score and the accuracy after nonlinear regression. The GLYPH<28>rst two metrics measure the prediction monotonicity of the indicator while the third one measures the linear correlation between the actual accuracy and the predicted one. These quality assessment metrics are made for evaluating such characteristics (GLYPH<28>delity and correlation) and are widely used in practice [38]GLYPH<21>[40]. To compute PLCC and RMSE, we apply the following logistic function as suggested in [41]:\n<!-- formula-not-decoded -->\nwhere x is the score (predicted accuracy) and GLYPH<12> i ( i D 1 ; 2 ; 3 ; 4 ; 5) are the set of parameters that minimize the least squares error between the output from indicator and the network's accuracy.\nSince our FI is a learning-based method, the optimal w may not generalize well on other test data. To prevent overGLYPH<28>tting, we use n-fold cross-validation. SpeciGLYPH<28>cally, we use 5-fold cross-validation and average the results obtained from 5 folds to get the overall performance. In all experiments, we run the 5-fold cross-validation 10 times and average the results to obtain the GLYPH<28>nal performance.\n\nEvaluates the indicator's performance using Spearman and Kendall correlation, Pearson correlation, and RMSE, employing 5-fold cross-validation to assess generalizability and prevent overfitting.",
    "original_text": "We use four metrics to evaluate the performance of the indicator. The GLYPH<28>rst two metrics are the Spearman Rank-Order Correlation CoefGLYPH<28>cient (SROCC) and the Kendall RankOrder Correlation CoefGLYPH<28>cient (KROCC). The third metric and the fourth metric are the Pearson Linear Correlation CoefGLYPH<28>cient (PLCC) and the Root Mean Square Error (RMSE) between the score and the accuracy after nonlinear regression. The GLYPH<28>rst two metrics measure the prediction monotonicity of the indicator while the third one measures the linear correlation between the actual accuracy and the predicted one. These quality assessment metrics are made for evaluating such characteristics (GLYPH<28>delity and correlation) and are widely used in practice [38]GLYPH<21>[40]. To compute PLCC and RMSE, we apply the following logistic function as suggested in [41]:\n<!-- formula-not-decoded -->\nwhere x is the score (predicted accuracy) and GLYPH<12> i ( i D 1 ; 2 ; 3 ; 4 ; 5) are the set of parameters that minimize the least squares error between the output from indicator and the network's accuracy.\nSince our FI is a learning-based method, the optimal w may not generalize well on other test data. To prevent overGLYPH<28>tting, we use n-fold cross-validation. SpeciGLYPH<28>cally, we use 5-fold cross-validation and average the results obtained from 5 folds to get the overall performance. In all experiments, we run the 5-fold cross-validation 10 times and average the results to obtain the GLYPH<28>nal performance.",
    "context": "Evaluates the indicator's performance using Spearman and Kendall correlation, Pearson correlation, and RMSE, employing 5-fold cross-validation to assess generalizability and prevent overfitting.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      5
    ],
    "id": "43cb228980e28ab7fa936d3ed19b5cef8541a9763e26f7c113e20dbfcc14f369"
  },
  {
    "text": "We compare the performance of our FI and standalone indicators, i.e. CJ, NLR, CNNTK, OS on CIFAR-10 of NASBench-101 search space. Because we have four indicators, there are 11 combinations for our FI. FI2 means two indicators are used and so on. We use f CJ, OS g for FI2, f CJ, CNNTK, OS g for FI3, and f CJ, NLR, CNNTK, OS g for FI4. We refer the reader to the Ablation section for the performance of other combinations. The results are shown in Table 1.\nAs shown in Table 1, the proposed FI outperforms other indicators signiGLYPH<28>cantly in all performance assessment methods. Notably, our FI achieves 5% higher KROCC and SROCCthan the CJ-based single indicator [16]. Compared to NLR, CNNTK and OS, the proposed FI has around 24% and 30% higher KROCC and SROCC, respectively. Additionally, it is worth noting that increasing the number of indicator does not improve the performance on NAS-Bench-101. One possible reason for this is that there are only three operations (i.e., 3 GLYPH<2> 3 convolution, 1 GLYPH<2> 1 convolution, 3 GLYPH<2> 3 max pool) in the search space. This reduces the diversity of the network architectures even though NAS-Bench-101 contains\nTABLE 1. Performance of several training-free indicators measured on CIFAR-10 of NAS-Bench-101. The first, second, and third ranked performances are highlighted in blue, red, and black bold, respectively.\nmore architectures than NAS-Bench-201. As a result, several indicators do not contribute to the overall performance.\n\nCompares the performance of the proposed Fusion Indicator (FI) to standalone indicators (CJ, NLR, CNNTK, OS) on CIFAR-10 of NAS-Bench-101, demonstrating that the FI significantly outperforms the individual indicators in terms of KROCC and SROCC, achieving a 5% higher KROCC and SROCC than the CJ-based single indicator.",
    "original_text": "We compare the performance of our FI and standalone indicators, i.e. CJ, NLR, CNNTK, OS on CIFAR-10 of NASBench-101 search space. Because we have four indicators, there are 11 combinations for our FI. FI2 means two indicators are used and so on. We use f CJ, OS g for FI2, f CJ, CNNTK, OS g for FI3, and f CJ, NLR, CNNTK, OS g for FI4. We refer the reader to the Ablation section for the performance of other combinations. The results are shown in Table 1.\nAs shown in Table 1, the proposed FI outperforms other indicators signiGLYPH<28>cantly in all performance assessment methods. Notably, our FI achieves 5% higher KROCC and SROCCthan the CJ-based single indicator [16]. Compared to NLR, CNNTK and OS, the proposed FI has around 24% and 30% higher KROCC and SROCC, respectively. Additionally, it is worth noting that increasing the number of indicator does not improve the performance on NAS-Bench-101. One possible reason for this is that there are only three operations (i.e., 3 GLYPH<2> 3 convolution, 1 GLYPH<2> 1 convolution, 3 GLYPH<2> 3 max pool) in the search space. This reduces the diversity of the network architectures even though NAS-Bench-101 contains\nTABLE 1. Performance of several training-free indicators measured on CIFAR-10 of NAS-Bench-101. The first, second, and third ranked performances are highlighted in blue, red, and black bold, respectively.\nmore architectures than NAS-Bench-201. As a result, several indicators do not contribute to the overall performance.",
    "context": "Compares the performance of the proposed Fusion Indicator (FI) to standalone indicators (CJ, NLR, CNNTK, OS) on CIFAR-10 of NAS-Bench-101, demonstrating that the FI significantly outperforms the individual indicators in terms of KROCC and SROCC, achieving a 5% higher KROCC and SROCC than the CJ-based single indicator.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      5
    ],
    "id": "28d263abe193d001cdcc1bb3c45c1ba0c213c48199be31515d797bda4f2be284"
  },
  {
    "text": "We further evaluate the performance of training-free indicators on NAS-Bench-201. We use f CNNTK, OS g for FI2, f NLR, CNNTK, OS g for FI3, and all indicators for FI4. The performance measured by PLCC, RMSE, SROCC, and KROCC for all competitors on this dataset are shown in Table 2.\nFrom Table 2, it can be seen that the proposed FI outperforms standalone indicators in all performance assessment methods for all datasets on NAS-Bench-201. It is worth noting that OS performs the best compared to CJ, NLR, and CNNTK. On CIFAR-10, FI2 has 7.29% and 8.58% higher SROCC and KROCC than OS. Adding more indicators to our FI further increases the performance. SpeciGLYPH<28>cally, FI3 has 7.43% and 8.85% SROCC and KROCC improvement over OS. When using all indicators, FI4 reaches its peak with 0.88 and 0.7017 SROCC and KROCC, respectively.\nOn CIFAR-100, our FI performs consistently well. Standalone indicator achieves less than 0.8 SROCC and 0.6 KROCC. By contrast, the proposed FI obtains more than 0.86 SROCC and 0.68 KROCC. On ImageNet-16-120, we can see that FI4 has the best performance for all assessment metrics. Compared to OS, FI4 has 9% and 11% higher SROCC and KROCC.\nAdditionally, we illustrate the scatter plots of the predicted scores versus the accuracy measured by CJ, NLR, CNNTK, and OS in Figure 2. We show the test accuracy with respect to the score of CJ, NLR, CNNTK, and OS in the GLYPH<28>rst, second, third, and fourth column of Figure 2, respectively. We also show the scatter plots of FI4 for 5-fold cross-validation (CV) in Figure 3. Each column in Figure 3 represents the test accuracy with respect to the score of each fold, namely Fold-1, Fold-2, Fold-3, Fold-4, and Fold-5 in CV. In both GLYPH<28>gures, the green line is the best GLYPH<28>tting curve. Figure 2 demonstrates that the OS indicator has a higher correlation than others. From Figure 3, we can see that there is a very strong correlation between the predicted score and the accuracy when using the proposed FI.\nIn summary, the performance of our FI improves consistently when adding more indicators. This may come from the diversity of the search space. Besides the three operations\nTABLE 2. Performance of several training-free indicators measured on CIFAR-10, CIFAR-100, and ImageNet-16-120 of NAS-Bench-201 search space. The first, second, and third ranked performances are highlighted in blue, red, and black bold, respectively.\nFIGURE 2. The plots of the score for all architectures in NAS-Bench-201 against the test accuracies on CIFAR-10, CIFAR-100, and ImageNet-16-120. For better visualization, the score is scaled to the range of 0 and 1. The best fitting curve is shown in green line. The Kendall Tau values show a strong correlation for all indicators.\nused in [32], NAS-Bench-201 uses two more operators, zero and skip-connect, which makes the search space richer. Thus, the proposed FI can fully utilize all characteristics of a network to evaluate the network correctly.\n\nEvaluates the performance of training-free indicators on NAS-Bench-201, demonstrating that adding more indicators to the fusion indicator consistently improves performance across datasets (CIFAR-10, CIFAR-100, and ImageNet-16-120) and metrics (SROCC, KROCC, PLCC, RMSE).",
    "original_text": "We further evaluate the performance of training-free indicators on NAS-Bench-201. We use f CNNTK, OS g for FI2, f NLR, CNNTK, OS g for FI3, and all indicators for FI4. The performance measured by PLCC, RMSE, SROCC, and KROCC for all competitors on this dataset are shown in Table 2.\nFrom Table 2, it can be seen that the proposed FI outperforms standalone indicators in all performance assessment methods for all datasets on NAS-Bench-201. It is worth noting that OS performs the best compared to CJ, NLR, and CNNTK. On CIFAR-10, FI2 has 7.29% and 8.58% higher SROCC and KROCC than OS. Adding more indicators to our FI further increases the performance. SpeciGLYPH<28>cally, FI3 has 7.43% and 8.85% SROCC and KROCC improvement over OS. When using all indicators, FI4 reaches its peak with 0.88 and 0.7017 SROCC and KROCC, respectively.\nOn CIFAR-100, our FI performs consistently well. Standalone indicator achieves less than 0.8 SROCC and 0.6 KROCC. By contrast, the proposed FI obtains more than 0.86 SROCC and 0.68 KROCC. On ImageNet-16-120, we can see that FI4 has the best performance for all assessment metrics. Compared to OS, FI4 has 9% and 11% higher SROCC and KROCC.\nAdditionally, we illustrate the scatter plots of the predicted scores versus the accuracy measured by CJ, NLR, CNNTK, and OS in Figure 2. We show the test accuracy with respect to the score of CJ, NLR, CNNTK, and OS in the GLYPH<28>rst, second, third, and fourth column of Figure 2, respectively. We also show the scatter plots of FI4 for 5-fold cross-validation (CV) in Figure 3. Each column in Figure 3 represents the test accuracy with respect to the score of each fold, namely Fold-1, Fold-2, Fold-3, Fold-4, and Fold-5 in CV. In both GLYPH<28>gures, the green line is the best GLYPH<28>tting curve. Figure 2 demonstrates that the OS indicator has a higher correlation than others. From Figure 3, we can see that there is a very strong correlation between the predicted score and the accuracy when using the proposed FI.\nIn summary, the performance of our FI improves consistently when adding more indicators. This may come from the diversity of the search space. Besides the three operations\nTABLE 2. Performance of several training-free indicators measured on CIFAR-10, CIFAR-100, and ImageNet-16-120 of NAS-Bench-201 search space. The first, second, and third ranked performances are highlighted in blue, red, and black bold, respectively.\nFIGURE 2. The plots of the score for all architectures in NAS-Bench-201 against the test accuracies on CIFAR-10, CIFAR-100, and ImageNet-16-120. For better visualization, the score is scaled to the range of 0 and 1. The best fitting curve is shown in green line. The Kendall Tau values show a strong correlation for all indicators.\nused in [32], NAS-Bench-201 uses two more operators, zero and skip-connect, which makes the search space richer. Thus, the proposed FI can fully utilize all characteristics of a network to evaluate the network correctly.",
    "context": "Evaluates the performance of training-free indicators on NAS-Bench-201, demonstrating that adding more indicators to the fusion indicator consistently improves performance across datasets (CIFAR-10, CIFAR-100, and ImageNet-16-120) and metrics (SROCC, KROCC, PLCC, RMSE).",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      5,
      6
    ],
    "id": "2a0165a2a3aaa6d62e3fd315101d8e2eea31bb5747d36937586887d56f3d194e"
  },
  {
    "text": "We perform cross dataset tests to verify the generalizability of our FI for different datasets. To do this, we obtain the weights for the FI trained on one dataset (e.g., CIFAR-10) and evaluate it on other datasets. For example, we use the score evaluated on CIFAR-10 to train the weight for the FI and use the same weights to perform testing on CIFAR-100 and ImageNet-16-120. We denote the weight obtained from CIFAR-10, CIFAR-100, and ImageNet-16-120 as w CIFAR-10, w CIFAR-100, and w ImageNet-16-120, respectively. The performance results are listed in Table 3.\nTABLE 3. Performance of FI 4 on NAS-Bench-201, measured by KROCC.\nAs shown in Table 3, we can see that the proposed FI achieves a high KROCC. For all datasets, the value of KROCC is greater than 0.67. The weights obtained on one dataset are highly compatible with other datasets. This shows the generality and robustness of our approach.\n\nThis section demonstrates the generalizability of the proposed indicator by transferring weights learned on one dataset (CIFAR-10) to evaluate performance on other datasets (CIFAR-100 and ImageNet-16-120), revealing consistent high KROCC values (greater than 0.67) and demonstrating the robustness of the approach.",
    "original_text": "We perform cross dataset tests to verify the generalizability of our FI for different datasets. To do this, we obtain the weights for the FI trained on one dataset (e.g., CIFAR-10) and evaluate it on other datasets. For example, we use the score evaluated on CIFAR-10 to train the weight for the FI and use the same weights to perform testing on CIFAR-100 and ImageNet-16-120. We denote the weight obtained from CIFAR-10, CIFAR-100, and ImageNet-16-120 as w CIFAR-10, w CIFAR-100, and w ImageNet-16-120, respectively. The performance results are listed in Table 3.\nTABLE 3. Performance of FI 4 on NAS-Bench-201, measured by KROCC.\nAs shown in Table 3, we can see that the proposed FI achieves a high KROCC. For all datasets, the value of KROCC is greater than 0.67. The weights obtained on one dataset are highly compatible with other datasets. This shows the generality and robustness of our approach.",
    "context": "This section demonstrates the generalizability of the proposed indicator by transferring weights learned on one dataset (CIFAR-10) to evaluate performance on other datasets (CIFAR-100 and ImageNet-16-120), revealing consistent high KROCC values (greater than 0.67) and demonstrating the robustness of the approach.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      6
    ],
    "id": "fceabdefb5548ecb215a2e03d2581a9689472226f748e9e3ab1770589dcfad9f"
  },
  {
    "text": "The ultimate goal of NAS without Training is to replace the heavy cost of training candidate networks with inexpensive\nFIGURE 3. Scatter plots of predicted score, FI 4 , against the accuracy for each fold on CIFAR-10, CIFAR-100, and ImageNet-16-120 of NAS-Bench-201 search space. The best fitting curve is shown in green line.\nones (e.g., CJ, NLR...). In order to demonstrate the effectiveness of training-free indicators, we incorporate the score from training-free indicator into Aging Evolution (AE), an evolutionary algorithm for NAS [42]. We replace the network accuracy obtained from the network training step ( train and evaluation in [42]) with the predicted score from the indicator. For the mutation phase, the network which has the highest predicted score is selected as a parent. After mutation, the child network is scored using the indicator. Finally, the output is the network which has the highest predicted score. We choose the AE algorithm due to its simplicity.\nWe compare the performance of AE using CJ, NLR, CNNTK,OS,andtheproposedFI4 on CIFAR-100 with NASBench-201 search space. We evaluate around 300 networks. Werunthe experiment 100 times and show the average results in Figure 4. As demonstrated in Figure 4, it is clear that the proposed FI achieves higher test accuracy for the network with the highest predicted score than others. SpeciGLYPH<28>cally, NLR has the lowest test accuracy for the network with the highest predicted score. The three indicators CJ, CNNTK, and OS achieve comparable test accuracy. It is noticeable that the proposed FI achieves much higher accuracy than others, where we perform experiments 100 times for each case and take an average of the 100 test accuracies.\n\nDemonstrates the effectiveness of training-free indicators by integrating them into an evolutionary algorithm (AE) for NAS, replacing network accuracy with indicator scores to guide network selection during mutation.",
    "original_text": "The ultimate goal of NAS without Training is to replace the heavy cost of training candidate networks with inexpensive\nFIGURE 3. Scatter plots of predicted score, FI 4 , against the accuracy for each fold on CIFAR-10, CIFAR-100, and ImageNet-16-120 of NAS-Bench-201 search space. The best fitting curve is shown in green line.\nones (e.g., CJ, NLR...). In order to demonstrate the effectiveness of training-free indicators, we incorporate the score from training-free indicator into Aging Evolution (AE), an evolutionary algorithm for NAS [42]. We replace the network accuracy obtained from the network training step ( train and evaluation in [42]) with the predicted score from the indicator. For the mutation phase, the network which has the highest predicted score is selected as a parent. After mutation, the child network is scored using the indicator. Finally, the output is the network which has the highest predicted score. We choose the AE algorithm due to its simplicity.\nWe compare the performance of AE using CJ, NLR, CNNTK,OS,andtheproposedFI4 on CIFAR-100 with NASBench-201 search space. We evaluate around 300 networks. Werunthe experiment 100 times and show the average results in Figure 4. As demonstrated in Figure 4, it is clear that the proposed FI achieves higher test accuracy for the network with the highest predicted score than others. SpeciGLYPH<28>cally, NLR has the lowest test accuracy for the network with the highest predicted score. The three indicators CJ, CNNTK, and OS achieve comparable test accuracy. It is noticeable that the proposed FI achieves much higher accuracy than others, where we perform experiments 100 times for each case and take an average of the 100 test accuracies.",
    "context": "Demonstrates the effectiveness of training-free indicators by integrating them into an evolutionary algorithm (AE) for NAS, replacing network accuracy with indicator scores to guide network selection during mutation.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      6,
      7
    ],
    "id": "42727955d1ac6f37c331024abe82ee090bc504b0a27ab55cb80aff99c55662f0"
  },
  {
    "text": "In this section, we study the behavior of the training-free indicator. We investigate how weight initialization affects performance. We also study the behavior of the proposed FI when more training-free indicators are added.\nFIGURE 4. Performance of Aging Evolution using different training-free indicators on CIFAR-100 with NAS-Bench-201 search space. The black dot horizontal line is the best test accuracy on this benchmark.\n\nThe study investigates the impact of weight initialization and the addition of more training-free indicators on the performance of a fusion indicator (FI) within the NAS-Bench-201 framework. Results demonstrate consistent performance improvements with increased indicator usage, highlighting the FI's effectiveness in ranking networks and its robustness across different initialization methods and combinations.",
    "original_text": "In this section, we study the behavior of the training-free indicator. We investigate how weight initialization affects performance. We also study the behavior of the proposed FI when more training-free indicators are added.\nFIGURE 4. Performance of Aging Evolution using different training-free indicators on CIFAR-100 with NAS-Bench-201 search space. The black dot horizontal line is the best test accuracy on this benchmark.",
    "context": "The study investigates the impact of weight initialization and the addition of more training-free indicators on the performance of a fusion indicator (FI) within the NAS-Bench-201 framework. Results demonstrate consistent performance improvements with increased indicator usage, highlighting the FI's effectiveness in ranking networks and its robustness across different initialization methods and combinations.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      7
    ],
    "id": "9ddc8b72473ad24147642d6495d392b254a079c69b557a3861590ab537bfb63c"
  },
  {
    "text": "When we develop an indicator for training-free NAS framework, the most critical factor is how robust our indicator is. At the initial state, our network is unstable. There are several initialization methodologies for a network. However, in this study, we assess the performance of training-free indicators using the following initialization methods V\n- GLYPH<15> Uniform distribution: For uniform distribution, the weights are initialized from U ( GLYPH<0> p k ; p k ), where k D groups = (C in GLYPH<3> Q 1 i D 0 k [ i ]).\n- GLYPH<15> Normal distribution: The weights are sampled from N (0 ; std 2 ). For OS indicator, we only use the standard\nTABLE 4. KROCC of indicators with different weight sampling methods on CIFAR-10 of NAS-Bench-201.\nnormal distribution without any batch normalization as mentioned in [17].\n- GLYPH<15> Kaiming He: Following [43], the weights are sampled from N (0 ; std 2 ), where std D gain = fanmode 1 = 2 .\nWe compute KROCC for each indicator with different weight sampling methods. Table 4 demonstrates the quantitative performance of each indicator for different weight sampling methods. As shown in Table 4, the performance of training-free indicators depends on how the weights are sampled, especially for CJ. One possible explanation for this is that the hypothesis of the Jacobian indicator requires the same local linear operators, where the linear maps are the Jacobian of augmented input x , should have low correlation. This means that if any weight initialization method violates the hypothesis, the Jacobian indicator will not work (e.g., strong correlation with different augmented input x ).\nTo avoid exhaustively computing all possible weight sampling combinations for FI, we use a simple technique that selects the best sampling method for each indicator. Thus, uniform distribution is used for computing CJ, NLR, CNNTK and normal distribution is used for OS.\n\nHighlights the importance of indicator robustness and the dependence of performance on weight initialization methods, specifically noting the impact on the Jacobian indicator and the strategy of selecting the best initialization method for each indicator.",
    "original_text": "When we develop an indicator for training-free NAS framework, the most critical factor is how robust our indicator is. At the initial state, our network is unstable. There are several initialization methodologies for a network. However, in this study, we assess the performance of training-free indicators using the following initialization methods V\n- GLYPH<15> Uniform distribution: For uniform distribution, the weights are initialized from U ( GLYPH<0> p k ; p k ), where k D groups = (C in GLYPH<3> Q 1 i D 0 k [ i ]).\n- GLYPH<15> Normal distribution: The weights are sampled from N (0 ; std 2 ). For OS indicator, we only use the standard\nTABLE 4. KROCC of indicators with different weight sampling methods on CIFAR-10 of NAS-Bench-201.\nnormal distribution without any batch normalization as mentioned in [17].\n- GLYPH<15> Kaiming He: Following [43], the weights are sampled from N (0 ; std 2 ), where std D gain = fanmode 1 = 2 .\nWe compute KROCC for each indicator with different weight sampling methods. Table 4 demonstrates the quantitative performance of each indicator for different weight sampling methods. As shown in Table 4, the performance of training-free indicators depends on how the weights are sampled, especially for CJ. One possible explanation for this is that the hypothesis of the Jacobian indicator requires the same local linear operators, where the linear maps are the Jacobian of augmented input x , should have low correlation. This means that if any weight initialization method violates the hypothesis, the Jacobian indicator will not work (e.g., strong correlation with different augmented input x ).\nTo avoid exhaustively computing all possible weight sampling combinations for FI, we use a simple technique that selects the best sampling method for each indicator. Thus, uniform distribution is used for computing CJ, NLR, CNNTK and normal distribution is used for OS.",
    "context": "Highlights the importance of indicator robustness and the dependence of performance on weight initialization methods, specifically noting the impact on the Jacobian indicator and the strategy of selecting the best initialization method for each indicator.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      8,
      7
    ],
    "id": "8c2ab019548833124e6b86c4671090fd547cf4c239561027ab13a9918f55cc11"
  },
  {
    "text": "We also investigate the performance of our FI by combining different indicators on CIFAR-10. There are 11 combinations in total. The weights are sampled following the above analysis. KROCC is used to measure performance. We list the results in Table 5.\nFrom Table 5, it can be seen that on NAS-Bench-101 combining indicators further improves the performance. For example, NLR, CNNTK, OS achieves around 0.27 KROCC but combining them enhances the performance to 0.30 GLYPH<24> 0.34 KROCC. The best combination is f CJ, OS g .\nOn NAS-Bench-201, we observe that when two indicators are used, the performance measured by KROCC has a high standard deviation (e.g., the value of KROCC is ranging from 0.5610 to 0.6965). On a search space that has higher diversity (e.g., more operators), fusing more indicators helps improve the performance of the network. SpeciGLYPH<28>cally, our best KROCC increases from 0.6965 to 0.6993 and 0.7016 when three, and four indicators are used respectively. We GLYPH<28>nd that when using two indicators, the combination of CNNTK and OS outperforms other combinations. It is natural since CNNTK and OS are the top-two standalone indicator. The overall performance increases when more indicators are used. The behaviour is consistent for all quality assessment methods. We can see that the best correlation is obtained when all indicators are used. Figure 5 shows the trend of\nTABLE 5. Comparison with different combination of indicators for FI on CIFAR-10, measured by KROCC.\nFIGURE 5. Overall comparison for different combination of indicators measured on CIFAR-10, NAS-Bench-201.\nincreasing the number of indicators for FI on NAS-Bench201 improves the performance.\nFor comparing the difference in performance, we conduct the statistical tests between the proposed Fusion Indicator and the standalone one. We GLYPH<28>rst deGLYPH<28>ne the absolute difference values between the predicted score O y (after nonlinear regression) and the actual accuracy y as 1 D j y GLYPH<0> O y j . The GLYPH<28>rst test veriGLYPH<28>es the normality of 1 and the second test determines whether the 1 from one indicator are statistically indistinguishable from another indicator. The signiGLYPH<28>cance level is 5% for both tests. We use Shapiro-Wilk test for normality and the results ( p -value) are shown in Table 6. For the second test, Wilcoxon rank-sum is performed because there is no normal distribution case as in Table 6. The results for the second test are shown in Table 7. In most cases, the indicators are statistically different from each other, except for CNNTK versus NLR on NAS-Bench-101, which is not different. In general, the proposed FI statistically improves the performance.\n\nEvaluating the performance of the Fusion Indicator by combining different indicators on CIFAR-10, specifically demonstrating that combining indicators like NLR, CNNTK, and OS enhances performance compared to standalone indicators, as evidenced by increased KROCC values.",
    "original_text": "We also investigate the performance of our FI by combining different indicators on CIFAR-10. There are 11 combinations in total. The weights are sampled following the above analysis. KROCC is used to measure performance. We list the results in Table 5.\nFrom Table 5, it can be seen that on NAS-Bench-101 combining indicators further improves the performance. For example, NLR, CNNTK, OS achieves around 0.27 KROCC but combining them enhances the performance to 0.30 GLYPH<24> 0.34 KROCC. The best combination is f CJ, OS g .\nOn NAS-Bench-201, we observe that when two indicators are used, the performance measured by KROCC has a high standard deviation (e.g., the value of KROCC is ranging from 0.5610 to 0.6965). On a search space that has higher diversity (e.g., more operators), fusing more indicators helps improve the performance of the network. SpeciGLYPH<28>cally, our best KROCC increases from 0.6965 to 0.6993 and 0.7016 when three, and four indicators are used respectively. We GLYPH<28>nd that when using two indicators, the combination of CNNTK and OS outperforms other combinations. It is natural since CNNTK and OS are the top-two standalone indicator. The overall performance increases when more indicators are used. The behaviour is consistent for all quality assessment methods. We can see that the best correlation is obtained when all indicators are used. Figure 5 shows the trend of\nTABLE 5. Comparison with different combination of indicators for FI on CIFAR-10, measured by KROCC.\nFIGURE 5. Overall comparison for different combination of indicators measured on CIFAR-10, NAS-Bench-201.\nincreasing the number of indicators for FI on NAS-Bench201 improves the performance.\nFor comparing the difference in performance, we conduct the statistical tests between the proposed Fusion Indicator and the standalone one. We GLYPH<28>rst deGLYPH<28>ne the absolute difference values between the predicted score O y (after nonlinear regression) and the actual accuracy y as 1 D j y GLYPH<0> O y j . The GLYPH<28>rst test veriGLYPH<28>es the normality of 1 and the second test determines whether the 1 from one indicator are statistically indistinguishable from another indicator. The signiGLYPH<28>cance level is 5% for both tests. We use Shapiro-Wilk test for normality and the results ( p -value) are shown in Table 6. For the second test, Wilcoxon rank-sum is performed because there is no normal distribution case as in Table 6. The results for the second test are shown in Table 7. In most cases, the indicators are statistically different from each other, except for CNNTK versus NLR on NAS-Bench-101, which is not different. In general, the proposed FI statistically improves the performance.",
    "context": "Evaluating the performance of the Fusion Indicator by combining different indicators on CIFAR-10, specifically demonstrating that combining indicators like NLR, CNNTK, and OS enhances performance compared to standalone indicators, as evidenced by increased KROCC values.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      8
    ],
    "id": "d42f8336a49eaf8d7d0634bf686ffb2a26214feb4a708e6c3af60943fcbc5534"
  },
  {
    "text": "There can be a few factors that threaten the validity of this research. We brieGLYPH<29>y list several potential threats V\n- GLYPH<15> Search space: There are several search spaces which are used in other NAS algorithms such as AmoebaNet [42],\nTABLE 6. The results from Shapiro-Wilk tests on CIFAR-10.\nTABLE 7. The statistical significance matrix on CIFAR-10 with 95% confidence. Each element in the table is a codeword for 2 symbols. The first and second position in the symbol indicate the result of the hypothesis test on NAS-Bench-201 and NAS-Bench-101.\nFBNet [44]. The search space of NAS-Bench-101/201 is much smaller than the aforementioned search space.\n- GLYPH<15> Dataset: Current NAS benchmark only uses small dataset such as CIFAR-10, CIFAR-100, and reduced ImageNet-16-120. Thus, it is common to ask whether the training-free indicator works well on large-scale datasets (e.g., ImageNet [1]).\n- GLYPH<15> Bias in the experiment: Since the training-free indicator evaluates the network before training, the score is affected by the value of the network's parameters.\nRegarding the GLYPH<28>rst and the second threats, there is no benchmark dataset for these search spaces at the current state. It is because training on a large-scale dataset requires a lot of computational costs, as mentioned in [33]. Due to this reason, most NAS benchmark datasets use smaller search spaces and train the networks on a small dataset such as CIFAR-10. However, we will validate the performance of training-free indicators as well as the Fusion indicator once they are available. In order to counter the last threat, we conduct the experiments multiple times (i.e., 10 times) with different random seeds and take the average as the GLYPH<28>nal results. This mitigates the inGLYPH<29>uence of random weights on the results. Extensive experiments on two recently released NAS-Bench-101/201 conGLYPH<28>rm that the Fusion Indicator brings many beneGLYPH<28>ts in ranking the networks.\n\nIdentifies potential threats to the research validity, specifically regarding search space limitations, dataset scale, and experimental bias. It outlines steps to mitigate these threats, including planned validation with larger datasets and repeated experiments with random seeds.",
    "original_text": "There can be a few factors that threaten the validity of this research. We brieGLYPH<29>y list several potential threats V\n- GLYPH<15> Search space: There are several search spaces which are used in other NAS algorithms such as AmoebaNet [42],\nTABLE 6. The results from Shapiro-Wilk tests on CIFAR-10.\nTABLE 7. The statistical significance matrix on CIFAR-10 with 95% confidence. Each element in the table is a codeword for 2 symbols. The first and second position in the symbol indicate the result of the hypothesis test on NAS-Bench-201 and NAS-Bench-101.\nFBNet [44]. The search space of NAS-Bench-101/201 is much smaller than the aforementioned search space.\n- GLYPH<15> Dataset: Current NAS benchmark only uses small dataset such as CIFAR-10, CIFAR-100, and reduced ImageNet-16-120. Thus, it is common to ask whether the training-free indicator works well on large-scale datasets (e.g., ImageNet [1]).\n- GLYPH<15> Bias in the experiment: Since the training-free indicator evaluates the network before training, the score is affected by the value of the network's parameters.\nRegarding the GLYPH<28>rst and the second threats, there is no benchmark dataset for these search spaces at the current state. It is because training on a large-scale dataset requires a lot of computational costs, as mentioned in [33]. Due to this reason, most NAS benchmark datasets use smaller search spaces and train the networks on a small dataset such as CIFAR-10. However, we will validate the performance of training-free indicators as well as the Fusion indicator once they are available. In order to counter the last threat, we conduct the experiments multiple times (i.e., 10 times) with different random seeds and take the average as the GLYPH<28>nal results. This mitigates the inGLYPH<29>uence of random weights on the results. Extensive experiments on two recently released NAS-Bench-101/201 conGLYPH<28>rm that the Fusion Indicator brings many beneGLYPH<28>ts in ranking the networks.",
    "context": "Identifies potential threats to the research validity, specifically regarding search space limitations, dataset scale, and experimental bias. It outlines steps to mitigate these threats, including planned validation with larger datasets and repeated experiments with random seeds.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      8,
      9
    ],
    "id": "a4de586ab7e0ebefe179cfc9aa7fd2d8cd4432e25be12b8cc4aff1a170cde144"
  },
  {
    "text": "In this paper, we proposed a simple yet effective method for training-free NAS, called FI. The core motivation of our work is to harmonize several characteristics, including correlation of Jacobian, the number of linear regions, the condition number of the neural tangent kernel, and the output sensitivity of a network in a weighted sum manner. Through extensive experiments, our work demonstrated that the proposed FI achieves a higher correlation between the predicted score and the network's accuracy than standalone indicator in all quality assessment methods. The best combination on NAS-Bench-101 and NAS-Bench-201 is f CJ, OS g and f CJ, NLR, CNNTK, OS g , respectively. The large search space such as NAS-Bench-201 can fully use the characteristics effectively because of the diversity in their search space. Interestingly, we GLYPH<28>nd that fusing only two indicators (e.g., f CNNTK, OS g on NAS-Bench-201) achieved comparable performance to fuse four indicators.\nWe encourage the researchers in this GLYPH<28>eld to discover more characteristics of a network so that we can develop a powerful training-free indicator. The proposed FI can be applied to query-based NAS algorithms such as random search or evolutionary search easily. We plan to develop an efGLYPH<28>cient search method that uses the proposed FI in our future work.\n\nSummarizes the core contribution of the paper: a new training-free NAS indicator (FI) that outperforms standalone indicators by harmonizing multiple network characteristics and achieving comparable performance to larger indicator combinations.",
    "original_text": "In this paper, we proposed a simple yet effective method for training-free NAS, called FI. The core motivation of our work is to harmonize several characteristics, including correlation of Jacobian, the number of linear regions, the condition number of the neural tangent kernel, and the output sensitivity of a network in a weighted sum manner. Through extensive experiments, our work demonstrated that the proposed FI achieves a higher correlation between the predicted score and the network's accuracy than standalone indicator in all quality assessment methods. The best combination on NAS-Bench-101 and NAS-Bench-201 is f CJ, OS g and f CJ, NLR, CNNTK, OS g , respectively. The large search space such as NAS-Bench-201 can fully use the characteristics effectively because of the diversity in their search space. Interestingly, we GLYPH<28>nd that fusing only two indicators (e.g., f CNNTK, OS g on NAS-Bench-201) achieved comparable performance to fuse four indicators.\nWe encourage the researchers in this GLYPH<28>eld to discover more characteristics of a network so that we can develop a powerful training-free indicator. The proposed FI can be applied to query-based NAS algorithms such as random search or evolutionary search easily. We plan to develop an efGLYPH<28>cient search method that uses the proposed FI in our future work.",
    "context": "Summarizes the core contribution of the paper: a new training-free NAS indicator (FI) that outperforms standalone indicators by harmonizing multiple network characteristics and achieving comparable performance to larger indicator combinations.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      9
    ],
    "id": "fe5207211383b3333adb2a41e8f489d3fc080a9e8a46cf53449feff449f26bb6"
  },
  {
    "text": "- [1] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ''ImageNet: A large-scale hierarchical image database,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , Jun. 2009, pp. 248GLYPH<21>255.\n- [2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ''ImageNet classiGLYPH<28>cation with deep convolutional neural networks,'' in Proc. Adv. Neural Inf. Process. Syst. , vol. 25, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, Eds. Red Hook, NY, USA: Curran Associates, 2012, pp. 1097GLYPH<21>1105.\n- [3] K. He, X. Zhang, S. Ren, and J. Sun, ''Deep residual learning for image recognition,'' 2015, arXiv:1512.03385 . [Online]. Available: https://arxiv.org/abs/1512.03385\n- [4] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, ''Densely connected convolutional networks,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jul. 2017, pp. 4700GLYPH<21>4708.\n- [5] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, ''MobileNets: EfGLYPH<28>cient convolutional neural networks for mobile vision applications,'' 2017, arXiv:1704.04861 . [Online]. Available: https://arxiv.org/abs/1704.04861\n- [6] B. Zoph and Q. V. Le, ''Neural architecture search with reinforcement learning,'' 2016, arXiv:1611.01578 . [Online]. Available: https://arxiv. org/abs/1611.01578\n- [7] H. Liu, K. Simonyan, and Y. Yang, ''DARTS: Differentiable architecture search,'' in Proc. Int. Conf. Learn. Represent. , 2019, pp. 1GLYPH<21>12.\n- [8] M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard, and Q. V. Le, ''MnasNet: Platform-aware neural architecture search for mobile,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2019, pp. 2820GLYPH<21>2828.\n- [9] B. Chen, G. Ghiasi, H. Liu, T.-Y. Lin, D. Kalenichenko, H. Adam, and Q. V. Le, ''MnasFPN: Learning latency-aware pyramid architecture for object detection on mobile devices,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2020, pp. 2820GLYPH<21>2828.\n- [10] N. Wang, Y. Gao, H. Chen, P. Wang, Z. Tian, C. Shen, and Y. Zhang, ''NAS-FCOS: Fast neural architecture search for object detection,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2020, pp. 11943GLYPH<21>11951.\n- [11] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, ''Learning transferable architectures for scalable image recognition,'' 2017, arXiv:1707.07012 . [Online]. Available: https://arxiv.org/abs/1707.07012\n- [12] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean, ''EfGLYPH<28>cient neural architecture search via parameter sharing,'' 2018, arXiv:1802.03268 . [Online]. Available: https://arxiv.org/abs/1802.03268\n\nIntroduces foundational image classification datasets and models, establishing the context for subsequent discussions on neural architecture search and benchmark datasets.",
    "original_text": "- [1] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ''ImageNet: A large-scale hierarchical image database,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , Jun. 2009, pp. 248GLYPH<21>255.\n- [2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ''ImageNet classiGLYPH<28>cation with deep convolutional neural networks,'' in Proc. Adv. Neural Inf. Process. Syst. , vol. 25, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, Eds. Red Hook, NY, USA: Curran Associates, 2012, pp. 1097GLYPH<21>1105.\n- [3] K. He, X. Zhang, S. Ren, and J. Sun, ''Deep residual learning for image recognition,'' 2015, arXiv:1512.03385 . [Online]. Available: https://arxiv.org/abs/1512.03385\n- [4] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, ''Densely connected convolutional networks,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jul. 2017, pp. 4700GLYPH<21>4708.\n- [5] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, ''MobileNets: EfGLYPH<28>cient convolutional neural networks for mobile vision applications,'' 2017, arXiv:1704.04861 . [Online]. Available: https://arxiv.org/abs/1704.04861\n- [6] B. Zoph and Q. V. Le, ''Neural architecture search with reinforcement learning,'' 2016, arXiv:1611.01578 . [Online]. Available: https://arxiv. org/abs/1611.01578\n- [7] H. Liu, K. Simonyan, and Y. Yang, ''DARTS: Differentiable architecture search,'' in Proc. Int. Conf. Learn. Represent. , 2019, pp. 1GLYPH<21>12.\n- [8] M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard, and Q. V. Le, ''MnasNet: Platform-aware neural architecture search for mobile,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2019, pp. 2820GLYPH<21>2828.\n- [9] B. Chen, G. Ghiasi, H. Liu, T.-Y. Lin, D. Kalenichenko, H. Adam, and Q. V. Le, ''MnasFPN: Learning latency-aware pyramid architecture for object detection on mobile devices,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2020, pp. 2820GLYPH<21>2828.\n- [10] N. Wang, Y. Gao, H. Chen, P. Wang, Z. Tian, C. Shen, and Y. Zhang, ''NAS-FCOS: Fast neural architecture search for object detection,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2020, pp. 11943GLYPH<21>11951.\n- [11] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, ''Learning transferable architectures for scalable image recognition,'' 2017, arXiv:1707.07012 . [Online]. Available: https://arxiv.org/abs/1707.07012\n- [12] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean, ''EfGLYPH<28>cient neural architecture search via parameter sharing,'' 2018, arXiv:1802.03268 . [Online]. Available: https://arxiv.org/abs/1802.03268",
    "context": "Introduces foundational image classification datasets and models, establishing the context for subsequent discussions on neural architecture search and benchmark datasets.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      9
    ],
    "id": "84e627c0c4d3f2f721013b401fa44b5cdc8a4ca8f4be88476e6f1bf67764d03c"
  },
  {
    "text": "- [13] J. Chen, K. Li, K. Bilal, X. Zhou, K. Li, and P. S. Yu, ''A bi-layered parallel training architecture for large-scale convolutional neural networks,'' IEEE Trans. Parallel Distrib. Syst. , vol. 30, no. 5, pp. 965GLYPH<21>976, May 2019.\n- [14] Y. Xu, L. Xie, X. Zhang, X. Chen, G. Qi, Q. Tian, and H. Xiong, ''PC-DARTS: Partial channel connections for memory-efGLYPH<28>cient differentiable architecture search,'' 2019, arXiv:1907.05737 . [Online]. Available: https://arxiv.org/abs/1907.05737\n- [15] C. Li, Z. Yu, Y. Fu, Y. Zhang, Y. Zhao, H. You, Q. Yu, Y. Wang, C. Hao, and Y. Lin, ''HW-NAS-Bench: Hardware-aware neural architecture search benchmark,'' in Proc. Int. Conf. Learn. Represent. , 2021. [Online]. Available: https://openreview.net/forum?id=_0kaDkv3dVf\n- [16] J. Mellor, J. Turner, A. Storkey, and E. J. Crowley, ''Neural architecture search without training,'' 2020, arXiv:2006.04647 . [Online]. Available: https://arxiv.org/abs/2006.04647\n- [17] M. Forouzesh, F. Salehi, and P. Thiran, ''Generalization comparison of deep neural networks via output sensitivity,'' in Proc. 25th Int. Conf. Pattern Recognit. (ICPR) , Jan. 2021, pp. 7411GLYPH<21>7418.\n- [18] R. Novak, Y. Bahri, D. A. AbolaGLYPH<28>a, J. Pennington, and J. Sohl-Dickstein, ''Sensitivity and generalization in neural networks: An empirical study,'' 2018, arXiv:1802.08760 . [Online]. Available: https://arxiv.org/abs/1802.08760\n- [19] K. Kawaguchi, L. P. Kaelbling, and Y. Bengio, ''Generalization in deep learning,'' 2017, arXiv:1710.05468 . [Online]. Available: https://arxiv.org/abs/1710.05468\n- [20] A. Jacot, F. Gabriel, and C. Hongler, ''Neural tangent kernel: Convergence and generalization in neural networks,'' 2018, arXiv:1806.07572 . [Online]. Available: https://arxiv.org/abs/1806.07572\n- [21] H. Xiong, L. Huang, M. Yu, L. Liu, F. Zhu, and L. Shao, ''On the number of linear regions of convolutional neural networks,'' in Proc. 37th Int. Conf. Mach. Learn. , vol. 119, H. D. III and A. Singh, Eds. Jul. 2020, pp. 10514GLYPH<21>10523.\n- [22] M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, and J. Sohl-Dickstein, ''On the expressive power of deep neural networks,'' 2017, arXiv:1606.05336 . [Online]. Available: https://arxiv.org/abs/1606.05336\n- [23] L. Xiao, J. Pennington, and S. S. Schoenholz, ''Disentangling trainability and generalization in deep learning,'' 2019, arXiv:1912.13053 . [Online]. Available: https://arxiv.org/abs/1912.13053\n- [24] W. Chen, X. Gong, and Z. Wang, ''Neural architecture search on ImageNet in four GPU hours: A theoretically inspired perspective,'' in Proc. Int. Conf. Learn. Represent. , 2021. [Online]. Available: https://openreview.net/forum?id=Cnon5ezMHtu\n- [25] T. Devries and G. W. Taylor, ''Improved regularization of convolutional neural networks with cutout,'' 2017, arXiv:1708.04552 . [Online]. Available: https://arxiv.org/abs/1708.04552\n- [26] B. Baker, O. Gupta, N. Naik, and R. Raskar, ''Designing neural network architectures using reinforcement learning,'' 2016, arXiv:1611.02167 . [Online]. Available: https://arxiv.org/abs/1611.02167\n- [27] Z. Zhong, J. Yan, W. Wu, J. Shao, and C.-L. Liu, ''Practical blockwise neural network architecture generation,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , Jun. 2018, pp. 2423GLYPH<21>2432.\n- [28] L. Xie and A. Yuille, ''Genetic CNN,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , Oct. 2017, pp. 1379GLYPH<21>1388.\n- [29] E. Real, S. Moore, A. Selle, S. Saxena, Y . L. Suematsu, J. Tan, Q. V . Le, and A. Kurakin, ''Large-scale evolution of image classiGLYPH<28>ers,'' in Proc. 34th Int. Conf. Mach. Learn. (ICML) , Sydney, NSW, Australia, vol. 70, Aug. 2017, pp. 2902GLYPH<21>2911.\n- [30] T. Elsken, J. H. Metzen, and F. Hutter, ''EfGLYPH<28>cient multi-objective neural architecture search via Lamarckian evolution,'' in Proc. Int. Conf. Learn. Represent. , Sep. 2019. [Online]. Available: https:// openreview.net/forum?id=ByME42AqK7\n- [31] X. Chen, L. Xie, J. Wu, and Q. Tian, ''Progressive differentiable architecture search: Bridging the depth gap between search and evaluation,'' 2019, arXiv:1904.12760 . [Online]. Available: https://arxiv.org/abs/1904.12760\n- [32] C. Ying, A. Klein, E. Christiansen, E. Real, K. Murphy, and F. Hutter, ''Nas-bench-101: Towards reproducible neural architecture search,'' in Proc. ICML , 2019, pp. 7105GLYPH<21>7114.\n- [33] X. Dong and Y. Yang, ''NAS-Bench-201: Extending the scope of reproducible neural architecture search,'' in Proc. Int. Conf. Learn. Represent. , 2020, pp. 2GLYPH<21>17.\n- [34] P. Chrabaszcz, I. Loshchilov, and F. Hutter, ''A downsampled variant of ImageNet as an alternative to the CIFAR datasets,'' 2017, arXiv:1707.08819 . [Online]. Available: https://arxiv.org/abs/1707.08819\n- [35] J. Yang, X. Zeng, S. Zhong, and S. Wu, ''Effective neural network ensemble approach for improving generalization performance,'' IEEE Trans. Neural Netw. Learn. Syst. , vol. 24, no. 6, pp. 878GLYPH<21>887, Jun. 2013.\n- [36] R. Pascanu, G. Montufar, and Y. Bengio, ''On the number of response regions of deep feed forward networks with piece-wise linear activations,'' 2014, arXiv:1312.6098 . [Online]. Available: https://arxiv.org/abs/1312.6098\n\nProvides a collection of research papers focused on neural architecture search techniques, including evolutionary algorithms, differentiable architecture search, and hardware-aware design.",
    "original_text": "- [13] J. Chen, K. Li, K. Bilal, X. Zhou, K. Li, and P. S. Yu, ''A bi-layered parallel training architecture for large-scale convolutional neural networks,'' IEEE Trans. Parallel Distrib. Syst. , vol. 30, no. 5, pp. 965GLYPH<21>976, May 2019.\n- [14] Y. Xu, L. Xie, X. Zhang, X. Chen, G. Qi, Q. Tian, and H. Xiong, ''PC-DARTS: Partial channel connections for memory-efGLYPH<28>cient differentiable architecture search,'' 2019, arXiv:1907.05737 . [Online]. Available: https://arxiv.org/abs/1907.05737\n- [15] C. Li, Z. Yu, Y. Fu, Y. Zhang, Y. Zhao, H. You, Q. Yu, Y. Wang, C. Hao, and Y. Lin, ''HW-NAS-Bench: Hardware-aware neural architecture search benchmark,'' in Proc. Int. Conf. Learn. Represent. , 2021. [Online]. Available: https://openreview.net/forum?id=_0kaDkv3dVf\n- [16] J. Mellor, J. Turner, A. Storkey, and E. J. Crowley, ''Neural architecture search without training,'' 2020, arXiv:2006.04647 . [Online]. Available: https://arxiv.org/abs/2006.04647\n- [17] M. Forouzesh, F. Salehi, and P. Thiran, ''Generalization comparison of deep neural networks via output sensitivity,'' in Proc. 25th Int. Conf. Pattern Recognit. (ICPR) , Jan. 2021, pp. 7411GLYPH<21>7418.\n- [18] R. Novak, Y. Bahri, D. A. AbolaGLYPH<28>a, J. Pennington, and J. Sohl-Dickstein, ''Sensitivity and generalization in neural networks: An empirical study,'' 2018, arXiv:1802.08760 . [Online]. Available: https://arxiv.org/abs/1802.08760\n- [19] K. Kawaguchi, L. P. Kaelbling, and Y. Bengio, ''Generalization in deep learning,'' 2017, arXiv:1710.05468 . [Online]. Available: https://arxiv.org/abs/1710.05468\n- [20] A. Jacot, F. Gabriel, and C. Hongler, ''Neural tangent kernel: Convergence and generalization in neural networks,'' 2018, arXiv:1806.07572 . [Online]. Available: https://arxiv.org/abs/1806.07572\n- [21] H. Xiong, L. Huang, M. Yu, L. Liu, F. Zhu, and L. Shao, ''On the number of linear regions of convolutional neural networks,'' in Proc. 37th Int. Conf. Mach. Learn. , vol. 119, H. D. III and A. Singh, Eds. Jul. 2020, pp. 10514GLYPH<21>10523.\n- [22] M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, and J. Sohl-Dickstein, ''On the expressive power of deep neural networks,'' 2017, arXiv:1606.05336 . [Online]. Available: https://arxiv.org/abs/1606.05336\n- [23] L. Xiao, J. Pennington, and S. S. Schoenholz, ''Disentangling trainability and generalization in deep learning,'' 2019, arXiv:1912.13053 . [Online]. Available: https://arxiv.org/abs/1912.13053\n- [24] W. Chen, X. Gong, and Z. Wang, ''Neural architecture search on ImageNet in four GPU hours: A theoretically inspired perspective,'' in Proc. Int. Conf. Learn. Represent. , 2021. [Online]. Available: https://openreview.net/forum?id=Cnon5ezMHtu\n- [25] T. Devries and G. W. Taylor, ''Improved regularization of convolutional neural networks with cutout,'' 2017, arXiv:1708.04552 . [Online]. Available: https://arxiv.org/abs/1708.04552\n- [26] B. Baker, O. Gupta, N. Naik, and R. Raskar, ''Designing neural network architectures using reinforcement learning,'' 2016, arXiv:1611.02167 . [Online]. Available: https://arxiv.org/abs/1611.02167\n- [27] Z. Zhong, J. Yan, W. Wu, J. Shao, and C.-L. Liu, ''Practical blockwise neural network architecture generation,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , Jun. 2018, pp. 2423GLYPH<21>2432.\n- [28] L. Xie and A. Yuille, ''Genetic CNN,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , Oct. 2017, pp. 1379GLYPH<21>1388.\n- [29] E. Real, S. Moore, A. Selle, S. Saxena, Y . L. Suematsu, J. Tan, Q. V . Le, and A. Kurakin, ''Large-scale evolution of image classiGLYPH<28>ers,'' in Proc. 34th Int. Conf. Mach. Learn. (ICML) , Sydney, NSW, Australia, vol. 70, Aug. 2017, pp. 2902GLYPH<21>2911.\n- [30] T. Elsken, J. H. Metzen, and F. Hutter, ''EfGLYPH<28>cient multi-objective neural architecture search via Lamarckian evolution,'' in Proc. Int. Conf. Learn. Represent. , Sep. 2019. [Online]. Available: https:// openreview.net/forum?id=ByME42AqK7\n- [31] X. Chen, L. Xie, J. Wu, and Q. Tian, ''Progressive differentiable architecture search: Bridging the depth gap between search and evaluation,'' 2019, arXiv:1904.12760 . [Online]. Available: https://arxiv.org/abs/1904.12760\n- [32] C. Ying, A. Klein, E. Christiansen, E. Real, K. Murphy, and F. Hutter, ''Nas-bench-101: Towards reproducible neural architecture search,'' in Proc. ICML , 2019, pp. 7105GLYPH<21>7114.\n- [33] X. Dong and Y. Yang, ''NAS-Bench-201: Extending the scope of reproducible neural architecture search,'' in Proc. Int. Conf. Learn. Represent. , 2020, pp. 2GLYPH<21>17.\n- [34] P. Chrabaszcz, I. Loshchilov, and F. Hutter, ''A downsampled variant of ImageNet as an alternative to the CIFAR datasets,'' 2017, arXiv:1707.08819 . [Online]. Available: https://arxiv.org/abs/1707.08819\n- [35] J. Yang, X. Zeng, S. Zhong, and S. Wu, ''Effective neural network ensemble approach for improving generalization performance,'' IEEE Trans. Neural Netw. Learn. Syst. , vol. 24, no. 6, pp. 878GLYPH<21>887, Jun. 2013.\n- [36] R. Pascanu, G. Montufar, and Y. Bengio, ''On the number of response regions of deep feed forward networks with piece-wise linear activations,'' 2014, arXiv:1312.6098 . [Online]. Available: https://arxiv.org/abs/1312.6098",
    "context": "Provides a collection of research papers focused on neural architecture search techniques, including evolutionary algorithms, differentiable architecture search, and hardware-aware design.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      10
    ],
    "id": "c5d60e8a2b8e555927e995f66f4402b07d4a183e4821aa3e5cf09bf4bf8605e5"
  },
  {
    "text": "- [37] J. Lee, L. Xiao, S. S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, and J. Pennington, ''Wide neural networks of any depth evolve as linear models under gradient descent,'' J. Stat. Mech. Theory Exp. , vol. 2020, no. 12, Dec. 2020, Art. no. 124002.\n- [38] L. Zhang, Y. Shen, and H. Li, ''VSI: A visual saliency-induced index for perceptual image quality assessment,'' IEEE Trans. Image Process. , vol. 23, no. 10, pp. 4270GLYPH<21>4281, Aug. 2014.\n- [39] S.-H. Bae and M. Kim, ''DCT-QM: A DCT-based quality degradation metric for image quality optimization problems,'' IEEE Trans. Image Process. , vol. 25, no. 10, pp. 4916GLYPH<21>4930, Oct. 2016.\n- [40] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, ''Image quality assessment: From error visibility to structural similarity,'' IEEE Trans. Image Process. , vol. 13, no. 4, pp. 600GLYPH<21>612, Apr. 2004.\n- [41] H. R. Sheikh, M. F. Sabir, and A. C. Bovik, ''A statistical evaluation of recent full reference image quality assessment algorithms,'' IEEE Trans. Image Process. , vol. 15, no. 11, pp. 3440GLYPH<21>3451, Nov. 2006.\n- [42] E. Real, A. Aggarwal, Y. Huang, and Q. V. Le, ''Regularized evolution for image classiGLYPH<28>er architecture search,'' in Proc. AAAI Conf. Artif. Intell. , vol. 33, Jul. 2019, pp. 4780GLYPH<21>4789.\n- [43] K. He, X. Zhang, S. Ren, and J. Sun, ''Delving deep into rectiGLYPH<28>ers: Surpassing human-level performance on imagenet classiGLYPH<28>cation,'' 2015, arXiv:1502.01852 . [Online]. Available: https://arxiv.org/abs/1502.01852\n- [44] B. Wu, K. Keutzer, X. Dai, P. Zhang, Y. Wang, F. Sun, Y. Wu, Y. Tian, P. Vajda, and Y. Jia, ''FBNet: Hardware-aware efGLYPH<28>cient ConvNet design via differentiable neural architecture search,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2019, pp. 10734GLYPH<21>10742.\nLINH-TAM TRAN received the bachelor's degree from the Department of Computer Science and Engineering, Ho Chi Minh City University of Technology, Vietnam, in 2014, and the M.S. degree from Hongik University, Seoul, South Korea, in 2018. He is currently pursuing the Ph.D. degree with the Department of Computer Science and Engineering, Kyung Hee University, Suwon, South Korea. His research interest includes neural architecture search for practical applications.\nMUHAMMAD SALMAN ALI received the bachelor's degree in computer science from the National University of Sciences and Technology (NUST), Islamabad, Pakistan, in 2018. He is currently pursuing the M.S. degree leading to the Ph.D. degree with Kyung Hee University, South Korea. His research interests include deep learning interpretation and the effect of soft errors on deep neural networks. He was a recipient of the gold medal for being a high-achiever during his UG studies.\nSUNG-HO BAE (Member, IEEE) received the B.S. degree from Kyung Hee University, South Korea, in 2011, and the M.S. and Ph.D. degrees from Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea, in 2012 and 2016, respectively. From 2016 to 2017, he was a Postdoctoral Associate with the Computer Science and ArtiGLYPH<28>cial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT), MA, USA. Since 2017, he has been an Assistant Professor with the Department of Computer Science and Engineering, Kyung Hee University. He has been involved in model compression/interpretation for deep neural networks and inverse problems in image processing and computer vision.\n\nProvides a collection of research papers focused on image quality assessment techniques.",
    "original_text": "- [37] J. Lee, L. Xiao, S. S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, and J. Pennington, ''Wide neural networks of any depth evolve as linear models under gradient descent,'' J. Stat. Mech. Theory Exp. , vol. 2020, no. 12, Dec. 2020, Art. no. 124002.\n- [38] L. Zhang, Y. Shen, and H. Li, ''VSI: A visual saliency-induced index for perceptual image quality assessment,'' IEEE Trans. Image Process. , vol. 23, no. 10, pp. 4270GLYPH<21>4281, Aug. 2014.\n- [39] S.-H. Bae and M. Kim, ''DCT-QM: A DCT-based quality degradation metric for image quality optimization problems,'' IEEE Trans. Image Process. , vol. 25, no. 10, pp. 4916GLYPH<21>4930, Oct. 2016.\n- [40] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, ''Image quality assessment: From error visibility to structural similarity,'' IEEE Trans. Image Process. , vol. 13, no. 4, pp. 600GLYPH<21>612, Apr. 2004.\n- [41] H. R. Sheikh, M. F. Sabir, and A. C. Bovik, ''A statistical evaluation of recent full reference image quality assessment algorithms,'' IEEE Trans. Image Process. , vol. 15, no. 11, pp. 3440GLYPH<21>3451, Nov. 2006.\n- [42] E. Real, A. Aggarwal, Y. Huang, and Q. V. Le, ''Regularized evolution for image classiGLYPH<28>er architecture search,'' in Proc. AAAI Conf. Artif. Intell. , vol. 33, Jul. 2019, pp. 4780GLYPH<21>4789.\n- [43] K. He, X. Zhang, S. Ren, and J. Sun, ''Delving deep into rectiGLYPH<28>ers: Surpassing human-level performance on imagenet classiGLYPH<28>cation,'' 2015, arXiv:1502.01852 . [Online]. Available: https://arxiv.org/abs/1502.01852\n- [44] B. Wu, K. Keutzer, X. Dai, P. Zhang, Y. Wang, F. Sun, Y. Wu, Y. Tian, P. Vajda, and Y. Jia, ''FBNet: Hardware-aware efGLYPH<28>cient ConvNet design via differentiable neural architecture search,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2019, pp. 10734GLYPH<21>10742.\nLINH-TAM TRAN received the bachelor's degree from the Department of Computer Science and Engineering, Ho Chi Minh City University of Technology, Vietnam, in 2014, and the M.S. degree from Hongik University, Seoul, South Korea, in 2018. He is currently pursuing the Ph.D. degree with the Department of Computer Science and Engineering, Kyung Hee University, Suwon, South Korea. His research interest includes neural architecture search for practical applications.\nMUHAMMAD SALMAN ALI received the bachelor's degree in computer science from the National University of Sciences and Technology (NUST), Islamabad, Pakistan, in 2018. He is currently pursuing the M.S. degree leading to the Ph.D. degree with Kyung Hee University, South Korea. His research interests include deep learning interpretation and the effect of soft errors on deep neural networks. He was a recipient of the gold medal for being a high-achiever during his UG studies.\nSUNG-HO BAE (Member, IEEE) received the B.S. degree from Kyung Hee University, South Korea, in 2011, and the M.S. and Ph.D. degrees from Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea, in 2012 and 2016, respectively. From 2016 to 2017, he was a Postdoctoral Associate with the Computer Science and ArtiGLYPH<28>cial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT), MA, USA. Since 2017, he has been an Assistant Professor with the Department of Computer Science and Engineering, Kyung Hee University. He has been involved in model compression/interpretation for deep neural networks and inverse problems in image processing and computer vision.",
    "context": "Provides a collection of research papers focused on image quality assessment techniques.",
    "document": "A_Feature_Fusion_Based_Indicator_for_Training-Free_Neural_Architecture_Search.pdf",
    "pages": [
      10
    ],
    "id": "a8ce875bbcef1cf37e2779e3b776eb75a4b37921d7d8eb3ef6e1bd90bd4e921d"
  },
  {
    "text": "Received July 9, 2021, accepted July 18, 2021, date of publication July 26, 2021, date of current version August 3, 2021.\nDigital Object Identifier 10.1 109/ACCESS.2021.3100316\n\nIntroduces the paper's context, publication details, and DOI.",
    "original_text": "Received July 9, 2021, accepted July 18, 2021, date of publication July 26, 2021, date of current version August 3, 2021.\nDigital Object Identifier 10.1 109/ACCESS.2021.3100316",
    "context": "Introduces the paper's context, publication details, and DOI.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      1
    ],
    "id": "06660b4f59094e9bccd86f11ac4654aadb9b137697af5c4c5abffdbf9e7a2659"
  },
  {
    "text": "A. B. M. BODRUL ALAM 1 , (Member, IEEE), ZUBAIR MD. FADLULLAH 1,2 , (Senior Member, IEEE), AND SALIMUR CHOUDHURY 2 , (Senior Member, IEEE)\n1 Thunder Bay Regional Health Research Institute (TBRHRI), Thunder Bay, ON P7B 7A5, Canada 2 Department of Computer Science, Lakehead University, Thunder Bay, ON P7B 5E1, Canada\nCorresponding author: A. B. M. Bodrul Alam (abalam@lakeheadu.ca)\nABSTRACT Allocating and managing resources while considering the quality of service is considered a fundamental and complex research problem in a cloud environment. An optimal resource allocation optimizes several parameters such as optimizing cost and resource utilization or maximizing any quality parameters. However, to ensure better customer service, Cloud Service Providers (CSPs) should consider most of the quality attributes while allocating resources to the cloud infrastructures. Existing research does not evaluate trust as a quantitative attribute, thus a trade-off between trust and performance in resource allocation is also absent in the research area. We propose a model to consider both trust and delay in this paper. The trust of a CSP is quantitatively estimated through some attributes and metrics. Availability, reliability, data integrity, and efGLYPH<28>ciency are considered to estimate the trust. The objective is to maximize the trust of the allocation while minimizing the communication delay. The proposed joint optimization model combines the previous credentials of the CSPs and the present resource constraints. To solve the problem heuristically, a genetic algorithm is applied. The model uses a number of parameters that provide the GLYPH<29>exibility to adapt several service requirements. The effectiveness and applicability of the proposed approach are demonstrated through experiments. The results ensure that the effectiveness in the estimation of trust evaluation for different CSPs with the proposed attributes. Moreover, integrating trust in the resource allocation model allocates appropriate resources while enhancing the trust and reducing the communication delay in the overall allocation.\nINDEX TERMS Cloud computing, trust evaluation, resource allocation, availability, reliability, optimization, genetic algorithm.\n\nProposes a model to quantify trust in cloud resource allocation, considering availability, reliability, data integrity, and efficiency, aiming to balance trust with communication delay.",
    "original_text": "A. B. M. BODRUL ALAM 1 , (Member, IEEE), ZUBAIR MD. FADLULLAH 1,2 , (Senior Member, IEEE), AND SALIMUR CHOUDHURY 2 , (Senior Member, IEEE)\n1 Thunder Bay Regional Health Research Institute (TBRHRI), Thunder Bay, ON P7B 7A5, Canada 2 Department of Computer Science, Lakehead University, Thunder Bay, ON P7B 5E1, Canada\nCorresponding author: A. B. M. Bodrul Alam (abalam@lakeheadu.ca)\nABSTRACT Allocating and managing resources while considering the quality of service is considered a fundamental and complex research problem in a cloud environment. An optimal resource allocation optimizes several parameters such as optimizing cost and resource utilization or maximizing any quality parameters. However, to ensure better customer service, Cloud Service Providers (CSPs) should consider most of the quality attributes while allocating resources to the cloud infrastructures. Existing research does not evaluate trust as a quantitative attribute, thus a trade-off between trust and performance in resource allocation is also absent in the research area. We propose a model to consider both trust and delay in this paper. The trust of a CSP is quantitatively estimated through some attributes and metrics. Availability, reliability, data integrity, and efGLYPH<28>ciency are considered to estimate the trust. The objective is to maximize the trust of the allocation while minimizing the communication delay. The proposed joint optimization model combines the previous credentials of the CSPs and the present resource constraints. To solve the problem heuristically, a genetic algorithm is applied. The model uses a number of parameters that provide the GLYPH<29>exibility to adapt several service requirements. The effectiveness and applicability of the proposed approach are demonstrated through experiments. The results ensure that the effectiveness in the estimation of trust evaluation for different CSPs with the proposed attributes. Moreover, integrating trust in the resource allocation model allocates appropriate resources while enhancing the trust and reducing the communication delay in the overall allocation.\nINDEX TERMS Cloud computing, trust evaluation, resource allocation, availability, reliability, optimization, genetic algorithm.",
    "context": "Proposes a model to quantify trust in cloud resource allocation, considering availability, reliability, data integrity, and efficiency, aiming to balance trust with communication delay.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      1
    ],
    "id": "267eba1200173fbc0bf2a11dcf56d5b9425739b5cb16ca3ab7a63b2a0ea693b1"
  },
  {
    "text": "Cloud is a popular computing paradigm and addressed intensively by both academia and industry. Cloud has become popular due to its unique characteristics. Some of the cloud characteristics also negatively impact the quality of service. The unique GLYPH<28>ve characteristics of cloud which improve performance can also increase risks [1]. For instance, there are several applications for which missing any provisioning of services is considered as a failure [2]. Broad network access introduces new challenges. The networking service reliability, availability, and latency directly impact the Quality of Service (QoS) in a cloud environment. Another characteristic is related to rapid elasticity. If the service is not capable of adapting to the current load then elasticity failure may occur that will impact the reliability of the service. The last feature concerns the measured service. The measured service is the computation of resource usage. If any loss or unavailability of resources occurs, it can lead to unreliable or inaccurate estimation in data-related measurements.\nThe associate editor coordinating the review of this manuscript and approving it for publication was Ali Kashif Bashir .\nThe fundamental characteristic of the cloud is resource sharing by multiple users. Resources in the cloud can be dynamically re-allocated based on users' demands and preferences. Allocating resources appropriately is considered one of the primary concerns in cloud computing. The word appropriate can be any of the performance metrics: cost-effectiveness, accuracy, and reliability. Allocating resource appropriately refers to such an allocation that integrates the resource utilization of the service providers while meeting customer's requirements. Allocation of an\noptimal resource must avoid resource contention, fragmentation, over-provisioning, and under-provisioning. Resource contention occurs when several applications request the same resources at a speciGLYPH<28>c time. Due to the unavailability of resources, scarcity of resources may happen. Fragmentation of resources may take place for resource isolation. Over-provisioning happens if providers allocate more resources than the demand and under-provisioning may happen if providers allocate fewer resources than its demand. We try to avoid all these cases in our resource allocation framework.\nThe main obstacles of this paradigm are reliability and security. To address reliability and security both, we propose a model using four quality attributes: availability, reliability, data integrity, and efGLYPH<28>ciency to estimate the trust value of a CSP. At GLYPH<28>rst, the experiments on the evaluation for the trust model are conducted. After that, the results are integrated into the optimization of the resource allocation model. Our contributions in this paper are threefold V\n- GLYPH<15> Most of the research work in the literature used trust as a quality attribute. In this work, we evaluate trust as a quantitative attribute and estimate the values of trust of different service providers based on our proposed four quality attributes. The values of the quality attributes are estimated based on a set of metrics through a simulation environment. We combine the four quality attributes as a weighted sum to obtain the trust of a provider.\n- GLYPH<15> We integrate the estimated trust value into a resource allocation model in a multi-cloud environment. The resource allocation model also considers the trafGLYPH<28>c and capacity of the CSPs to evaluate the communication delay. We design the model as a joint optimization problem and aim to maximize trust and minimize communication delay to improve the QoS.\n- GLYPH<15> We solve the resource allocation problem optimally and heuristically. Intlinprog in MATLAB is used as an optimizer to solve the proposed resource allocation model. The optimizer requires several hours to obtain an optimized solution in some scenarios. To avoid such situations, we use a meta-heuristic, a Genetic Algorithm for generating computationally efGLYPH<28>cient approximated solutions. We compare the results of our heuristics with the optimal solution. The solution of the Genetic algorithm achieves 90% similarity to the exact solution. The execution time of the Genetic Algorithm increases linearly with an increase in the number of the CSPs and servers. These trends ensure the validity of the proposed trust model in a practical cloud environment.\nThe trust evaluation model considers most of the main elements of a cloud that impact the service performance. Results demonstrated that evaluation of trust and integrating that into the resource allocation model are effectively conducted. The simulation of the model conGLYPH<28>rms the trust maximization and delay minimization in the allocation. Thereby improves the quality of services. The model introduces GLYPH<29>exibility into the resource allocation process as adjusting some parameters can provide customized users' requirements. As trust and delay are conGLYPH<29>icting objectives of the model, the trade-off should be estimated with proper caution.\nThe organization of the rest of the paper is following. The existing research work regarding the evaluation of quality attributes of cloud, optimization of resource utilization, and application of the meta-heuristic algorithm in cloud resource allocation are discussed in Section II. The proposed trust evaluation model is described in Section III. The resource allocation and problem formulation of our optimization model are presented in Section IV. This section also presents the proposed algorithm to solve the problem. The experimental setup and results analysis are discussed in Section V. Finally, Section VI concludes the paper by mentioning some future research directions.\n\nIntroduces the core problem of resource allocation in cloud environments, highlighting the importance of quality of service and the need to consider multiple performance metrics.",
    "original_text": "Cloud is a popular computing paradigm and addressed intensively by both academia and industry. Cloud has become popular due to its unique characteristics. Some of the cloud characteristics also negatively impact the quality of service. The unique GLYPH<28>ve characteristics of cloud which improve performance can also increase risks [1]. For instance, there are several applications for which missing any provisioning of services is considered as a failure [2]. Broad network access introduces new challenges. The networking service reliability, availability, and latency directly impact the Quality of Service (QoS) in a cloud environment. Another characteristic is related to rapid elasticity. If the service is not capable of adapting to the current load then elasticity failure may occur that will impact the reliability of the service. The last feature concerns the measured service. The measured service is the computation of resource usage. If any loss or unavailability of resources occurs, it can lead to unreliable or inaccurate estimation in data-related measurements.\nThe associate editor coordinating the review of this manuscript and approving it for publication was Ali Kashif Bashir .\nThe fundamental characteristic of the cloud is resource sharing by multiple users. Resources in the cloud can be dynamically re-allocated based on users' demands and preferences. Allocating resources appropriately is considered one of the primary concerns in cloud computing. The word appropriate can be any of the performance metrics: cost-effectiveness, accuracy, and reliability. Allocating resource appropriately refers to such an allocation that integrates the resource utilization of the service providers while meeting customer's requirements. Allocation of an\noptimal resource must avoid resource contention, fragmentation, over-provisioning, and under-provisioning. Resource contention occurs when several applications request the same resources at a speciGLYPH<28>c time. Due to the unavailability of resources, scarcity of resources may happen. Fragmentation of resources may take place for resource isolation. Over-provisioning happens if providers allocate more resources than the demand and under-provisioning may happen if providers allocate fewer resources than its demand. We try to avoid all these cases in our resource allocation framework.\nThe main obstacles of this paradigm are reliability and security. To address reliability and security both, we propose a model using four quality attributes: availability, reliability, data integrity, and efGLYPH<28>ciency to estimate the trust value of a CSP. At GLYPH<28>rst, the experiments on the evaluation for the trust model are conducted. After that, the results are integrated into the optimization of the resource allocation model. Our contributions in this paper are threefold V\n- GLYPH<15> Most of the research work in the literature used trust as a quality attribute. In this work, we evaluate trust as a quantitative attribute and estimate the values of trust of different service providers based on our proposed four quality attributes. The values of the quality attributes are estimated based on a set of metrics through a simulation environment. We combine the four quality attributes as a weighted sum to obtain the trust of a provider.\n- GLYPH<15> We integrate the estimated trust value into a resource allocation model in a multi-cloud environment. The resource allocation model also considers the trafGLYPH<28>c and capacity of the CSPs to evaluate the communication delay. We design the model as a joint optimization problem and aim to maximize trust and minimize communication delay to improve the QoS.\n- GLYPH<15> We solve the resource allocation problem optimally and heuristically. Intlinprog in MATLAB is used as an optimizer to solve the proposed resource allocation model. The optimizer requires several hours to obtain an optimized solution in some scenarios. To avoid such situations, we use a meta-heuristic, a Genetic Algorithm for generating computationally efGLYPH<28>cient approximated solutions. We compare the results of our heuristics with the optimal solution. The solution of the Genetic algorithm achieves 90% similarity to the exact solution. The execution time of the Genetic Algorithm increases linearly with an increase in the number of the CSPs and servers. These trends ensure the validity of the proposed trust model in a practical cloud environment.\nThe trust evaluation model considers most of the main elements of a cloud that impact the service performance. Results demonstrated that evaluation of trust and integrating that into the resource allocation model are effectively conducted. The simulation of the model conGLYPH<28>rms the trust maximization and delay minimization in the allocation. Thereby improves the quality of services. The model introduces GLYPH<29>exibility into the resource allocation process as adjusting some parameters can provide customized users' requirements. As trust and delay are conGLYPH<29>icting objectives of the model, the trade-off should be estimated with proper caution.\nThe organization of the rest of the paper is following. The existing research work regarding the evaluation of quality attributes of cloud, optimization of resource utilization, and application of the meta-heuristic algorithm in cloud resource allocation are discussed in Section II. The proposed trust evaluation model is described in Section III. The resource allocation and problem formulation of our optimization model are presented in Section IV. This section also presents the proposed algorithm to solve the problem. The experimental setup and results analysis are discussed in Section V. Finally, Section VI concludes the paper by mentioning some future research directions.",
    "context": "Introduces the core problem of resource allocation in cloud environments, highlighting the importance of quality of service and the need to consider multiple performance metrics.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      1,
      2
    ],
    "id": "06655edbbfd91f61791b74897d6919a43f632b94d61b97ebac8a0bcfb18fd03a"
  },
  {
    "text": "The demand for cloud resources changes rapidly and impacts the availability of cloud resources over time. The occurrence of failures is unpredictable and directly impacts the reliability of the cloud. Optimal allocation of resources in a cloud is crucial and can be solved using heuristics. The general process for allocating resources requires three parties: customer, broker, and service provider. At GLYPH<28>rst, customers specify the amounts of resources along with the constraints related to service performance. Then, a broker is responsible for GLYPH<28>nding a proper CSP or multiple CSPs that can satisfy customers' requirements and transfers the requests to the chosen CSPs. After that, CSPs allocate resources to their data centers based on availability. In such allocation, resource utilization, and customers' requirements both constraints are fulGLYPH<28>lled. Managing and optimizing the allocation of resources appropriately in a cloud environment is crucial. There have been many recent efforts to optimize resource allocation and we acknowledge a few of them below.\nAnalysis of hardware reliability of cloud and the causes of the failures impacting the reliability are discussed in [3]. The authors investigate different parameters such as server age, conGLYPH<28>guration, location, load executing on the infrastructure and conclude that none of the parameters directly impact the failures signiGLYPH<28>cantly. Selecting an optimal channel for improving reliability and minimizing energy budget is the objective of this work [4]. A machine learning approach with reliability improvement and energy awareness is applied in the proposed method. The unique dimensions of resource management in IoT such as managing memory, energy, and process are discussed with pros and cons [5]. A modiGLYPH<28>ed Max-Min algorithm is applied for allocating resources in a cloud environment [6]. The modiGLYPH<28>cation improves the average waiting time and completion time. The method maximizes the overall resource utilization. A model to evaluate cloud reliability is presented in [7]. The authors use a state transition graph with three states for the evaluation. A Semi-Markov model is used for estimating the reliability using the state transition probability. Some of the software failures are considered in the evaluation. For the physical\nresource breakdown, the authors use the mean-time to recovery and failure for availability and downtime. Failures of all main elements of a cloud are not considered in the estimation of cloud reliability. In the model, the authors calculate the reliability of each physical resource there is no focus on performing a reliable allocation of resources for the requested services. A reliability evaluation model for cloud environment is presented in [8]. The model proposes some attributes and metrics for reliability evaluation. Four quality attributes are chosen for calculating the reliability of a cloud. For each of the attributes, the authors proposed some metrics for the estimation and GLYPH<28>nally combine them using a weighted sum technique. Based on response time, reliability and performance in a cloud are presented in [9]. The response time was calculated based on three parameters: waiting time, service time, and execution time. Queuing theory is applied in the estimation of waiting time and service time. Response time was estimated by combining the above attributes. Faragardi et al. [10] proposed an evaluation model for cloud reliability. To evaluate system reliability, they considered the reliability of servers and links.\nThe primary concern of most of the research is optimizing resource utilization. Resource allocation techniques are discussed with an objective to control various parameters: costs, execution time, and scheduling policy. For instance, Obtaining the minimum servers is the objective of this scheme [11]. The authors propose a heuristic for an allocation that minimizes the requirements of the servers through response time distribution. The effectiveness of the mechanism is evaluated with several performance measurements. The approach does not consider any other parameters that also directly impact service performance. Scheduling task in an efGLYPH<28>cient manner while minimizing the power consumption is the aim of this work [12]. A prediction mechanism is used for allocating resources and reducing power consumption. A resource allocation problem in a cloud radio access network is shown in the model for choosing the optimal technique to combine all the available spectral bands [13]. A modiGLYPH<28>ed version of the cloud radio access network is proposed for handling spectral resources efGLYPH<28>ciently. Another model for minimizing the server quantity of cloud is presented in [14]. A prediction algorithm that can model the load in advance is applied to predict future resource usages. The authors presented an evaluation to prove the validity of the proposed model. The evaluation shows that it can avoid the occurring of overload and achieve green computing. A resource allocation scheme for monitoring and predicting future requirements is proposed in [15]. The scheme adjusts VMs based on actual resource needs. There are two primary challenges to this task. The GLYPH<28>rst one is when to reallocate resources and the second one is how much resource needs adjustment. However, the performance shows the applicability of the method but adjusting and selecting the threshold is critical and it significantly impacts resource optimization. For a hybrid Cloud, Grewal and Pateria [16] propose a resource provisioning technique. A private cloud is capable of scaling up using\npublic cloud resources with the help of a resource manager. In allocating resources, most of the approaches are concerned only with resource utilization. To improve the performance, integration of several other performance metrics like reliability, efGLYPH<28>ciency should also be required into the allocation model.\nFor an appropriate VM allocation in a cloud, a trafGLYPH<28>c-aware algorithm is presented in [17]. A swarm optimization algorithm is applied for solving the VM allocation problem. The simulation results show that the algorithm can signiGLYPH<28>cantly improve energy consumption and control the loss of communication links. The authors propose a model to maximize resource utilization and minimize energy consumption [18]. A particle swarm optimization method is used to solve the problem. A cloud resource allocation model to improve security is proposed by Halabi et al. [19]. The authors designed the allocation problem as an optimization problem and propose a heuristic for the solution. For forecasting the utilization of resources in a data center, a Genetic Algorithm is proposed in [20]. The GA forecasts the requirements of the resources depending on the previous data. A resource allocation approach depending on resource prediction is also proposed in the work. The validity and applicability of the method are presented through simulation results. The results show that GA is superior for an accurate prediction. Moreover, it improves the utilization of CPU and memory and the consumption of energy. Finally, a fault-aware resource allocation for cloud is proposed by Gupta and Ghrera [21]. The authors aim for the maximization of the utilization of resources and consumption of power for the servers.\nMost of the research in this area proposes approaches to evaluate some quality attributes and optimize those attributes. The absence of standard quantitative attributes and metrics and the incompleteness in considering the major elements of cloud infrastructure are ensured through the existing cloud literature. No research work in the literature considers most of the major cloud elements in the evaluation to improve QoS. The objective of our approach is to maximize trust and minimize the communication delay to improve service performance. Trust will be calculated based on the availability, reliability, data integrity, and efGLYPH<28>ciency of a CSP while allocating resources efGLYPH<28>ciently based on users' requirements. Consideration of trust and communication delay together in the optimization problem makes our model demanding and appropriate.\n\nIntroduces the core concept of resource allocation in cloud environments, emphasizing the need to balance cost, utilization, and quality of service.",
    "original_text": "The demand for cloud resources changes rapidly and impacts the availability of cloud resources over time. The occurrence of failures is unpredictable and directly impacts the reliability of the cloud. Optimal allocation of resources in a cloud is crucial and can be solved using heuristics. The general process for allocating resources requires three parties: customer, broker, and service provider. At GLYPH<28>rst, customers specify the amounts of resources along with the constraints related to service performance. Then, a broker is responsible for GLYPH<28>nding a proper CSP or multiple CSPs that can satisfy customers' requirements and transfers the requests to the chosen CSPs. After that, CSPs allocate resources to their data centers based on availability. In such allocation, resource utilization, and customers' requirements both constraints are fulGLYPH<28>lled. Managing and optimizing the allocation of resources appropriately in a cloud environment is crucial. There have been many recent efforts to optimize resource allocation and we acknowledge a few of them below.\nAnalysis of hardware reliability of cloud and the causes of the failures impacting the reliability are discussed in [3]. The authors investigate different parameters such as server age, conGLYPH<28>guration, location, load executing on the infrastructure and conclude that none of the parameters directly impact the failures signiGLYPH<28>cantly. Selecting an optimal channel for improving reliability and minimizing energy budget is the objective of this work [4]. A machine learning approach with reliability improvement and energy awareness is applied in the proposed method. The unique dimensions of resource management in IoT such as managing memory, energy, and process are discussed with pros and cons [5]. A modiGLYPH<28>ed Max-Min algorithm is applied for allocating resources in a cloud environment [6]. The modiGLYPH<28>cation improves the average waiting time and completion time. The method maximizes the overall resource utilization. A model to evaluate cloud reliability is presented in [7]. The authors use a state transition graph with three states for the evaluation. A Semi-Markov model is used for estimating the reliability using the state transition probability. Some of the software failures are considered in the evaluation. For the physical\nresource breakdown, the authors use the mean-time to recovery and failure for availability and downtime. Failures of all main elements of a cloud are not considered in the estimation of cloud reliability. In the model, the authors calculate the reliability of each physical resource there is no focus on performing a reliable allocation of resources for the requested services. A reliability evaluation model for cloud environment is presented in [8]. The model proposes some attributes and metrics for reliability evaluation. Four quality attributes are chosen for calculating the reliability of a cloud. For each of the attributes, the authors proposed some metrics for the estimation and GLYPH<28>nally combine them using a weighted sum technique. Based on response time, reliability and performance in a cloud are presented in [9]. The response time was calculated based on three parameters: waiting time, service time, and execution time. Queuing theory is applied in the estimation of waiting time and service time. Response time was estimated by combining the above attributes. Faragardi et al. [10] proposed an evaluation model for cloud reliability. To evaluate system reliability, they considered the reliability of servers and links.\nThe primary concern of most of the research is optimizing resource utilization. Resource allocation techniques are discussed with an objective to control various parameters: costs, execution time, and scheduling policy. For instance, Obtaining the minimum servers is the objective of this scheme [11]. The authors propose a heuristic for an allocation that minimizes the requirements of the servers through response time distribution. The effectiveness of the mechanism is evaluated with several performance measurements. The approach does not consider any other parameters that also directly impact service performance. Scheduling task in an efGLYPH<28>cient manner while minimizing the power consumption is the aim of this work [12]. A prediction mechanism is used for allocating resources and reducing power consumption. A resource allocation problem in a cloud radio access network is shown in the model for choosing the optimal technique to combine all the available spectral bands [13]. A modiGLYPH<28>ed version of the cloud radio access network is proposed for handling spectral resources efGLYPH<28>ciently. Another model for minimizing the server quantity of cloud is presented in [14]. A prediction algorithm that can model the load in advance is applied to predict future resource usages. The authors presented an evaluation to prove the validity of the proposed model. The evaluation shows that it can avoid the occurring of overload and achieve green computing. A resource allocation scheme for monitoring and predicting future requirements is proposed in [15]. The scheme adjusts VMs based on actual resource needs. There are two primary challenges to this task. The GLYPH<28>rst one is when to reallocate resources and the second one is how much resource needs adjustment. However, the performance shows the applicability of the method but adjusting and selecting the threshold is critical and it significantly impacts resource optimization. For a hybrid Cloud, Grewal and Pateria [16] propose a resource provisioning technique. A private cloud is capable of scaling up using\npublic cloud resources with the help of a resource manager. In allocating resources, most of the approaches are concerned only with resource utilization. To improve the performance, integration of several other performance metrics like reliability, efGLYPH<28>ciency should also be required into the allocation model.\nFor an appropriate VM allocation in a cloud, a trafGLYPH<28>c-aware algorithm is presented in [17]. A swarm optimization algorithm is applied for solving the VM allocation problem. The simulation results show that the algorithm can signiGLYPH<28>cantly improve energy consumption and control the loss of communication links. The authors propose a model to maximize resource utilization and minimize energy consumption [18]. A particle swarm optimization method is used to solve the problem. A cloud resource allocation model to improve security is proposed by Halabi et al. [19]. The authors designed the allocation problem as an optimization problem and propose a heuristic for the solution. For forecasting the utilization of resources in a data center, a Genetic Algorithm is proposed in [20]. The GA forecasts the requirements of the resources depending on the previous data. A resource allocation approach depending on resource prediction is also proposed in the work. The validity and applicability of the method are presented through simulation results. The results show that GA is superior for an accurate prediction. Moreover, it improves the utilization of CPU and memory and the consumption of energy. Finally, a fault-aware resource allocation for cloud is proposed by Gupta and Ghrera [21]. The authors aim for the maximization of the utilization of resources and consumption of power for the servers.\nMost of the research in this area proposes approaches to evaluate some quality attributes and optimize those attributes. The absence of standard quantitative attributes and metrics and the incompleteness in considering the major elements of cloud infrastructure are ensured through the existing cloud literature. No research work in the literature considers most of the major cloud elements in the evaluation to improve QoS. The objective of our approach is to maximize trust and minimize the communication delay to improve service performance. Trust will be calculated based on the availability, reliability, data integrity, and efGLYPH<28>ciency of a CSP while allocating resources efGLYPH<28>ciently based on users' requirements. Consideration of trust and communication delay together in the optimization problem makes our model demanding and appropriate.",
    "context": "Introduces the core concept of resource allocation in cloud environments, emphasizing the need to balance cost, utilization, and quality of service.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      2,
      3
    ],
    "id": "dc3b06a81174ed4365d3a79e01fa4a3863d0be4d0ff2d92bdbf526e36b63b0cc"
  },
  {
    "text": "Trust evaluation in a cloud requires data of several parameters obtained from a real cloud computing environment. The unavailability of such practical environment's data makes any research problem more complex [22]. The problem is to evaluate the trust value of different service providers and design a model for the selection of CSPs while improving the QoS. We assume that all resources of a CSP stay in one data center. The proposed evaluation metrics are used to estimate the trust of a CSP.\n\nData scarcity in real cloud environments complicates trust evaluation research; the goal is to assess CSP trustworthiness and select optimal providers while enhancing quality of service, assuming resources reside within a single data center.",
    "original_text": "Trust evaluation in a cloud requires data of several parameters obtained from a real cloud computing environment. The unavailability of such practical environment's data makes any research problem more complex [22]. The problem is to evaluate the trust value of different service providers and design a model for the selection of CSPs while improving the QoS. We assume that all resources of a CSP stay in one data center. The proposed evaluation metrics are used to estimate the trust of a CSP.",
    "context": "Data scarcity in real cloud environments complicates trust evaluation research; the goal is to assess CSP trustworthiness and select optimal providers while enhancing quality of service, assuming resources reside within a single data center.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      3
    ],
    "id": "fe7a4d413a2c6ba8fc641563154b231ac89e7f70bbe53fae1a550bf4c370a534"
  },
  {
    "text": "Trust is evaluated quantitatively in the proposed model. We use two steps to evaluate the trust of a CSP. One is workload modeling that involves the assessment of the arrival rates of requests for resources such as CPU, memory, bandwidth requirements placed by users. In the second step, system modeling aims at evaluating the performance of a cloud system at the allocation based on the performance of the service providers. The model estimates trust based on the value of the QoS metrics which are availability, reliability, data integrity, and time efGLYPH<28>ciency.\nFIGURE 1. Proposed trust evaluation approach.\nWe compute trust based on the previous credentials of the CSPs using the four quality attributes and our proposed evaluation metrics shown in Fig. 1. Availability is one of the major concerning parameters in a cloud environment. Availability is calculated based on the resource availability of a CSP. All cloud service providers emphasize the availability of their service in their Service Level Agreement (SLA). Amazon, Google, Microsoft, IBM, and Salesforce maintain the availability of 99.9% which is still lower than the expectation [23]GLYPH<21>[26]. The availability of a CSP is calculated based on the rate of the accepted and total requests. We consider a request accepted if the request queue can process the request within the prescribed time threshold. The availability of CSP n is calculated using the equation below where ACrq is the rate of accepted requests and TOrq is the rate of incoming total requests.\n<!-- formula-not-decoded -->\nReliability ensures the ability of a service to keep functioning with a speciGLYPH<28>c performance level for a speciGLYPH<28>ed time. Reliability of cloud is not addressed or appropriately considered by the previous cloud researchers [27], [28]. However, due to many outages in a cloud, reliability issues have become a major concern in a cloud environment. We consider reliability as a failure-free operation. In terms of failures in the cloud, we consider hardware and software failures. Reliability is calculated based on the rate of successful and accepted requests. If a request completes the execution without any failure occurrence, we consider that request a successful one. The reliability of CSP n is calculated as below where SUrq is the rate of successful requests and ACrq is the rate of accepted requests.\n<!-- formula-not-decoded -->\nData integrity considers data loss due to latency and network failures. If a service is unable to grow fast then elasticity failure occurs that will impact the quality of service. We consider the elasticity failure as a network failure as it occurs due to an error in measuring load. Data integrity is checked by estimating the data loss. We use the equation below to estimate the data integrity of CSP n where SUrq is the rate of successful requests and NFrq is the rate of requests where a data failure occurs and generates a data loss.\n<!-- formula-not-decoded -->\nIn terms of efGLYPH<28>ciency, time efGLYPH<28>ciency is evaluated that considers the execution time and waiting time of each CSP. EfGLYPH<28>ciency is calculated based on the rate of promised and actual execution time. In an ideal case, the promised time ( PRt ) should be equal to the execution time ( EXt ) as the waiting time ( WAt ) is supposed to be 0.\n<!-- formula-not-decoded -->\nFinally, we combine all the individual rates multiplying with their corresponding weighting factors to compute the trust of a CSP as follows V\n<!-- formula-not-decoded -->\nWe repeat the same process for each of the CSP and evaluate the trust for all of them. The following architecture is used to evaluate the trust.\n\nEvaluates trust quantitatively using workload and system modeling, incorporating availability, reliability, data integrity, and time efficiency to determine CSP trust.  The model uses weighted averages of these quality attributes to estimate trust, considering the rates of accepted and total requests for availability and successful versus accepted requests for reliability.",
    "original_text": "Trust is evaluated quantitatively in the proposed model. We use two steps to evaluate the trust of a CSP. One is workload modeling that involves the assessment of the arrival rates of requests for resources such as CPU, memory, bandwidth requirements placed by users. In the second step, system modeling aims at evaluating the performance of a cloud system at the allocation based on the performance of the service providers. The model estimates trust based on the value of the QoS metrics which are availability, reliability, data integrity, and time efGLYPH<28>ciency.\nFIGURE 1. Proposed trust evaluation approach.\nWe compute trust based on the previous credentials of the CSPs using the four quality attributes and our proposed evaluation metrics shown in Fig. 1. Availability is one of the major concerning parameters in a cloud environment. Availability is calculated based on the resource availability of a CSP. All cloud service providers emphasize the availability of their service in their Service Level Agreement (SLA). Amazon, Google, Microsoft, IBM, and Salesforce maintain the availability of 99.9% which is still lower than the expectation [23]GLYPH<21>[26]. The availability of a CSP is calculated based on the rate of the accepted and total requests. We consider a request accepted if the request queue can process the request within the prescribed time threshold. The availability of CSP n is calculated using the equation below where ACrq is the rate of accepted requests and TOrq is the rate of incoming total requests.\n<!-- formula-not-decoded -->\nReliability ensures the ability of a service to keep functioning with a speciGLYPH<28>c performance level for a speciGLYPH<28>ed time. Reliability of cloud is not addressed or appropriately considered by the previous cloud researchers [27], [28]. However, due to many outages in a cloud, reliability issues have become a major concern in a cloud environment. We consider reliability as a failure-free operation. In terms of failures in the cloud, we consider hardware and software failures. Reliability is calculated based on the rate of successful and accepted requests. If a request completes the execution without any failure occurrence, we consider that request a successful one. The reliability of CSP n is calculated as below where SUrq is the rate of successful requests and ACrq is the rate of accepted requests.\n<!-- formula-not-decoded -->\nData integrity considers data loss due to latency and network failures. If a service is unable to grow fast then elasticity failure occurs that will impact the quality of service. We consider the elasticity failure as a network failure as it occurs due to an error in measuring load. Data integrity is checked by estimating the data loss. We use the equation below to estimate the data integrity of CSP n where SUrq is the rate of successful requests and NFrq is the rate of requests where a data failure occurs and generates a data loss.\n<!-- formula-not-decoded -->\nIn terms of efGLYPH<28>ciency, time efGLYPH<28>ciency is evaluated that considers the execution time and waiting time of each CSP. EfGLYPH<28>ciency is calculated based on the rate of promised and actual execution time. In an ideal case, the promised time ( PRt ) should be equal to the execution time ( EXt ) as the waiting time ( WAt ) is supposed to be 0.\n<!-- formula-not-decoded -->\nFinally, we combine all the individual rates multiplying with their corresponding weighting factors to compute the trust of a CSP as follows V\n<!-- formula-not-decoded -->\nWe repeat the same process for each of the CSP and evaluate the trust for all of them. The following architecture is used to evaluate the trust.",
    "context": "Evaluates trust quantitatively using workload and system modeling, incorporating availability, reliability, data integrity, and time efficiency to determine CSP trust.  The model uses weighted averages of these quality attributes to estimate trust, considering the rates of accepted and total requests for availability and successful versus accepted requests for reliability.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      4
    ],
    "id": "39c9d8b65658d620c6eef25bebe364bdb99a3735461e6daff8fed21a14c0a12a"
  },
  {
    "text": "Cloud providers can have multiple data centers in different geographic locations [29]. The failure rates and reliability of servers have different values that exist on different racks. The primary cause of a rack failure is the interruption on the power distribution or the rack switches [30]. Each of the servers is a combination of resources such as CPU, storage, RAM, and bandwidth and each of these resources has different failure rates and reliability levels. An entire data center may fail due to a natural disaster or due to a prolonged power outage that can cause a network interruption, and as a consequence service outage can occur. In our model, for simulating a cloud architecture, we consider a layered software framework and\ncloud components through CloudSim. The core functionalities are implemented at the lowest layer such as arriving and processing requests, resource creation (data centers, servers, VMs), communication among resources, and management of the load execution. We implement our customized VM allocation policy extending the functionality. We develop user requests, and resource availability, and different scenarios at this layer. We model a data center for handling users' requests. This request requires the allocation of VM to a data center's servers. In our considered architecture, the data center of a CSP is composed of many servers and is responsible for managing resources. Each server has a GLYPH<28>xed amount of pre-conGLYPH<28>gured resources for allocating to VMs [31]. The primary concerns in the cloud such as allocation of VMs, managing load execution, and monitoring are controlled at this layer. We used the aforementioned architecture for emulating a cloud environment and collected the necessary data to estimate the value of different attributes.\n\nDetails the architecture for simulating a cloud environment and data collection for evaluating trust attributes.",
    "original_text": "Cloud providers can have multiple data centers in different geographic locations [29]. The failure rates and reliability of servers have different values that exist on different racks. The primary cause of a rack failure is the interruption on the power distribution or the rack switches [30]. Each of the servers is a combination of resources such as CPU, storage, RAM, and bandwidth and each of these resources has different failure rates and reliability levels. An entire data center may fail due to a natural disaster or due to a prolonged power outage that can cause a network interruption, and as a consequence service outage can occur. In our model, for simulating a cloud architecture, we consider a layered software framework and\ncloud components through CloudSim. The core functionalities are implemented at the lowest layer such as arriving and processing requests, resource creation (data centers, servers, VMs), communication among resources, and management of the load execution. We implement our customized VM allocation policy extending the functionality. We develop user requests, and resource availability, and different scenarios at this layer. We model a data center for handling users' requests. This request requires the allocation of VM to a data center's servers. In our considered architecture, the data center of a CSP is composed of many servers and is responsible for managing resources. Each server has a GLYPH<28>xed amount of pre-conGLYPH<28>gured resources for allocating to VMs [31]. The primary concerns in the cloud such as allocation of VMs, managing load execution, and monitoring are controlled at this layer. We used the aforementioned architecture for emulating a cloud environment and collected the necessary data to estimate the value of different attributes.",
    "context": "Details the architecture for simulating a cloud environment and data collection for evaluating trust attributes.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      4,
      5
    ],
    "id": "5b373cbd590f59e2ce8dee118bbdce5941cdd1b5605d42f785e7df131bfd844d"
  },
  {
    "text": "CloudSim simulator is used in the simulation due to its acceptance in the research community for representing real cloud environments [32]GLYPH<21>[34]. To validate the proposed evaluation and integration models, it is essential to create a cloud using a simulator that can imitate practical cloud instances and failures by executing different loads in different scenarios. Failure injection and failure data collection in VM granularity levels are restricted to real cloud provider's conGLYPH<28>gurations and infrastructures. To verify the effectiveness of the proposed models, modeling and simulations are used as an alternative. Our proposed approach creates an option to improve the performance of quality attributes in a cloud scenario. The problem modeling and simulation environments ensure the applicability and validity of the proposed approach in a real cloud environment. Moreover, CSPs can achieve beneGLYPH<28>ts from the results by integrating new features and extensions to their infrastructures.\nAll the above-mentioned metrics are used in the trust evaluation model using the simulator. In the simulation, Poisson distribution is used for generating requests and occurring failures. Due to the popularity and applicability of Poisson distribution in modeling request and failure generation in cloud, it is the most appropriate distribution [35], [36], [37]. At GLYPH<28>rst, the initialization of all required parameters for simulation is set to obtain the necessary data. Multiple environments with different problem sizes are being created in the simulator. Weexecute the model in distinct environments and workloads for validation. We run each of the experiments ten times and obtain the average value to evaluate all the required attributes.\n\nValidates the proposed trust evaluation and integration models through simulation, emphasizing the use of CloudSim to mimic real cloud environments and failures.",
    "original_text": "CloudSim simulator is used in the simulation due to its acceptance in the research community for representing real cloud environments [32]GLYPH<21>[34]. To validate the proposed evaluation and integration models, it is essential to create a cloud using a simulator that can imitate practical cloud instances and failures by executing different loads in different scenarios. Failure injection and failure data collection in VM granularity levels are restricted to real cloud provider's conGLYPH<28>gurations and infrastructures. To verify the effectiveness of the proposed models, modeling and simulations are used as an alternative. Our proposed approach creates an option to improve the performance of quality attributes in a cloud scenario. The problem modeling and simulation environments ensure the applicability and validity of the proposed approach in a real cloud environment. Moreover, CSPs can achieve beneGLYPH<28>ts from the results by integrating new features and extensions to their infrastructures.\nAll the above-mentioned metrics are used in the trust evaluation model using the simulator. In the simulation, Poisson distribution is used for generating requests and occurring failures. Due to the popularity and applicability of Poisson distribution in modeling request and failure generation in cloud, it is the most appropriate distribution [35], [36], [37]. At GLYPH<28>rst, the initialization of all required parameters for simulation is set to obtain the necessary data. Multiple environments with different problem sizes are being created in the simulator. Weexecute the model in distinct environments and workloads for validation. We run each of the experiments ten times and obtain the average value to evaluate all the required attributes.",
    "context": "Validates the proposed trust evaluation and integration models through simulation, emphasizing the use of CloudSim to mimic real cloud environments and failures.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      5
    ],
    "id": "58551f7f79449db2d25ace9cb8f312722132e4b3b10cb4616032c461994a1e5d"
  },
  {
    "text": "The objective of this model is to allocate resources in such a way that increases trust and decreases delay. Availability, reliability, data integrity, and efGLYPH<28>ciency are considered to obtain the trust of a CSP. Allocating resources in the cloud while maximizing the service quality is treated as one of the\ncomplex problems. The resource allocation model integrates trust obtained from our trust evaluation model. The optimization model is designed for joint optimization criteria as a Mixed Integer Linear Programming (MILP) problem. The joint optimization aims at maximizing trust and minimizing delay to improve the service performance.\nFIGURE 2. Maximizing trust in the resource allocation model.\nAn example of the proposed resource allocation process is shown in Fig. 2 with N CSPs. At GLYPH<28>rst, users provide requests for resources to a broker. Broker has access to the value of trust and delay for all the data centers of all CSPs. The broker executes the proposed resource allocation model and allocates resources while maintaining the objectives and required constraints. The model aims for an optimized allocation of resources that maximizes trust to improve the quality of service.\n\nSummarizes the model's objective: to allocate resources in a cloud environment to maximize trust (based on availability, reliability, data integrity, and efficiency) while minimizing delay. It utilizes a Mixed Integer Linear Programming (MILP) model and a Genetic Algorithm to achieve this, integrating trust scores from a separate evaluation model.",
    "original_text": "The objective of this model is to allocate resources in such a way that increases trust and decreases delay. Availability, reliability, data integrity, and efGLYPH<28>ciency are considered to obtain the trust of a CSP. Allocating resources in the cloud while maximizing the service quality is treated as one of the\ncomplex problems. The resource allocation model integrates trust obtained from our trust evaluation model. The optimization model is designed for joint optimization criteria as a Mixed Integer Linear Programming (MILP) problem. The joint optimization aims at maximizing trust and minimizing delay to improve the service performance.\nFIGURE 2. Maximizing trust in the resource allocation model.\nAn example of the proposed resource allocation process is shown in Fig. 2 with N CSPs. At GLYPH<28>rst, users provide requests for resources to a broker. Broker has access to the value of trust and delay for all the data centers of all CSPs. The broker executes the proposed resource allocation model and allocates resources while maintaining the objectives and required constraints. The model aims for an optimized allocation of resources that maximizes trust to improve the quality of service.",
    "context": "Summarizes the model's objective: to allocate resources in a cloud environment to maximize trust (based on availability, reliability, data integrity, and efficiency) while minimizing delay. It utilizes a Mixed Integer Linear Programming (MILP) model and a Genetic Algorithm to achieve this, integrating trust scores from a separate evaluation model.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      5
    ],
    "id": "39a11cf74cd9616e8e1db01bbf6291c57e672b0f53b7112bfa5486786b7a0c72"
  },
  {
    "text": "The research problem that we want to formulate is allocating resources in multiple CSPs while increasing trust and decreasing communication delay. The capacity of the servers located in the data center and total trafGLYPH<28>c are used in estimating communication delay. The description of all the notations of our model is presented in Table 1.\nWe allocate a set of servers for each of the CSP. A number of VMs can be allocated inside each of the servers based on the required amount of resources. A VM is created by applying the users' requirements for the combination of processing unit, storage, memory, and bandwidth. We use R to determine the amount of resources of a VM.\n\nFormulates a problem of allocating resources across multiple cloud service providers to maximize trust and minimize communication delay.",
    "original_text": "The research problem that we want to formulate is allocating resources in multiple CSPs while increasing trust and decreasing communication delay. The capacity of the servers located in the data center and total trafGLYPH<28>c are used in estimating communication delay. The description of all the notations of our model is presented in Table 1.\nWe allocate a set of servers for each of the CSP. A number of VMs can be allocated inside each of the servers based on the required amount of resources. A VM is created by applying the users' requirements for the combination of processing unit, storage, memory, and bandwidth. We use R to determine the amount of resources of a VM.",
    "context": "Formulates a problem of allocating resources across multiple cloud service providers to maximize trust and minimize communication delay.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      5
    ],
    "id": "0f223b7d8be580f4f7c417d073dde63c1160e84e64d3371f6c457bfc55068d84"
  },
  {
    "text": "We want to maximize fTR , the overall trust value of the resource allocation. TRisn is the trust of VM i in server s in CSP n , and Aisn is used as a GLYPH<29>ag to determine the allocation of VM i in server s in CSP n .\n<!-- formula-not-decoded -->\nThe communication delay of a server is evaluated using the values of the total trafGLYPH<28>c between VMs and the capacity of the server where the VMs will be placed. In DLisn , we estimate the delay to allocate VM i of server s in CSP n . For measuring\nTABLE 1. Description of the symbols.\ntotal trafGLYPH<28>c, we combine the communication of VM i with all other VMs of the corresponding server s . The delay of a VM i is evaluated by combining all the above trafGLYPH<28>c divided by the capacity of server s in CSP n using the equation below V\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nWe set all the constraints in the above. With constraint 10, we conGLYPH<28>rm the allocation of one VM to one server of a CSP only. The limitations and availability of resources are maintained by constraint 11. Integrity constraint is fulGLYPH<28>lled using constraint 12. In the literature, it is shown that such type of problems are NP-hard problems [38].\n\nFocuses on optimizing trust and delay in resource allocation through a genetic algorithm, specifically targeting the overall trust value (fTR) and VM allocation within cloud service providers.",
    "original_text": "We want to maximize fTR , the overall trust value of the resource allocation. TRisn is the trust of VM i in server s in CSP n , and Aisn is used as a GLYPH<29>ag to determine the allocation of VM i in server s in CSP n .\n<!-- formula-not-decoded -->\nThe communication delay of a server is evaluated using the values of the total trafGLYPH<28>c between VMs and the capacity of the server where the VMs will be placed. In DLisn , we estimate the delay to allocate VM i of server s in CSP n . For measuring\nTABLE 1. Description of the symbols.\ntotal trafGLYPH<28>c, we combine the communication of VM i with all other VMs of the corresponding server s . The delay of a VM i is evaluated by combining all the above trafGLYPH<28>c divided by the capacity of server s in CSP n using the equation below V\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nWe set all the constraints in the above. With constraint 10, we conGLYPH<28>rm the allocation of one VM to one server of a CSP only. The limitations and availability of resources are maintained by constraint 11. Integrity constraint is fulGLYPH<28>lled using constraint 12. In the literature, it is shown that such type of problems are NP-hard problems [38].",
    "context": "Focuses on optimizing trust and delay in resource allocation through a genetic algorithm, specifically targeting the overall trust value (fTR) and VM allocation within cloud service providers.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      5,
      6
    ],
    "id": "b008d65f2eb868624488ddf9f540d838a0c5fa1feda23ce4600808c3ff2e172c"
  },
  {
    "text": "We solve the joint optimization model using the optimizer varying several weight combinations to the objective functions. The variation of weights is required due to multiple reasons such as the requirements of the type of the application or QoS parameters. Due to the NP-hard nature of the problem, formal methods are suitable only for small-scale instances. To overcome this situation, a heuristic is proposed for generating an approximated near-optimal solution.\nFor solving the resource allocation problem, we propose a Genetic Algorithm that is usually applied in several machine learning approaches [39]. This type of meta-heuristic algorithm is appropriate to GLYPH<28>nd a near-optimal solution in large-scale problem instances. The complexity of the three major operations of the genetic algorithm determines the total\nAlgorithm 1 Optimizing Trust and Delay in Resource Allocation\n\nThis chunk details the approach to solving the resource allocation problem, specifically using a Genetic Algorithm to overcome the challenges posed by the NP-hard nature of the optimization. It highlights the need for weight variations based on application requirements and the algorithm's suitability for large-scale instances, emphasizing its role as a heuristic for near-optimal solutions.",
    "original_text": "We solve the joint optimization model using the optimizer varying several weight combinations to the objective functions. The variation of weights is required due to multiple reasons such as the requirements of the type of the application or QoS parameters. Due to the NP-hard nature of the problem, formal methods are suitable only for small-scale instances. To overcome this situation, a heuristic is proposed for generating an approximated near-optimal solution.\nFor solving the resource allocation problem, we propose a Genetic Algorithm that is usually applied in several machine learning approaches [39]. This type of meta-heuristic algorithm is appropriate to GLYPH<28>nd a near-optimal solution in large-scale problem instances. The complexity of the three major operations of the genetic algorithm determines the total\nAlgorithm 1 Optimizing Trust and Delay in Resource Allocation",
    "context": "This chunk details the approach to solving the resource allocation problem, specifically using a Genetic Algorithm to overcome the challenges posed by the NP-hard nature of the optimization. It highlights the need for weight variations based on application requirements and the algorithm's suitability for large-scale instances, emphasizing its role as a heuristic for near-optimal solutions.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      6
    ],
    "id": "fdf2541ec87513ee893d24921e900ad98ba8b2052ef10a87ad1d1b629dd74b83"
  },
  {
    "text": "- Resource capacities for all servers of all CSPs\n- Availability, reliability, data integrity, and time efGLYPH<28>ciency of the servers of all CSPs\n- Matrix for communication trafGLYPH<28>c of all VMs\n\nThis chunk details the inputs and data used in the resource allocation model, specifically outlining the resource capacities, performance metrics (availability, reliability, etc.), and communication traffic matrices for all servers and CSPs.",
    "original_text": "- Resource capacities for all servers of all CSPs\n- Availability, reliability, data integrity, and time efGLYPH<28>ciency of the servers of all CSPs\n- Matrix for communication trafGLYPH<28>c of all VMs",
    "context": "This chunk details the inputs and data used in the resource allocation model, specifically outlining the resource capacities, performance metrics (availability, reliability, etc.), and communication traffic matrices for all servers and CSPs.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      6
    ],
    "id": "0699892e5c75d7c048a3b9564d03ef29c89311ac222398817c338ab61c7ef0ce"
  },
  {
    "text": "- The best VM allocation set with maximum GLYPH<28>tness value CRb\n- 1: for all incoming requests rq 2 R Q\n- 2: for all existing CSPs n 2 N\n- 3: for all available servers s 2 S n do\n- 4: Estimate trust\n5:\nEstimate delay\n- 6: end for\n- 7: end for\n- 8: end for\n- 9: CRb Initialize this set empty\n- 10: Randomly generate a set of C chromosomes\n- 11: Evaluate GLYPH<28>tness (trust and delay) for the above C chromosomes\n- 12: generation D 0\n- 13: while generation < D MAXgeneration\n14:\nwhile new chromosomes set\n6D\nC\n- 15: Select two chromosomes applying roulette wheel operator\n16:\nBased on the crossover probability, crossover occurs\n- 17: According to the mutation probability, mutation occurs\n- 18: end while\n- 19: Estimate GLYPH<28>tness of the new set of chromosomes\n- 20: Select the GLYPH<28>ttest chromosomes CRn using selection operation\n- 21: if f ( CRn ) > f ( CRb )\n- 22: CRb D CRn\n- 23: end if\n- 24: generation D generation C 1\n- 25: end while\n- 26: return the best allocation set ( CRb )\ntime complexity of such an algorithm. The time complexity for selection operation depends on the size of the request queue and it is O ( RQ ). On the other hand, crossover and mutation operations have a time complexity of O ( N GLYPH<3> V GLYPH<3> RQ ). We estimate the total complexity by combining the above operations and it is O ( N GLYPH<3> V GLYPH<3> RQ GLYPH<3> MAXgen ). The complexity of the algorithm provides the feasibility for implementing the model in a large-scale real cloud environment to ensure the allocation of VMs while jointly optimizing trust and delay.\nWe design the chromosome CR as a vector where the size of the vector depends on the requests. The size of the vector can be S GLYPH<3> N -dimensional while we keep the number of VMs the same in all requests. The value of the chromosome indicates the location of a VM to a server of a speciGLYPH<28>c CSP.\nWedeGLYPH<28>ne trust and delay as the GLYPH<28>tness of the chromosome. Weestimate the trust and delay and combine them to compute the GLYPH<28>tness. The GLYPH<28>ttest set of chromosomes represents the best solution for allocating VMs. We deGLYPH<28>ne the GLYPH<28>tness function by combining the objective functions using a weighted sum technique. Multiple combinations of the weights are examined for the objective functions to determine the proper tradeoff. We present the pseudo-code of the proposed algorithm in Algorithm 1. As input of the algorithm, it takes the following: available resource capacities, availability, reliability, data integrity, time efGLYPH<28>ciency, and trafGLYPH<28>c matrix of VMs for all servers and CSPs. At GLYPH<28>rst, a random chromosome population of size C is generated and the algorithm calculates the GLYPH<28>tness of each of them. After the GLYPH<28>tness being evaluated, based on the GLYPH<28>tness value selection, crossover, and mutation are applied and a new set of chromosomes are generated [40] (lines 12-20).\nThe roulette wheel method is used for selection operations. We use a two-point crossover process to have the better parts of parent chromosomes. In the process, the selected chromosomes exchange their parts and create two new child chromosomes. A single bit of the chromosome is changed in mutation operation, and thereby a new chromosome is generated. We assign the crossover and mutation rates and based on the values the operations occur in the algorithm. After obtaining a new generation of chromosomes, we estimate the GLYPH<28>tness of each of them, and the set of chromosomes with maximum GLYPH<28>tness value is selected as our new solution CRn . If the GLYPH<28>tness of this set is more than that of the previous set, the algorithm selects this set as the best solution CRb . The algorithm will return the best set of solutions when it reaches the termination conditions. In our algorithm, we set the maximum number of generations MAXgeneration to terminate the proposed algorithm.\nThe proposed optimization problem is modeled with several constraints and the constraints handling method depends on the application of selection operation. We enforce several rules while comparing two solutions such as for any solution in terms of feasibility, we discard the infeasible solution and choose the feasible one. The algorithm GLYPH<28>nally returns CRb , the best VM allocation set that maximizes trust and minimizes delay.\n\nIntroduces the genetic algorithm approach for resource allocation, outlining its key steps and components.",
    "original_text": "- The best VM allocation set with maximum GLYPH<28>tness value CRb\n- 1: for all incoming requests rq 2 R Q\n- 2: for all existing CSPs n 2 N\n- 3: for all available servers s 2 S n do\n- 4: Estimate trust\n5:\nEstimate delay\n- 6: end for\n- 7: end for\n- 8: end for\n- 9: CRb Initialize this set empty\n- 10: Randomly generate a set of C chromosomes\n- 11: Evaluate GLYPH<28>tness (trust and delay) for the above C chromosomes\n- 12: generation D 0\n- 13: while generation < D MAXgeneration\n14:\nwhile new chromosomes set\n6D\nC\n- 15: Select two chromosomes applying roulette wheel operator\n16:\nBased on the crossover probability, crossover occurs\n- 17: According to the mutation probability, mutation occurs\n- 18: end while\n- 19: Estimate GLYPH<28>tness of the new set of chromosomes\n- 20: Select the GLYPH<28>ttest chromosomes CRn using selection operation\n- 21: if f ( CRn ) > f ( CRb )\n- 22: CRb D CRn\n- 23: end if\n- 24: generation D generation C 1\n- 25: end while\n- 26: return the best allocation set ( CRb )\ntime complexity of such an algorithm. The time complexity for selection operation depends on the size of the request queue and it is O ( RQ ). On the other hand, crossover and mutation operations have a time complexity of O ( N GLYPH<3> V GLYPH<3> RQ ). We estimate the total complexity by combining the above operations and it is O ( N GLYPH<3> V GLYPH<3> RQ GLYPH<3> MAXgen ). The complexity of the algorithm provides the feasibility for implementing the model in a large-scale real cloud environment to ensure the allocation of VMs while jointly optimizing trust and delay.\nWe design the chromosome CR as a vector where the size of the vector depends on the requests. The size of the vector can be S GLYPH<3> N -dimensional while we keep the number of VMs the same in all requests. The value of the chromosome indicates the location of a VM to a server of a speciGLYPH<28>c CSP.\nWedeGLYPH<28>ne trust and delay as the GLYPH<28>tness of the chromosome. Weestimate the trust and delay and combine them to compute the GLYPH<28>tness. The GLYPH<28>ttest set of chromosomes represents the best solution for allocating VMs. We deGLYPH<28>ne the GLYPH<28>tness function by combining the objective functions using a weighted sum technique. Multiple combinations of the weights are examined for the objective functions to determine the proper tradeoff. We present the pseudo-code of the proposed algorithm in Algorithm 1. As input of the algorithm, it takes the following: available resource capacities, availability, reliability, data integrity, time efGLYPH<28>ciency, and trafGLYPH<28>c matrix of VMs for all servers and CSPs. At GLYPH<28>rst, a random chromosome population of size C is generated and the algorithm calculates the GLYPH<28>tness of each of them. After the GLYPH<28>tness being evaluated, based on the GLYPH<28>tness value selection, crossover, and mutation are applied and a new set of chromosomes are generated [40] (lines 12-20).\nThe roulette wheel method is used for selection operations. We use a two-point crossover process to have the better parts of parent chromosomes. In the process, the selected chromosomes exchange their parts and create two new child chromosomes. A single bit of the chromosome is changed in mutation operation, and thereby a new chromosome is generated. We assign the crossover and mutation rates and based on the values the operations occur in the algorithm. After obtaining a new generation of chromosomes, we estimate the GLYPH<28>tness of each of them, and the set of chromosomes with maximum GLYPH<28>tness value is selected as our new solution CRn . If the GLYPH<28>tness of this set is more than that of the previous set, the algorithm selects this set as the best solution CRb . The algorithm will return the best set of solutions when it reaches the termination conditions. In our algorithm, we set the maximum number of generations MAXgeneration to terminate the proposed algorithm.\nThe proposed optimization problem is modeled with several constraints and the constraints handling method depends on the application of selection operation. We enforce several rules while comparing two solutions such as for any solution in terms of feasibility, we discard the infeasible solution and choose the feasible one. The algorithm GLYPH<28>nally returns CRb , the best VM allocation set that maximizes trust and minimizes delay.",
    "context": "Introduces the genetic algorithm approach for resource allocation, outlining its key steps and components.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      6,
      7
    ],
    "id": "00e4b5853c2692886fd94325f13941f8f1b68d04e68422f78cac133c0b884273"
  },
  {
    "text": "The details of the experiments and results analysis are discussed in this section. CloudSim [31] is used for cloud data collection, and MATLAB is used for the optimized resource allocation in the experiments. We perform the experiments in multiple scenarios for testing the applicability of the approach. We implement the trust evaluation model using CloudSim and obtain the required data to calculate the trust of a CSP. Finally, the performance of the optimization model for resource allocation is compared with the exact optimized solution and the performance of the proposed heuristic is presented.\nWe obtain the necessary cloud data to compute trust from the experiments executed in the CloudSim environment. Based on the collected data, we calculate the trust of different CSPs. We use the trust value of our model and integrate this in the optimization model to execute the optimizer and GA in MATLAB environments to analyze the performance. To run experiments, we use an Intel Core i7 machine with a 4 GHz processor. In GA, we set the population size based on our requirements and generation size to 1000.\nWe execute the proposed model in four distinct scenarios. The number of servers and CSPs are varied in each of the scenarios. We modify the number of servers and CSPs from 30 to 60 and 6 to 12 respectively. In each scenario, the number of servers is increased by 10 and CSPs by 2. The above scenarios may represent real scenarios as the number of CSPs that can participate together cannot be more as several constraints need to be fulGLYPH<28>lled.\nThe trust of a CSP is estimated using the weighted sum of the value of availability, reliability, data integrity, and time efGLYPH<28>ciency. Fig. 3 depicts the trend of the availability metric with respect to different loads. The number of incoming requests and executing loads are varied, and three distinct cases are considered. A small load represents the number of requests and executing loads are minimum, on the other hand, there is one case with a maximum or heavy load. In between the minimum and maximum loads, we also consider one case which we name medium load. The trend in the GLYPH<28>gure shows that with a small load the availability is maximum and with a heavy load it is minimum. For calculating the availability value, we consider several such scenarios and take the average of 10 executions of those scenarios for the availability value.\nFIGURE 3. Resources availability with different loads.\nFor calculating reliability, we consider some of the hardware and software failures and combine them together to get the reliability attribute. In case of hardware failures, failures of virtual machines and processing elements are considered. The required software that is involved to process requests and execute workload is considered software failures. We execute several experiments with variations in the requests and loads\nFIGURE 4. Reliability of resources with different loads.\nFIGURE 5. Data integrity with different loads.\nand show that in Fig. 4. The reliability trend depicts that failures are more frequent in the case of a maximum number of requests and loads.\nFig. 5 shows the trend of the value of data integrity that was varied with the variations of the different loads. Fig. 6 conGLYPH<28>rms the efGLYPH<28>ciency in terms of the required time for all the different scenarios. In an ideal case, it is supposed to be 1 but practically we can try to get closer to the maximum possible value. After obtaining all the four attributes, we take the averages of all the attributes and combined them together as a weighted sum to get the trust value of one CSP.\nFig. 7 shows an important performance trend of the approach on the basis of trust value. The trust value is calculated using the weighted sum of availability, reliability, data integrity, and time efGLYPH<28>ciency. The results show the variation in the values of trust of our model with a variation in the weight. A 0.25 value in the weight of trust means trust is considered partially but the delay has more weight which is 0.75 in the objective function. Similarly, the other side is also considered in the objective function. A weight value of 0.5 indicates an equal weight on both of the objective functions. Fig. 7 shows that the trust of the model improves approximately 10% in all distinct scenarios. This performance gain ensures the validity of the approach.\nFIGURE 6. Time efficiency with different loads.\nFIGURE 7. Trust with different weight values.\nFIGURE 8. Delay with different weight values.\nFig. 8 depicts the trends of delay. The same weight variations are considered to show the variations in the values of communication delay. Applying the model delay is decreased by approximately 10%. The performance improvement is not massive but even a small reduction of delay improves the quality of service. The results can be more prominent in the real environment.\nFIGURE 9. Objective value comparison between optimal and genetic algorithm.\nFIGURE 10. Execution time comparison between optimal and genetic algorithm.\nFig. 9 shows one of the principal performance trends of the approach that is the objective value. The objectives of trust and delay are combined to calculate the objective value. The obtained value of the objective in the approach is nearly the same as an optimal solution. The aim is to maximize trust and minimize delay, and the proposed GA achieves almost the same objective values as an optimizer. The performance trend remains the same while varying the size of the problem in terms of CSPs and servers. This trend conGLYPH<28>rms that the proposed model is appropriate in a large cloud environment.\nFig. 10 also shows the performance of the approach on the basis of execution time. Results depict a linear increase in the execution time with an increase in the number of CSPs and servers. The above trend conGLYPH<28>rms the usability of the approach in real scenarios. The results exhibit that increase in the problem size does not require any noticeable execution time. This trend provides the assurance of the approach while increasing trust and decreasing delay. Moreover, the execution time of the approach is more signiGLYPH<28>cant compare to the optimizer in a large scenario.\nFig. 11 reveals the Pareto front obtained from the proposed heuristic. The trade-off between the objective functions is depicted using the Pareto front. Allocating resources in the\nFIGURE 11. Pareto font between trust and delay.\nmost trusted CSPs may increase trust but it will acquire a massive delay. Distributing resources will reduce delay as the load will be divided but trust will be decreased.\nThe performance results show that the trust of different CSPs is effectively estimated while varying the simulation environments. The performance results exhibit that the proposed approach can be an appropriate solution for optimizing resource allocation in a large cloud environment while fulGLYPH<28>lling the objectives. The improvement of the performance can be more prominent with an application in a real large cloud environment.\n\nDescribes the experimental setup and data collection methods used to evaluate the resource allocation model.",
    "original_text": "The details of the experiments and results analysis are discussed in this section. CloudSim [31] is used for cloud data collection, and MATLAB is used for the optimized resource allocation in the experiments. We perform the experiments in multiple scenarios for testing the applicability of the approach. We implement the trust evaluation model using CloudSim and obtain the required data to calculate the trust of a CSP. Finally, the performance of the optimization model for resource allocation is compared with the exact optimized solution and the performance of the proposed heuristic is presented.\nWe obtain the necessary cloud data to compute trust from the experiments executed in the CloudSim environment. Based on the collected data, we calculate the trust of different CSPs. We use the trust value of our model and integrate this in the optimization model to execute the optimizer and GA in MATLAB environments to analyze the performance. To run experiments, we use an Intel Core i7 machine with a 4 GHz processor. In GA, we set the population size based on our requirements and generation size to 1000.\nWe execute the proposed model in four distinct scenarios. The number of servers and CSPs are varied in each of the scenarios. We modify the number of servers and CSPs from 30 to 60 and 6 to 12 respectively. In each scenario, the number of servers is increased by 10 and CSPs by 2. The above scenarios may represent real scenarios as the number of CSPs that can participate together cannot be more as several constraints need to be fulGLYPH<28>lled.\nThe trust of a CSP is estimated using the weighted sum of the value of availability, reliability, data integrity, and time efGLYPH<28>ciency. Fig. 3 depicts the trend of the availability metric with respect to different loads. The number of incoming requests and executing loads are varied, and three distinct cases are considered. A small load represents the number of requests and executing loads are minimum, on the other hand, there is one case with a maximum or heavy load. In between the minimum and maximum loads, we also consider one case which we name medium load. The trend in the GLYPH<28>gure shows that with a small load the availability is maximum and with a heavy load it is minimum. For calculating the availability value, we consider several such scenarios and take the average of 10 executions of those scenarios for the availability value.\nFIGURE 3. Resources availability with different loads.\nFor calculating reliability, we consider some of the hardware and software failures and combine them together to get the reliability attribute. In case of hardware failures, failures of virtual machines and processing elements are considered. The required software that is involved to process requests and execute workload is considered software failures. We execute several experiments with variations in the requests and loads\nFIGURE 4. Reliability of resources with different loads.\nFIGURE 5. Data integrity with different loads.\nand show that in Fig. 4. The reliability trend depicts that failures are more frequent in the case of a maximum number of requests and loads.\nFig. 5 shows the trend of the value of data integrity that was varied with the variations of the different loads. Fig. 6 conGLYPH<28>rms the efGLYPH<28>ciency in terms of the required time for all the different scenarios. In an ideal case, it is supposed to be 1 but practically we can try to get closer to the maximum possible value. After obtaining all the four attributes, we take the averages of all the attributes and combined them together as a weighted sum to get the trust value of one CSP.\nFig. 7 shows an important performance trend of the approach on the basis of trust value. The trust value is calculated using the weighted sum of availability, reliability, data integrity, and time efGLYPH<28>ciency. The results show the variation in the values of trust of our model with a variation in the weight. A 0.25 value in the weight of trust means trust is considered partially but the delay has more weight which is 0.75 in the objective function. Similarly, the other side is also considered in the objective function. A weight value of 0.5 indicates an equal weight on both of the objective functions. Fig. 7 shows that the trust of the model improves approximately 10% in all distinct scenarios. This performance gain ensures the validity of the approach.\nFIGURE 6. Time efficiency with different loads.\nFIGURE 7. Trust with different weight values.\nFIGURE 8. Delay with different weight values.\nFig. 8 depicts the trends of delay. The same weight variations are considered to show the variations in the values of communication delay. Applying the model delay is decreased by approximately 10%. The performance improvement is not massive but even a small reduction of delay improves the quality of service. The results can be more prominent in the real environment.\nFIGURE 9. Objective value comparison between optimal and genetic algorithm.\nFIGURE 10. Execution time comparison between optimal and genetic algorithm.\nFig. 9 shows one of the principal performance trends of the approach that is the objective value. The objectives of trust and delay are combined to calculate the objective value. The obtained value of the objective in the approach is nearly the same as an optimal solution. The aim is to maximize trust and minimize delay, and the proposed GA achieves almost the same objective values as an optimizer. The performance trend remains the same while varying the size of the problem in terms of CSPs and servers. This trend conGLYPH<28>rms that the proposed model is appropriate in a large cloud environment.\nFig. 10 also shows the performance of the approach on the basis of execution time. Results depict a linear increase in the execution time with an increase in the number of CSPs and servers. The above trend conGLYPH<28>rms the usability of the approach in real scenarios. The results exhibit that increase in the problem size does not require any noticeable execution time. This trend provides the assurance of the approach while increasing trust and decreasing delay. Moreover, the execution time of the approach is more signiGLYPH<28>cant compare to the optimizer in a large scenario.\nFig. 11 reveals the Pareto front obtained from the proposed heuristic. The trade-off between the objective functions is depicted using the Pareto front. Allocating resources in the\nFIGURE 11. Pareto font between trust and delay.\nmost trusted CSPs may increase trust but it will acquire a massive delay. Distributing resources will reduce delay as the load will be divided but trust will be decreased.\nThe performance results show that the trust of different CSPs is effectively estimated while varying the simulation environments. The performance results exhibit that the proposed approach can be an appropriate solution for optimizing resource allocation in a large cloud environment while fulGLYPH<28>lling the objectives. The improvement of the performance can be more prominent with an application in a real large cloud environment.",
    "context": "Describes the experimental setup and data collection methods used to evaluate the resource allocation model.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      8,
      9,
      7
    ],
    "id": "370d559156b8e3e4ea7c5398b8a9872bb9682995b4c0b664da5ff774bb1e71b2"
  },
  {
    "text": "The contributions of this work can be outlined into three main parts: evaluation of trust of cloud computing, integration of trust in resource allocation model for improving QoS, and the application of Genetic algorithm to allocate resources effectively in a reasonable amount of time. The GLYPH<28>rst contribution is the trust evaluation in multi-cloud settings. These include consideration of proper attributes and metrics and the quantiGLYPH<28>cation of the metrics for estimating trust. The evaluation is shown in several environments for providing practical solutions. The trust evaluation is crucial in the adoption of a multi-cloud environment. The proposed evaluation model will attract more users as it estimates the trust of the cloud more accurately. This type of evaluation model will be of high importance to both service providers and customers.\nThe performance analysis of the resource allocation model depicts that the model effectively allocates resources while maintaining the objectives. Applying a Genetic algorithm to get an optimal solution in a reasonable amount of time is our third contribution. The execution time of the algorithm to GLYPH<28>nd a solution requires signiGLYPH<28>cantly less time compared to an optimizer and, this feature validates the applicability of the method. Achieving the same performance trend while varying numbers of resources conGLYPH<28>rms that the improvement of the performance will not be reduced even if a real cloud environment is used with a large number of resources. In the future, we plan to provide a reliable and secure model for trust management among different cloud service providers.\n\nOutlines the three main contributions of the work: trust evaluation in multi-cloud settings, integration of trust into a resource allocation model for improved QoS, and the application of a genetic algorithm for efficient resource allocation.",
    "original_text": "The contributions of this work can be outlined into three main parts: evaluation of trust of cloud computing, integration of trust in resource allocation model for improving QoS, and the application of Genetic algorithm to allocate resources effectively in a reasonable amount of time. The GLYPH<28>rst contribution is the trust evaluation in multi-cloud settings. These include consideration of proper attributes and metrics and the quantiGLYPH<28>cation of the metrics for estimating trust. The evaluation is shown in several environments for providing practical solutions. The trust evaluation is crucial in the adoption of a multi-cloud environment. The proposed evaluation model will attract more users as it estimates the trust of the cloud more accurately. This type of evaluation model will be of high importance to both service providers and customers.\nThe performance analysis of the resource allocation model depicts that the model effectively allocates resources while maintaining the objectives. Applying a Genetic algorithm to get an optimal solution in a reasonable amount of time is our third contribution. The execution time of the algorithm to GLYPH<28>nd a solution requires signiGLYPH<28>cantly less time compared to an optimizer and, this feature validates the applicability of the method. Achieving the same performance trend while varying numbers of resources conGLYPH<28>rms that the improvement of the performance will not be reduced even if a real cloud environment is used with a large number of resources. In the future, we plan to provide a reliable and secure model for trust management among different cloud service providers.",
    "context": "Outlines the three main contributions of the work: trust evaluation in multi-cloud settings, integration of trust into a resource allocation model for improved QoS, and the application of a genetic algorithm for efficient resource allocation.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      9
    ],
    "id": "17d285ecaa73e7e507292ac1558b28c3ed6b702c991fb7405d9d4e6e896c2384"
  },
  {
    "text": "- [1] E. Buer and R. Adams, Reliability and Availability of Cloud Computing . Hoboken, NJ, USA: Wiley, 2012.\n- [2] A. B. M. B. Alam, T. Halabi, A. Haque, and M. Zulkernine, ''Multiobjective interdependent VM placement model based on cloud reliability evaluation,'' in Proc. IEEE Int. Conf. Commun. (ICC) , Dublin, Ireland, Jun. 2020, pp. 1GLYPH<21>7.\n- [3] K. V. Vishwanath and N. Nagappan, ''Characterizing cloud computing hardware reliability,'' in Proc. 1st ACM Symp. Cloud Comput. (SoCC) , 2010, pp. 193GLYPH<21>204.\n- [4] H. Liao, Z. Zhou, X. Zhao, L. Zhang, S. Mumtaz, A. Jolfaei, S. H. Ahmed, and A. K. Bashir, ''Learning-based context-aware resource allocation for edge-computing-empowered industrial IoT,'' IEEE Internet Things J. , vol. 7, no. 5, pp. 4260GLYPH<21>4277, May 2020.\n- [5] A. Musaddiq, Y. B. Zikria, O. Hahm, H. Yu, A. K. Bashir, and S. W. Kim, ''A survey on resource management in IoT operating systems,'' IEEE Access , vol. 6, pp. 8459GLYPH<21>8482, 2018.\n- [6] P. Pradhan, P. K. Behera, and B. N. B. Ray, ''Improved max-min algorithm for resource allocation in cloud computing,'' in Proc. 6th Int. Conf. Parallel, Distrib. Grid Comput. (PDGC) , Nov. 2020, pp. 22GLYPH<21>24.\n- [7] Z. Xuejie, W. Zhijian, and X. Feng, ''Reliability evaluation of cloud computing systems using hybrid methods,'' Intell. Autom. Soft Comput. , vol. 19, no. 2, pp. 165GLYPH<21>174, Apr. 2013.\n- [8] N. Padmapriya and R. Rajmohan, ''Reliability evaluation suite for cloud services,'' in Proc. 3rd Int. Conf. Comput., Commun. Netw. Technol. (ICCCNT) , Jul. 2012, pp. 1GLYPH<21>6.\n- [9] B. Yang, F. Tan, Y.-S. Dai, and S. Guo, ''Performance evaluation of cloud service considering fault recovery,'' in Proc. IEEE Int. Conf. Cloud Comput. , Dec. 2009, pp. 571GLYPH<21>576.\n- [10] H. R. Faragardi, R. Shojaee, H. Tabani, and A. Rajabi, ''An analytical model to evaluate reliability of cloud computing systems in the presence of QoS requirements,'' in Proc. IEEE/ACIS 12th Int. Conf. Comput. Inf. Sci. (ICIS) , Niigata, Japan, Jun. 2013, pp. 315GLYPH<21>321.\n- [11] Y. Hu, J. Wong, G. Iszlai, and M. Litoiu, ''Resource provisioning for cloud computing,'' in Proc. Conf. Center Adv. Stud. Collaborative Res. , 2009, pp. 101GLYPH<21>111.\n- [12] J. Praveenchandar and A. Tamilarasi, ''Dynamic resource allocation with optimized task scheduling and improved power management in cloud computing,'' J. Ambient Intell. Humanized Comput. , vol. 12, no. 3, pp. 4147GLYPH<21>4159, Mar. 2021.\n- [13] A. K. Bashir, R. Arul, S. Basheer, G. Raja, R. Jayaraman, and N. M. F. Qureshi, ''An optimal multitier resource allocation of cloud RAN in 5G using machine learning,'' Trans. Emerg. Telecommun. Technol. , vol. 30, no. 8, p. e3627, Aug. 2019.\n- [14] Z. Xiao, W. Song, and Q. Chen, ''Dynamic resource allocation using virtual machines for cloud computing environment,'' IEEETrans. Parallel Distrib. Syst. , vol. 24, no. 6, pp. 1107GLYPH<21>1117, Jun. 2013.\n- [15] W. Lin, J. Z. Wang, C. Liang, and D. Qi, ''A threshold-based dynamic resource allocation scheme for cloud computing,'' Procedia Eng. , vol. 23, pp. 695GLYPH<21>703, Jan. 2011.\n- [16] R. K. Grewal and P. K. Pateriya, ''A rule-based approach for effective resource provisioning in hybrid cloud environment,'' in New Paradigms in Internet Computing . Berlin, Germany: Springer, 2013, pp. 41GLYPH<21>57.\n- [17] J. Luo, W. Song, and L. Yin, ''Reliable virtual machine placement based on multi-objective optimization with trafGLYPH<28>c-aware algorithm in industrial cloud,'' IEEE Access , vol. 6, pp. 23043GLYPH<21>23052, 2018.\n- [18] X. Wang, H. Gu, and Y. Yue, ''The optimization of virtual resource allocation in cloud computing based on RBPSO,'' Concurrency Comput., Pract. Exper. , vol. 32, no. 16, p. e5113, Aug. 2020.\n- [19] T. Halabi, M. Bellaiche, and A. Abusitta, ''Online allocation of cloud resources based on security satisfaction,'' in Proc. 17th IEEE Int. Conf. Trust, Secur. Privacy Comput. Commun./12th IEEE Int. Conf. Big Data Sci. Eng. (TrustCom/BigDataSE) , Aug. 2018, pp. 379GLYPH<21>384.\n- [20] F.-H. Tseng, X. Wang, L.-D. Chou, H.-C. Chao, and V. C. M. Leung, ''Dynamic resource prediction and allocation for cloud data center using the multiobjective genetic algorithm,'' IEEE Syst. J. , vol. 12, no. 2, pp. 1688GLYPH<21>1699, Jun. 2018.\n- [21] P. Gupta and S. P. Ghrera, ''Power and fault aware reliable resource allocation for cloud infrastructure,'' Procedia Comput. Sci. , vol. 78, pp. 457GLYPH<21>463, Jan. 2016.\n- [22] A. B. M. B. Alam, T. Halabi, A. Haque, and M. Zulkernine, ''Optimizing virtual machine migration in multi-clouds,'' in Proc. Int. Symp. Netw., Comput. Commun. (ISNCC) , Montreal, QC, Canada, Oct. 2020, pp. 1GLYPH<21>7.\n- [23] (2013). Windows Azure . Accessed: Apr. 2019. [Online]. Available: http://www.windowsazure.com/en-us/support/legal/sla/\n- [24] (2020). Amazon EC2 . Accessed: May 2020. [Online]. Available: http://aws.amazon.com/ec2-sla/\n- [25] (2020). Google Gmail . Accessed: May 2020. [Online]. Available: https://developers.google.com/storage/docs/sla\n- [26] (2020). Salesforce . Accessed: May 2020. [Online]. Available: https://www.salesforce.com/editions-pricing/service-cloud/\n- [27] O. Beaumont, L. Eyraud-Dubois, and H. LarchevŒque, ''Reliable service allocation in clouds,'' in Proc. IEEE 27th Int. Symp. Parallel Distrib. Process. , May 2013, pp. 55GLYPH<21>66.\n- [28] A. B. M. B. Alam, M. Zulkernine, and A. Haque, ''A reliability-based resource allocation approach for cloud computing,'' in Proc. IEEE 7th Int. Symp. Cloud Service Comput. (SC) , Kanazawa, Japan, Nov. 2017, pp. 249GLYPH<21>252.\n\nIntroduces the evaluation of trust in cloud computing, including a reliability-based resource allocation approach and VM migration optimization.",
    "original_text": "- [1] E. Buer and R. Adams, Reliability and Availability of Cloud Computing . Hoboken, NJ, USA: Wiley, 2012.\n- [2] A. B. M. B. Alam, T. Halabi, A. Haque, and M. Zulkernine, ''Multiobjective interdependent VM placement model based on cloud reliability evaluation,'' in Proc. IEEE Int. Conf. Commun. (ICC) , Dublin, Ireland, Jun. 2020, pp. 1GLYPH<21>7.\n- [3] K. V. Vishwanath and N. Nagappan, ''Characterizing cloud computing hardware reliability,'' in Proc. 1st ACM Symp. Cloud Comput. (SoCC) , 2010, pp. 193GLYPH<21>204.\n- [4] H. Liao, Z. Zhou, X. Zhao, L. Zhang, S. Mumtaz, A. Jolfaei, S. H. Ahmed, and A. K. Bashir, ''Learning-based context-aware resource allocation for edge-computing-empowered industrial IoT,'' IEEE Internet Things J. , vol. 7, no. 5, pp. 4260GLYPH<21>4277, May 2020.\n- [5] A. Musaddiq, Y. B. Zikria, O. Hahm, H. Yu, A. K. Bashir, and S. W. Kim, ''A survey on resource management in IoT operating systems,'' IEEE Access , vol. 6, pp. 8459GLYPH<21>8482, 2018.\n- [6] P. Pradhan, P. K. Behera, and B. N. B. Ray, ''Improved max-min algorithm for resource allocation in cloud computing,'' in Proc. 6th Int. Conf. Parallel, Distrib. Grid Comput. (PDGC) , Nov. 2020, pp. 22GLYPH<21>24.\n- [7] Z. Xuejie, W. Zhijian, and X. Feng, ''Reliability evaluation of cloud computing systems using hybrid methods,'' Intell. Autom. Soft Comput. , vol. 19, no. 2, pp. 165GLYPH<21>174, Apr. 2013.\n- [8] N. Padmapriya and R. Rajmohan, ''Reliability evaluation suite for cloud services,'' in Proc. 3rd Int. Conf. Comput., Commun. Netw. Technol. (ICCCNT) , Jul. 2012, pp. 1GLYPH<21>6.\n- [9] B. Yang, F. Tan, Y.-S. Dai, and S. Guo, ''Performance evaluation of cloud service considering fault recovery,'' in Proc. IEEE Int. Conf. Cloud Comput. , Dec. 2009, pp. 571GLYPH<21>576.\n- [10] H. R. Faragardi, R. Shojaee, H. Tabani, and A. Rajabi, ''An analytical model to evaluate reliability of cloud computing systems in the presence of QoS requirements,'' in Proc. IEEE/ACIS 12th Int. Conf. Comput. Inf. Sci. (ICIS) , Niigata, Japan, Jun. 2013, pp. 315GLYPH<21>321.\n- [11] Y. Hu, J. Wong, G. Iszlai, and M. Litoiu, ''Resource provisioning for cloud computing,'' in Proc. Conf. Center Adv. Stud. Collaborative Res. , 2009, pp. 101GLYPH<21>111.\n- [12] J. Praveenchandar and A. Tamilarasi, ''Dynamic resource allocation with optimized task scheduling and improved power management in cloud computing,'' J. Ambient Intell. Humanized Comput. , vol. 12, no. 3, pp. 4147GLYPH<21>4159, Mar. 2021.\n- [13] A. K. Bashir, R. Arul, S. Basheer, G. Raja, R. Jayaraman, and N. M. F. Qureshi, ''An optimal multitier resource allocation of cloud RAN in 5G using machine learning,'' Trans. Emerg. Telecommun. Technol. , vol. 30, no. 8, p. e3627, Aug. 2019.\n- [14] Z. Xiao, W. Song, and Q. Chen, ''Dynamic resource allocation using virtual machines for cloud computing environment,'' IEEETrans. Parallel Distrib. Syst. , vol. 24, no. 6, pp. 1107GLYPH<21>1117, Jun. 2013.\n- [15] W. Lin, J. Z. Wang, C. Liang, and D. Qi, ''A threshold-based dynamic resource allocation scheme for cloud computing,'' Procedia Eng. , vol. 23, pp. 695GLYPH<21>703, Jan. 2011.\n- [16] R. K. Grewal and P. K. Pateriya, ''A rule-based approach for effective resource provisioning in hybrid cloud environment,'' in New Paradigms in Internet Computing . Berlin, Germany: Springer, 2013, pp. 41GLYPH<21>57.\n- [17] J. Luo, W. Song, and L. Yin, ''Reliable virtual machine placement based on multi-objective optimization with trafGLYPH<28>c-aware algorithm in industrial cloud,'' IEEE Access , vol. 6, pp. 23043GLYPH<21>23052, 2018.\n- [18] X. Wang, H. Gu, and Y. Yue, ''The optimization of virtual resource allocation in cloud computing based on RBPSO,'' Concurrency Comput., Pract. Exper. , vol. 32, no. 16, p. e5113, Aug. 2020.\n- [19] T. Halabi, M. Bellaiche, and A. Abusitta, ''Online allocation of cloud resources based on security satisfaction,'' in Proc. 17th IEEE Int. Conf. Trust, Secur. Privacy Comput. Commun./12th IEEE Int. Conf. Big Data Sci. Eng. (TrustCom/BigDataSE) , Aug. 2018, pp. 379GLYPH<21>384.\n- [20] F.-H. Tseng, X. Wang, L.-D. Chou, H.-C. Chao, and V. C. M. Leung, ''Dynamic resource prediction and allocation for cloud data center using the multiobjective genetic algorithm,'' IEEE Syst. J. , vol. 12, no. 2, pp. 1688GLYPH<21>1699, Jun. 2018.\n- [21] P. Gupta and S. P. Ghrera, ''Power and fault aware reliable resource allocation for cloud infrastructure,'' Procedia Comput. Sci. , vol. 78, pp. 457GLYPH<21>463, Jan. 2016.\n- [22] A. B. M. B. Alam, T. Halabi, A. Haque, and M. Zulkernine, ''Optimizing virtual machine migration in multi-clouds,'' in Proc. Int. Symp. Netw., Comput. Commun. (ISNCC) , Montreal, QC, Canada, Oct. 2020, pp. 1GLYPH<21>7.\n- [23] (2013). Windows Azure . Accessed: Apr. 2019. [Online]. Available: http://www.windowsazure.com/en-us/support/legal/sla/\n- [24] (2020). Amazon EC2 . Accessed: May 2020. [Online]. Available: http://aws.amazon.com/ec2-sla/\n- [25] (2020). Google Gmail . Accessed: May 2020. [Online]. Available: https://developers.google.com/storage/docs/sla\n- [26] (2020). Salesforce . Accessed: May 2020. [Online]. Available: https://www.salesforce.com/editions-pricing/service-cloud/\n- [27] O. Beaumont, L. Eyraud-Dubois, and H. LarchevŒque, ''Reliable service allocation in clouds,'' in Proc. IEEE 27th Int. Symp. Parallel Distrib. Process. , May 2013, pp. 55GLYPH<21>66.\n- [28] A. B. M. B. Alam, M. Zulkernine, and A. Haque, ''A reliability-based resource allocation approach for cloud computing,'' in Proc. IEEE 7th Int. Symp. Cloud Service Comput. (SC) , Kanazawa, Japan, Nov. 2017, pp. 249GLYPH<21>252.",
    "context": "Introduces the evaluation of trust in cloud computing, including a reliability-based resource allocation approach and VM migration optimization.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      10
    ],
    "id": "aa550c849656866bbb10699f72402ece02975c352e4cae1fc07d44a13936f281"
  },
  {
    "text": "- [29] E. Bauer and R. Adams, Reliability and Availability of Cloud Computing . Hoboken, NJ, USA: Wiley, 2012.\n- [30] M. Jammal, A. Kanso, and A. Shami, ''CHASE: Component high availability-aware scheduler in cloud computing environment,'' in Proc. IEEE 8th Int. Conf. Cloud Comput. , Jun. 2015, pp. 477GLYPH<21>484.\n- [31] R. Buyya, R. Ranjan, and R. N. Calheiros, ''Modeling and simulation of scalable cloud computing environments and the CloudSim toolkit: Challenges and opportunities,'' in Proc. Int. Conf. High Perform. Comput. Simulation , Jun. 2009, pp. 1GLYPH<21>11.\n- [32] F. M. Alturkistani and S. S. Alaboodi, ''An analytical model for availability evaluation of cloud service provisioning system,'' Int. J. Adv. Comput. Sci. Appl. , vol. 8, no. 6, pp. 240GLYPH<21>247, 2017.\n- [33] B. G. Batista, J. C. Estrella, C. H. G. Ferreira, D. M. L. Filho, L. H. V. Nakamura, S. Reiff-Marganiec, M. J. Santana, and R. H. C. Santana, ''Performance evaluation of resource management in cloud computing environments,'' PLoS ONE , vol. 10, no. 11, 2015, Art. no. e0141914.\n- [34] S. Meng, X. Qiu, L. Luo, H. Xu, and M. Lei, ''Reliability simulation in cloud computing system,'' Int. J. Performability Eng. , vol. 14, no. 9, p. 2015, 2018.\n- [35] A. B. M. B. Alam, T. Halabi, A. Haque, and M. Zulkernine, ''Reliabilitybased formation of cloud federations using game theory,'' in Proc. IEEE Global Commun. Conf. (GLOBECOM) , Taipei, Taiwan, Dec. 2020, pp. 1GLYPH<21>6.\n- [36] Y.-S. Dai, B. Yang, J. Dongarra, and G. Zhang, ''Cloud service reliability: Modeling and analysis,'' in Proc. 15th IEEE PaciGLYPH<28>c Rim Int. Symp. Dependable Comput. , Nov. 2009, pp. 1GLYPH<21>17.\n- [37] N. Limrungsi, J. Zhao, Y. Xiang, T. Lan, H. H. Huang, and S. Subramaniam, ''Providing reliability as an elastic service in cloud computing,'' in Proc. IEEE Int. Conf. Commun. (ICC) , Jun. 2012, pp. 2912GLYPH<21>2917.\n- [38] H. Kellerer, U. Pferschy, and D. Pisinger, ''Multidimensional knapsack problems,'' in Knapsack Problems . Berlin, Germany: Springer, 2004, pp. 235GLYPH<21>283.\n- [39] T. Murata and H. Ishibuchi, ''MOGA: Multi-objective genetic algorithms,'' in Proc. IEEE Int. Conf. Evol. Comput. , vol. 1, Nov. 1995, pp. 289GLYPH<21>294.\n- [40] A. E. Eiben and J. E. Smith, Introduction to Evolutionary Computing (Natural Computing Series). New York, NY, USA: Springer-Verlag, 2008.\n- A. B. M. BODRUL ALAM (Member, IEEE) received the B.Sc. degree (Hons.) in computer science from Jahangirnagar University, Bangladesh, in 2005, the M.Sc. degree from the Department of Computer Science, Ryerson University, Canada, in 2012, and the Ph.D. degree from Queen's University, Canada, in 2020. He is currently working as a Postdoctoral Researcher with the Department of Computer Science, Lakehead University, Thunder Bay, ON, Canada. Before joining\nRyerson University, he served as a full-time Lecturer with the Department of Computer Science, Stamford University, Bangladesh, for GLYPH<28>ve years. He has published several peer-reviewed journals and conference papers. He also collaborated with two other universities: Western University and the University of Winnipeg while conducting his research. His research interests include optimization, mathematical modeling, reliability analysis, reliable resource management, VM placement, VM migration, survivable network design, and cloud computing.\nZUBAIR MD. FADLULLAH (Senior Member, IEEE) was an Associate Professor at the Graduate School of Information Sciences (GSIS), Tohoku University, Japan, from 2017 to 2019. He is currently an Associate Professor with the Computer Science Department, Lakehead University, and the Research Chair of Thunder Bay Regional Health Research Institute (TBRHRI), Thunder Bay, ON, Canada. His research interests include the areas of emerging communication systems, such as 5G\nnew radio and beyond, deep learning applications on solving computer science and communication system problems, UAV-based systems, smart health technology, cyber security, game theory, smart grid, and emerging communication systems. He is a Senior Member of IEEE Communications Society (ComSoc). He received several best paper awards at conferences, including IEEE/ACM IWCMC, IEEE GLOBECOM, and IEEE IC-NIDC. He is currently an Editor of IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY (TVT), IEEE Network magazine, IEEE ACCESS, IEEE OPEN JOURNAL OF THE COMMUNICATIONS SOCIETY, and Ad Hoc & Sensor Wireless Networks (AHSWN) journal.\nSALIMUR CHOUDHURY (Senior Member, IEEE) received the Ph.D. degree in computing from Queen's University, Kingston, ON, Canada, in 2012. He is currently an Assistant Professor with the Department of Computer Science, Lakehead University, Thunder Bay, ON. He is also the Director of the Lakehead Optimization Research Group (LORG). His research interests include designing algorithms for wireless communications, optimization, cellular automata, and approximation algorithms. He was the Technical Program Chair of the SGIoT 2017, SGIoT 2018, and iThings 2018 conferences. He is also an Editor of Parallel Processing Letters .\n\nThis chunk focuses on the evaluation of cloud computing reliability, presenting various research approaches including modeling, simulation, and analysis of system components.",
    "original_text": "- [29] E. Bauer and R. Adams, Reliability and Availability of Cloud Computing . Hoboken, NJ, USA: Wiley, 2012.\n- [30] M. Jammal, A. Kanso, and A. Shami, ''CHASE: Component high availability-aware scheduler in cloud computing environment,'' in Proc. IEEE 8th Int. Conf. Cloud Comput. , Jun. 2015, pp. 477GLYPH<21>484.\n- [31] R. Buyya, R. Ranjan, and R. N. Calheiros, ''Modeling and simulation of scalable cloud computing environments and the CloudSim toolkit: Challenges and opportunities,'' in Proc. Int. Conf. High Perform. Comput. Simulation , Jun. 2009, pp. 1GLYPH<21>11.\n- [32] F. M. Alturkistani and S. S. Alaboodi, ''An analytical model for availability evaluation of cloud service provisioning system,'' Int. J. Adv. Comput. Sci. Appl. , vol. 8, no. 6, pp. 240GLYPH<21>247, 2017.\n- [33] B. G. Batista, J. C. Estrella, C. H. G. Ferreira, D. M. L. Filho, L. H. V. Nakamura, S. Reiff-Marganiec, M. J. Santana, and R. H. C. Santana, ''Performance evaluation of resource management in cloud computing environments,'' PLoS ONE , vol. 10, no. 11, 2015, Art. no. e0141914.\n- [34] S. Meng, X. Qiu, L. Luo, H. Xu, and M. Lei, ''Reliability simulation in cloud computing system,'' Int. J. Performability Eng. , vol. 14, no. 9, p. 2015, 2018.\n- [35] A. B. M. B. Alam, T. Halabi, A. Haque, and M. Zulkernine, ''Reliabilitybased formation of cloud federations using game theory,'' in Proc. IEEE Global Commun. Conf. (GLOBECOM) , Taipei, Taiwan, Dec. 2020, pp. 1GLYPH<21>6.\n- [36] Y.-S. Dai, B. Yang, J. Dongarra, and G. Zhang, ''Cloud service reliability: Modeling and analysis,'' in Proc. 15th IEEE PaciGLYPH<28>c Rim Int. Symp. Dependable Comput. , Nov. 2009, pp. 1GLYPH<21>17.\n- [37] N. Limrungsi, J. Zhao, Y. Xiang, T. Lan, H. H. Huang, and S. Subramaniam, ''Providing reliability as an elastic service in cloud computing,'' in Proc. IEEE Int. Conf. Commun. (ICC) , Jun. 2012, pp. 2912GLYPH<21>2917.\n- [38] H. Kellerer, U. Pferschy, and D. Pisinger, ''Multidimensional knapsack problems,'' in Knapsack Problems . Berlin, Germany: Springer, 2004, pp. 235GLYPH<21>283.\n- [39] T. Murata and H. Ishibuchi, ''MOGA: Multi-objective genetic algorithms,'' in Proc. IEEE Int. Conf. Evol. Comput. , vol. 1, Nov. 1995, pp. 289GLYPH<21>294.\n- [40] A. E. Eiben and J. E. Smith, Introduction to Evolutionary Computing (Natural Computing Series). New York, NY, USA: Springer-Verlag, 2008.\n- A. B. M. BODRUL ALAM (Member, IEEE) received the B.Sc. degree (Hons.) in computer science from Jahangirnagar University, Bangladesh, in 2005, the M.Sc. degree from the Department of Computer Science, Ryerson University, Canada, in 2012, and the Ph.D. degree from Queen's University, Canada, in 2020. He is currently working as a Postdoctoral Researcher with the Department of Computer Science, Lakehead University, Thunder Bay, ON, Canada. Before joining\nRyerson University, he served as a full-time Lecturer with the Department of Computer Science, Stamford University, Bangladesh, for GLYPH<28>ve years. He has published several peer-reviewed journals and conference papers. He also collaborated with two other universities: Western University and the University of Winnipeg while conducting his research. His research interests include optimization, mathematical modeling, reliability analysis, reliable resource management, VM placement, VM migration, survivable network design, and cloud computing.\nZUBAIR MD. FADLULLAH (Senior Member, IEEE) was an Associate Professor at the Graduate School of Information Sciences (GSIS), Tohoku University, Japan, from 2017 to 2019. He is currently an Associate Professor with the Computer Science Department, Lakehead University, and the Research Chair of Thunder Bay Regional Health Research Institute (TBRHRI), Thunder Bay, ON, Canada. His research interests include the areas of emerging communication systems, such as 5G\nnew radio and beyond, deep learning applications on solving computer science and communication system problems, UAV-based systems, smart health technology, cyber security, game theory, smart grid, and emerging communication systems. He is a Senior Member of IEEE Communications Society (ComSoc). He received several best paper awards at conferences, including IEEE/ACM IWCMC, IEEE GLOBECOM, and IEEE IC-NIDC. He is currently an Editor of IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY (TVT), IEEE Network magazine, IEEE ACCESS, IEEE OPEN JOURNAL OF THE COMMUNICATIONS SOCIETY, and Ad Hoc & Sensor Wireless Networks (AHSWN) journal.\nSALIMUR CHOUDHURY (Senior Member, IEEE) received the Ph.D. degree in computing from Queen's University, Kingston, ON, Canada, in 2012. He is currently an Assistant Professor with the Department of Computer Science, Lakehead University, Thunder Bay, ON. He is also the Director of the Lakehead Optimization Research Group (LORG). His research interests include designing algorithms for wireless communications, optimization, cellular automata, and approximation algorithms. He was the Technical Program Chair of the SGIoT 2017, SGIoT 2018, and iThings 2018 conferences. He is also an Editor of Parallel Processing Letters .",
    "context": "This chunk focuses on the evaluation of cloud computing reliability, presenting various research approaches including modeling, simulation, and analysis of system components.",
    "document": "A_Resource_Allocation_Model_Based_on_Trust_Evaluation_in_Multi-Cloud_Environments.pdf",
    "pages": [
      10,
      11
    ],
    "id": "9003f1fa4ab4dc44fd0fe78f9853c8b559e0b70ca7de02d59759fc2378aacafa"
  },
  {
    "text": "Received February 7, 2022, accepted February 22, 2022, date of publication March 2, 2022, date of current version March 11, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.31561 18\n\nIntroduces the document’s publication details and DOI.",
    "original_text": "Received February 7, 2022, accepted February 22, 2022, date of publication March 2, 2022, date of current version March 11, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.31561 18",
    "context": "Introduces the document’s publication details and DOI.",
    "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
    "pages": [
      1
    ],
    "id": "ced7da322a4c089bc6e250f16b3cce510dc9cafef75170bb71275124b8a4ec10"
  },
  {
    "text": "SANGHO YU 1 , SANG MIN WON 1 , HYOUNG WON BAAC 1 , DONGHEE SON 1 , AND CHANGHWAN SHIN 2 , (Senior Member, IEEE)\n1 Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon 16419, Republic of Korea 2 School of Electrical Engineering, Korea University, Seoul 02841, Republic of Korea\nCorresponding authors: Changhwan Shin (cshin@korea.ac.kr) and Donghee Son (daniel3600@g.skku.edu)\nThis work was supported in part by the National Research Foundation of Korea (NRF) Grant funded by the Korea Government Ministry of Science and ICT (MSIT) under Grant 2020R1A2C1009063; and in part by the Korea Medical Device Development Fund Grant funded by the Korean Government (the Ministry of Trade, Industry and Energy, the Ministry of Health & Welfare, and the Ministry of Food and Drug Safety) under Grant 202012D28.\n- ABSTRACT To design a device that is robust to process-induced random variation, this study proposes a machine-learning-based predictive model that can simulate the electrical characteristics of FinFETs with process-induced line-edge roughness. This model, i.e., a Bayesian neural network (BNN) model with horseshoe priors (Horseshoe-BNN), can signiGLYPH<28>cantly reduce the simulation time (as compared to the conventional technology computer-aided design (TCAD) simulation method) in a sufGLYPH<28>ciently accurate manner. Moreover, this model can perform autonomous model selection over the most compact layer size, which is necessary when the amount of data must be limited. The mean absolute percentage error for the mean and standard deviation of the drain-to-source current . IDS / were GLYPH<24> 0.5% and GLYPH<24> 6%, respectively. By estimating the distribution of the current-voltage characteristics, the distributions of the other device metrics, such as off-state leakage current and threshold voltage, can be estimated as well.\nINDEX TERMS Line edge roughness (LER), process-induced random variation, Bayesian neural network, automatic model selection.\n\nIntroduces a machine-learning-based predictive model for FinFETs with process-induced line-edge roughness, highlighting its ability to reduce simulation time and perform automatic model selection.",
    "original_text": "SANGHO YU 1 , SANG MIN WON 1 , HYOUNG WON BAAC 1 , DONGHEE SON 1 , AND CHANGHWAN SHIN 2 , (Senior Member, IEEE)\n1 Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon 16419, Republic of Korea 2 School of Electrical Engineering, Korea University, Seoul 02841, Republic of Korea\nCorresponding authors: Changhwan Shin (cshin@korea.ac.kr) and Donghee Son (daniel3600@g.skku.edu)\nThis work was supported in part by the National Research Foundation of Korea (NRF) Grant funded by the Korea Government Ministry of Science and ICT (MSIT) under Grant 2020R1A2C1009063; and in part by the Korea Medical Device Development Fund Grant funded by the Korean Government (the Ministry of Trade, Industry and Energy, the Ministry of Health & Welfare, and the Ministry of Food and Drug Safety) under Grant 202012D28.\n- ABSTRACT To design a device that is robust to process-induced random variation, this study proposes a machine-learning-based predictive model that can simulate the electrical characteristics of FinFETs with process-induced line-edge roughness. This model, i.e., a Bayesian neural network (BNN) model with horseshoe priors (Horseshoe-BNN), can signiGLYPH<28>cantly reduce the simulation time (as compared to the conventional technology computer-aided design (TCAD) simulation method) in a sufGLYPH<28>ciently accurate manner. Moreover, this model can perform autonomous model selection over the most compact layer size, which is necessary when the amount of data must be limited. The mean absolute percentage error for the mean and standard deviation of the drain-to-source current . IDS / were GLYPH<24> 0.5% and GLYPH<24> 6%, respectively. By estimating the distribution of the current-voltage characteristics, the distributions of the other device metrics, such as off-state leakage current and threshold voltage, can be estimated as well.\nINDEX TERMS Line edge roughness (LER), process-induced random variation, Bayesian neural network, automatic model selection.",
    "context": "Introduces a machine-learning-based predictive model for FinFETs with process-induced line-edge roughness, highlighting its ability to reduce simulation time and perform automatic model selection.",
    "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
    "pages": [
      1
    ],
    "id": "e9078adc0f19121c9bdf19b95484326ad38c44565abca94f0181966dca6de015"
  },
  {
    "text": "As the lateral/vertical dimension of transistors in integrated circuits (ICs) (e.g., channel length, channel width, junction depth, etc.) has been scaled down, the process-induced randomvariation of device parameters becomes signiGLYPH<28>cant. This would signiGLYPH<28>cantly damage the yield of devices. Processinduced line-edge-roughness (LER) is one type of random variation sources, and it induces a signiGLYPH<28>cant amount of variation in aggressively scaled transistors. Because the amplitude of LER did not shrink as much as the feature size shrinkage, the portion of LER in the nominal/physical channel length and width became signiGLYPH<28>cant, resulting in a larger variation in IDS GLYPH<0> VGS characteristics of devices. Hence, the LER in device has become a primary limit when the physical dimensions of device has been scaled down. In this regard, precise proGLYPH<28>ling of the impact of LER on the device performance has become\nThe associate editor coordinating the review of this manuscript and approving it for publication was Anisul Haque.\nan indispensable requisite for designing a variation-immune device. To date, technology computer-aided design (TCAD) simulation has been used to evaluate the impact of LER on device characteristics; however, it takes a signiGLYPH<28>cant amount of time. Thousands of devices should be simulated to determine the exact amount of LER-induced variation (in real, a few weeks even for a single LER proGLYPH<28>le). For this reason, a novel approach for less time-consuming simulation is necessary. In this context, we proposed a linear regression model in [1]; however, this linear regression model was limited in dealing with nonlinearity. Conversely, artiGLYPH<28>cial neural network (ANN) is the most popular machine-learning model because it shows a powerful ability to GLYPH<28>nd patterns as well as to detect trends in complex real-world data. By stacking multiple layers (in detail, neurons/nodes aggregations that receive inputs and compute/give outputs, based on their predeGLYPH<28>ned simple non-linear functions), an ANN can well deGLYPH<28>ne highly nonlinear functions and capture non-linear dependencies in data. However, standard ANN models are prone\nto overGLYPH<28>tting and exhibit poor generalization performance. In addition, they tend to make unjustiGLYPH<28>ed over-conGLYPH<28>dent predictions for the inputs far away from the training data, which can sometimes result in suboptimal predictions [2]. In particular, in the regime of small data (where the uncertainty of data is severe), ANNs are prone to poorly specify their weights. To solve these problems, the Bayesian neural network (BNN) has received lots of attention. The BNNs treat their weights as random variables (described by distributions), and thereby capturing uncertainties in the training data and their weights. This study employed BNN models to predict the IDS GLYPH<0> VGS characteristics in various FinFETs with different device structures. In real, there is a demand for FinFET devices with multiple heights [3], [4]. The BNN model proposed in this work assigns horseshoe priors [5] as a prior distribution, which we call the Horseshoe-BNN (HS-BNN). The prior (i.e., prior distribution) is a prior belief of the weight distribution's conGLYPH<28>guration before observing the data. According to the Bayesian theorem, the beliefs are updated after the data observance [6]. Herein, depending on which prior distribution is to be speciGLYPH<28>ed, it leads to a different model training process. While the Gaussian prior is the most commonly-used prior, the horseshoe distribution has been chosen to introduce sparsity and shrinkage over the weights. The horseshoe prior makes it available the model selection over a number of nodes and can reduce arduous/tiresome work in optimizing the layer sizes.\n\nIntroduces the central thesis about climate policy reform.",
    "original_text": "As the lateral/vertical dimension of transistors in integrated circuits (ICs) (e.g., channel length, channel width, junction depth, etc.) has been scaled down, the process-induced randomvariation of device parameters becomes signiGLYPH<28>cant. This would signiGLYPH<28>cantly damage the yield of devices. Processinduced line-edge-roughness (LER) is one type of random variation sources, and it induces a signiGLYPH<28>cant amount of variation in aggressively scaled transistors. Because the amplitude of LER did not shrink as much as the feature size shrinkage, the portion of LER in the nominal/physical channel length and width became signiGLYPH<28>cant, resulting in a larger variation in IDS GLYPH<0> VGS characteristics of devices. Hence, the LER in device has become a primary limit when the physical dimensions of device has been scaled down. In this regard, precise proGLYPH<28>ling of the impact of LER on the device performance has become\nThe associate editor coordinating the review of this manuscript and approving it for publication was Anisul Haque.\nan indispensable requisite for designing a variation-immune device. To date, technology computer-aided design (TCAD) simulation has been used to evaluate the impact of LER on device characteristics; however, it takes a signiGLYPH<28>cant amount of time. Thousands of devices should be simulated to determine the exact amount of LER-induced variation (in real, a few weeks even for a single LER proGLYPH<28>le). For this reason, a novel approach for less time-consuming simulation is necessary. In this context, we proposed a linear regression model in [1]; however, this linear regression model was limited in dealing with nonlinearity. Conversely, artiGLYPH<28>cial neural network (ANN) is the most popular machine-learning model because it shows a powerful ability to GLYPH<28>nd patterns as well as to detect trends in complex real-world data. By stacking multiple layers (in detail, neurons/nodes aggregations that receive inputs and compute/give outputs, based on their predeGLYPH<28>ned simple non-linear functions), an ANN can well deGLYPH<28>ne highly nonlinear functions and capture non-linear dependencies in data. However, standard ANN models are prone\nto overGLYPH<28>tting and exhibit poor generalization performance. In addition, they tend to make unjustiGLYPH<28>ed over-conGLYPH<28>dent predictions for the inputs far away from the training data, which can sometimes result in suboptimal predictions [2]. In particular, in the regime of small data (where the uncertainty of data is severe), ANNs are prone to poorly specify their weights. To solve these problems, the Bayesian neural network (BNN) has received lots of attention. The BNNs treat their weights as random variables (described by distributions), and thereby capturing uncertainties in the training data and their weights. This study employed BNN models to predict the IDS GLYPH<0> VGS characteristics in various FinFETs with different device structures. In real, there is a demand for FinFET devices with multiple heights [3], [4]. The BNN model proposed in this work assigns horseshoe priors [5] as a prior distribution, which we call the Horseshoe-BNN (HS-BNN). The prior (i.e., prior distribution) is a prior belief of the weight distribution's conGLYPH<28>guration before observing the data. According to the Bayesian theorem, the beliefs are updated after the data observance [6]. Herein, depending on which prior distribution is to be speciGLYPH<28>ed, it leads to a different model training process. While the Gaussian prior is the most commonly-used prior, the horseshoe distribution has been chosen to introduce sparsity and shrinkage over the weights. The horseshoe prior makes it available the model selection over a number of nodes and can reduce arduous/tiresome work in optimizing the layer sizes.",
    "context": "Introduces the central thesis about climate policy reform.",
    "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
    "pages": [
      1,
      2
    ],
    "id": "975d97ababdbc33169d57b2a71bc113ba3f2811d21281758d0961d84c45ea489"
  },
  {
    "text": "Various FinFETs with different structures were designed on the basis of [7]GLYPH<21>[10], and thereafter, 3D LER proGLYPH<28>les on those FinFETs were applied/simulated using the MATLAB model (which was proposed in [11]) and Sentaurus TCAD. Specifically, the 3D LER sequences (i.e., points of the randomly rough surfaces) were generated by the MATLAB model with three main parameters for LER [i.e., RMS amplitude ( GLYPH<27> ), x(y)-axis correlation length .GLYPH<24> X ( GLYPH<24> Y / ), and roughness exponent ( GLYPH<11> )], and the sequences were imported into the TCAD tool. Herein, the RMS amplitude ( GLYPH<27> ) indicates the standard deviation of the LER amplitudes, and it is often referred to as the LER or LER magnitude. Note that GLYPH<27> is the major factor for LER. The correlation length ( GLYPH<24> ) is corresponding to the wavelength of LER proGLYPH<28>le, and the roughness exponent ( GLYPH<11> ) quantitatively indicates the way how high-frequency components in LER proGLYPH<28>le diminishes. The device parameters for FinFET (including the parameters for LER and device structure) are summarized in Table 1. Figure 1(a) shows an isometric view and current density distribution of a FinFET with Lg D 20 nm ; WGLYPH<28>n D 7 nm ; HGLYPH<28>n D 42 nm . Note that the feature parameters for LER used in the FinFET is as follows: GLYPH<27> D 0 : 5 nm ; GLYPH<24> X D 20 nm ; GLYPH<24> Y D 50 nm ; GLYPH<11> D 1. Figure 1(b) shows the IDS GLYPH<0> VGS characteristics with 100 sample devices. Herein, the GLYPH<28>n width variation by LER causes the variations in the IDS GLYPH<0> VGS characteristics [12]. It is noteworthy that the 3D TCAD device simulations were run using various\nTABLE 1. Device parameters used in this work.\nCorrelation length along y direction (5x), 1 = 1~500 nm. Roughness exponent, 1 = 0.1~1.3\n(a)\nFIGURE 1. (a) Isometric view of FinFET with LER (left) and its current density distribution (right), and (b) simulated I DS -V GS of nominal device and 100 sample devices with LER. The parameters for LER and FinFET device structure is GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1, and Lg D 20 nm, W fin D 7 nm, H fin D 42 nm, respectively.\nphysics models, i.e., the ShockleyGLYPH<21>ReadGLYPH<21>Hall model for carrier generation and recombination, the Old Slotboom model for bandgap narrowing, the Lombardi model for thin-layer mobility, and the density gradient quantization model for quantum mechanics effects.\nWe have noted that creating a number of data is very time-consuming because of a long TCAD simulation run\nFIGURE 2. Probability density of Horseshoe prior and Gaussian prior.\ntime. This led to a dilemma when satisfying (i) the number of LER proGLYPH<28>les or (ii) the number of sample devices with each LER proGLYPH<28>le. In other words, to appropriately estimate the distribution of electrical characteristics, the number of sample devices should be sufGLYPH<28>cient, but at the same time, the number of LER proGLYPH<28>les is quite important. In considering those two aspects, 169 different datasets were composed for training and testing the model, and each dataset consisted of 50 different sample FinFETs with identical LER and device parameters. Eighteen types of FinFET device structure were chosen based on [7]GLYPH<21>[10], and the LER parameters were randomly selected in the relevant ranges shown in Table 1.\nAs mentioned in the previous study [1], the distribution of log . IDS / for a given VGS can be considered as a normal distribution in terms of small kurtosis and skewness values. From this perspective, the mean and standard deviation of log . IDS / (denoted as GLYPH<22> IDS and GLYPH<27> IDS , respectively) were selected as target variables. And gate voltage . VGS / , LER parameters (i.e., RMS amplitude ( GLYPH<27> ), correlation length along the x-axis ( GLYPH<24> X ), correlation length along the y-axis .GLYPH<24> Y / , and roughness exponent ( GLYPH<11> )) and device structure parameters (i.e., gate length ( Lg ), GLYPH<28>n width ( WGLYPH<28>n ), and GLYPH<28>n height ( HGLYPH<28>n )) were selected as feature variables to predict the LER-induced variation in the IDS-VGS characteristics of various FinFET structures.\n\nDescribes the simulation process, detailing the use of MATLAB and Sentaurus TCAD to generate 3D LER profiles from MATLAB parameters (RMS amplitude, x-axis correlation length, and roughness exponent) and their subsequent import into the TCAD tool for device simulation.",
    "original_text": "Various FinFETs with different structures were designed on the basis of [7]GLYPH<21>[10], and thereafter, 3D LER proGLYPH<28>les on those FinFETs were applied/simulated using the MATLAB model (which was proposed in [11]) and Sentaurus TCAD. Specifically, the 3D LER sequences (i.e., points of the randomly rough surfaces) were generated by the MATLAB model with three main parameters for LER [i.e., RMS amplitude ( GLYPH<27> ), x(y)-axis correlation length .GLYPH<24> X ( GLYPH<24> Y / ), and roughness exponent ( GLYPH<11> )], and the sequences were imported into the TCAD tool. Herein, the RMS amplitude ( GLYPH<27> ) indicates the standard deviation of the LER amplitudes, and it is often referred to as the LER or LER magnitude. Note that GLYPH<27> is the major factor for LER. The correlation length ( GLYPH<24> ) is corresponding to the wavelength of LER proGLYPH<28>le, and the roughness exponent ( GLYPH<11> ) quantitatively indicates the way how high-frequency components in LER proGLYPH<28>le diminishes. The device parameters for FinFET (including the parameters for LER and device structure) are summarized in Table 1. Figure 1(a) shows an isometric view and current density distribution of a FinFET with Lg D 20 nm ; WGLYPH<28>n D 7 nm ; HGLYPH<28>n D 42 nm . Note that the feature parameters for LER used in the FinFET is as follows: GLYPH<27> D 0 : 5 nm ; GLYPH<24> X D 20 nm ; GLYPH<24> Y D 50 nm ; GLYPH<11> D 1. Figure 1(b) shows the IDS GLYPH<0> VGS characteristics with 100 sample devices. Herein, the GLYPH<28>n width variation by LER causes the variations in the IDS GLYPH<0> VGS characteristics [12]. It is noteworthy that the 3D TCAD device simulations were run using various\nTABLE 1. Device parameters used in this work.\nCorrelation length along y direction (5x), 1 = 1~500 nm. Roughness exponent, 1 = 0.1~1.3\n(a)\nFIGURE 1. (a) Isometric view of FinFET with LER (left) and its current density distribution (right), and (b) simulated I DS -V GS of nominal device and 100 sample devices with LER. The parameters for LER and FinFET device structure is GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1, and Lg D 20 nm, W fin D 7 nm, H fin D 42 nm, respectively.\nphysics models, i.e., the ShockleyGLYPH<21>ReadGLYPH<21>Hall model for carrier generation and recombination, the Old Slotboom model for bandgap narrowing, the Lombardi model for thin-layer mobility, and the density gradient quantization model for quantum mechanics effects.\nWe have noted that creating a number of data is very time-consuming because of a long TCAD simulation run\nFIGURE 2. Probability density of Horseshoe prior and Gaussian prior.\ntime. This led to a dilemma when satisfying (i) the number of LER proGLYPH<28>les or (ii) the number of sample devices with each LER proGLYPH<28>le. In other words, to appropriately estimate the distribution of electrical characteristics, the number of sample devices should be sufGLYPH<28>cient, but at the same time, the number of LER proGLYPH<28>les is quite important. In considering those two aspects, 169 different datasets were composed for training and testing the model, and each dataset consisted of 50 different sample FinFETs with identical LER and device parameters. Eighteen types of FinFET device structure were chosen based on [7]GLYPH<21>[10], and the LER parameters were randomly selected in the relevant ranges shown in Table 1.\nAs mentioned in the previous study [1], the distribution of log . IDS / for a given VGS can be considered as a normal distribution in terms of small kurtosis and skewness values. From this perspective, the mean and standard deviation of log . IDS / (denoted as GLYPH<22> IDS and GLYPH<27> IDS , respectively) were selected as target variables. And gate voltage . VGS / , LER parameters (i.e., RMS amplitude ( GLYPH<27> ), correlation length along the x-axis ( GLYPH<24> X ), correlation length along the y-axis .GLYPH<24> Y / , and roughness exponent ( GLYPH<11> )) and device structure parameters (i.e., gate length ( Lg ), GLYPH<28>n width ( WGLYPH<28>n ), and GLYPH<28>n height ( HGLYPH<28>n )) were selected as feature variables to predict the LER-induced variation in the IDS-VGS characteristics of various FinFET structures.",
    "context": "Describes the simulation process, detailing the use of MATLAB and Sentaurus TCAD to generate 3D LER profiles from MATLAB parameters (RMS amplitude, x-axis correlation length, and roughness exponent) and their subsequent import into the TCAD tool for device simulation.",
    "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
    "pages": [
      2,
      3
    ],
    "id": "48dad3f022235b00c7e6e3980ea3e5af9bc9b1bcdb345fc023f80f9f8a477a8c"
  },
  {
    "text": "Predictive models are just a function in the form of y D f ( x ) that GLYPH<28>ts the given data D D GLYPH<8>GLYPH<0> xn ; y n GLYPH<1>GLYPH<9> N 1 , and they are generalized/extended to the cases that have not been seen in training. For example, the function form of a linear regression model is y D P wixi C b . However, this model has its own limit for determining the nonlinear relationship between x and y . Conversely, an ANN with an L hidden layer is in the form of y D W . L C 1 / h . L / C b . L C 1 / , where W l is a matrix of weights between layer ( l GLYPH<0> 1 ) and layer ( l ) in the size of R . Kl GLYPH<0> 1 C 1 / × Kl , and h . l / is a matrix of\nFIGURE 3. Architecture of the HS-BNN, predicting the mean and standard deviation of log(I DS ) in various FinFET structures with arbitrary LER profiles.\nFIGURE 4. Diagram of K-fold cross-validation.\nFIGURE 5. MAPE, RMSE, MAE, and PLL values of each model along different numbers of nodes, showing model selection over the number of HS-BNN nodes.\noutputs from layer ( l ) . Herein, Kl is the number of units in layer l , h . l / D a GLYPH<0> W . l / h . l GLYPH<0> 1 / C b GLYPH<1> ; h . 0 / D x ; and a ( · ) is an activation function for each hidden layer. In the case of the ANN with three hidden layers, the function is\nFIGURE 6. (a) Predicted standard deviation and (b) predicted mean of log(I OFF ) for FinFET structure of Lg D 20 nm, H fin D 42 nm, W fin D 7 nm with the LER profile of GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1. Note that each parameter (i.e., GLYPH<27> , GLYPH<24> X , GLYPH<24> Y , GLYPH<11> , Lg, H fin , and W fin ) is varied in relevant ranges.\ny D W 4 a GLYPH<0> W 3 a GLYPH<0> W 2 a GLYPH<0> W 1 x C b 1 GLYPH<1> C b 2 GLYPH<1> C b 3 GLYPH<1> C b 4 . The ANN model is a composite function of multiple activation functions (i.e., activation integral) [13], which can make it complex nonlinear regression predictions.\nA Bayesian neural network is an extended standard ANN with Bayesian/posterior inference [14]. While the ANN is a deterministic model, the BNN is a stochastic/probabilistic model. The BNN incorporates uncertainty in modeling tasks by introducing distributions over the weights W GLYPH<24> p ( W ) (i.e., W D GLYPH<8> W l GLYPH<9> L C 1 1 , a set of weight matrices over all the layers) [15]. By doing so, BNNs can demonstrate/reconGLYPH<28>gure epistemic uncertainty when the amount of data is limited. However, at the same time, there are inherent neural network problems like over\n(under)-parameterization. The larger(smaller) the number of data points is, the larger(smaller) the number of required nodes is. When the number of nodes is much larger(smaller) than that estimated from the data, the variance around the BNN's predictions would be too large(small). The layer sizes should be as large as necessary. Especially, in the case of estimating the LER-induced variation, data were bound to be small, so that over-parameterization had to be prevented. In this regard, this study assigns horseshoe priors over weights using Bayesian inference. The horseshoe prior is given by: GLYPH<16> w . l / i ; j j GLYPH<28> i ; v GLYPH<17> ∼ N GLYPH<0> 0 ;GLYPH<28> 2 i v 2 GLYPH<1> where GLYPH<28> i ∼ C C . 0 ; b 0 / and v ∼ C C GLYPH<0> 0 ; bg GLYPH<1> . Herein, w . l / i ; j is the weight connecting the i th node of layer ( l -1) and j th node of\nFIGURE 7. (a) Predicted standard deviation and (b) mean of V TH distribution for FinFET structure of Lg D 20 nm, H fin D 42 nm, W fin D 7 nm with the LER profile of GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1. Note that each parameter (i.e., GLYPH<27> , GLYPH<24> X , GLYPH<24> Y , GLYPH<11> , Lg, H fin , and W fin ) is varied in relevant ranges.\nlayer . l / , and C C . 0 ; b / is a half-Cauchy distribution, and b 0 and bg are shrinkage parameters. The horseshoe prior can introduce shrinkage and sparsity over the weights and bring model selection that automatically GLYPH<28>nds the most compact neural network structure (i.e., GLYPH<28>nding the best number of nodes). This is due to the two features of the horseshoe distribution: (1) a tall spike at zero and (2) heavy (relatively GLYPH<29>at) tails (see Figure 2). The tall spike at zero promotes shrinkage of the weights, which seems unnecessary in making predictions. However, its heavy tail allows large weights to avoid shrinkage.\nThe complete architecture of the HS-BNN is illustrated in Figure 3: Three hidden layers with a RELU activation function, horseshoe priors for the weights into the hidden layers, and a Gaussian prior for the weights into the output layer. The Gaussian prior was used to prevent overGLYPH<28>tting [16].\n\nDescribes the architecture of the Bayesian neural network (HS-BNN) and its role in predicting the mean and standard deviation of log(I DS ) in various FinFET structures with arbitrary LER profiles.",
    "original_text": "Predictive models are just a function in the form of y D f ( x ) that GLYPH<28>ts the given data D D GLYPH<8>GLYPH<0> xn ; y n GLYPH<1>GLYPH<9> N 1 , and they are generalized/extended to the cases that have not been seen in training. For example, the function form of a linear regression model is y D P wixi C b . However, this model has its own limit for determining the nonlinear relationship between x and y . Conversely, an ANN with an L hidden layer is in the form of y D W . L C 1 / h . L / C b . L C 1 / , where W l is a matrix of weights between layer ( l GLYPH<0> 1 ) and layer ( l ) in the size of R . Kl GLYPH<0> 1 C 1 / × Kl , and h . l / is a matrix of\nFIGURE 3. Architecture of the HS-BNN, predicting the mean and standard deviation of log(I DS ) in various FinFET structures with arbitrary LER profiles.\nFIGURE 4. Diagram of K-fold cross-validation.\nFIGURE 5. MAPE, RMSE, MAE, and PLL values of each model along different numbers of nodes, showing model selection over the number of HS-BNN nodes.\noutputs from layer ( l ) . Herein, Kl is the number of units in layer l , h . l / D a GLYPH<0> W . l / h . l GLYPH<0> 1 / C b GLYPH<1> ; h . 0 / D x ; and a ( · ) is an activation function for each hidden layer. In the case of the ANN with three hidden layers, the function is\nFIGURE 6. (a) Predicted standard deviation and (b) predicted mean of log(I OFF ) for FinFET structure of Lg D 20 nm, H fin D 42 nm, W fin D 7 nm with the LER profile of GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1. Note that each parameter (i.e., GLYPH<27> , GLYPH<24> X , GLYPH<24> Y , GLYPH<11> , Lg, H fin , and W fin ) is varied in relevant ranges.\ny D W 4 a GLYPH<0> W 3 a GLYPH<0> W 2 a GLYPH<0> W 1 x C b 1 GLYPH<1> C b 2 GLYPH<1> C b 3 GLYPH<1> C b 4 . The ANN model is a composite function of multiple activation functions (i.e., activation integral) [13], which can make it complex nonlinear regression predictions.\nA Bayesian neural network is an extended standard ANN with Bayesian/posterior inference [14]. While the ANN is a deterministic model, the BNN is a stochastic/probabilistic model. The BNN incorporates uncertainty in modeling tasks by introducing distributions over the weights W GLYPH<24> p ( W ) (i.e., W D GLYPH<8> W l GLYPH<9> L C 1 1 , a set of weight matrices over all the layers) [15]. By doing so, BNNs can demonstrate/reconGLYPH<28>gure epistemic uncertainty when the amount of data is limited. However, at the same time, there are inherent neural network problems like over\n(under)-parameterization. The larger(smaller) the number of data points is, the larger(smaller) the number of required nodes is. When the number of nodes is much larger(smaller) than that estimated from the data, the variance around the BNN's predictions would be too large(small). The layer sizes should be as large as necessary. Especially, in the case of estimating the LER-induced variation, data were bound to be small, so that over-parameterization had to be prevented. In this regard, this study assigns horseshoe priors over weights using Bayesian inference. The horseshoe prior is given by: GLYPH<16> w . l / i ; j j GLYPH<28> i ; v GLYPH<17> ∼ N GLYPH<0> 0 ;GLYPH<28> 2 i v 2 GLYPH<1> where GLYPH<28> i ∼ C C . 0 ; b 0 / and v ∼ C C GLYPH<0> 0 ; bg GLYPH<1> . Herein, w . l / i ; j is the weight connecting the i th node of layer ( l -1) and j th node of\nFIGURE 7. (a) Predicted standard deviation and (b) mean of V TH distribution for FinFET structure of Lg D 20 nm, H fin D 42 nm, W fin D 7 nm with the LER profile of GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1. Note that each parameter (i.e., GLYPH<27> , GLYPH<24> X , GLYPH<24> Y , GLYPH<11> , Lg, H fin , and W fin ) is varied in relevant ranges.\nlayer . l / , and C C . 0 ; b / is a half-Cauchy distribution, and b 0 and bg are shrinkage parameters. The horseshoe prior can introduce shrinkage and sparsity over the weights and bring model selection that automatically GLYPH<28>nds the most compact neural network structure (i.e., GLYPH<28>nding the best number of nodes). This is due to the two features of the horseshoe distribution: (1) a tall spike at zero and (2) heavy (relatively GLYPH<29>at) tails (see Figure 2). The tall spike at zero promotes shrinkage of the weights, which seems unnecessary in making predictions. However, its heavy tail allows large weights to avoid shrinkage.\nThe complete architecture of the HS-BNN is illustrated in Figure 3: Three hidden layers with a RELU activation function, horseshoe priors for the weights into the hidden layers, and a Gaussian prior for the weights into the output layer. The Gaussian prior was used to prevent overGLYPH<28>tting [16].",
    "context": "Describes the architecture of the Bayesian neural network (HS-BNN) and its role in predicting the mean and standard deviation of log(I DS ) in various FinFET structures with arbitrary LER profiles.",
    "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
    "pages": [
      3,
      4,
      5
    ],
    "id": "aad5d978d36fc9f91ba5ca5bf315bf6e3d4b31aa73ac15df9c8fb7c64dd98ca0"
  },
  {
    "text": "The HS-BNN predictions were evaluated using K-fold crossvalidation [splitting data in a K number of sections, and then, iteratively using one of the sections as a test set and the others as a training set (see Figure 4)]. This method is expected to be a proper choice when data are limited, by ensuring all data to be used as a test set at least once, which makes less biased evaluation and well estimates how a model will perform in general cases [17], [18]. Mean absolute percentage errors (MAPEs), root-mean-squared errors (RMSEs), mean absolute errors (MAEs) (herein, note that the lower they are,\nTABLE 2. MAPE and RMSE values of each model.\nTABLE 3. MAPE, RMSE, MAE, and PLL values of each model.\nthe better they are), and predictive log likelihoods (PLLs) (herein, note that the higher it is, the better it is) are calculated in each test set. Afterwards, the averages of those are the representative evaluation process values.\nThe overall prediction performance was improved, as compared against the previous work [1] which is the Bayesian linear regression (BLR) model (see Table 2 ). Note that the HS-BNN model for this comparison is only for the FinFET with Lg D 14 nm, WGLYPH<28>n D 7 nm, HGLYPH<28>n D 30 nm because the BLR model made predictions only for the corresponding FinFET structure. Table 2 summarizes the MAPE and RMSEof the two target variables (i.e., GLYPH<22> IDS and GLYPH<27> IDS) of the HS-BNN and BLR. The predictions of both GLYPH<22> IDS and GLYPH<27> IDS were much improved over those in the previous work. The prediction for GLYPH<22> IDS was improved (i.e., 0.55% vs. 0.81%), but there was signiGLYPH<28>cant prediction improvement for GLYPH<27> IDS (i.e., 6.66% vs. 19.59%).\nTable 3 and Figure 5 show the results of HS-BNN model for various FinFET structures. To verify the beneGLYPH<28>t of the HS-BNN model, we compared it with the Gaussian-BNN model. Table 3 summarizes the predictive performance, showing that the HS-BNN showed much better results. Figure 5 demonstrates the performance of the model selection with respect to the layer sizes. For the number of nodes over 200, the HS-BNN's prediction for GLYPH<27> IDS showed almost the same results (MAPEs GLYPH<24> 7%), while the Gaussian-BNN showed totally different results for different numbers of nodes. It is noteworthy that it is a log-scale for the MAPE, RMSE, and MAE in Figure 5. This indicates that, as mentioned in [19], even if the number of nodes was excessively overestimated, the horseshoe prior made it possible to GLYPH<28>nd the most compact layer sizes. However, it is meaningful, not to merely make a simple comparison of predictive performance, but to focus on the fact that the HS-BNN achieved the model selection while making a GLYPH<28>ne predictive performance. This is because the number of nodes for every hidden layer is speciGLYPH<28>ed as the same. Manual layer size optimization was not performed, and the optimization was performed automatically\nin the HS-BNN. Thus, it is an open question whether the HS-BNNis better than all the Gaussian-BNNs. However, it is certain that the HS-BNN has saved a signiGLYPH<28>cant optimization time.\nFigures 6 and 7 show the predictions of the mean and standard deviation of log off-state current [log(IOFF)] and threshold voltage (VTH) of the FinFET with Lg D 20 nm, WGLYPH<28>n D 7 nm, HGLYPH<28>n D 42 nm, and an LER proGLYPH<28>le [i.e., GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1]. By GLYPH<28>xing all the parameters other than a single parameter, GLYPH<27> log(IOFF), GLYPH<22> log(IOFF), GLYPH<27> VTH, and GLYPH<22> VTH were predicted.\nThe device performance trends shown in Figures 6 and 7 seem to be well matched to the general trends, when considering several studies shown in [20]GLYPH<21>[25]. Using this HS-BNN model, we can show how both the standard deviation and mean of the electrical characteristics vary with arbitrary LER proGLYPH<28>les in various FinFET structures.\n\nEvaluates the HS-BNN predictions using K-fold crossvalidation, calculating MAPEs, RMSEs, MAEs, and PLLs to determine the model's performance compared to the Bayesian linear regression (BLR) model.",
    "original_text": "The HS-BNN predictions were evaluated using K-fold crossvalidation [splitting data in a K number of sections, and then, iteratively using one of the sections as a test set and the others as a training set (see Figure 4)]. This method is expected to be a proper choice when data are limited, by ensuring all data to be used as a test set at least once, which makes less biased evaluation and well estimates how a model will perform in general cases [17], [18]. Mean absolute percentage errors (MAPEs), root-mean-squared errors (RMSEs), mean absolute errors (MAEs) (herein, note that the lower they are,\nTABLE 2. MAPE and RMSE values of each model.\nTABLE 3. MAPE, RMSE, MAE, and PLL values of each model.\nthe better they are), and predictive log likelihoods (PLLs) (herein, note that the higher it is, the better it is) are calculated in each test set. Afterwards, the averages of those are the representative evaluation process values.\nThe overall prediction performance was improved, as compared against the previous work [1] which is the Bayesian linear regression (BLR) model (see Table 2 ). Note that the HS-BNN model for this comparison is only for the FinFET with Lg D 14 nm, WGLYPH<28>n D 7 nm, HGLYPH<28>n D 30 nm because the BLR model made predictions only for the corresponding FinFET structure. Table 2 summarizes the MAPE and RMSEof the two target variables (i.e., GLYPH<22> IDS and GLYPH<27> IDS) of the HS-BNN and BLR. The predictions of both GLYPH<22> IDS and GLYPH<27> IDS were much improved over those in the previous work. The prediction for GLYPH<22> IDS was improved (i.e., 0.55% vs. 0.81%), but there was signiGLYPH<28>cant prediction improvement for GLYPH<27> IDS (i.e., 6.66% vs. 19.59%).\nTable 3 and Figure 5 show the results of HS-BNN model for various FinFET structures. To verify the beneGLYPH<28>t of the HS-BNN model, we compared it with the Gaussian-BNN model. Table 3 summarizes the predictive performance, showing that the HS-BNN showed much better results. Figure 5 demonstrates the performance of the model selection with respect to the layer sizes. For the number of nodes over 200, the HS-BNN's prediction for GLYPH<27> IDS showed almost the same results (MAPEs GLYPH<24> 7%), while the Gaussian-BNN showed totally different results for different numbers of nodes. It is noteworthy that it is a log-scale for the MAPE, RMSE, and MAE in Figure 5. This indicates that, as mentioned in [19], even if the number of nodes was excessively overestimated, the horseshoe prior made it possible to GLYPH<28>nd the most compact layer sizes. However, it is meaningful, not to merely make a simple comparison of predictive performance, but to focus on the fact that the HS-BNN achieved the model selection while making a GLYPH<28>ne predictive performance. This is because the number of nodes for every hidden layer is speciGLYPH<28>ed as the same. Manual layer size optimization was not performed, and the optimization was performed automatically\nin the HS-BNN. Thus, it is an open question whether the HS-BNNis better than all the Gaussian-BNNs. However, it is certain that the HS-BNN has saved a signiGLYPH<28>cant optimization time.\nFigures 6 and 7 show the predictions of the mean and standard deviation of log off-state current [log(IOFF)] and threshold voltage (VTH) of the FinFET with Lg D 20 nm, WGLYPH<28>n D 7 nm, HGLYPH<28>n D 42 nm, and an LER proGLYPH<28>le [i.e., GLYPH<27> D 0.5 nm, GLYPH<24> X D 20 nm, GLYPH<24> Y D 50 nm, GLYPH<11> D 1]. By GLYPH<28>xing all the parameters other than a single parameter, GLYPH<27> log(IOFF), GLYPH<22> log(IOFF), GLYPH<27> VTH, and GLYPH<22> VTH were predicted.\nThe device performance trends shown in Figures 6 and 7 seem to be well matched to the general trends, when considering several studies shown in [20]GLYPH<21>[25]. Using this HS-BNN model, we can show how both the standard deviation and mean of the electrical characteristics vary with arbitrary LER proGLYPH<28>les in various FinFET structures.",
    "context": "Evaluates the HS-BNN predictions using K-fold crossvalidation, calculating MAPEs, RMSEs, MAEs, and PLLs to determine the model's performance compared to the Bayesian linear regression (BLR) model.",
    "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
    "pages": [
      5,
      6
    ],
    "id": "907ee6bd1249aa8cfc54c14f66a6290d0804ea2687ca417b39b85f6825d3e492"
  },
  {
    "text": "A Bayesian neural network model with horseshoe priors (HS-BNN) has been proposed for the design of LERimmune FinFETs. This model can quantitatively estimate the LER-induced random variation of IDS-VGS characteristics in various FinFET structures within a few seconds, which is much shorter than the conventional TCAD simulation running time. With the capability of neural networks to deGLYPH<28>ne highly nonlinear functions, the prediction has been improved [vs. the Bayesian linear regression model [1] (i.e., GLYPH<24> 6 % vs. GLYPH<24> 19 %)]. Moreover, the model selection performance over the most compact layer size was veriGLYPH<28>ed, which is essential when the amount of data is very limited.\n\nIntroduces a Bayesian neural network model for predicting LER-induced FinFET variations, highlighting its speed advantage over TCAD simulations and improved prediction accuracy compared to a Bayesian linear regression model.",
    "original_text": "A Bayesian neural network model with horseshoe priors (HS-BNN) has been proposed for the design of LERimmune FinFETs. This model can quantitatively estimate the LER-induced random variation of IDS-VGS characteristics in various FinFET structures within a few seconds, which is much shorter than the conventional TCAD simulation running time. With the capability of neural networks to deGLYPH<28>ne highly nonlinear functions, the prediction has been improved [vs. the Bayesian linear regression model [1] (i.e., GLYPH<24> 6 % vs. GLYPH<24> 19 %)]. Moreover, the model selection performance over the most compact layer size was veriGLYPH<28>ed, which is essential when the amount of data is very limited.",
    "context": "Introduces a Bayesian neural network model for predicting LER-induced FinFET variations, highlighting its speed advantage over TCAD simulations and improved prediction accuracy compared to a Bayesian linear regression model.",
    "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
    "pages": [
      6
    ],
    "id": "a46e0e09053ca021f82200a107eaef751089702af5e42cd98622737900981c6e"
  },
  {
    "text": "- [1] S. Yu and C. Shin, ''Quantitative evaluation of process-induced lineedge roughness in FinFET: Bayesian regression model,'' Semicond. Sci. Technol. , vol. 32, no. 2, 2021, Art. no. 025020.\n- [2] L. Valentin Jospin, W. Buntine, F. Boussaid, H. Laga, and M. Bennamoun, ''Hands-on Bayesian neural networksGLYPH<21>A tutorial for deep learning users,'' 2020, arXiv:2007.06823 .\n- [3] R. Courtland, ''3-D transistors with different in heights make better memory cells,'' IEEE Spectr. , Jun. 2013. [Online]. Available: https://spectrum.ieee.org/3d-transistors-with-different-GLYPH<28>ns-heights-makebetter-memory-cells\n- [4] I. Aller and B. Rainey, ''Multi-height FinFETs,'' U.S. Patent 6 909 147 B2, Jun. 21, 2005.\n- [5] C. M. Carvalho, N. G. Polson, and J. G. Scott, ''Handling sparsity via the horseshoe,'' presented at the Artif. Intell. Statist., 2009. [Online]. Available: http://procee ings.mlr.press/v5/carvalho09a\n- [6] J. Miguel HernÆndez-Lobato and R. P. Adams, ''Probabilistic backpropagation for scalable learning of Bayesian neural networks,'' 2015, arXiv:1502.05336 .\n- [7] M. Badaroglu. (2018). International Roadmap for Device and Systems More Moore . [Online]. Available: https://irds.ieee.org/editions/2018\n- [8] A. Allan. (2016). International Roadmap for Device and Systems More Moore . [Online]. Available: https://irds.ieee.org/editions/2016\n- [9] WikiChip. Technology Node . Accessed: Mar. 15, 2021. [Online]. Available: https://en.wikichip.org/wiki/technology_node\n- [10] Wikipedia. Semiconductor Device Fabrication . Accessed: Mar. 15, 2021. [Online]. Available: https://en.wikipedia.org/wiki/Semiconductor_ device_fabrication\n- [11] S. Oh and C. Shin, ''3-D quasi-atomistic model for line edge roughness in nonplanar MOSFETs,'' IEEE Trans. Electron Devices , vol. 63, no. 12, pp. 4617GLYPH<21>4623, Dec. 2016.\n- [12] C.-Y. Chen, W.-T. Huang, and Y. Li, ''Electrical characteristic and power consumption GLYPH<29>uctuations of trapezoidal bulk FinFET devices and circuits induced by random line edge roughness,'' in Proc. 16th Int. Symp. Quality Electron. Design , Mar. 2015, pp. 61GLYPH<21>64, doi: 10.1109/ISQED.2015.7085399.\n- [13] Z. Ma, ''The function representation of artiGLYPH<28>cial neural network,'' 2019, arXiv:1908.10493 .\n- [14] J. Gordon and J. Miguel HernÆndez-Lobato, ''Bayesian semisupervised learning with deep generative models,'' 2017, arXiv:1706.09751 .\n- [15] H. Overweg, A.-L. Popkes, A. Ercole, Y. Li, J. Miguel HernÆndez-Lobato, Y. Zaykov, and C. Zhang, ''Interpretable outcome prediction with sparse Bayesian neural networks in intensive care,'' 2019, arXiv:1905.02599 .\n- [16] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, ''Weight uncertainty in neural networks,'' 2015, arXiv:1505.05424 .\n- [17] S. Shinmura, ''Comparison of linear discriminant function by K-fold cross validation,'' Data Anal. , vol. 2014, pp. 1GLYPH<21>6, Sep. 2014.\n- [18] J. Brownlee. (2018). A Gentle Introduction to K-Fold CrossValidation . Machine Learning Mastery. [Online]. Available: https:// machinelearningmastery.com/k-fold-cross-validation/\n- [19] S. Ghosh, J. Yao, and F. Doshi-Velez, ''Model selection in Bayesian neural networks via horseshoe priors,'' J. Mach. Learn. Res. , vol. 20, no. 182, pp. 1GLYPH<21>46, 2019.\n- [20] A. Asenov, S. Kaya, and A. R. Brown, ''Intrinsic parameter GLYPH<29>uctuations in decananometer MOSFETs introduced by gate line edge roughness,'' IEEE Trans. Electron Devices , vol. 50, no. 5, pp. 1254GLYPH<21>1260, May 2003.\n- [21] M. Wong, K. D. Holland, S. Anderson, and S. Rizw, ''Impact of shortwavelength and long-wavelength line-edge roughness on the variability of ultrascaled FinFETs,'' IEEE Trans. Electron Devices , vol. 64, no. 3, pp. 1231GLYPH<21>1238, Mar. 2017.\n- [22] Y. Ma, H. J. Levinson, and T. Wallow, ''Line edge roughness impact on critical dimension variation,'' Proc. SPIE , vol. 6518, Apr. 2007, Art. no. 651824.\n- [23] K. Patel, T. Wallow, H. J. Levinson, and C. J. Spanos, ''Comparative study of line width roughness (LWR) in next-generation lithography (NGL) processes,'' Proc. SPIE , vol. 7640, Mar. 2010, Art. no. 76400T.\n- [24] Y. Ban, ''Electrical impact of line-edge roughness on sub-45 nm node standard cell,'' J. Micro/Nanolithography, MEMS, MOEMS , vol. 9, no. 4, 2009, Art. no. 041206.\n- [25] E. Liu, K. Lutker-Lee, Q. Lou, Y.-M. Chen, A. Raley, and P. Biolsi, ''Line edge roughness (LER) reduction strategies for EUV self-aligned double patterning (SADP),'' Proc. SPIE , vol. 11615, Apr. 2021, Art. no. 1161506.\nSANGHO YU received the M.S. degree in electronic and electrical engineering from Sungkyunkwan University (SKKU), Suwon, Republic of Korea, in 2021. He is currently working as a Researcher with SKKU. His current research interests include process-induced random variation and machine learning model.\nSANG MIN WON received the B.S., M.S., and Ph.D. degrees in electrical and computer engineering from the University of Illinois at UrbanaGLYPH<21> Champaign. He is currently an Assistant Professor with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include sensors/stimulators with unique applications in advanced biomedical and/or health monitoring systems.\nHYOUNG WON BAAC received the B.S. degree (Hons.) in electronic engineering from Sungkyunkwan University, Suwon, Republic of Korea, in 1999, and the Ph.D. degree in electrical engineering and computer sciences from the University of Michigan, Ann Arbor, MI, USA, in 2011. He is currently an Associate Professor with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include opti- cal/acoustic/electrical sensors and systems for biomedical therapy, healthcare, and non-destructive evaluation.\nDONGHEE SON received the Ph.D. degree in chemical and biological engineering from Seoul National University, in 2015. He is currently an Assistant Professor with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include soft GLYPH<29>exible electronic devices and systems.\n\nProvides a quantitative model for estimating LER-induced random variation in FinFET characteristics, offering a faster prediction time compared to TCAD simulations.",
    "original_text": "- [1] S. Yu and C. Shin, ''Quantitative evaluation of process-induced lineedge roughness in FinFET: Bayesian regression model,'' Semicond. Sci. Technol. , vol. 32, no. 2, 2021, Art. no. 025020.\n- [2] L. Valentin Jospin, W. Buntine, F. Boussaid, H. Laga, and M. Bennamoun, ''Hands-on Bayesian neural networksGLYPH<21>A tutorial for deep learning users,'' 2020, arXiv:2007.06823 .\n- [3] R. Courtland, ''3-D transistors with different in heights make better memory cells,'' IEEE Spectr. , Jun. 2013. [Online]. Available: https://spectrum.ieee.org/3d-transistors-with-different-GLYPH<28>ns-heights-makebetter-memory-cells\n- [4] I. Aller and B. Rainey, ''Multi-height FinFETs,'' U.S. Patent 6 909 147 B2, Jun. 21, 2005.\n- [5] C. M. Carvalho, N. G. Polson, and J. G. Scott, ''Handling sparsity via the horseshoe,'' presented at the Artif. Intell. Statist., 2009. [Online]. Available: http://procee ings.mlr.press/v5/carvalho09a\n- [6] J. Miguel HernÆndez-Lobato and R. P. Adams, ''Probabilistic backpropagation for scalable learning of Bayesian neural networks,'' 2015, arXiv:1502.05336 .\n- [7] M. Badaroglu. (2018). International Roadmap for Device and Systems More Moore . [Online]. Available: https://irds.ieee.org/editions/2018\n- [8] A. Allan. (2016). International Roadmap for Device and Systems More Moore . [Online]. Available: https://irds.ieee.org/editions/2016\n- [9] WikiChip. Technology Node . Accessed: Mar. 15, 2021. [Online]. Available: https://en.wikichip.org/wiki/technology_node\n- [10] Wikipedia. Semiconductor Device Fabrication . Accessed: Mar. 15, 2021. [Online]. Available: https://en.wikipedia.org/wiki/Semiconductor_ device_fabrication\n- [11] S. Oh and C. Shin, ''3-D quasi-atomistic model for line edge roughness in nonplanar MOSFETs,'' IEEE Trans. Electron Devices , vol. 63, no. 12, pp. 4617GLYPH<21>4623, Dec. 2016.\n- [12] C.-Y. Chen, W.-T. Huang, and Y. Li, ''Electrical characteristic and power consumption GLYPH<29>uctuations of trapezoidal bulk FinFET devices and circuits induced by random line edge roughness,'' in Proc. 16th Int. Symp. Quality Electron. Design , Mar. 2015, pp. 61GLYPH<21>64, doi: 10.1109/ISQED.2015.7085399.\n- [13] Z. Ma, ''The function representation of artiGLYPH<28>cial neural network,'' 2019, arXiv:1908.10493 .\n- [14] J. Gordon and J. Miguel HernÆndez-Lobato, ''Bayesian semisupervised learning with deep generative models,'' 2017, arXiv:1706.09751 .\n- [15] H. Overweg, A.-L. Popkes, A. Ercole, Y. Li, J. Miguel HernÆndez-Lobato, Y. Zaykov, and C. Zhang, ''Interpretable outcome prediction with sparse Bayesian neural networks in intensive care,'' 2019, arXiv:1905.02599 .\n- [16] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, ''Weight uncertainty in neural networks,'' 2015, arXiv:1505.05424 .\n- [17] S. Shinmura, ''Comparison of linear discriminant function by K-fold cross validation,'' Data Anal. , vol. 2014, pp. 1GLYPH<21>6, Sep. 2014.\n- [18] J. Brownlee. (2018). A Gentle Introduction to K-Fold CrossValidation . Machine Learning Mastery. [Online]. Available: https:// machinelearningmastery.com/k-fold-cross-validation/\n- [19] S. Ghosh, J. Yao, and F. Doshi-Velez, ''Model selection in Bayesian neural networks via horseshoe priors,'' J. Mach. Learn. Res. , vol. 20, no. 182, pp. 1GLYPH<21>46, 2019.\n- [20] A. Asenov, S. Kaya, and A. R. Brown, ''Intrinsic parameter GLYPH<29>uctuations in decananometer MOSFETs introduced by gate line edge roughness,'' IEEE Trans. Electron Devices , vol. 50, no. 5, pp. 1254GLYPH<21>1260, May 2003.\n- [21] M. Wong, K. D. Holland, S. Anderson, and S. Rizw, ''Impact of shortwavelength and long-wavelength line-edge roughness on the variability of ultrascaled FinFETs,'' IEEE Trans. Electron Devices , vol. 64, no. 3, pp. 1231GLYPH<21>1238, Mar. 2017.\n- [22] Y. Ma, H. J. Levinson, and T. Wallow, ''Line edge roughness impact on critical dimension variation,'' Proc. SPIE , vol. 6518, Apr. 2007, Art. no. 651824.\n- [23] K. Patel, T. Wallow, H. J. Levinson, and C. J. Spanos, ''Comparative study of line width roughness (LWR) in next-generation lithography (NGL) processes,'' Proc. SPIE , vol. 7640, Mar. 2010, Art. no. 76400T.\n- [24] Y. Ban, ''Electrical impact of line-edge roughness on sub-45 nm node standard cell,'' J. Micro/Nanolithography, MEMS, MOEMS , vol. 9, no. 4, 2009, Art. no. 041206.\n- [25] E. Liu, K. Lutker-Lee, Q. Lou, Y.-M. Chen, A. Raley, and P. Biolsi, ''Line edge roughness (LER) reduction strategies for EUV self-aligned double patterning (SADP),'' Proc. SPIE , vol. 11615, Apr. 2021, Art. no. 1161506.\nSANGHO YU received the M.S. degree in electronic and electrical engineering from Sungkyunkwan University (SKKU), Suwon, Republic of Korea, in 2021. He is currently working as a Researcher with SKKU. His current research interests include process-induced random variation and machine learning model.\nSANG MIN WON received the B.S., M.S., and Ph.D. degrees in electrical and computer engineering from the University of Illinois at UrbanaGLYPH<21> Champaign. He is currently an Assistant Professor with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include sensors/stimulators with unique applications in advanced biomedical and/or health monitoring systems.\nHYOUNG WON BAAC received the B.S. degree (Hons.) in electronic engineering from Sungkyunkwan University, Suwon, Republic of Korea, in 1999, and the Ph.D. degree in electrical engineering and computer sciences from the University of Michigan, Ann Arbor, MI, USA, in 2011. He is currently an Associate Professor with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include opti- cal/acoustic/electrical sensors and systems for biomedical therapy, healthcare, and non-destructive evaluation.\nDONGHEE SON received the Ph.D. degree in chemical and biological engineering from Seoul National University, in 2015. He is currently an Assistant Professor with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include soft GLYPH<29>exible electronic devices and systems.",
    "context": "Provides a quantitative model for estimating LER-induced random variation in FinFET characteristics, offering a faster prediction time compared to TCAD simulations.",
    "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
    "pages": [
      6,
      7
    ],
    "id": "0c99593db324f7e427a5fe2b52c6ac15308dc333385e0c7476ca77a6a3545827"
  },
  {
    "text": "CHANGHWAN SHIN (Senior Member, IEEE) received the B.S. degree (Hons.) in electrical engineering from Korea University, Seoul, South Korea, in 2006, and the Ph.D. degree in electrical engineering and computer sciences from the University of California at Berkeley, Berkeley, CA, USA, in 2011. Since 2017, he has been the Board of Directors with SK Hynix. He is currently Professor with the School of Electrical Engineering, Korea University. His current research interests include advanced CMOS device designs and their applications to memory/logic devices.\n\nDetails his academic background and current position at Korea University and SK Hynix.",
    "original_text": "CHANGHWAN SHIN (Senior Member, IEEE) received the B.S. degree (Hons.) in electrical engineering from Korea University, Seoul, South Korea, in 2006, and the Ph.D. degree in electrical engineering and computer sciences from the University of California at Berkeley, Berkeley, CA, USA, in 2011. Since 2017, he has been the Board of Directors with SK Hynix. He is currently Professor with the School of Electrical Engineering, Korea University. His current research interests include advanced CMOS device designs and their applications to memory/logic devices.",
    "context": "Details his academic background and current position at Korea University and SK Hynix.",
    "document": "Quantitative_Evaluation_of_Line-Edge_Roughness_in_Various_FinFET_Structures_Bayesian_Neural_Network_With_Automatic_Model_Selection.pdf",
    "pages": [
      7
    ],
    "id": "bb12d010210110042d07877954fddc21c85dcbc9145af6a0edc2c6a844c89b57"
  },
  {
    "text": "Received May 11, 2021, accepted June 6, 2021, date of publication June 11, 2021, date of current version June 22, 2021.\nDigital Object Identifier 10.1 109/ACCESS.2021.3088461\n\nThis chunk introduces a machine learning approach using an artificial neural network (ANN) to predict transistor performance metrics impacted by line edge roughness (LER). It details the generation of datasets, evaluation methods (including Earth Mover's Distance), and the architecture of the ANN, specifically a mixture of multivariate normal distributions, to address non-Gaussian distribution issues in transistor performance.",
    "original_text": "Received May 11, 2021, accepted June 6, 2021, date of publication June 11, 2021, date of current version June 22, 2021.\nDigital Object Identifier 10.1 109/ACCESS.2021.3088461",
    "context": "This chunk introduces a machine learning approach using an artificial neural network (ANN) to predict transistor performance metrics impacted by line edge roughness (LER). It details the generation of datasets, evaluation methods (including Earth Mover's Distance), and the architecture of the ANN, specifically a mixture of multivariate normal distributions, to address non-Gaussian distribution issues in transistor performance.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      1
    ],
    "id": "13edd023ab6e240494f754af75c8884c586b1b4c9469f03c99a3a9a6939ba1b6"
  },
  {
    "text": "Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon 16419, Republic of Korea\nCorresponding author: Changhwan Shin (cshin@skku.edu)\nThis work was supported in part by the National Research Foundation of Korea (NRF) grant funded by the Korea Government Ministry of Science and ICT (MSIT) under Grant 2020R1A2C1009063, and in part by the Future Semiconductor Device Technology Development Program under Grant 20003551 funded by the Ministry of Trade, Industry and Energy (MOTIE) and the Korea Semiconductor Research Consortium (KSRC).\n- ABSTRACT Line-edge-roughness (LER) is one of undesirable process-induced random variation sources. LER is mostly occurred in the process of photo-lithography and etching, and it provokes random variation in performance of transistors such as metal oxide semiconductor GLYPH<28>eld effect transistor (MOSFET), GLYPH<28>n-shaped GLYPH<28>eld effect transistor (FinFET), and gate-all-around GLYPH<28>eld effect transistor (GAAFET). LER was analyzed/characterized with technology computer-aided design (TCAD), but it is fundamentally very time consuming. To tackle this issue, machine learning (ML)-based method is proposed in this work. LER parameters (i.e., amplitude, and correlation length X, Y) are provided as inputs. Then, artiGLYPH<28>cial neural network (ANN) predicts 7-parameters [i.e., off-state leakage current (Ioff), saturation drain current (Idsat), linear drain current (Idlin), low drain current (Idlo), high drain current (Idhi), saturation threshold voltage (Vtsat), and linear threshold voltage (Vtlin)] which are usually used to evaluate the performance of FinFET. First, how datasets for training process of ANN were generated is explained. Next, the evaluation method for probabilistic problem is introduced. Finally, the architecture of ANN, training process and our new proposition is presented. It turned out that the prediction results (i.e., non-Gaussian distribution of device performance metrics) obtained from the ANN were very similar to that from TCAD in the respect of both qualitative and quantitative comparison.\nLine edge roughness, process-induced random variation, FinFET, machine learning,\n- INDEX TERMS artiGLYPH<28>cial neural network.\n\nThis chunk details the funding sources and institutional affiliation for the research, alongside a brief overview of the proposed machine learning approach to address line edge roughness in FinFET devices.",
    "original_text": "Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon 16419, Republic of Korea\nCorresponding author: Changhwan Shin (cshin@skku.edu)\nThis work was supported in part by the National Research Foundation of Korea (NRF) grant funded by the Korea Government Ministry of Science and ICT (MSIT) under Grant 2020R1A2C1009063, and in part by the Future Semiconductor Device Technology Development Program under Grant 20003551 funded by the Ministry of Trade, Industry and Energy (MOTIE) and the Korea Semiconductor Research Consortium (KSRC).\n- ABSTRACT Line-edge-roughness (LER) is one of undesirable process-induced random variation sources. LER is mostly occurred in the process of photo-lithography and etching, and it provokes random variation in performance of transistors such as metal oxide semiconductor GLYPH<28>eld effect transistor (MOSFET), GLYPH<28>n-shaped GLYPH<28>eld effect transistor (FinFET), and gate-all-around GLYPH<28>eld effect transistor (GAAFET). LER was analyzed/characterized with technology computer-aided design (TCAD), but it is fundamentally very time consuming. To tackle this issue, machine learning (ML)-based method is proposed in this work. LER parameters (i.e., amplitude, and correlation length X, Y) are provided as inputs. Then, artiGLYPH<28>cial neural network (ANN) predicts 7-parameters [i.e., off-state leakage current (Ioff), saturation drain current (Idsat), linear drain current (Idlin), low drain current (Idlo), high drain current (Idhi), saturation threshold voltage (Vtsat), and linear threshold voltage (Vtlin)] which are usually used to evaluate the performance of FinFET. First, how datasets for training process of ANN were generated is explained. Next, the evaluation method for probabilistic problem is introduced. Finally, the architecture of ANN, training process and our new proposition is presented. It turned out that the prediction results (i.e., non-Gaussian distribution of device performance metrics) obtained from the ANN were very similar to that from TCAD in the respect of both qualitative and quantitative comparison.\nLine edge roughness, process-induced random variation, FinFET, machine learning,\n- INDEX TERMS artiGLYPH<28>cial neural network.",
    "context": "This chunk details the funding sources and institutional affiliation for the research, alongside a brief overview of the proposed machine learning approach to address line edge roughness in FinFET devices.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      1
    ],
    "id": "67ee8ff59e145ba0f792599de7302dd62ece3933ee9becd56d809fc78b410611"
  },
  {
    "text": "Over the past a few decades, complementary metal oxide semiconductor (CMOS) technology has been evolved with advanced techniques such as stress engineering in 90 nm technology node [1], high-k/metal-gate (HK/MG) in 45 nm technology node [2], and three-dimensional advanced device structure in 22 nm technology node [3]. Those new techniques have enabled the physical dimension of metal oxide semiconductor GLYPH<28>eld effect transistor (MOSFET) to be successfully scaled down, resulting in the improved functions of integrated circuit (IC) per cost. However, there still exists secondary effects in aggressively scaled MOSFETs, and they should be overcome. Especially, one of those challenges, i.e., process-induced random variation, which randomly cause\nThe associate editor coordinating the review of this manuscript and approving it for publication was Kalyan Koley .\nvariation in transistor performance metrics such as threshold voltage, on-state drive current, and off-state leakage current, become signiGLYPH<28>cant as CMOS technology is evolved [4]. The primary causes of process-induced random variation can be classiGLYPH<28>ed as (i) line edge roughness (LER), (ii) random dopant GLYPH<29>uctuation (RDF), and (iii) work function variation (WFV) [5]. Among them, because LER can affect the other random variation sources (i.e., RDF and WFV) by inducing the deformation of device structure [6], it would degrade the device performance more severely. With the most radical shift in device architecture at 22 nm node, i.e., from planar bulk MOSFET to three-dimensional GLYPH<28>n-shaped GLYPH<28>eld effect transistor (FinFET), the process-induced random variation becomes much more severe [7]. As expected that a more 45 complicated device structure such as multiple bridge channel GLYPH<28>eld effect transistor (MBCFET), stacked nano-wire FET, complementary FET (CFET) would be adopted at 3 nm node\n[9], understanding and analyzing the impact of LER on device performance is essential for designing variation-robust silicon device.\nTo understand and quantify the impact of LER on device performance, TCAD has been used so far. TCAD simulation is, however, fundamentally very time-consuming. As another approach, compact model [10] has been used to overcome the time-inefGLYPH<28>ciency of using TCAD tools. However, the compact model for LER is still based on 2-D analysis, despite that 3-D analysis for LER is necessary [7]. As an alternative way to avoid those obstacles, we have focused on machine learning (ML)-based artiGLYPH<28>cial neural network (ANN). Machine Learning technology has already been in spotlight in various GLYPH<28>elds such as geology, and biology [11]GLYPH<21>[13]. Especially in semiconductor technologies, a number of studies have been reported in many branches such as fabrication [14], [15], optimization [16], and modeling [17]. Following this trend, we suggested a ML-based ANN model in our previous work [18]. However, it turned out that the ML-based ANN had limits, in that target performance metrics were assumed to follow multi-variate Gaussian distribution only. This assumption would cause some inevitable errors. Moreover, the ANN model has its own intrinsic limit in estimating the other performance metrics such as Idlo , and Idhi (which, in real, do not follow Gaussian distribution). Therefore, we would like to develop and propose an upgraded ANN model with enhanced accuracy (note that this new ANN model can overcome the limit mentioned above).\nIn this work, we show the way how FinFETs with LER are simulated as well as how those data are preprocessed for training process of ANN. Afterwards, the evaluation method used to assess the results of proposed work is introduced. Finally, it is shown how ANN was composed and built, including its geometrical structure, hyper parameters, and the process of grafting probability.\n\nThe document introduces a machine learning approach to predict FinFET performance metrics, specifically addressing the challenge of line edge roughness (LER) and its impact on transistor variation. It details the use of an ANN model to overcome limitations of previous Gaussian distribution assumptions and highlights the model's similarity to TCAD simulations.",
    "original_text": "Over the past a few decades, complementary metal oxide semiconductor (CMOS) technology has been evolved with advanced techniques such as stress engineering in 90 nm technology node [1], high-k/metal-gate (HK/MG) in 45 nm technology node [2], and three-dimensional advanced device structure in 22 nm technology node [3]. Those new techniques have enabled the physical dimension of metal oxide semiconductor GLYPH<28>eld effect transistor (MOSFET) to be successfully scaled down, resulting in the improved functions of integrated circuit (IC) per cost. However, there still exists secondary effects in aggressively scaled MOSFETs, and they should be overcome. Especially, one of those challenges, i.e., process-induced random variation, which randomly cause\nThe associate editor coordinating the review of this manuscript and approving it for publication was Kalyan Koley .\nvariation in transistor performance metrics such as threshold voltage, on-state drive current, and off-state leakage current, become signiGLYPH<28>cant as CMOS technology is evolved [4]. The primary causes of process-induced random variation can be classiGLYPH<28>ed as (i) line edge roughness (LER), (ii) random dopant GLYPH<29>uctuation (RDF), and (iii) work function variation (WFV) [5]. Among them, because LER can affect the other random variation sources (i.e., RDF and WFV) by inducing the deformation of device structure [6], it would degrade the device performance more severely. With the most radical shift in device architecture at 22 nm node, i.e., from planar bulk MOSFET to three-dimensional GLYPH<28>n-shaped GLYPH<28>eld effect transistor (FinFET), the process-induced random variation becomes much more severe [7]. As expected that a more 45 complicated device structure such as multiple bridge channel GLYPH<28>eld effect transistor (MBCFET), stacked nano-wire FET, complementary FET (CFET) would be adopted at 3 nm node\n[9], understanding and analyzing the impact of LER on device performance is essential for designing variation-robust silicon device.\nTo understand and quantify the impact of LER on device performance, TCAD has been used so far. TCAD simulation is, however, fundamentally very time-consuming. As another approach, compact model [10] has been used to overcome the time-inefGLYPH<28>ciency of using TCAD tools. However, the compact model for LER is still based on 2-D analysis, despite that 3-D analysis for LER is necessary [7]. As an alternative way to avoid those obstacles, we have focused on machine learning (ML)-based artiGLYPH<28>cial neural network (ANN). Machine Learning technology has already been in spotlight in various GLYPH<28>elds such as geology, and biology [11]GLYPH<21>[13]. Especially in semiconductor technologies, a number of studies have been reported in many branches such as fabrication [14], [15], optimization [16], and modeling [17]. Following this trend, we suggested a ML-based ANN model in our previous work [18]. However, it turned out that the ML-based ANN had limits, in that target performance metrics were assumed to follow multi-variate Gaussian distribution only. This assumption would cause some inevitable errors. Moreover, the ANN model has its own intrinsic limit in estimating the other performance metrics such as Idlo , and Idhi (which, in real, do not follow Gaussian distribution). Therefore, we would like to develop and propose an upgraded ANN model with enhanced accuracy (note that this new ANN model can overcome the limit mentioned above).\nIn this work, we show the way how FinFETs with LER are simulated as well as how those data are preprocessed for training process of ANN. Afterwards, the evaluation method used to assess the results of proposed work is introduced. Finally, it is shown how ANN was composed and built, including its geometrical structure, hyper parameters, and the process of grafting probability.",
    "context": "The document introduces a machine learning approach to predict FinFET performance metrics, specifically addressing the challenge of line edge roughness (LER) and its impact on transistor variation. It details the use of an ANN model to overcome limitations of previous Gaussian distribution assumptions and highlights the model's similarity to TCAD simulations.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      1,
      2
    ],
    "id": "67cc20048dc953ca4401bea493596bb83b8a68e769d8f220e5c6b739a717009b"
  },
  {
    "text": "Asdone in the previous study [18], 3-D quasi atomistic model for line edge roughness was used [19]. Three parameters (i.e., 1 , 3 x, and 3 y) are used to describe and reconGLYPH<28>gure LER proGLYPH<28>le. The physical meaning of those parameters are as follows [see Fig 2]:\n- (i) Amplitude ( 1 ): it indicates the rms (root-meansquared) value of surface roughness.\n- (ii) Correlation length ( 3 ): it means how line edge is closely correlated with its neighboring edge. A larger 3 indicates a smoother line.\nFIGURE 1. A bird's-eye view of FinFET with a three-dimensional line-edge-roughness (LER) on its sidewall. LER parameters used in this example are as follows: 1 D 0 : 5 nm, 3 x D 20 nm, 3 y D 50 nm, GLYPH<11> D 1, and 2 D 0.\nFIGURE 2. Examples of roughness amplitude when (a) 3 D 10, and (b) 1 D 0.5.\nIn Eq. (1), as shown at the bottom of the page, 3 x and 3 y are the correlation length along x-direction and y-direction, respectively [see Fig. 1]. 2 indicates the relation between x-direction and y-direction.\nUsing parameters mentioned above and two-dimensional auto covariance function (ACVF), we have simulated FinFET with MATLAB and TCAD Sentaurus Structure Editor and Device [19], [20]. The detailed steps how to create a rough sidewall surface of FinFET are provided in [18], [19].\nNominal device parameters of FinFET is summarized in Table 1. Drift-diffusion simulation for the FinFETs with surface roughness are executed, using various models such as doping-dependent mobility model, thin-layer mobility model for carrier transport, the Shockley-Read-Hall (SRH) model for generation and recombination, high GLYPH<28>eld saturation model\n<!-- formula-not-decoded -->\nTABLE 1. Nominal device parameters of FinFET [8].\nTABLE 2. Performance metrics.\nfor velocity saturation, and a density-gradient quantization model for quantum-mechanical effects.\n\nThis chunk details the methodology for simulating FinFETs with line edge roughness (LER), utilizing a 3D quasi-atomistic model and parameters describing LER amplitude and correlation length. It outlines the simulation process, including the use of MATLAB and TCAD Sentaurus, and references previous work on LER modeling.",
    "original_text": "Asdone in the previous study [18], 3-D quasi atomistic model for line edge roughness was used [19]. Three parameters (i.e., 1 , 3 x, and 3 y) are used to describe and reconGLYPH<28>gure LER proGLYPH<28>le. The physical meaning of those parameters are as follows [see Fig 2]:\n- (i) Amplitude ( 1 ): it indicates the rms (root-meansquared) value of surface roughness.\n- (ii) Correlation length ( 3 ): it means how line edge is closely correlated with its neighboring edge. A larger 3 indicates a smoother line.\nFIGURE 1. A bird's-eye view of FinFET with a three-dimensional line-edge-roughness (LER) on its sidewall. LER parameters used in this example are as follows: 1 D 0 : 5 nm, 3 x D 20 nm, 3 y D 50 nm, GLYPH<11> D 1, and 2 D 0.\nFIGURE 2. Examples of roughness amplitude when (a) 3 D 10, and (b) 1 D 0.5.\nIn Eq. (1), as shown at the bottom of the page, 3 x and 3 y are the correlation length along x-direction and y-direction, respectively [see Fig. 1]. 2 indicates the relation between x-direction and y-direction.\nUsing parameters mentioned above and two-dimensional auto covariance function (ACVF), we have simulated FinFET with MATLAB and TCAD Sentaurus Structure Editor and Device [19], [20]. The detailed steps how to create a rough sidewall surface of FinFET are provided in [18], [19].\nNominal device parameters of FinFET is summarized in Table 1. Drift-diffusion simulation for the FinFETs with surface roughness are executed, using various models such as doping-dependent mobility model, thin-layer mobility model for carrier transport, the Shockley-Read-Hall (SRH) model for generation and recombination, high GLYPH<28>eld saturation model\n<!-- formula-not-decoded -->\nTABLE 1. Nominal device parameters of FinFET [8].\nTABLE 2. Performance metrics.\nfor velocity saturation, and a density-gradient quantization model for quantum-mechanical effects.",
    "context": "This chunk details the methodology for simulating FinFETs with line edge roughness (LER), utilizing a 3D quasi-atomistic model and parameters describing LER amplitude and correlation length. It outlines the simulation process, including the use of MATLAB and TCAD Sentaurus, and references previous work on LER modeling.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      2,
      3
    ],
    "id": "ba77fab3e371f17c8d4404f048c643f41e908f572dac971dcfcd1e0016e09b90"
  },
  {
    "text": "To train, validate, and test the ANN model that we have newly built up, two kinds of datasets were separately generated: (1) The GLYPH<28>rst kind of dataset has 130 datasets. Each dataset contains the performance metrics of 50 different FinFETs, so that total 6,500 FinFETs are used in 130 datasets. 70% of them are used as the training datasets, and the others are used as the validation datasets. (2) The second kind of dataset has 10 different datasets. Each dataset contains the performance metrics of 250 different FinFETs, so that total 2,500 FinFETs are used in 10 datasets.\nThe LER parameters are chosen in the range below V\nAmplitude ( 1 )\nV 0 : 1 nm GLYPH<24> 0 : 8 nm\nCorrelation length X( 3 x) V 10 nm GLYPH<24> 100 nm\nCorrelation length Y( 3 y) V 20 nm GLYPH<24> 200 nm\nThe reference parameter set ( 1 D 0.5 nm, 3 x D 20 nm, 3 y D 50 nm) is obtained, based on experimental results [21]GLYPH<21>[24]. Then, the range is selected on the basis of the reference parameter set. Note that the performance metrics of transistor were extracted from simulated drain current versus gate voltage (Id-vs.-Vg) characteristic. The details on the device performance metrics are summarized in Table 2. These metrics are extracted using the Sentaurus TCAD inspect [20].\n\nThis chunk details the creation and use of two datasets for training an artificial neural network (ANN) designed to predict FinFET performance metrics influenced by line edge roughness (LER). Specifically, it outlines the generation of 130 datasets (70% training, 30% validation) and 10 datasets (all training) containing performance metrics from 50 and 250 FinFETs, respectively, based on simulated drain current versus gate voltage characteristics. The LER parameters (amplitude, correlation length X and Y) are defined within a specific voltage range, and a reference parameter set is established for selection.",
    "original_text": "To train, validate, and test the ANN model that we have newly built up, two kinds of datasets were separately generated: (1) The GLYPH<28>rst kind of dataset has 130 datasets. Each dataset contains the performance metrics of 50 different FinFETs, so that total 6,500 FinFETs are used in 130 datasets. 70% of them are used as the training datasets, and the others are used as the validation datasets. (2) The second kind of dataset has 10 different datasets. Each dataset contains the performance metrics of 250 different FinFETs, so that total 2,500 FinFETs are used in 10 datasets.\nThe LER parameters are chosen in the range below V\nAmplitude ( 1 )\nV 0 : 1 nm GLYPH<24> 0 : 8 nm\nCorrelation length X( 3 x) V 10 nm GLYPH<24> 100 nm\nCorrelation length Y( 3 y) V 20 nm GLYPH<24> 200 nm\nThe reference parameter set ( 1 D 0.5 nm, 3 x D 20 nm, 3 y D 50 nm) is obtained, based on experimental results [21]GLYPH<21>[24]. Then, the range is selected on the basis of the reference parameter set. Note that the performance metrics of transistor were extracted from simulated drain current versus gate voltage (Id-vs.-Vg) characteristic. The details on the device performance metrics are summarized in Table 2. These metrics are extracted using the Sentaurus TCAD inspect [20].",
    "context": "This chunk details the creation and use of two datasets for training an artificial neural network (ANN) designed to predict FinFET performance metrics influenced by line edge roughness (LER). Specifically, it outlines the generation of 130 datasets (70% training, 30% validation) and 10 datasets (all training) containing performance metrics from 50 and 250 FinFETs, respectively, based on simulated drain current versus gate voltage characteristics. The LER parameters (amplitude, correlation length X and Y) are defined within a specific voltage range, and a reference parameter set is established for selection.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      3
    ],
    "id": "ebd31e6ca79e50e00bd8ac62e6e03f71b4d485ce9b7a2d16c9c21eb800778c85"
  },
  {
    "text": "To quantitatively verify the distribution of values obtained from the new ANN model, we used earth-mover's distance (EMD) score (or be referred to as Wasserstein metric\nFIGURE 3. Conceptual diagram for showing the mixture of multivariate normal distributions.\nin mathematics). The EMD score is used to measure how two probability distributions are different from each other [25]. The deGLYPH<28>nition of the score is ''The minimal amount of work needed to transform one distribution to another distribution''. The EMD score can be calculated following the steps below:\nStep I: Calculate the difference of cumulative distribution function (CDF) of TCAD datasets and ANN prediction datasets.\nStep II: Normalize the calculated value in Step I.\nIn this work, we used Gaussian kernel density estimation (KDE) to estimate CDF of datasets. The EMD score is ''0'' when two distributions are exactly identical.\n\nQuantifies the difference between TCAD dataset and ANN predictions using the Earth-mover's distance (EMD) metric, outlining the steps for calculating the EMD score and its significance in comparing probability distributions.",
    "original_text": "To quantitatively verify the distribution of values obtained from the new ANN model, we used earth-mover's distance (EMD) score (or be referred to as Wasserstein metric\nFIGURE 3. Conceptual diagram for showing the mixture of multivariate normal distributions.\nin mathematics). The EMD score is used to measure how two probability distributions are different from each other [25]. The deGLYPH<28>nition of the score is ''The minimal amount of work needed to transform one distribution to another distribution''. The EMD score can be calculated following the steps below:\nStep I: Calculate the difference of cumulative distribution function (CDF) of TCAD datasets and ANN prediction datasets.\nStep II: Normalize the calculated value in Step I.\nIn this work, we used Gaussian kernel density estimation (KDE) to estimate CDF of datasets. The EMD score is ''0'' when two distributions are exactly identical.",
    "context": "Quantifies the difference between TCAD dataset and ANN predictions using the Earth-mover's distance (EMD) metric, outlining the steps for calculating the EMD score and its significance in comparing probability distributions.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      3
    ],
    "id": "061a8d07cee4b5796e315775572201e0a08ef619d49126f342a9283bcfbd91ea"
  },
  {
    "text": "Different from the previous study [10], the mixture of multivariate normal distributions (MVN) is used in this work. By using the mixture of MVNs, we can respond to many other unknown distribution shapes. It is trivial that performance metrics of transistor approximately follow Gaussian distribution [26]GLYPH<21>[28]. However, triggered by many non-ideal effects in transistors (i.e., short-channel effects [29]), there are some skewness, kurtosis, and/or non-linear correlation in-between the performance metrics. Moreover, distribution shapes are also quite different, for each LER parameter. For those reasons, the mixture of MVNs was used to deal with the non-ideal cases. A conceptual diagram for the mixture of MVNs is shown in Fig. 3.\nTo determine the number of components (i.e., MVN distributions) used in generating the mixture of MVNs, an optimization was GLYPH<28>rst done. V alidation datasets were used in the training process, to GLYPH<28>nd the best working model. In Fig. 4, it can be noted that the validation loss was minimized with the number of components of 11 at 7,800 epochs. This means that the ANN model with 11 MVNs works best to describe the distribution of performance metrics. The optimized ANN with the mixture of MVNs has 3 neurons for the input layer, 81 neurons for the GLYPH<28>rst hidden layer, 162 neurons for the second hidden layer, 324 for the third hidden layer, and 324 neurons for the output layer. This output neurons are\nFIGURE 4. Validation loss vs. the number of epochs, when the number of distribution used to mixture is varied from 9 to 13.\nFIGURE 5. The flow chart how to build/train/test the ANN model.\nconnected to probabilistic layer with the mixture of multivariate normal distribution, for the generation of power density function (PDF) of output variables (performance metrics).\n\nIntroduces the use of a mixture of multivariate normal distributions to account for non-Gaussian behavior in transistor performance metrics.",
    "original_text": "Different from the previous study [10], the mixture of multivariate normal distributions (MVN) is used in this work. By using the mixture of MVNs, we can respond to many other unknown distribution shapes. It is trivial that performance metrics of transistor approximately follow Gaussian distribution [26]GLYPH<21>[28]. However, triggered by many non-ideal effects in transistors (i.e., short-channel effects [29]), there are some skewness, kurtosis, and/or non-linear correlation in-between the performance metrics. Moreover, distribution shapes are also quite different, for each LER parameter. For those reasons, the mixture of MVNs was used to deal with the non-ideal cases. A conceptual diagram for the mixture of MVNs is shown in Fig. 3.\nTo determine the number of components (i.e., MVN distributions) used in generating the mixture of MVNs, an optimization was GLYPH<28>rst done. V alidation datasets were used in the training process, to GLYPH<28>nd the best working model. In Fig. 4, it can be noted that the validation loss was minimized with the number of components of 11 at 7,800 epochs. This means that the ANN model with 11 MVNs works best to describe the distribution of performance metrics. The optimized ANN with the mixture of MVNs has 3 neurons for the input layer, 81 neurons for the GLYPH<28>rst hidden layer, 162 neurons for the second hidden layer, 324 for the third hidden layer, and 324 neurons for the output layer. This output neurons are\nFIGURE 4. Validation loss vs. the number of epochs, when the number of distribution used to mixture is varied from 9 to 13.\nFIGURE 5. The flow chart how to build/train/test the ANN model.\nconnected to probabilistic layer with the mixture of multivariate normal distribution, for the generation of power density function (PDF) of output variables (performance metrics).",
    "context": "Introduces the use of a mixture of multivariate normal distributions to account for non-Gaussian behavior in transistor performance metrics.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      3,
      4
    ],
    "id": "162dab2f62129f1b2441783099b32df98f5b8926a1457eb43ad2f723462cd3f3"
  },
  {
    "text": "Unlike the proposed ANN in the previous section, another simple ANN was built only for estimating mean and standard deviation. With this simple ANN, the performance metrics were standardized in the training process of ANN with the mixture of MVNs [see Fig. 5]. By using additional simple ANN, we can limit the role of ANN with the mixture of MVNs, to just estimate the shape of probability distribution. Thus, the time spent to train the ANN model can be largely reduced (see Table 3). The simple ANN has 3 neurons for the input layer, 3 neurons for the GLYPH<28>rst hidden layer, 7 neurons for the second hidden layer, 14 neurons for the third hidden layer, and 14 neurons for the output layer. To compare this work against the non-separated ANN (note that the ANN model\nTABLE 3. Time spent to train ANN model.\ncan predict the mean, standard deviation, and the shape of distribution, at once), the same work mentioned in Part A was repeated to GLYPH<28>nd the optimized ANN (i.e., the number of MVNs, training epochs, etc.).\n\nIntroduces a simpler ANN designed solely to estimate mean and standard deviation, streamlining the training process and reducing simulation time compared to the more complex model.",
    "original_text": "Unlike the proposed ANN in the previous section, another simple ANN was built only for estimating mean and standard deviation. With this simple ANN, the performance metrics were standardized in the training process of ANN with the mixture of MVNs [see Fig. 5]. By using additional simple ANN, we can limit the role of ANN with the mixture of MVNs, to just estimate the shape of probability distribution. Thus, the time spent to train the ANN model can be largely reduced (see Table 3). The simple ANN has 3 neurons for the input layer, 3 neurons for the GLYPH<28>rst hidden layer, 7 neurons for the second hidden layer, 14 neurons for the third hidden layer, and 14 neurons for the output layer. To compare this work against the non-separated ANN (note that the ANN model\nTABLE 3. Time spent to train ANN model.\ncan predict the mean, standard deviation, and the shape of distribution, at once), the same work mentioned in Part A was repeated to GLYPH<28>nd the optimized ANN (i.e., the number of MVNs, training epochs, etc.).",
    "context": "Introduces a simpler ANN designed solely to estimate mean and standard deviation, streamlining the training process and reducing simulation time compared to the more complex model.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      4
    ],
    "id": "0683b94871fa28ca24ee920927d7e7580363e697b18b1ef0198bf02ea2bd7c9c"
  },
  {
    "text": "The weight matrices and bias vectors of ANN model are updated for the given/speciGLYPH<28>ed number of iterations. These matrices and vectors determine the output of ANN model. The probabilistic layer attached to output neurons returns the PDF of variables while training process. Thus, conventional mean-squared error cannot be used as loss function. Instead, ''Negative log likelihood'' (Negloglik) was used as a loss function [see Eq. (2)]. The training process is executed to minimize this loss function. That is, training ANN becomes the process of Maximum Likelihood Estimation (MLE) [30].\n<!-- formula-not-decoded -->\nIn Eq. (2), P(x) and Q(x) denotes the PDF of observation and hypothesis, respectively.\nUsing Adam Optimizer [31], the training process was executed for 14,880 epochs (84 sec) for mean and standard deviation of ANN model and 7,800 epochs (101 sec) for the mixture of MVNs ANN model. Both models are trained with the learning rate of 10 GLYPH<0> 4 , and then optimized to prevent the occurrence of over-GLYPH<28>tting by using validation datasets. ReLU was used as the activation function for both models. Note that the ANN model was built using the TensorGLYPH<29>ow 2.0 and TensorGLYPH<29>ow-probability python library [32], [33].\n\nDetails the ANN training process, specifically outlining the use of Negative log likelihood as a loss function and Maximum Likelihood Estimation.",
    "original_text": "The weight matrices and bias vectors of ANN model are updated for the given/speciGLYPH<28>ed number of iterations. These matrices and vectors determine the output of ANN model. The probabilistic layer attached to output neurons returns the PDF of variables while training process. Thus, conventional mean-squared error cannot be used as loss function. Instead, ''Negative log likelihood'' (Negloglik) was used as a loss function [see Eq. (2)]. The training process is executed to minimize this loss function. That is, training ANN becomes the process of Maximum Likelihood Estimation (MLE) [30].\n<!-- formula-not-decoded -->\nIn Eq. (2), P(x) and Q(x) denotes the PDF of observation and hypothesis, respectively.\nUsing Adam Optimizer [31], the training process was executed for 14,880 epochs (84 sec) for mean and standard deviation of ANN model and 7,800 epochs (101 sec) for the mixture of MVNs ANN model. Both models are trained with the learning rate of 10 GLYPH<0> 4 , and then optimized to prevent the occurrence of over-GLYPH<28>tting by using validation datasets. ReLU was used as the activation function for both models. Note that the ANN model was built using the TensorGLYPH<29>ow 2.0 and TensorGLYPH<29>ow-probability python library [32], [33].",
    "context": "Details the ANN training process, specifically outlining the use of Negative log likelihood as a loss function and Maximum Likelihood Estimation.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      4
    ],
    "id": "cee1f43f262f5b67a8a64cb41b5a475b435d0a7ce1cd04b532a8618ab6a6131a"
  },
  {
    "text": "Based on the PDF determined by the mixture of MVNs, the standardized prediction data was randomly extracted. Then, these standardized values are recovered with the predicted mean and standard deviation to the original scale (see Fig. 5).\nFig. 6 shows the comparison between the previous work, non-separated ANN, and this work, for a given LER of 1 D 0.505 nm, 3 x D 48.62 nm, 3 y D 67.99 nm. As shown in Fig. 5(a, b), previous work with plain MVN cannot predict metrics with non-linear correlation, skewness and kurtosis. On the other hand, ANN with mixture of MVN [see Fig. 6(c-f)] successfully predicts skewness, kurtosis, and non-linear correlation, which is distinctly different from plain MVN. EMD score also proves that prediction accuracy is highly improved (0.0170 vs 0.00928). As shown in Fig. 6(c-f), there is no signiGLYPH<28>cant performance degradation\nFIGURE 6. Histograms of (a, b) previous work, (c, d) non-separated model, and (e, f) this work. EMD score of (b), (d), and (f) is 0.0170, 0.00785, and 0.00928, respectively.\nTABLE 4. Comparison of EMD score between this work and non-separated ANN model.\nFIGURE 7. Pair plot of performance metrics for 1 D 0.505 nm, 3 x D 48.62 nm, 3 y D 67.99 nm.\nbetween non-separated ANN model and this work, in spite of huge time saving (1412 sec to 185 sec) [see Table 3].\nThe pair plots for 1 D 0.505 nm, 3 x D 48.62 nm, 3 y D 67.99 nm, and 1 D 0.690 nm, 3 x D 85.09 nm,\n3 y D 80.98 nm are shown in [See Fig. 7 and Fig. 8]. We can notify that distribution for each parameter (a diagonal line) and correlation between parameters (the rest except for a diagonal line) are well predicted.\nVtlin\nFIGURE 8. Pair plot of performance metrics 1 D 0.690 nm, 3 x D 85.09 nm, 3 y D 80.98 nm.\nTable 4 shows the EMD score comparison of performance metric. In the respect of quantitative analysis, it is shown that this work shows quite similar performance with non-separated ANN, when the amplitude of LER proGLYPH<28>le is around 0.5 or 0.6. However, it shows enhanced performance in a wider range than the non-separated ANN model. This work not only predicts the mean and standard deviation at a high level with the additional simple ANN, but also\nshows higher consistency for speciGLYPH<28>c points such as the tail of distribution by using the mixture of MVNs.\n\nProvides a quantitative comparison of prediction accuracy between the proposed work and a non-separated ANN model, highlighting improved performance across a wider range of LER profiles.",
    "original_text": "Based on the PDF determined by the mixture of MVNs, the standardized prediction data was randomly extracted. Then, these standardized values are recovered with the predicted mean and standard deviation to the original scale (see Fig. 5).\nFig. 6 shows the comparison between the previous work, non-separated ANN, and this work, for a given LER of 1 D 0.505 nm, 3 x D 48.62 nm, 3 y D 67.99 nm. As shown in Fig. 5(a, b), previous work with plain MVN cannot predict metrics with non-linear correlation, skewness and kurtosis. On the other hand, ANN with mixture of MVN [see Fig. 6(c-f)] successfully predicts skewness, kurtosis, and non-linear correlation, which is distinctly different from plain MVN. EMD score also proves that prediction accuracy is highly improved (0.0170 vs 0.00928). As shown in Fig. 6(c-f), there is no signiGLYPH<28>cant performance degradation\nFIGURE 6. Histograms of (a, b) previous work, (c, d) non-separated model, and (e, f) this work. EMD score of (b), (d), and (f) is 0.0170, 0.00785, and 0.00928, respectively.\nTABLE 4. Comparison of EMD score between this work and non-separated ANN model.\nFIGURE 7. Pair plot of performance metrics for 1 D 0.505 nm, 3 x D 48.62 nm, 3 y D 67.99 nm.\nbetween non-separated ANN model and this work, in spite of huge time saving (1412 sec to 185 sec) [see Table 3].\nThe pair plots for 1 D 0.505 nm, 3 x D 48.62 nm, 3 y D 67.99 nm, and 1 D 0.690 nm, 3 x D 85.09 nm,\n3 y D 80.98 nm are shown in [See Fig. 7 and Fig. 8]. We can notify that distribution for each parameter (a diagonal line) and correlation between parameters (the rest except for a diagonal line) are well predicted.\nVtlin\nFIGURE 8. Pair plot of performance metrics 1 D 0.690 nm, 3 x D 85.09 nm, 3 y D 80.98 nm.\nTable 4 shows the EMD score comparison of performance metric. In the respect of quantitative analysis, it is shown that this work shows quite similar performance with non-separated ANN, when the amplitude of LER proGLYPH<28>le is around 0.5 or 0.6. However, it shows enhanced performance in a wider range than the non-separated ANN model. This work not only predicts the mean and standard deviation at a high level with the additional simple ANN, but also\nshows higher consistency for speciGLYPH<28>c points such as the tail of distribution by using the mixture of MVNs.",
    "context": "Provides a quantitative comparison of prediction accuracy between the proposed work and a non-separated ANN model, highlighting improved performance across a wider range of LER profiles.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      4,
      5,
      6,
      7,
      8
    ],
    "id": "d5f0ec052fdd40c9f5c8a0e7e5ed2fa80518ec3d41592c76c7d3c079e744d28f"
  },
  {
    "text": "We have proposed newly developed ANN models with enhanced accuracy. A ML-based model [14] was GLYPH<28>rst suggested to estimate LER-induced random variation, and its simulation time was shorter than using compact models. Herein, compared against the previous ML-based model [14], the newly proposed ANN models have shortened the simulation time by GLYPH<24> 6 times (from 1,191 seconds to 185 seconds). Especially, non-Gaussian features of device performance metrics' distribution (i.e., skewness, kurtosis, and non-linear correlation) are successfully predicted while the previous ML model only did with a shape of Gaussian distribution and linear correlation. Thus, the accuracy of the ANN model is signiGLYPH<28>cantly improved in the respect of both quantitative and qualitative comparisons. Especially, we extend the prediction target from 4 parameters such as Ioff, Idsat, Vtsat, and SS (Subthreshold Swing) to 7 parameters such as Ioff, Idsat, Idlin, Idlo , Idhi , Vtsat, and Vtlin. This enables simulating electrical behavior of transistor as well as DC behavior of digital circuit blocks such as SRAM bit cell [34]. This work can pave a new road to analyzing the impact of LER, and thereby, to timely design the process integration for integrated circuits.\n\nHighlights the improved accuracy and reduced simulation time of the new ANN models compared to a previous ML-based model, specifically noting the ability to predict non-Gaussian features and expanding the prediction target.",
    "original_text": "We have proposed newly developed ANN models with enhanced accuracy. A ML-based model [14] was GLYPH<28>rst suggested to estimate LER-induced random variation, and its simulation time was shorter than using compact models. Herein, compared against the previous ML-based model [14], the newly proposed ANN models have shortened the simulation time by GLYPH<24> 6 times (from 1,191 seconds to 185 seconds). Especially, non-Gaussian features of device performance metrics' distribution (i.e., skewness, kurtosis, and non-linear correlation) are successfully predicted while the previous ML model only did with a shape of Gaussian distribution and linear correlation. Thus, the accuracy of the ANN model is signiGLYPH<28>cantly improved in the respect of both quantitative and qualitative comparisons. Especially, we extend the prediction target from 4 parameters such as Ioff, Idsat, Vtsat, and SS (Subthreshold Swing) to 7 parameters such as Ioff, Idsat, Idlin, Idlo , Idhi , Vtsat, and Vtlin. This enables simulating electrical behavior of transistor as well as DC behavior of digital circuit blocks such as SRAM bit cell [34]. This work can pave a new road to analyzing the impact of LER, and thereby, to timely design the process integration for integrated circuits.",
    "context": "Highlights the improved accuracy and reduced simulation time of the new ANN models compared to a previous ML-based model, specifically noting the ability to predict non-Gaussian features and expanding the prediction target.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      8
    ],
    "id": "e5641ee5926b64216a040b28fefa9fc5eab8f7053062fe751b20c411f0aee4cf"
  },
  {
    "text": "The EDA Tool was supported by the IC Design Education Center (IDEC), Republic of Korea.\n\nProvides support and funding for the research, specifically mentioning the IC Design Education Center in Korea.",
    "original_text": "The EDA Tool was supported by the IC Design Education Center (IDEC), Republic of Korea.",
    "context": "Provides support and funding for the research, specifically mentioning the IC Design Education Center in Korea.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      8
    ],
    "id": "8e527ea97e0d87289b7b8525dfb4f6fc1bbb0a48744f093c80e090402abf1f52"
  },
  {
    "text": "- [1] K. Mistry, M. Armstrong, C. Auth, S. Cea, T. Coan, T. Ghani, T. Hoffmann, A. Murthy, J. Sandford, R. Shaheed, K. Zawadzki, K. Zhang, S. Thompson, and M. Bohr, ''Delaying forever: Uniaxial strained silicon transistors in a 90 nm CMOS technology,'' in Proc. Dig. Tech. Papers. Symp. VLSI Technol. , 2004, pp. 50GLYPH<21>51.\n- [2] C. Auth et al. , ''45 nm high-k C metal gate strain-enhanced transistors,'' in Proc. Symp. VLSI Technol. , Jun. 2008, pp. 128GLYPH<21>129.\n- [3] C. Auth et al. , ''A 22 nm high performance and low-power CMOS technology featuring fully-depleted tri-gate transistors, self-aligned contacts and high density MIM capacitors,'' in Proc. Symp. VLSI Technol. (VLSIT) , Jun. 2012, pp. 131GLYPH<21>132.\n- [4] K. Agarwal and S. Nassif, ''The impact of random device variation on SRAMcell stability in sub-90-nm CMOS technologies,'' IEEE Trans. Very Large Scale Integr. (VLSI) Syst. , vol. 16, no. 1, pp. 86GLYPH<21>97, Jan. 2008.\n- [5] S. Markov, A. S. M. Zain, B. Cheng, and A. Asenov, ''Statistical variability in scaled generations of n-channel UTB-FD-SOI MOSFETs under the inGLYPH<29>uence of RDF, LER, OTF and MGG,'' in Proc. IEEE Int. SOI Conf. (SOI) , Oct. 2012, pp. 1GLYPH<21>2.\n- [6] G. Leung and C. O. Chui, ''Interactions between line edge roughness and random dopant GLYPH<29>uctuation in nonplanar GLYPH<28>eld-effect transistor variability,'' IEEE Trans. Electron Devices , vol. 60, no. 10, pp. 3277GLYPH<21>3284, Oct. 2013.\n- [7] W.-T. Huang and Y. Li, ''The impact of GLYPH<28>n/sidewall/gate line edge roughness on trapezoidal bulk FinFET devices,'' in Proc. Int. Conf. Simul. Semiconductor Processes Devices (SISPAD) , Sep. 2014, pp. 281GLYPH<21>284.\n- [8] M. Badaroglu. (2018). International Roadmap for Device and Systems (IRDS) . [Online]. Available: http://irds.ieee.org\n- [9] G. Bae et al. , ''3 nm GAA technology featuring multi-bridge-channel FET for low power and high performance applications,'' in IEDM Tech. Dig. , Dec. 2018, pp. 28.7.1GLYPH<21>28.7.4.\n- [10] X. Jiang, X. Wang, R. Wang, B. Cheng, A. Asenov, and R. Huang, ''Predictive compact modeling of random variations in FinFET technology for 16/14 nm node and beyond,'' in IEDM Tech. Dig. , Dec. 2015, pp. 28.3.1GLYPH<21>28.3.4.\n- [11] M. H. A. Banna, K. A. Taher, M. S. Kaiser, M. Mahmud, M. S. Rahman, A. S. M. S. Hosen, and G. H. Cho, ''Application of artiGLYPH<28>cial intelligence in predicting earthquakes: State-of-the-art and future challenges,'' IEEE Access , vol. 8, pp. 192880GLYPH<21>192923, 2020.\n- [12] M. B. T. Noor, N. Z. Zenia, M. S. Kaiser, S. A. Mamun, and M. Mahmud, ''Application of deep learning in detecting neurological disorders from magnetic resonance images: A survey on the detection of Alzheimer's disease, Parkinson's disease and schizophrenia,'' Brain Informat. , vol. 7, no. 1, pp. 1GLYPH<21>21, Dec. 2020.\n- [13] A. Whata and C. Chimedza, ''Deep learning for SARS COV-2 genome sequences,'' IEEE Access , vol. 9, pp. 59597GLYPH<21>59611, 2021.\n- [14] H.-C. Choi, H. Yun, J.-S. Yoon, and R.-H. Baek, ''Neural approach for modeling and optimizing Si-MOSFET manufacturing,'' IEEE Access , vol. 8, pp. 159351GLYPH<21>159370, 2020.\n- [15] D. Jiang, W. Lin, and N. Raghavan, ''A Gaussian mixture model clustering ensemble regressor for semiconductor manufacturing GLYPH<28>nal test yield prediction,'' IEEE Access , vol. 9, pp. 22253GLYPH<21>22263, 2021.\n- [16] J.-S. Yoon, S. Lee, H. Yun, and R.-H. Baek, ''Digital/analog performance optimization of vertical nanowire FETs using machine learning,'' IEEE Access , vol. 9, pp. 29071GLYPH<21>29077, 2021.\n- [17] K. Ko, J. K. Lee, and H. Shin, ''Variability-aware machine learning strategy for 3-D NAND GLYPH<29>ash memories,'' IEEE Trans. Electron Devices , vol. 67, no. 4, pp. 1575GLYPH<21>1580, Apr. 2020.\n- [18] J. Lim and C. Shin, ''Machine learning (ML)-based model to characterize the line edge roughness (LER)-induced random variation in FinFET,'' IEEE Access , vol. 8, pp. 158237GLYPH<21>158242, 2020.\n- [19] S. Oh and C. Shin, ''3-D quasi-atomistic model for line edge roughness in nonplanar MOSFETs,'' IEEE Trans. Electron Devices , vol. 63, no. 12, pp. 4617GLYPH<21>4623, Dec. 2016.\n- [20] TCAD Sentaurus? User Guide Version P-2019.03 , Synopsys, Inc., Mountain View, CA, USA, 2019.\n- [21] C. Shin, Variation-Aware Advanced CMOS Devices and SRAM , vol. 56. Dordrecht, The Netherlands: Springer, 2016.\n- [22] E. Dornel, T. Ernst, J. C. BarbØ, J. M. Hartmann, V. Delaye, F. Aussenac, C. Vizioz, S. Borel, V . MafGLYPH<28>ni-Alvaro, C. Isheden, and J. Foucher, ''Hydrogen annealing of arrays of planar and vertically stacked Si nanowires,'' Appl. Phys. Lett. , vol. 91, no. 23, Dec. 2007, Art. no. 233502.\n- [23] T. Tezuka, N. Hirashita, Y. Moriyama, N. Sugiyama, K. Usuda, E. Toyoda, K. Murayama, and S.-I. Takagi, ''110-facets formation by hydrogen thermal etching on sidewalls of Si and strained-Si GLYPH<28>n structures,'' Appl. Phys. Lett. , vol. 92, no. 19, May 2008, Art. no. 191903.\n- [24] Y. Ma, H. J. Levinson, and T. Wallow, ''Line edge roughness impact on critical dimension variation,'' Proc. SPIE , vol. 6518, Apr. 2007, Art. no. 651824.\n- [25] Y. Rubner, C. Tomasi, and L. J. Guibas, ''A metric for distributions with applications to image databases,'' in Proc. 6th Int. Conf. Comput. Vis. , Jan. 1998, pp. 59GLYPH<21>66.\n\nProvides a historical overview of transistor technology advancements, including techniques for mitigating process variations and exploring machine learning applications in semiconductor manufacturing.",
    "original_text": "- [1] K. Mistry, M. Armstrong, C. Auth, S. Cea, T. Coan, T. Ghani, T. Hoffmann, A. Murthy, J. Sandford, R. Shaheed, K. Zawadzki, K. Zhang, S. Thompson, and M. Bohr, ''Delaying forever: Uniaxial strained silicon transistors in a 90 nm CMOS technology,'' in Proc. Dig. Tech. Papers. Symp. VLSI Technol. , 2004, pp. 50GLYPH<21>51.\n- [2] C. Auth et al. , ''45 nm high-k C metal gate strain-enhanced transistors,'' in Proc. Symp. VLSI Technol. , Jun. 2008, pp. 128GLYPH<21>129.\n- [3] C. Auth et al. , ''A 22 nm high performance and low-power CMOS technology featuring fully-depleted tri-gate transistors, self-aligned contacts and high density MIM capacitors,'' in Proc. Symp. VLSI Technol. (VLSIT) , Jun. 2012, pp. 131GLYPH<21>132.\n- [4] K. Agarwal and S. Nassif, ''The impact of random device variation on SRAMcell stability in sub-90-nm CMOS technologies,'' IEEE Trans. Very Large Scale Integr. (VLSI) Syst. , vol. 16, no. 1, pp. 86GLYPH<21>97, Jan. 2008.\n- [5] S. Markov, A. S. M. Zain, B. Cheng, and A. Asenov, ''Statistical variability in scaled generations of n-channel UTB-FD-SOI MOSFETs under the inGLYPH<29>uence of RDF, LER, OTF and MGG,'' in Proc. IEEE Int. SOI Conf. (SOI) , Oct. 2012, pp. 1GLYPH<21>2.\n- [6] G. Leung and C. O. Chui, ''Interactions between line edge roughness and random dopant GLYPH<29>uctuation in nonplanar GLYPH<28>eld-effect transistor variability,'' IEEE Trans. Electron Devices , vol. 60, no. 10, pp. 3277GLYPH<21>3284, Oct. 2013.\n- [7] W.-T. Huang and Y. Li, ''The impact of GLYPH<28>n/sidewall/gate line edge roughness on trapezoidal bulk FinFET devices,'' in Proc. Int. Conf. Simul. Semiconductor Processes Devices (SISPAD) , Sep. 2014, pp. 281GLYPH<21>284.\n- [8] M. Badaroglu. (2018). International Roadmap for Device and Systems (IRDS) . [Online]. Available: http://irds.ieee.org\n- [9] G. Bae et al. , ''3 nm GAA technology featuring multi-bridge-channel FET for low power and high performance applications,'' in IEDM Tech. Dig. , Dec. 2018, pp. 28.7.1GLYPH<21>28.7.4.\n- [10] X. Jiang, X. Wang, R. Wang, B. Cheng, A. Asenov, and R. Huang, ''Predictive compact modeling of random variations in FinFET technology for 16/14 nm node and beyond,'' in IEDM Tech. Dig. , Dec. 2015, pp. 28.3.1GLYPH<21>28.3.4.\n- [11] M. H. A. Banna, K. A. Taher, M. S. Kaiser, M. Mahmud, M. S. Rahman, A. S. M. S. Hosen, and G. H. Cho, ''Application of artiGLYPH<28>cial intelligence in predicting earthquakes: State-of-the-art and future challenges,'' IEEE Access , vol. 8, pp. 192880GLYPH<21>192923, 2020.\n- [12] M. B. T. Noor, N. Z. Zenia, M. S. Kaiser, S. A. Mamun, and M. Mahmud, ''Application of deep learning in detecting neurological disorders from magnetic resonance images: A survey on the detection of Alzheimer's disease, Parkinson's disease and schizophrenia,'' Brain Informat. , vol. 7, no. 1, pp. 1GLYPH<21>21, Dec. 2020.\n- [13] A. Whata and C. Chimedza, ''Deep learning for SARS COV-2 genome sequences,'' IEEE Access , vol. 9, pp. 59597GLYPH<21>59611, 2021.\n- [14] H.-C. Choi, H. Yun, J.-S. Yoon, and R.-H. Baek, ''Neural approach for modeling and optimizing Si-MOSFET manufacturing,'' IEEE Access , vol. 8, pp. 159351GLYPH<21>159370, 2020.\n- [15] D. Jiang, W. Lin, and N. Raghavan, ''A Gaussian mixture model clustering ensemble regressor for semiconductor manufacturing GLYPH<28>nal test yield prediction,'' IEEE Access , vol. 9, pp. 22253GLYPH<21>22263, 2021.\n- [16] J.-S. Yoon, S. Lee, H. Yun, and R.-H. Baek, ''Digital/analog performance optimization of vertical nanowire FETs using machine learning,'' IEEE Access , vol. 9, pp. 29071GLYPH<21>29077, 2021.\n- [17] K. Ko, J. K. Lee, and H. Shin, ''Variability-aware machine learning strategy for 3-D NAND GLYPH<29>ash memories,'' IEEE Trans. Electron Devices , vol. 67, no. 4, pp. 1575GLYPH<21>1580, Apr. 2020.\n- [18] J. Lim and C. Shin, ''Machine learning (ML)-based model to characterize the line edge roughness (LER)-induced random variation in FinFET,'' IEEE Access , vol. 8, pp. 158237GLYPH<21>158242, 2020.\n- [19] S. Oh and C. Shin, ''3-D quasi-atomistic model for line edge roughness in nonplanar MOSFETs,'' IEEE Trans. Electron Devices , vol. 63, no. 12, pp. 4617GLYPH<21>4623, Dec. 2016.\n- [20] TCAD Sentaurus? User Guide Version P-2019.03 , Synopsys, Inc., Mountain View, CA, USA, 2019.\n- [21] C. Shin, Variation-Aware Advanced CMOS Devices and SRAM , vol. 56. Dordrecht, The Netherlands: Springer, 2016.\n- [22] E. Dornel, T. Ernst, J. C. BarbØ, J. M. Hartmann, V. Delaye, F. Aussenac, C. Vizioz, S. Borel, V . MafGLYPH<28>ni-Alvaro, C. Isheden, and J. Foucher, ''Hydrogen annealing of arrays of planar and vertically stacked Si nanowires,'' Appl. Phys. Lett. , vol. 91, no. 23, Dec. 2007, Art. no. 233502.\n- [23] T. Tezuka, N. Hirashita, Y. Moriyama, N. Sugiyama, K. Usuda, E. Toyoda, K. Murayama, and S.-I. Takagi, ''110-facets formation by hydrogen thermal etching on sidewalls of Si and strained-Si GLYPH<28>n structures,'' Appl. Phys. Lett. , vol. 92, no. 19, May 2008, Art. no. 191903.\n- [24] Y. Ma, H. J. Levinson, and T. Wallow, ''Line edge roughness impact on critical dimension variation,'' Proc. SPIE , vol. 6518, Apr. 2007, Art. no. 651824.\n- [25] Y. Rubner, C. Tomasi, and L. J. Guibas, ''A metric for distributions with applications to image databases,'' in Proc. 6th Int. Conf. Comput. Vis. , Jan. 1998, pp. 59GLYPH<21>66.",
    "context": "Provides a historical overview of transistor technology advancements, including techniques for mitigating process variations and exploring machine learning applications in semiconductor manufacturing.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      8
    ],
    "id": "bc21eaee24a6e02ab8f0a8bcc2cdf5629cd92485aa12a4b2fe235cf07f488da0"
  },
  {
    "text": "- [26] A. Asenov, S. Kaya, and A. R. Brown, ''Intrinsic parameter GLYPH<29>uctuations in decananometer MOSFETs introduced by gate line edge roughness,'' IEEE Trans. Electron Devices , vol. 50, no. 5, pp. 1254GLYPH<21>1260, May 2003.\n- [27] J. Min and C. Shin, ''Study of line edge roughness on various types of gateall-around GLYPH<28>eld effect transistor,'' Semicond. Sci. Technol. , vol. 35, no. 1, Jan. 2020, Art. no. 015004.\n- [28] Y.-N. Chen, C.-J. Chen, M.-L. Fan, V. Hu, P. Su, and C.-T. Chuang, ''Impacts of work function variation and line-edge roughness on TFET and FinFET devices and 32-bit CLA circuits,'' J. Low Power Electron. Appl. , vol. 5, no. 2, pp. 101GLYPH<21>115, May 2015.\n- [29] S. Kaya, A. Brown, A. Asenov, D. Magot, and T. LintonI, ''Analysis of statistical GLYPH<29>uctuations due to line edge roughness in sub-0.1 GLYPH<22> mMOSFETs,'' in Simulation of Semiconductor Processes and Devices . Vienna, Austria: Springer, 2001, pp. 78GLYPH<21>81.\n- [30] J.-X. Pan and K.-T. Fang, ''Maximum likelihood estimation,'' in Growth Curve Models and Statistical Diagnostics . New York, NY, USA: Springer, 2002, pp. 77GLYPH<21>158.\n- [31] D. P. Kingma and J. Ba, ''Adam: A method for stochastic optimization,'' 2014, arXiv:1412.6980 . [Online]. Available: http://arxiv.org/abs/1412.6980\n- [32] M. Abadi et al. , ''TensorFlow: Large-scale machine learning on heterogeneous distributed systems,'' 2016, arXiv:1603.04467 . [Online]. Available: https://arxiv.org/abs/1603.04467\n- [33] M. Abadi et al. , ''TensorFlow: A system for large-scale machine learning,'' in Proc. 12th USENIX Symp. Operating Syst. Design Implement. (OSDI) , 2016, pp. 265GLYPH<21>283.\n- [34] A. E. Carlson, ''Device and circuit techniques for reducing variation in nanoscale SRAM,'' Univ, California, Berkeley, CA, USA, Tech. Rep., 2008, pp. 23GLYPH<21>51.\nJAEHYUK LIM received the B.S. degree in electronic and electrical engineering from Sungkyunkwan University (SKKU), Suwon, Republic of Korea, in 2020, where he is currently pursuing the M.S. degree in electrical and computer engineering. His current research interests include machine learning and process induced random variations.\nJINWOONG LEE received the B.S. degree from the Department of Physics, KyungHee University (KHU), Seoul, Republic of Korea, in 2017. He is currently pursuing the M.S. degree in electrical and computer engineering with Sungkyunkwan University (SKKU). His current research interests include machine learning and process induced random variations.\nCHANGHWAN SHIN (Senior Member, IEEE) received the B.S. degree (Hons.) in electrical engineering from Korea University, Seoul, Republic of Korea, in 2006, and the Ph.D. degree in electrical engineering and computer sciences from the University of California at Berkeley, in 2011. Since 2017, he has been with the Board of Directors in SK Hynix. He is currently with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include advanced CMOS device designs and their applications to memory/logic devices.\n\nProvides research on line edge roughness and its impact on MOSFET devices, aligning with the authors' research interests in process-induced random variations.",
    "original_text": "- [26] A. Asenov, S. Kaya, and A. R. Brown, ''Intrinsic parameter GLYPH<29>uctuations in decananometer MOSFETs introduced by gate line edge roughness,'' IEEE Trans. Electron Devices , vol. 50, no. 5, pp. 1254GLYPH<21>1260, May 2003.\n- [27] J. Min and C. Shin, ''Study of line edge roughness on various types of gateall-around GLYPH<28>eld effect transistor,'' Semicond. Sci. Technol. , vol. 35, no. 1, Jan. 2020, Art. no. 015004.\n- [28] Y.-N. Chen, C.-J. Chen, M.-L. Fan, V. Hu, P. Su, and C.-T. Chuang, ''Impacts of work function variation and line-edge roughness on TFET and FinFET devices and 32-bit CLA circuits,'' J. Low Power Electron. Appl. , vol. 5, no. 2, pp. 101GLYPH<21>115, May 2015.\n- [29] S. Kaya, A. Brown, A. Asenov, D. Magot, and T. LintonI, ''Analysis of statistical GLYPH<29>uctuations due to line edge roughness in sub-0.1 GLYPH<22> mMOSFETs,'' in Simulation of Semiconductor Processes and Devices . Vienna, Austria: Springer, 2001, pp. 78GLYPH<21>81.\n- [30] J.-X. Pan and K.-T. Fang, ''Maximum likelihood estimation,'' in Growth Curve Models and Statistical Diagnostics . New York, NY, USA: Springer, 2002, pp. 77GLYPH<21>158.\n- [31] D. P. Kingma and J. Ba, ''Adam: A method for stochastic optimization,'' 2014, arXiv:1412.6980 . [Online]. Available: http://arxiv.org/abs/1412.6980\n- [32] M. Abadi et al. , ''TensorFlow: Large-scale machine learning on heterogeneous distributed systems,'' 2016, arXiv:1603.04467 . [Online]. Available: https://arxiv.org/abs/1603.04467\n- [33] M. Abadi et al. , ''TensorFlow: A system for large-scale machine learning,'' in Proc. 12th USENIX Symp. Operating Syst. Design Implement. (OSDI) , 2016, pp. 265GLYPH<21>283.\n- [34] A. E. Carlson, ''Device and circuit techniques for reducing variation in nanoscale SRAM,'' Univ, California, Berkeley, CA, USA, Tech. Rep., 2008, pp. 23GLYPH<21>51.\nJAEHYUK LIM received the B.S. degree in electronic and electrical engineering from Sungkyunkwan University (SKKU), Suwon, Republic of Korea, in 2020, where he is currently pursuing the M.S. degree in electrical and computer engineering. His current research interests include machine learning and process induced random variations.\nJINWOONG LEE received the B.S. degree from the Department of Physics, KyungHee University (KHU), Seoul, Republic of Korea, in 2017. He is currently pursuing the M.S. degree in electrical and computer engineering with Sungkyunkwan University (SKKU). His current research interests include machine learning and process induced random variations.\nCHANGHWAN SHIN (Senior Member, IEEE) received the B.S. degree (Hons.) in electrical engineering from Korea University, Seoul, Republic of Korea, in 2006, and the Ph.D. degree in electrical engineering and computer sciences from the University of California at Berkeley, in 2011. Since 2017, he has been with the Board of Directors in SK Hynix. He is currently with the Department of Electrical and Computer Engineering, Sungkyunkwan University. His current research interests include advanced CMOS device designs and their applications to memory/logic devices.",
    "context": "Provides research on line edge roughness and its impact on MOSFET devices, aligning with the authors' research interests in process-induced random variations.",
    "document": "Probabilistic_Artificial_Neural_Network_for_Line-Edge-Roughness-Induced_Random_Variation_in_FinFET.pdf",
    "pages": [
      8,
      9
    ],
    "id": "7ea35574b1bed28c4123d815c2ace9ef3f0b6600f70908f3af114855f4d6894f"
  },
  {
    "text": "Received June 20, 2020, accepted July 6, 2020, date of publication July 9, 2020, date of current version July 22, 2020.\nDigital Object Identifier 10.1 109/ACCESS.2020.3008195\n\nThe chunk introduces the paper's focus on utilizing singular value decomposition (SVD) in convolutional neural networks, addressing challenges posed by SVD's inherent properties like sign ambiguity and its difficulty in learning due to its manifold nature. It proposes a solution: transforming singular vectors into Euclidean space to facilitate learning and introduces a novel pooling method called singular vector pooling (SVP).",
    "original_text": "Received June 20, 2020, accepted July 6, 2020, date of publication July 9, 2020, date of current version July 22, 2020.\nDigital Object Identifier 10.1 109/ACCESS.2020.3008195",
    "context": "The chunk introduces the paper's focus on utilizing singular value decomposition (SVD) in convolutional neural networks, addressing challenges posed by SVD's inherent properties like sign ambiguity and its difficulty in learning due to its manifold nature. It proposes a solution: transforming singular vectors into Euclidean space to facilitate learning and introduces a novel pooling method called singular vector pooling (SVP).",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      1
    ],
    "id": "8a95b9951edb4fadfa691dd448ba0164867ec693e842d3e52c37ef242dcd62d8"
  },
  {
    "text": "Department of Electronic Engineering, Inha University, Incheon 22212, South Korea\nCorresponding author: Byung Cheol Song (bcsong@inha.ac.kr)\nThis work was supported in part by the Industrial Technology Innovation Program funded by the Ministry of Trade, Industry & Energy (MI, South Korea) (Development on Deep Learning based 4K30P Edge Computing enabled IP Camera System) under Grant 20006483, in part by the Institute of Information & Communications Technology Planning & Evaluation (IITP) funded by the Korea Government (MSIT) (ArtiGLYPH<28>cial Intelligence Convergence Research Center, Inha University) under Grant 2020-0-01389, and in part by the Industrial Technology Innovation Program through the Ministry of Trade, Industry, and Energy (MI, South Korea) (Development of Human-Friendly Human-Robot Interaction Technologies Using Human Internal Emotional States) under Grant 10073154.\n- ABSTRACT Singular value decomposition (SVD) is a popular technique to extract essential information by reducing the dimension of a feature set. SVD is able to analyze a vast matrix in spite of a relatively low computational cost. However, singular vectors produced by SVD have been seldom used in convolutional neural networks (CNNs). This is because the inherent properties of singular vectors such as sign ambiguity and manifold features make CNNs difGLYPH<28>cult to learn singular vectors. In order to overcome the limitations, this paper analyzes the undesirable properties of singular vectors and presents the transformation of singular vectors into Euclidean space as a smart solution. If the singular vectors are transformed to follow Euclidean geometry, SVD can be used for pooling to maintain the feature information well, which is called singular vector pooling (SVP). Since SVP can extract essential information from a feature map, it is robust against adversarial attacks in comparison to global average pooling. Thus, SVP shows a quantitative performance improvement of about 36% for the CIFAR10 dataset. In addition, we applied SVP to a knowledge distillation scheme that uses singular vectors in a restricted manner. As a result, SVP improved the performance by up to 1.7% for the CIFAR100 dataset.\nINDEX TERMS Neural networks, pattern analysis, principal component analysis.\n\nThis section details the funding sources and institutional affiliations for the research, introducing the core problem: the difficulty of using singular vectors in CNNs due to their inherent properties, and outlining the proposed solution of transforming them into Euclidean space via singular vector pooling (SVP).",
    "original_text": "Department of Electronic Engineering, Inha University, Incheon 22212, South Korea\nCorresponding author: Byung Cheol Song (bcsong@inha.ac.kr)\nThis work was supported in part by the Industrial Technology Innovation Program funded by the Ministry of Trade, Industry & Energy (MI, South Korea) (Development on Deep Learning based 4K30P Edge Computing enabled IP Camera System) under Grant 20006483, in part by the Institute of Information & Communications Technology Planning & Evaluation (IITP) funded by the Korea Government (MSIT) (ArtiGLYPH<28>cial Intelligence Convergence Research Center, Inha University) under Grant 2020-0-01389, and in part by the Industrial Technology Innovation Program through the Ministry of Trade, Industry, and Energy (MI, South Korea) (Development of Human-Friendly Human-Robot Interaction Technologies Using Human Internal Emotional States) under Grant 10073154.\n- ABSTRACT Singular value decomposition (SVD) is a popular technique to extract essential information by reducing the dimension of a feature set. SVD is able to analyze a vast matrix in spite of a relatively low computational cost. However, singular vectors produced by SVD have been seldom used in convolutional neural networks (CNNs). This is because the inherent properties of singular vectors such as sign ambiguity and manifold features make CNNs difGLYPH<28>cult to learn singular vectors. In order to overcome the limitations, this paper analyzes the undesirable properties of singular vectors and presents the transformation of singular vectors into Euclidean space as a smart solution. If the singular vectors are transformed to follow Euclidean geometry, SVD can be used for pooling to maintain the feature information well, which is called singular vector pooling (SVP). Since SVP can extract essential information from a feature map, it is robust against adversarial attacks in comparison to global average pooling. Thus, SVP shows a quantitative performance improvement of about 36% for the CIFAR10 dataset. In addition, we applied SVP to a knowledge distillation scheme that uses singular vectors in a restricted manner. As a result, SVP improved the performance by up to 1.7% for the CIFAR100 dataset.\nINDEX TERMS Neural networks, pattern analysis, principal component analysis.",
    "context": "This section details the funding sources and institutional affiliations for the research, introducing the core problem: the difficulty of using singular vectors in CNNs due to their inherent properties, and outlining the proposed solution of transforming them into Euclidean space via singular vector pooling (SVP).",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      1
    ],
    "id": "e48cea70177b6e462537498a97a4f63745db576d881284fe22cb5a1f7aeb5815"
  },
  {
    "text": "Principal component analysis (PCA) is a technique used for the identiGLYPH<28>cation of a smaller number of uncorrelated variables known as principal components from a larger set of data. Several principal components have most information of a given data, so the dimension of the data can be signiGLYPH<28>cantly reduced or compressed by choosing only some of the principal components. Therefore, PCA is a beneGLYPH<28>cial technique for analyzing high-dimensional data. Among PCA techniques, singular value decomposition (SVD) is most popular because it can produce global solutions to various problems in computer vision, pattern recognition, and machine learning [1]GLYPH<21>[3]. As the derivative function of SVD was developed [4], SVD has been applied even to convolutional neural networks (CNNs) that can deal with very high-dimensional data. For instance, SVD was employed to obtain a unique square root of a symmetric positive deGLYPH<28>nite (SPD) matrix for subspace analysis [5]GLYPH<21>[7] or higher-order pooling [4], [8]GLYPH<21>[10].\nThe associate editor coordinating the review of this manuscript and approving it for publication was Claudio Cusano .\nPrevious works show that SVD can effectively extract essential information from feature maps. However, since singular vectors have undesirable properties such as sign ambiguity [11] and manifold features, they have been seldom studied to be directly learned in spite that they have feature map's essential information. In order to avoid undesirable properties, conventional approaches adopted an SPD matrix consisting of singular vectors. However, since they should expand the compressed feature's dimension again, they require additional computational and memory cost.\nLee et al. proposed a knowledge distillation method using SVD (KD-SVD) the harmful properties of singular vectors and deGLYPH<28>ned singular vectors as knowledge [12]. But this algorithm has a limitation that it needs guidance from a teacher network.\nIn this paper, we deGLYPH<28>ne a singular vector as a feature vector and make its information directly learnable without re-composition and guidance. To do this, we analyze two problematic properties of singular vectors and proposes solutions to effectively remove the unfavorable properties based on the analysis. The sign ambiguity is eliminated so that singular vectors with similar information are gathered, and then singular vectors are converted from manifold to Euclidean space. Here, Euclidean geometry that is useful for ordinary learning schemes is employed. This procedure does not simply make singular vectors learnable but transform them to follow Euclidean geometry. Thus, SVD can be easily analyzed with layers based on Euclidean geometry.\nAlso, we propose a method called singular vector pooling (SVP), which is the only pooling method using SVD to our knowledge. SVP is basically robust to noise due to the nature of SVD. For example, in adversarial leaning with the CIFAR10 dataset, SVP provides about 36% better performance than global average pooling (GAP). As shown in the feature distributions of Fig. 1, SVP has smaller intra-class variance and larger inter-class variance than GAP, which is a great advantage of SVP. In addition, when the SVP is applied to KD-SVD, which has learned singular vectors relatively naively, the performance of a student network can be improved by about 1.7% for the CIFAR100 dataset. The contribution of this paper is summarized as follows:\n- GLYPH<15> This paper analyzes the properties of disturbing learning of singular vectors. Then, we propose skills to eliminate the unfavorable properties and present a method to transform singular vectors on non-Euclidean space to Euclidean space for effective learning.\n- GLYPH<15> We propose SVP as a novel pooling method using SVD, and prove that applying SVP to CNN's feature maps is very useful for obtaining essential information.\n- GLYPH<15> In addition, this paper shows that since singular vectors preserve feature map's information well, they are not only robust to adversarial attack, but also effective in knowledge distillation.\n\nAnalyzes the problematic properties of singular vectors and proposes solutions for direct, learnable singular vector utilization in deep learning.",
    "original_text": "Principal component analysis (PCA) is a technique used for the identiGLYPH<28>cation of a smaller number of uncorrelated variables known as principal components from a larger set of data. Several principal components have most information of a given data, so the dimension of the data can be signiGLYPH<28>cantly reduced or compressed by choosing only some of the principal components. Therefore, PCA is a beneGLYPH<28>cial technique for analyzing high-dimensional data. Among PCA techniques, singular value decomposition (SVD) is most popular because it can produce global solutions to various problems in computer vision, pattern recognition, and machine learning [1]GLYPH<21>[3]. As the derivative function of SVD was developed [4], SVD has been applied even to convolutional neural networks (CNNs) that can deal with very high-dimensional data. For instance, SVD was employed to obtain a unique square root of a symmetric positive deGLYPH<28>nite (SPD) matrix for subspace analysis [5]GLYPH<21>[7] or higher-order pooling [4], [8]GLYPH<21>[10].\nThe associate editor coordinating the review of this manuscript and approving it for publication was Claudio Cusano .\nPrevious works show that SVD can effectively extract essential information from feature maps. However, since singular vectors have undesirable properties such as sign ambiguity [11] and manifold features, they have been seldom studied to be directly learned in spite that they have feature map's essential information. In order to avoid undesirable properties, conventional approaches adopted an SPD matrix consisting of singular vectors. However, since they should expand the compressed feature's dimension again, they require additional computational and memory cost.\nLee et al. proposed a knowledge distillation method using SVD (KD-SVD) the harmful properties of singular vectors and deGLYPH<28>ned singular vectors as knowledge [12]. But this algorithm has a limitation that it needs guidance from a teacher network.\nIn this paper, we deGLYPH<28>ne a singular vector as a feature vector and make its information directly learnable without re-composition and guidance. To do this, we analyze two problematic properties of singular vectors and proposes solutions to effectively remove the unfavorable properties based on the analysis. The sign ambiguity is eliminated so that singular vectors with similar information are gathered, and then singular vectors are converted from manifold to Euclidean space. Here, Euclidean geometry that is useful for ordinary learning schemes is employed. This procedure does not simply make singular vectors learnable but transform them to follow Euclidean geometry. Thus, SVD can be easily analyzed with layers based on Euclidean geometry.\nAlso, we propose a method called singular vector pooling (SVP), which is the only pooling method using SVD to our knowledge. SVP is basically robust to noise due to the nature of SVD. For example, in adversarial leaning with the CIFAR10 dataset, SVP provides about 36% better performance than global average pooling (GAP). As shown in the feature distributions of Fig. 1, SVP has smaller intra-class variance and larger inter-class variance than GAP, which is a great advantage of SVP. In addition, when the SVP is applied to KD-SVD, which has learned singular vectors relatively naively, the performance of a student network can be improved by about 1.7% for the CIFAR100 dataset. The contribution of this paper is summarized as follows:\n- GLYPH<15> This paper analyzes the properties of disturbing learning of singular vectors. Then, we propose skills to eliminate the unfavorable properties and present a method to transform singular vectors on non-Euclidean space to Euclidean space for effective learning.\n- GLYPH<15> We propose SVP as a novel pooling method using SVD, and prove that applying SVP to CNN's feature maps is very useful for obtaining essential information.\n- GLYPH<15> In addition, this paper shows that since singular vectors preserve feature map's information well, they are not only robust to adversarial attack, but also effective in knowledge distillation.",
    "context": "Analyzes the problematic properties of singular vectors and proposes solutions for direct, learnable singular vector utilization in deep learning.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      1,
      2
    ],
    "id": "9af4ca68d2a7af19771c7857a1d2a536d09fdac354acc2005e4b4b3354785181"
  },
  {
    "text": "As one of the most popular PCA techniques, SVD is often used to reduce the dimension of a matrix or to extract essential information from a matrix or dataset. Even in the GLYPH<28>eld of machine learning, the use of SVD is expanding more and more [4], [5], [8]GLYPH<21>[10], [12]GLYPH<21>[15]. For example, SVD has been applied to a typical CNN as the derivative of SVD has been mathematically derived recently. The most common example is the computation of a unique square root of an SPD matrix. This method is used for various purposes, such as\nFIGURE 1. Comparison of feature vectors obtained by global average pooling and singular vector pooling.\ncovariance pooling [4], [8], [14], [15] and manifold embedding [5], [8]GLYPH<21>[10]. In addition, there are several cases where a good property that singular vector(s) obtained from SVD has the compressed information is used for global pooling.\nSince global pooling was GLYPH<28>rst introduced in [16], it has evolved in various ways, such as employing trainable variables [17] and adopting non-linear functions [18], [19]. On the other hand, a global pooling method using SVD was proposed [20], which was based on covariance pooling. Covariance pooling obtains the covariance matrix FF T of a feature matrix F and extracts meaningful information from covariance. Reference [4] showed that if matrix logarithm is obtained by SVD, tangent space mapping is applicable. Recently, various covariance pooling methods have been proposed, inspired by [4]. For instance, [14] presented a method to solve the instability of SVD gradients, and [8] reduced the computational cost of SVD through Newton-Schulz iteration. Also, [15] showed that covariance pooling can be effectively applied in the middle of the network.\nHowever, very few studies directly use singular vectors in deep learning. As far as we know, KD-SVD [12] may be the GLYPH<28>rst deep learning technique that uses essential information of singular vectors directly. In KD-SVD, singular vectors are extracted from two feature maps by SVD, a correlation between two singular vectors is computed, and the correlation is distilled as knowledge. To facilitate the learning of singular vectors, KD-SVD uses a teacher network as a guide. However, such an approach is not applicable to general architectures other than the student-teacher network where the teacher network plays a guide role.\nAccording to our survey, the use of SVD in deep learning is continuously being studied, but there is no way to learn singular vectors directly. On the other hand, it is also true that many methods using singular vectors for various computer vision tasks have been studied before the deep learning era [11], [21], [22]. We argue that the reason for this recent research trend is that although singular vectors have good information, there are some bad properties making difGLYPH<28>cult to learn them with common deep learning schemes. Therefore, by analyzing and removing these properties, we intend to propose a way that singular vectors can be utilized in deep learning scheme. Ultimately, the proposed algorithm will provide an opportunity to use SVD in various machine learning applications.\nFIGURE 2. An example of CNN architecture using SVP. At the bottom of the figure, we show the process of transforming the singular vectors obtained through SVD prior to learning and the process of changing the feature distribution through each process.\n\nContextualizes SVD’s role in PCA and its limited application in deep learning due to singular vector properties, highlighting the proposed solution of transforming them to Euclidean space for effective learning and a novel pooling method (SVP).",
    "original_text": "As one of the most popular PCA techniques, SVD is often used to reduce the dimension of a matrix or to extract essential information from a matrix or dataset. Even in the GLYPH<28>eld of machine learning, the use of SVD is expanding more and more [4], [5], [8]GLYPH<21>[10], [12]GLYPH<21>[15]. For example, SVD has been applied to a typical CNN as the derivative of SVD has been mathematically derived recently. The most common example is the computation of a unique square root of an SPD matrix. This method is used for various purposes, such as\nFIGURE 1. Comparison of feature vectors obtained by global average pooling and singular vector pooling.\ncovariance pooling [4], [8], [14], [15] and manifold embedding [5], [8]GLYPH<21>[10]. In addition, there are several cases where a good property that singular vector(s) obtained from SVD has the compressed information is used for global pooling.\nSince global pooling was GLYPH<28>rst introduced in [16], it has evolved in various ways, such as employing trainable variables [17] and adopting non-linear functions [18], [19]. On the other hand, a global pooling method using SVD was proposed [20], which was based on covariance pooling. Covariance pooling obtains the covariance matrix FF T of a feature matrix F and extracts meaningful information from covariance. Reference [4] showed that if matrix logarithm is obtained by SVD, tangent space mapping is applicable. Recently, various covariance pooling methods have been proposed, inspired by [4]. For instance, [14] presented a method to solve the instability of SVD gradients, and [8] reduced the computational cost of SVD through Newton-Schulz iteration. Also, [15] showed that covariance pooling can be effectively applied in the middle of the network.\nHowever, very few studies directly use singular vectors in deep learning. As far as we know, KD-SVD [12] may be the GLYPH<28>rst deep learning technique that uses essential information of singular vectors directly. In KD-SVD, singular vectors are extracted from two feature maps by SVD, a correlation between two singular vectors is computed, and the correlation is distilled as knowledge. To facilitate the learning of singular vectors, KD-SVD uses a teacher network as a guide. However, such an approach is not applicable to general architectures other than the student-teacher network where the teacher network plays a guide role.\nAccording to our survey, the use of SVD in deep learning is continuously being studied, but there is no way to learn singular vectors directly. On the other hand, it is also true that many methods using singular vectors for various computer vision tasks have been studied before the deep learning era [11], [21], [22]. We argue that the reason for this recent research trend is that although singular vectors have good information, there are some bad properties making difGLYPH<28>cult to learn them with common deep learning schemes. Therefore, by analyzing and removing these properties, we intend to propose a way that singular vectors can be utilized in deep learning scheme. Ultimately, the proposed algorithm will provide an opportunity to use SVD in various machine learning applications.\nFIGURE 2. An example of CNN architecture using SVP. At the bottom of the figure, we show the process of transforming the singular vectors obtained through SVD prior to learning and the process of changing the feature distribution through each process.",
    "context": "Contextualizes SVD’s role in PCA and its limited application in deep learning due to singular vector properties, highlighting the proposed solution of transforming them to Euclidean space for effective learning and a novel pooling method (SVP).",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      2,
      3
    ],
    "id": "0c33a10f3144f7d81394a5d861d7c37943e24f9e62bf73be57f536e0e140bc41"
  },
  {
    "text": "We theoretically analyze the undesirable properties that generally make the learning of singular vectors difGLYPH<28>cult. Based on the theoretical analysis, we propose SVP as a solution to overcome the limitations of using singular vectors. Fig. 2 shows the overall structure of SVP, which is a sort of post-processing for learning singular vectors. Feature maps obtained from CNN are GLYPH<28>rst decomposed by SVD, and then the output singular vectors are transformed to facilitate learning by SVP. Finally, the transformed information is classiGLYPH<28>ed.\n\nAnalysis of singular vector learning challenges and introduction of a novel pooling method (SVP) to address these limitations, specifically focusing on eliminating sign ambiguity and mapping non-Euclidean space to Euclidean space for improved learning.",
    "original_text": "We theoretically analyze the undesirable properties that generally make the learning of singular vectors difGLYPH<28>cult. Based on the theoretical analysis, we propose SVP as a solution to overcome the limitations of using singular vectors. Fig. 2 shows the overall structure of SVP, which is a sort of post-processing for learning singular vectors. Feature maps obtained from CNN are GLYPH<28>rst decomposed by SVD, and then the output singular vectors are transformed to facilitate learning by SVP. Finally, the transformed information is classiGLYPH<28>ed.",
    "context": "Analysis of singular vector learning challenges and introduction of a novel pooling method (SVP) to address these limitations, specifically focusing on eliminating sign ambiguity and mapping non-Euclidean space to Euclidean space for improved learning.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      3
    ],
    "id": "8412b8c408d036af00a80def43cbc48e74cabc894234da49bdc924f902cee7dd"
  },
  {
    "text": "Assume that a feature map obtained by a CNN has a spatial shape of H GLYPH<2> W and a feature depth of D , and it can be transformed into a matrix form. Then, the feature map of matrix form can be interpreted as a set composed of HWD -dimensional vectors. We deGLYPH<28>ne this as F . Decomposing F using SVD results in:\n<!-- formula-not-decoded -->\nLeft-hand matrix U has feature speciGLYPH<28>c information, right-hand matrix V has global feature information, and the central matrix 6 indicates singular value magnitudes [1]. Since the singular values are decomposed in descending order, SVD of the same matrices always yields the same 6 . On the other hand, U and V are created in pairs as follows:\n<!-- formula-not-decoded -->\nwhere\n<!-- formula-not-decoded -->\nEq. (2) indicates that singular vectors with identical information can be randomly distributed in two different positions due to random variable pk . This phenomenon is called sign ambiguity [11]. This is an undesirable property that must be removed because it makes learning singular vectors difGLYPH<28>cult. Meanwhile, each singular vector is a unit vector with a norm of 1. In other words, singular vectors are the points on a unit hypersphereGLYPH<22>i.e., a manifold. Therefore, learning singular vectors with a generic neural network and Euclidean geometry can be very inefGLYPH<28>cient or impossible. This is the second problem. In summary, singular vectors have two fundamental problems as follows.\n- GLYPH<15> Singular vectors with similar information are randomly distributed in two areas due to sign ambiguity.\n- GLYPH<15> It is inefGLYPH<28>cient to learn singular vectors on manifold space through a general neural network scheme.\n\nDetails of SVD decomposition and the resulting U and V matrices, highlighting the sign ambiguity issue and the difficulty of learning singular vectors with standard neural networks.",
    "original_text": "Assume that a feature map obtained by a CNN has a spatial shape of H GLYPH<2> W and a feature depth of D , and it can be transformed into a matrix form. Then, the feature map of matrix form can be interpreted as a set composed of HWD -dimensional vectors. We deGLYPH<28>ne this as F . Decomposing F using SVD results in:\n<!-- formula-not-decoded -->\nLeft-hand matrix U has feature speciGLYPH<28>c information, right-hand matrix V has global feature information, and the central matrix 6 indicates singular value magnitudes [1]. Since the singular values are decomposed in descending order, SVD of the same matrices always yields the same 6 . On the other hand, U and V are created in pairs as follows:\n<!-- formula-not-decoded -->\nwhere\n<!-- formula-not-decoded -->\nEq. (2) indicates that singular vectors with identical information can be randomly distributed in two different positions due to random variable pk . This phenomenon is called sign ambiguity [11]. This is an undesirable property that must be removed because it makes learning singular vectors difGLYPH<28>cult. Meanwhile, each singular vector is a unit vector with a norm of 1. In other words, singular vectors are the points on a unit hypersphereGLYPH<22>i.e., a manifold. Therefore, learning singular vectors with a generic neural network and Euclidean geometry can be very inefGLYPH<28>cient or impossible. This is the second problem. In summary, singular vectors have two fundamental problems as follows.\n- GLYPH<15> Singular vectors with similar information are randomly distributed in two areas due to sign ambiguity.\n- GLYPH<15> It is inefGLYPH<28>cient to learn singular vectors on manifold space through a general neural network scheme.",
    "context": "Details of SVD decomposition and the resulting U and V matrices, highlighting the sign ambiguity issue and the difficulty of learning singular vectors with standard neural networks.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      3
    ],
    "id": "4a5cab8b87efe1ae58e0ae6e270551604be45019f31cf289d3709867f5afcc9a"
  },
  {
    "text": "We GLYPH<28>rst describe a solution for learning singular vectors with the manifold property. Since the manifold is a unit hypersphere, it can be mapped to Euclidean space by disentangling each coordinate through a speciGLYPH<28>c coordinate conversion. As a result, it becomes possible to use SVD as a layer of CNN because it is easier to learn singular vectors based on Euclidean geometry and to transform them. (see Sec. III-E).\nHowever, since each coordinate in the spherical space is bounded, the singular vector distribution can be discontinuous near the boundary. This phenomenon can negatively affect the feature embedding of CNN. Therefore, rotation is applied so that the singular vectors are not located near the boundary before coordinate conversion. (see Sec. III-C).\nNext, we present a method to remove the sign ambiguity phenomenon. Suppose that singular vectors are distributed in two regions around a speciGLYPH<28>c center vector due to sign ambiguity. Similarity of each singular vector is calculated based on the center vector. By reversing the sign of a singular vector with negative similarity, the sign ambiguity can be resolved. This method is very simple and efGLYPH<28>cient as in Sec. III-D.\nOn the other hand, it is not easy to determine an accurate center vector because the singular vector distribution can change as learning progresses. Thus, we propose a way to approximate the center vector during learning. This process is performed after coordinate conversion for convenient calculation. (see Sec. III-F)\n\nAddresses challenges in learning singular vectors by mapping a unit hypersphere (manifold) to Euclidean space through coordinate conversion and rotation to mitigate discontinuity and sign ambiguity.",
    "original_text": "We GLYPH<28>rst describe a solution for learning singular vectors with the manifold property. Since the manifold is a unit hypersphere, it can be mapped to Euclidean space by disentangling each coordinate through a speciGLYPH<28>c coordinate conversion. As a result, it becomes possible to use SVD as a layer of CNN because it is easier to learn singular vectors based on Euclidean geometry and to transform them. (see Sec. III-E).\nHowever, since each coordinate in the spherical space is bounded, the singular vector distribution can be discontinuous near the boundary. This phenomenon can negatively affect the feature embedding of CNN. Therefore, rotation is applied so that the singular vectors are not located near the boundary before coordinate conversion. (see Sec. III-C).\nNext, we present a method to remove the sign ambiguity phenomenon. Suppose that singular vectors are distributed in two regions around a speciGLYPH<28>c center vector due to sign ambiguity. Similarity of each singular vector is calculated based on the center vector. By reversing the sign of a singular vector with negative similarity, the sign ambiguity can be resolved. This method is very simple and efGLYPH<28>cient as in Sec. III-D.\nOn the other hand, it is not easy to determine an accurate center vector because the singular vector distribution can change as learning progresses. Thus, we propose a way to approximate the center vector during learning. This process is performed after coordinate conversion for convenient calculation. (see Sec. III-F)",
    "context": "Addresses challenges in learning singular vectors by mapping a unit hypersphere (manifold) to Euclidean space through coordinate conversion and rotation to mitigate discontinuity and sign ambiguity.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      3
    ],
    "id": "98ef740ce6a0ea099f42e369f568298bba0b935dd31045ef14496c77d77c9a0c"
  },
  {
    "text": "Mapping of singular vectors in the spherical coordinate system is equivalent to converting them from non-Euclidean space to Euclidean space. However, when singular vectors\nFIGURE 3. Discontinuity and rotation effects in the spherical coordinate system. Red, green, yellow, and blue lines indicate a discontinuity boundary, SP1, SP2, and rotation, respectively. (a) Discontinuity in spherical coordinates (b) SP1 and SP2 before rotation (c) SP1 and SP2 after rotation.\nare directly mapped to spherical coordinates, a discontinuity problem may occur.\nLet . r ; GLYPH<30> 1 ; GLYPH<30> 2 ; : : : ; GLYPH<30> N GLYPH<0> 1 / denote N -dimensional spherical coordinates. Each component of .GLYPH<30> 1 ; GLYPH<30> 2 ; : : : ; GLYPH<30> N GLYPH<0> 2 / has a bounded range of [0 ; GLYPH<25> ], and GLYPH<30> N GLYPH<0> 1 is limited to [ GLYPH<0> GLYPH<25> ; GLYPH<25> ], which means that discontinuity may occur at the boundary of each component, as shown in Fig. 3(a). For convenience, the three-dimensional space is assumed in the GLYPH<28>gure. If two feature vectors v and v t are present near the discontinuity boundary, as in Fig. 3(b), we can consider two different shortest paths: one through the discontinuity boundary (SP1) and another to avoid the discontinuity boundary (SP2). Therefore, the learning efGLYPH<28>ciency may be reduced because singular vectors can practically move to longer paths such as SP2 rather than the real shortest path (SP1) during the learning process. Thus, singular vectors should be kept as distant from the discontinuity boundary as possible. In other words, the singular vector distribution must be intentionally centered in the spherical coordinate system. The center of the spherical coordinate system GLYPH<8> GLYPH<25> 2 ; GLYPH<25> 2 ; : : : ; GLYPH<25> 2 ; 0 GLYPH<9> is converted to f 0 ; : : : ; 0 ; 1 ; 0 g of the Cartesian coordinate using spherical to Cartesian coordinates conversion equation in [23].\nAs a result, the center of the singular vector distribution v c must be rotated so that it becomes the coordinate center, e v c which is f 0 ; : : : ; 0 ; 1 ; 0 g . How to obtain v c will be given in Sec. III-F. Then, the rotation matrix R is computed by using the well-known Rodrigues rotation as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere k is the direction of the rotation axis. Using the rotation matrix of Eq. (6), e V is obtained by:\n<!-- formula-not-decoded -->\nWe can obtain e U by applying the same process to U .\nAs shown in Fig. 3(c), the shortest path and the real path now coincide. However, if singular vector variance is too high, some singular vectors still exist near the discontinuity boundary. Fortunately, SVD computes a least-square solution which is not sparse, and regularization skills such as weight\ndecay and batch normalization [24] also control feature map's variance to some extent. So, we can ignore this high variance problem. Therefore, the discontinuity problem is eliminated so that a singular vector can be effectively learned. However, the two problems of singular vectors mentioned in Section 3.1 still remain unsolved.\n\nAddresses the discontinuity problem arising from mapping spherical coordinates to Euclidean space, proposing a rotation strategy to center singular vectors and minimize learning inefficiencies.",
    "original_text": "Mapping of singular vectors in the spherical coordinate system is equivalent to converting them from non-Euclidean space to Euclidean space. However, when singular vectors\nFIGURE 3. Discontinuity and rotation effects in the spherical coordinate system. Red, green, yellow, and blue lines indicate a discontinuity boundary, SP1, SP2, and rotation, respectively. (a) Discontinuity in spherical coordinates (b) SP1 and SP2 before rotation (c) SP1 and SP2 after rotation.\nare directly mapped to spherical coordinates, a discontinuity problem may occur.\nLet . r ; GLYPH<30> 1 ; GLYPH<30> 2 ; : : : ; GLYPH<30> N GLYPH<0> 1 / denote N -dimensional spherical coordinates. Each component of .GLYPH<30> 1 ; GLYPH<30> 2 ; : : : ; GLYPH<30> N GLYPH<0> 2 / has a bounded range of [0 ; GLYPH<25> ], and GLYPH<30> N GLYPH<0> 1 is limited to [ GLYPH<0> GLYPH<25> ; GLYPH<25> ], which means that discontinuity may occur at the boundary of each component, as shown in Fig. 3(a). For convenience, the three-dimensional space is assumed in the GLYPH<28>gure. If two feature vectors v and v t are present near the discontinuity boundary, as in Fig. 3(b), we can consider two different shortest paths: one through the discontinuity boundary (SP1) and another to avoid the discontinuity boundary (SP2). Therefore, the learning efGLYPH<28>ciency may be reduced because singular vectors can practically move to longer paths such as SP2 rather than the real shortest path (SP1) during the learning process. Thus, singular vectors should be kept as distant from the discontinuity boundary as possible. In other words, the singular vector distribution must be intentionally centered in the spherical coordinate system. The center of the spherical coordinate system GLYPH<8> GLYPH<25> 2 ; GLYPH<25> 2 ; : : : ; GLYPH<25> 2 ; 0 GLYPH<9> is converted to f 0 ; : : : ; 0 ; 1 ; 0 g of the Cartesian coordinate using spherical to Cartesian coordinates conversion equation in [23].\nAs a result, the center of the singular vector distribution v c must be rotated so that it becomes the coordinate center, e v c which is f 0 ; : : : ; 0 ; 1 ; 0 g . How to obtain v c will be given in Sec. III-F. Then, the rotation matrix R is computed by using the well-known Rodrigues rotation as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere k is the direction of the rotation axis. Using the rotation matrix of Eq. (6), e V is obtained by:\n<!-- formula-not-decoded -->\nWe can obtain e U by applying the same process to U .\nAs shown in Fig. 3(c), the shortest path and the real path now coincide. However, if singular vector variance is too high, some singular vectors still exist near the discontinuity boundary. Fortunately, SVD computes a least-square solution which is not sparse, and regularization skills such as weight\ndecay and batch normalization [24] also control feature map's variance to some extent. So, we can ignore this high variance problem. Therefore, the discontinuity problem is eliminated so that a singular vector can be effectively learned. However, the two problems of singular vectors mentioned in Section 3.1 still remain unsolved.",
    "context": "Addresses the discontinuity problem arising from mapping spherical coordinates to Euclidean space, proposing a rotation strategy to center singular vectors and minimize learning inefficiencies.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      3,
      4
    ],
    "id": "e473d704fd3f305edc8be733a6418f2acc3ab0dd7ae8cc7d20a6a947ca7c68be"
  },
  {
    "text": "U and V in Eq. (1) exist as a pair, and one of them is used as a reference. We adopt an approach of aligning e V based on e U as a reference. Let e u k be the k -th singular vector. Then e u k and GLYPH<0> e u k exist concurrently with the same information but opposite signs. Therefore, a domain in which a singular vector exists can be divided into a positive half-sphere that is decomposed in the forward direction and a negative half-sphere that is decomposed in the reverse direction. Thus, we eliminate the randomness property by multiplying a singular vector on the negative half-sphere by GLYPH<0> 1. In the previous section, the center vector of the singular vector distribution is rotated to e u c , which is f 0 ; 0 ; : : : ; 0 ; 1 ; 0 g . So we can verify the hyper-sphere where e u k is in according to the sign of e uHW GLYPH<0> 1 ; k . Finally, O U and O V that are arranged based on e u c are deGLYPH<28>ned as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nSince the sign ambiguity phenomenon in O U and O V has been removed, O U and O V are easier to analyze than the original e U and e V .\n\nThe chunk describes a method to address sign ambiguity in singular vectors, achieved by aligning one singular vector (V) relative to another (U) as a reference and rotating the distribution of V to center it based on the sign of the alignment. This process eliminates randomness and simplifies analysis compared to the original singular vectors.",
    "original_text": "U and V in Eq. (1) exist as a pair, and one of them is used as a reference. We adopt an approach of aligning e V based on e U as a reference. Let e u k be the k -th singular vector. Then e u k and GLYPH<0> e u k exist concurrently with the same information but opposite signs. Therefore, a domain in which a singular vector exists can be divided into a positive half-sphere that is decomposed in the forward direction and a negative half-sphere that is decomposed in the reverse direction. Thus, we eliminate the randomness property by multiplying a singular vector on the negative half-sphere by GLYPH<0> 1. In the previous section, the center vector of the singular vector distribution is rotated to e u c , which is f 0 ; 0 ; : : : ; 0 ; 1 ; 0 g . So we can verify the hyper-sphere where e u k is in according to the sign of e uHW GLYPH<0> 1 ; k . Finally, O U and O V that are arranged based on e u c are deGLYPH<28>ned as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nSince the sign ambiguity phenomenon in O U and O V has been removed, O U and O V are easier to analyze than the original e U and e V .",
    "context": "The chunk describes a method to address sign ambiguity in singular vectors, achieved by aligning one singular vector (V) relative to another (U) as a reference and rotating the distribution of V to center it based on the sign of the alignment. This process eliminates randomness and simplifies analysis compared to the original singular vectors.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      4
    ],
    "id": "5af1b979401dce08bea1c289fa93b9ce0c0c34adbd78c050d2bc01dfa47edc8e"
  },
  {
    "text": "To convert the coordinate system of O V , it is rewritten as:\n<!-- formula-not-decoded -->\nThen, the D -dimensional transformation of O V in Cartesian coordinates into spherical coordinates is deGLYPH<28>ned as follows:\n<!-- formula-not-decoded -->\nwhere\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nIn Eq. (13), GLYPH<15> is a constant to prevent zero-division and is set to 10 GLYPH<0> 3 . So, N V on the spherical coordinate system is obtained. However, the 1 st derivative of the arccosine in Eq. (12) can easily diverge, as shown in Eq. (14).\n<!-- formula-not-decoded -->\nSo, we approximate the arccosine to GLYPH<0> z C GLYPH<25> 2 by the GLYPH<28>rst order Taylor expansion. Next, we add a constant term for keeping a zero-centered feature. Finally, Eq. (12) is re-written by\n<!-- formula-not-decoded -->\nAs a result, O V is converted to N V that can be learned by a generic neural network. At the same manner, N U is derived.\n\nThis chunk details the mathematical transformation of O V into N V, involving an approximation of the arccosine function and the addition of a constant term to maintain a zero-centered feature. It highlights the process of converting O V from Cartesian to spherical coordinates, emphasizing the need to avoid divergence in the derivative of the arccosine function.",
    "original_text": "To convert the coordinate system of O V , it is rewritten as:\n<!-- formula-not-decoded -->\nThen, the D -dimensional transformation of O V in Cartesian coordinates into spherical coordinates is deGLYPH<28>ned as follows:\n<!-- formula-not-decoded -->\nwhere\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nIn Eq. (13), GLYPH<15> is a constant to prevent zero-division and is set to 10 GLYPH<0> 3 . So, N V on the spherical coordinate system is obtained. However, the 1 st derivative of the arccosine in Eq. (12) can easily diverge, as shown in Eq. (14).\n<!-- formula-not-decoded -->\nSo, we approximate the arccosine to GLYPH<0> z C GLYPH<25> 2 by the GLYPH<28>rst order Taylor expansion. Next, we add a constant term for keeping a zero-centered feature. Finally, Eq. (12) is re-written by\n<!-- formula-not-decoded -->\nAs a result, O V is converted to N V that can be learned by a generic neural network. At the same manner, N U is derived.",
    "context": "This chunk details the mathematical transformation of O V into N V, involving an approximation of the arccosine function and the addition of a constant term to maintain a zero-centered feature. It highlights the process of converting O V from Cartesian to spherical coordinates, emphasizing the need to avoid divergence in the derivative of the arccosine function.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      4,
      5
    ],
    "id": "b7043fe9d636a147413ffa99723c6cc58e39d76fe6924560d1cb8d2206781bef"
  },
  {
    "text": "In this section, the process of estimating the center vector mentioned in Sec. III-C is described. Since this process has to be explained in connection with Sec. III-D, it is described with u c instead of v c . The proposed method rotates u around u c , moves the center of the distribution to f 0 ; 0 ; : : : ; 0 ; 1 ; 0 g , and determines which half-sphere it belongs to through the sign of e uHW GLYPH<0> 1 ; k . If u c is inaccurate, this process will be erroneous. Thereby, the sign ambiguity problem cannot be solved properly. To observe this phenomenon, the distribution of N u obtained from u c is shown in the GLYPH<28>rst row of Fig. 4. Here u c is experimentally set to a unit vector with all components of the same size. We can GLYPH<28>nd that the distributions of the two half-spheres are not aligned correctly because of the sign ambiguity problem caused by inaccurate u c . To obtain the precise u c , we suggest learning of u c . Since we are hard to understand the statistical characteristics of singular vectors due to two inherent bad properties, we learn u c based on N u . Note that each component of features produced by neural networks follows the Gaussian distribution in general. So, assuming that each component of the aligned N u has the Gaussian distribution, u c is learned through KL-divergence [25] of Eq. (16) so that the assumption is correct.\n<!-- formula-not-decoded -->\nwhere Norm GLYPH<16> GLYPH<22> N u b ; k ; GLYPH<27> 2 N u b ; k GLYPH<17> indicates a Gaussian distribution. Furthermore, GLYPH<27> 2 N u b ; k D var GLYPH<0> N u b ; k GLYPH<1> , and mean GLYPH<22> N u b ; k is empirically set to 0. B is the batch size. As shown in Fig. 4, in order to minimize L KL N u b ; k , two distributions of N u get merged and form a single Gaussian distribution, which means that the sign ambiguity problem is solved. Therefore, the proposed method can estimate u c overcoming two bad properties of singular vectors. In the same manner, v c is estimated by minimizing L KL N v b ; k . Note that L KL N u b ; k and L KL N v b ; k are loss functions for learning u c and v c only and do not affect the network parameters.\n\nDescribes the process of estimating the center vector, highlighting the challenge of inaccurate center vector estimation and proposing a learning-based approach to correct it.",
    "original_text": "In this section, the process of estimating the center vector mentioned in Sec. III-C is described. Since this process has to be explained in connection with Sec. III-D, it is described with u c instead of v c . The proposed method rotates u around u c , moves the center of the distribution to f 0 ; 0 ; : : : ; 0 ; 1 ; 0 g , and determines which half-sphere it belongs to through the sign of e uHW GLYPH<0> 1 ; k . If u c is inaccurate, this process will be erroneous. Thereby, the sign ambiguity problem cannot be solved properly. To observe this phenomenon, the distribution of N u obtained from u c is shown in the GLYPH<28>rst row of Fig. 4. Here u c is experimentally set to a unit vector with all components of the same size. We can GLYPH<28>nd that the distributions of the two half-spheres are not aligned correctly because of the sign ambiguity problem caused by inaccurate u c . To obtain the precise u c , we suggest learning of u c . Since we are hard to understand the statistical characteristics of singular vectors due to two inherent bad properties, we learn u c based on N u . Note that each component of features produced by neural networks follows the Gaussian distribution in general. So, assuming that each component of the aligned N u has the Gaussian distribution, u c is learned through KL-divergence [25] of Eq. (16) so that the assumption is correct.\n<!-- formula-not-decoded -->\nwhere Norm GLYPH<16> GLYPH<22> N u b ; k ; GLYPH<27> 2 N u b ; k GLYPH<17> indicates a Gaussian distribution. Furthermore, GLYPH<27> 2 N u b ; k D var GLYPH<0> N u b ; k GLYPH<1> , and mean GLYPH<22> N u b ; k is empirically set to 0. B is the batch size. As shown in Fig. 4, in order to minimize L KL N u b ; k , two distributions of N u get merged and form a single Gaussian distribution, which means that the sign ambiguity problem is solved. Therefore, the proposed method can estimate u c overcoming two bad properties of singular vectors. In the same manner, v c is estimated by minimizing L KL N v b ; k . Note that L KL N u b ; k and L KL N v b ; k are loss functions for learning u c and v c only and do not affect the network parameters.",
    "context": "Describes the process of estimating the center vector, highlighting the challenge of inaccurate center vector estimation and proposing a learning-based approach to correct it.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      5
    ],
    "id": "cc76a4baade3dbd9df09debf62c0bad7df09f12d10c1ffa7c8ac66858786ce11"
  },
  {
    "text": "We performed three experiments to verify the proposed method. First, we evaluated the performance of SVP in a CNN structure when using SVD. Because classiGLYPH<28>cation task outputs only class information, we use one singular vector\nFIGURE 4. Learning process of singular vectors which are converted to the spherical coordinate. The left column is plotting the last two components and the right column is the distribution. The black line shows a Gaussian distribution, and the blue and yellow lines indicate the distributions of singular vectors in two half-spheres obtained by applying the proposed method. During the learning process, it gradually gets closer to the Gaussian distribution, indicating that the basis is well learned.\nfor the classiGLYPH<28>cation task. Second, we applied the proposed method to KD-SVD, which is a representative network using singular vectors. Since KD-SVD compresses CNN's feature maps with several dominant singular vectors, the performance of the student network improves. In this experiment, weadditionally show an effect of using multiple singular vectors for extracting important information from feature maps. Third, the performance of the proposed method is analyzed through an ablation study.\n\nEvaluates the proposed singular vector pooling method (SVP) in CNNs using SVD; demonstrates improved performance with KD-SVD and highlights the benefit of using multiple singular vectors; and presents results from an ablation study confirming SVP’s effectiveness.",
    "original_text": "We performed three experiments to verify the proposed method. First, we evaluated the performance of SVP in a CNN structure when using SVD. Because classiGLYPH<28>cation task outputs only class information, we use one singular vector\nFIGURE 4. Learning process of singular vectors which are converted to the spherical coordinate. The left column is plotting the last two components and the right column is the distribution. The black line shows a Gaussian distribution, and the blue and yellow lines indicate the distributions of singular vectors in two half-spheres obtained by applying the proposed method. During the learning process, it gradually gets closer to the Gaussian distribution, indicating that the basis is well learned.\nfor the classiGLYPH<28>cation task. Second, we applied the proposed method to KD-SVD, which is a representative network using singular vectors. Since KD-SVD compresses CNN's feature maps with several dominant singular vectors, the performance of the student network improves. In this experiment, weadditionally show an effect of using multiple singular vectors for extracting important information from feature maps. Third, the performance of the proposed method is analyzed through an ablation study.",
    "context": "Evaluates the proposed singular vector pooling method (SVP) in CNNs using SVD; demonstrates improved performance with KD-SVD and highlights the benefit of using multiple singular vectors; and presents results from an ablation study confirming SVP’s effectiveness.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      5
    ],
    "id": "00c79b643eb8046e6f72a9958c56c406901c14c40df1cde35c960dc4f4e2bef2"
  },
  {
    "text": "The datasets used in the following experiments are CIFAR10, CIFAR100 [26], Tiny-ImageNet, and ImageNet2012 [27], which are all normalized to have values of [-0.5, 0.5]. CIFAR10 and CIFAR100 have 10 and 100 labels, respectively, and are compact datasets consisting of 50,000 color images of 32 GLYPH<2> 32 pixels. Because these datasets require relatively little cost, they are used for basic veriGLYPH<28>cation of the proposed method. Tiny-ImageNet is a medium-sized dataset with 100,000 color images of 64 GLYPH<2> 64 pixels and has 200 labels. ImageNet2012 is a large dataset with more than 1.5 million high-resolution color images and 1,000 labels. The last two datasets are used to prove that the proposed method works well even for huge datasets with large resolutions.\nA particular augmentation is applied to each dataset. Horizontal random GLYPH<29>ip is applied to all the datasets. Also, the images of CIFAR10 and CIFAR100 are zero-padded by 4 pixels, and the images of Tiny-ImageNet [27] are zero-padded by 8 pixels. Then, the zero-padded images are randomly cropped to the original size. In case of ImageNet2012, all\nFIGURE 5. The block diagrams of the networks introduced in this paper.\nthe images are resized to 256 GLYPH<2> 256, and then the training image are produced by randomly cropping the resized images to 224 GLYPH<2> 224. The test set is constructed by cutting the central area of each image to 224 GLYPH<2> 224.\nAll algorithms are implemented using TensorGLYPH<29>ow [28], and their network architectures are shown in Fig. 5. 'CONV' and 'Max Pool' indicate convolutional layer and max pooling layer, respectively. 'FC' stands for fully connected layer. Also H GLYPH<2> W is the kernel size, D is the output depth. s means stride and the same layer repeats by N . (Label) indicates the number of labels in the target dataset. The activation function for convolution layers and all FC layers except the end of the network is ReLU [29]. In Fig. 5(a), 5(b) and 5(c), one of SVP, global average pooling (GAP) [16], global max pooling (GMP) [18], and matrix power normalized covariance pooling (MPN) [8] is adopted for pooling block. Also, in Fig. 5(a), 5(b), 5(c) and 5(d), batch normalization [24] is used next to the convolutional layer. The dotted gray boxes in Fig. 5(e) and 5(f) are the layer modules for knowledge distillation. The input and output feature maps of this module are sensed and applied to both the authentic KD-SVD [12] and the KD-SVP where SVP was applied to KD-SVD.\nThe weights of all networks are initialized with He's initialization [30], and L 2 regularization is applied. A stochastic gradient descent (SGD) [31] is used as the optimizer, and a Nesterov accelerated gradient [32] is applied. All numerical values in the tables and GLYPH<28>gures are the averages of all GLYPH<28>ve trials.\n\nDescribes the datasets and augmentation techniques used in the experiments.",
    "original_text": "The datasets used in the following experiments are CIFAR10, CIFAR100 [26], Tiny-ImageNet, and ImageNet2012 [27], which are all normalized to have values of [-0.5, 0.5]. CIFAR10 and CIFAR100 have 10 and 100 labels, respectively, and are compact datasets consisting of 50,000 color images of 32 GLYPH<2> 32 pixels. Because these datasets require relatively little cost, they are used for basic veriGLYPH<28>cation of the proposed method. Tiny-ImageNet is a medium-sized dataset with 100,000 color images of 64 GLYPH<2> 64 pixels and has 200 labels. ImageNet2012 is a large dataset with more than 1.5 million high-resolution color images and 1,000 labels. The last two datasets are used to prove that the proposed method works well even for huge datasets with large resolutions.\nA particular augmentation is applied to each dataset. Horizontal random GLYPH<29>ip is applied to all the datasets. Also, the images of CIFAR10 and CIFAR100 are zero-padded by 4 pixels, and the images of Tiny-ImageNet [27] are zero-padded by 8 pixels. Then, the zero-padded images are randomly cropped to the original size. In case of ImageNet2012, all\nFIGURE 5. The block diagrams of the networks introduced in this paper.\nthe images are resized to 256 GLYPH<2> 256, and then the training image are produced by randomly cropping the resized images to 224 GLYPH<2> 224. The test set is constructed by cutting the central area of each image to 224 GLYPH<2> 224.\nAll algorithms are implemented using TensorGLYPH<29>ow [28], and their network architectures are shown in Fig. 5. 'CONV' and 'Max Pool' indicate convolutional layer and max pooling layer, respectively. 'FC' stands for fully connected layer. Also H GLYPH<2> W is the kernel size, D is the output depth. s means stride and the same layer repeats by N . (Label) indicates the number of labels in the target dataset. The activation function for convolution layers and all FC layers except the end of the network is ReLU [29]. In Fig. 5(a), 5(b) and 5(c), one of SVP, global average pooling (GAP) [16], global max pooling (GMP) [18], and matrix power normalized covariance pooling (MPN) [8] is adopted for pooling block. Also, in Fig. 5(a), 5(b), 5(c) and 5(d), batch normalization [24] is used next to the convolutional layer. The dotted gray boxes in Fig. 5(e) and 5(f) are the layer modules for knowledge distillation. The input and output feature maps of this module are sensed and applied to both the authentic KD-SVD [12] and the KD-SVP where SVP was applied to KD-SVD.\nThe weights of all networks are initialized with He's initialization [30], and L 2 regularization is applied. A stochastic gradient descent (SGD) [31] is used as the optimizer, and a Nesterov accelerated gradient [32] is applied. All numerical values in the tables and GLYPH<28>gures are the averages of all GLYPH<28>ve trials.",
    "context": "Describes the datasets and augmentation techniques used in the experiments.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      5,
      6
    ],
    "id": "4c75524b7630f60d6eec171cd1306599bed278b713001658dd306f2fe4fa6113"
  },
  {
    "text": "Most CNNs increase the size of the receptive GLYPH<28>eld and reduce the size of the feature map through pooling. GMP and GAP are the most popular pooling methods, but both methods suffer from the inherent loss of information. SVP can improve the performance of a given network because it can reduce the size of feature maps while keeping as much of their information as possible.\nThe base network used for this experiment is ResNet32 [30] which described in Fig. 5(a). We analyzed the results by replacing GAP of ResNet-32 with GMP [18] and SVP, respectively. In case of CIFAR10, learning is proceeded for 200 epochs and the initial learning rate is equal to 10 GLYPH<0> 2 , which is reduced by 0.1 times at 100 and 150 epochs. The batch size is set to 128, and the weight decay of L 2 regularization is GLYPH<28>xed to 10 GLYPH<0> 4 . ImageNet2012 is trained for 90 epochs, and the initial learning rate is set to 10 GLYPH<0> 2 , and the learning rate is reduced by 0.1 times at 30 and 60 epochs. The batch size in this experiment is 128 and the weight decay of L 2 regularization is GLYPH<28>xed to 5 GLYPH<2> 10 GLYPH<0> 4 .\nTABLE 1. The comparison of the proposed SVP with GAP and GMP for CIFAR10 dataset. 'ACC' indicates accuracy, 'Silh' indicates silhouette score, and T indicates forward time for each pooling layer.\nTable 1 shows the experimental results for CIFAR10. We can GLYPH<28>nd the validation accuracy of SVP is 0.67% and 0.91% better than that of GAP and GMP, respectively. However, this is not a big improvement and almost same with MPN. Nevertheless, the silhouette score [33] of SVP on training dataset is better by 0.444, 0.510, and 0.551 than that of GAP, GMP, and MPN, respectively. SVP shows excellent silhouette scores that are more than twice those of the\nFIGURE 6. The distribution of feature vectors obtained by several pooling techniques for CIFAR10 training set.\nother pooling methods. This indicate that SVP makes feature vectors get a high inter-class variance and a low intra-class variance. Also, the proposed method has a lower forward time than MPN, which is a light-weight second-order pooling method. Therefore, we can GLYPH<28>nd that the proposed method can provide better clustering performance with less computation time.\nAlso, to illustrate the clustering performance clearly, we performed the same experiment through a network with an additional FC layer having a two-dimensional output as shown in Fig. 5(b). The network is learned for CIFAR10 dataset until overGLYPH<28>tting occurs in order to discriminate clusters clearly. The feature distributions are shown in Fig. 6. The feature distributions of GAP, GMP, and MPN have very large intra-class variation, and the features tend to gather near the origin. That is, their inter-class variation is low. However, the feature distribution of SVP shows better separation than those of GAP, GMP, and MPN. This means that SVP makes a given network operate more robustly to noise and attack. In addition, we show the evaluation results for ImageNet2012 through ResNet-18 of Fig. 5(c) to generalize the proposed method (see Table 2). We could observe a trend similar to the results in CIFAR10 dataset, which means that SVP extracts essential information well even with large feature maps.\nTABLE 2. The comparison of SVP with GAP for ImageNet2012 dataset.\n\nEvaluates the proposed SVP method against GAP and GMP for CIFAR10, demonstrating SVP’s improved accuracy and silhouette score compared to the other pooling techniques.  The analysis highlights SVP’s ability to reduce feature map size while maintaining information, alongside its lower computational cost.",
    "original_text": "Most CNNs increase the size of the receptive GLYPH<28>eld and reduce the size of the feature map through pooling. GMP and GAP are the most popular pooling methods, but both methods suffer from the inherent loss of information. SVP can improve the performance of a given network because it can reduce the size of feature maps while keeping as much of their information as possible.\nThe base network used for this experiment is ResNet32 [30] which described in Fig. 5(a). We analyzed the results by replacing GAP of ResNet-32 with GMP [18] and SVP, respectively. In case of CIFAR10, learning is proceeded for 200 epochs and the initial learning rate is equal to 10 GLYPH<0> 2 , which is reduced by 0.1 times at 100 and 150 epochs. The batch size is set to 128, and the weight decay of L 2 regularization is GLYPH<28>xed to 10 GLYPH<0> 4 . ImageNet2012 is trained for 90 epochs, and the initial learning rate is set to 10 GLYPH<0> 2 , and the learning rate is reduced by 0.1 times at 30 and 60 epochs. The batch size in this experiment is 128 and the weight decay of L 2 regularization is GLYPH<28>xed to 5 GLYPH<2> 10 GLYPH<0> 4 .\nTABLE 1. The comparison of the proposed SVP with GAP and GMP for CIFAR10 dataset. 'ACC' indicates accuracy, 'Silh' indicates silhouette score, and T indicates forward time for each pooling layer.\nTable 1 shows the experimental results for CIFAR10. We can GLYPH<28>nd the validation accuracy of SVP is 0.67% and 0.91% better than that of GAP and GMP, respectively. However, this is not a big improvement and almost same with MPN. Nevertheless, the silhouette score [33] of SVP on training dataset is better by 0.444, 0.510, and 0.551 than that of GAP, GMP, and MPN, respectively. SVP shows excellent silhouette scores that are more than twice those of the\nFIGURE 6. The distribution of feature vectors obtained by several pooling techniques for CIFAR10 training set.\nother pooling methods. This indicate that SVP makes feature vectors get a high inter-class variance and a low intra-class variance. Also, the proposed method has a lower forward time than MPN, which is a light-weight second-order pooling method. Therefore, we can GLYPH<28>nd that the proposed method can provide better clustering performance with less computation time.\nAlso, to illustrate the clustering performance clearly, we performed the same experiment through a network with an additional FC layer having a two-dimensional output as shown in Fig. 5(b). The network is learned for CIFAR10 dataset until overGLYPH<28>tting occurs in order to discriminate clusters clearly. The feature distributions are shown in Fig. 6. The feature distributions of GAP, GMP, and MPN have very large intra-class variation, and the features tend to gather near the origin. That is, their inter-class variation is low. However, the feature distribution of SVP shows better separation than those of GAP, GMP, and MPN. This means that SVP makes a given network operate more robustly to noise and attack. In addition, we show the evaluation results for ImageNet2012 through ResNet-18 of Fig. 5(c) to generalize the proposed method (see Table 2). We could observe a trend similar to the results in CIFAR10 dataset, which means that SVP extracts essential information well even with large feature maps.\nTABLE 2. The comparison of SVP with GAP for ImageNet2012 dataset.",
    "context": "Evaluates the proposed SVP method against GAP and GMP for CIFAR10, demonstrating SVP’s improved accuracy and silhouette score compared to the other pooling techniques.  The analysis highlights SVP’s ability to reduce feature map size while maintaining information, alongside its lower computational cost.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      6,
      7
    ],
    "id": "33456d172209fdb82cdd35e5857d51a327b14ebb670d68fc25f3c43e07fe4de2"
  },
  {
    "text": "We next examined the robustness of SVP against adversarial attacks. The attack methods include the fast gradient sign method (FGSM) [34] and basic iterative method (BIM) [35]. FGSM is a gradient-based adversarial attack and one of the simplest and most effective attack methods. The image X adv generated by FGSM is deGLYPH<28>ned by:\n<!-- formula-not-decoded -->\nwhere X is an input image, ytrue is the label, and J stands for the Jacobian. The attack rate GLYPH<11> is set to 0.01 in this experiment.\nSince BIM performs FGSM iteratively, BIM generally attacks more strongly than FGSM. The I -th image generated by BIM is deGLYPH<28>ned by Eqs. (18) and (19):\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere GLYPH<12> is the clipping threshold. The following values are used in this experiment: GLYPH<11> D 0 : 01, GLYPH<12> D 0 : 002, and T D 5. All hyper-parameters are set as in Sec. IV-B.\nTABLE 3. Experimental result on white-box adversarial attack. The first three rows correspond to natural training, and the second three rows are the result of adversarial training.\nTo compare the results of Sec. IV-B, the CIFAR10 dataset was employed in this experiment, using ResNet-32 in Fig. 5(a) and ResNet-32-plot in Fig. 5(b) Table 3 shows the white-box attack results for several pooling methods. For FGSM with natural and adversarial training, SVP shows 36.07%, 28.18%, and 34.21% higher accuracies than GAP, GMP, and MPN, respectively. Overall, SVP provides the best performance for all other cases. The performance improvement is interpreted through the high silhouette score of the feature vector obtained by SVP. Fig 7 plots the feature vector distributions by employing the network used in Fig. 6. In the other pooling methods, the clusters are scattered due to the attack, but SVP maintains a relatively strong inter-cluster\nFIGURE 7. The distribution of feature vectors obtained by several pooling techniques for an adversarial attacked CIFAR10 training set.\nTABLE 4. Experimental results on black-box adversarial attack. The first three rows represent the natural training, and the second three rows represent the adversarial training results.\ndistance. Furthermore, Fig. 8 shows the performance changes according to the attack rates. As the attack rate increases, the gaps between SVP and the other pooling methods increase. For example, when GLYPH<11> is 0.02, SVP shows 40.99%, 33.60%, and 40.62% better than GAP, GMP, and MPN, respectively.\nA black-box attack is a method of training and testing a target network using attacked data generated by an arbitrary network. The network used for attacking is VGG-16 [36], and the experimental results are shown in Table 4. SVP still outperforms GAP, GMP, and MPN, but the gap is somewhat reduced. This occurs because SVP is located at the end of the network and it is difGLYPH<28>cult for it to extract essential information from a feature map that has already been destroyed. The experiments with adversarial attacks prove that the proposed method is robust to noise. Despite the limitations, the proposed method makes the given network robust against noise without requiring additional techniques for singular vectors.\n\nAnalyzes the robustness of SVP against adversarial attacks, demonstrating its superior performance compared to other pooling methods like GAP and GMP in both white-box and black-box settings.",
    "original_text": "We next examined the robustness of SVP against adversarial attacks. The attack methods include the fast gradient sign method (FGSM) [34] and basic iterative method (BIM) [35]. FGSM is a gradient-based adversarial attack and one of the simplest and most effective attack methods. The image X adv generated by FGSM is deGLYPH<28>ned by:\n<!-- formula-not-decoded -->\nwhere X is an input image, ytrue is the label, and J stands for the Jacobian. The attack rate GLYPH<11> is set to 0.01 in this experiment.\nSince BIM performs FGSM iteratively, BIM generally attacks more strongly than FGSM. The I -th image generated by BIM is deGLYPH<28>ned by Eqs. (18) and (19):\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere GLYPH<12> is the clipping threshold. The following values are used in this experiment: GLYPH<11> D 0 : 01, GLYPH<12> D 0 : 002, and T D 5. All hyper-parameters are set as in Sec. IV-B.\nTABLE 3. Experimental result on white-box adversarial attack. The first three rows correspond to natural training, and the second three rows are the result of adversarial training.\nTo compare the results of Sec. IV-B, the CIFAR10 dataset was employed in this experiment, using ResNet-32 in Fig. 5(a) and ResNet-32-plot in Fig. 5(b) Table 3 shows the white-box attack results for several pooling methods. For FGSM with natural and adversarial training, SVP shows 36.07%, 28.18%, and 34.21% higher accuracies than GAP, GMP, and MPN, respectively. Overall, SVP provides the best performance for all other cases. The performance improvement is interpreted through the high silhouette score of the feature vector obtained by SVP. Fig 7 plots the feature vector distributions by employing the network used in Fig. 6. In the other pooling methods, the clusters are scattered due to the attack, but SVP maintains a relatively strong inter-cluster\nFIGURE 7. The distribution of feature vectors obtained by several pooling techniques for an adversarial attacked CIFAR10 training set.\nTABLE 4. Experimental results on black-box adversarial attack. The first three rows represent the natural training, and the second three rows represent the adversarial training results.\ndistance. Furthermore, Fig. 8 shows the performance changes according to the attack rates. As the attack rate increases, the gaps between SVP and the other pooling methods increase. For example, when GLYPH<11> is 0.02, SVP shows 40.99%, 33.60%, and 40.62% better than GAP, GMP, and MPN, respectively.\nA black-box attack is a method of training and testing a target network using attacked data generated by an arbitrary network. The network used for attacking is VGG-16 [36], and the experimental results are shown in Table 4. SVP still outperforms GAP, GMP, and MPN, but the gap is somewhat reduced. This occurs because SVP is located at the end of the network and it is difGLYPH<28>cult for it to extract essential information from a feature map that has already been destroyed. The experiments with adversarial attacks prove that the proposed method is robust to noise. Despite the limitations, the proposed method makes the given network robust against noise without requiring additional techniques for singular vectors.",
    "context": "Analyzes the robustness of SVP against adversarial attacks, demonstrating its superior performance compared to other pooling methods like GAP and GMP in both white-box and black-box settings.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      8,
      7
    ],
    "id": "4b1097296e4f342c435ec2f7e4ac692fedb233cfec826260aa3bf380d0cb1371"
  },
  {
    "text": "The feature vectors obtained by KD-SVP are easier to analyze than original singular vectors. In KD-SVD, the sign ambiguity is removed by aligning the singular vectors of the student network based on the singular vector of the teacher network. In addition, KD-SVD learns only similar features to learn the manifold features and deGLYPH<28>nes the correlation by a radial basis function (RBF), which has smaller values as the distance becomes larger. In other words, singular vectors\nFIGURE 8. The performance change according to the intensity of adversarial attack.\nare post-processed in a very naive manner in KD-SVD. Therefore, if SVP plays a post-processing role, the learning accuracy will improve. The correlation between feature vectors obtained by SVP is computed by RBF, similarly to KD-SVD.TheCIFAR100andTiny-ImageNet are used in this experiment. The teacher network is VGG-16, and the student network is a condensed network that uses only one convolution of the same depth as VGG-16, which are described in Fig. 5(e) and 5(f). KD-SVD and KD-SVP are all learned with the same hyper-parameters. Their learning progressed by 200 epochs, with an initial learning rate of 10 GLYPH<0> 2 and a reduction of 0.1 in 100 and 150 epochs. Also, the batch size is set to 128 and the weight decay of L 2 regularization is 10 GLYPH<0> 4 .\nTABLE 5. Performance of KD-SVP according to the number of singular vectors.\nTable 5 shows that when the proposed method is applied to KD-SVD, the performance is improved by 1.69% for CIFAR100 and 0.97% for Tiny-ImageNet. This experiment shows that singular vectors are learned much better by KD-SVP. Thus, the experiment demonstrates that the\nTABLE 6. Performance of KD-SVP according to the number of singular vectors.\nTABLE 7. Performance change whenever each step of SVP is removed.\nFIGURE 9. Performance comparison whenever removing each step of SVP. Red triangle means NaN.\napplication of SVP signiGLYPH<28>cantly improves the performance of a given network. Next, we analyze the performance of KD-SVP according to the number of used singular vectors. Table 6 shows that since more singular vectors give more information, they improve the performance of the student network.\n\nDemonstrates how singular vectors are learned more effectively by KD-SVP, highlighting the method’s impact on student network performance as the number of singular vectors increases.",
    "original_text": "The feature vectors obtained by KD-SVP are easier to analyze than original singular vectors. In KD-SVD, the sign ambiguity is removed by aligning the singular vectors of the student network based on the singular vector of the teacher network. In addition, KD-SVD learns only similar features to learn the manifold features and deGLYPH<28>nes the correlation by a radial basis function (RBF), which has smaller values as the distance becomes larger. In other words, singular vectors\nFIGURE 8. The performance change according to the intensity of adversarial attack.\nare post-processed in a very naive manner in KD-SVD. Therefore, if SVP plays a post-processing role, the learning accuracy will improve. The correlation between feature vectors obtained by SVP is computed by RBF, similarly to KD-SVD.TheCIFAR100andTiny-ImageNet are used in this experiment. The teacher network is VGG-16, and the student network is a condensed network that uses only one convolution of the same depth as VGG-16, which are described in Fig. 5(e) and 5(f). KD-SVD and KD-SVP are all learned with the same hyper-parameters. Their learning progressed by 200 epochs, with an initial learning rate of 10 GLYPH<0> 2 and a reduction of 0.1 in 100 and 150 epochs. Also, the batch size is set to 128 and the weight decay of L 2 regularization is 10 GLYPH<0> 4 .\nTABLE 5. Performance of KD-SVP according to the number of singular vectors.\nTable 5 shows that when the proposed method is applied to KD-SVD, the performance is improved by 1.69% for CIFAR100 and 0.97% for Tiny-ImageNet. This experiment shows that singular vectors are learned much better by KD-SVP. Thus, the experiment demonstrates that the\nTABLE 6. Performance of KD-SVP according to the number of singular vectors.\nTABLE 7. Performance change whenever each step of SVP is removed.\nFIGURE 9. Performance comparison whenever removing each step of SVP. Red triangle means NaN.\napplication of SVP signiGLYPH<28>cantly improves the performance of a given network. Next, we analyze the performance of KD-SVP according to the number of used singular vectors. Table 6 shows that since more singular vectors give more information, they improve the performance of the student network.",
    "context": "Demonstrates how singular vectors are learned more effectively by KD-SVP, highlighting the method’s impact on student network performance as the number of singular vectors increases.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      8,
      9
    ],
    "id": "4ad93a66dfa9bc84888159ae9a90b07e99098a28137029112b4eda0dd9f3805e"
  },
  {
    "text": "The effect of each part of the proposed SVP on the overall performance of a given network is analyzed. SVP consists of rotation (step 1), removing sign ambiguity (step 2), and coordinate conversion (step 3). The model used for this experiment is ResNet-18, and CIFAR100 is chosen as the dataset. Wemeasuredthe performance changes by removing each part one by one. Table 7 shows the results, and Fig. 9 shows the training plots.\nOf course, whenever each step is omitted, the overall performance deteriorates. If step 1 is omitted (rotation), the sign ambiguity cannot be eliminated accurately because the singular vectors on the two half-spheres are not superimposed on each other. The performance decreases by about 1.46% because the error increases as the distance from the center of the coordinates deviates. However, the performance degradation is smaller than when omitting other steps. When the coordinate conversion step is removed, manifold features are not sufGLYPH<28>ciently learned, which resulted in performance degradation by 5.01%. This occurred because singular vectors that are deGLYPH<28>ned in non-Euclidean space are hard to learn by Euclidean geometry. Finally, if the sign ambiguity removal of step 2 is absent, learning is impossible. This experiment shows that each step of SVP is indispensable for proper learning of singular vectors.\n\nAnalysis of the impact of each component of the proposed SVP on network performance, demonstrating that each step is crucial for effective singular vector learning and overall network improvement.",
    "original_text": "The effect of each part of the proposed SVP on the overall performance of a given network is analyzed. SVP consists of rotation (step 1), removing sign ambiguity (step 2), and coordinate conversion (step 3). The model used for this experiment is ResNet-18, and CIFAR100 is chosen as the dataset. Wemeasuredthe performance changes by removing each part one by one. Table 7 shows the results, and Fig. 9 shows the training plots.\nOf course, whenever each step is omitted, the overall performance deteriorates. If step 1 is omitted (rotation), the sign ambiguity cannot be eliminated accurately because the singular vectors on the two half-spheres are not superimposed on each other. The performance decreases by about 1.46% because the error increases as the distance from the center of the coordinates deviates. However, the performance degradation is smaller than when omitting other steps. When the coordinate conversion step is removed, manifold features are not sufGLYPH<28>ciently learned, which resulted in performance degradation by 5.01%. This occurred because singular vectors that are deGLYPH<28>ned in non-Euclidean space are hard to learn by Euclidean geometry. Finally, if the sign ambiguity removal of step 2 is absent, learning is impossible. This experiment shows that each step of SVP is indispensable for proper learning of singular vectors.",
    "context": "Analysis of the impact of each component of the proposed SVP on network performance, demonstrating that each step is crucial for effective singular vector learning and overall network improvement.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      9
    ],
    "id": "7f8eccd03d9917c715b580e59a70274cf9cf3c98461677fe6fd0aa6074778a08"
  },
  {
    "text": "One advantage of the proposed method is that converting singular vectors into Euclidean space enables to use Euclidean geometry. The merit makes a generic CNN learn the singular vectors. As another advantage, the proposed SVP is superior to other techniques that treat singular vectors as manifold features in terms of scalability. SVP outperforms conventional pooling methods such as GAP and GMP, and also it is more robust against noise. In addition, SVP is experimentally proven to provide further performance enhancement when it is applied to an existing knowledge distillation scheme such as [17]. On the other hand, SVP has a disadvantage that its computational complexity is somewhat higher than that of general pooling methods. Also, SVD requires burdensome computations, so it may be difGLYPH<28>cult to apply it directly to an embedded system or a mobile environment.\n\nHighlights the benefits of SVP, including its ability to leverage Euclidean geometry for CNN learning and its scalability advantage over conventional pooling methods, while acknowledging its higher computational complexity.",
    "original_text": "One advantage of the proposed method is that converting singular vectors into Euclidean space enables to use Euclidean geometry. The merit makes a generic CNN learn the singular vectors. As another advantage, the proposed SVP is superior to other techniques that treat singular vectors as manifold features in terms of scalability. SVP outperforms conventional pooling methods such as GAP and GMP, and also it is more robust against noise. In addition, SVP is experimentally proven to provide further performance enhancement when it is applied to an existing knowledge distillation scheme such as [17]. On the other hand, SVP has a disadvantage that its computational complexity is somewhat higher than that of general pooling methods. Also, SVD requires burdensome computations, so it may be difGLYPH<28>cult to apply it directly to an embedded system or a mobile environment.",
    "context": "Highlights the benefits of SVP, including its ability to leverage Euclidean geometry for CNN learning and its scalability advantage over conventional pooling methods, while acknowledging its higher computational complexity.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      9
    ],
    "id": "c935dde40e31dd09914acd77f8120dacf1ebdd556e807b16008ccd49b22cc8bd"
  },
  {
    "text": "SVDisone of the most important techniques in many areas of high-level data manipulation. Therefore, if SVD can be used in deep learning methods such as CNN, it could be useful for various purposes. However, singular vectors decomposed by SVD have not been widely used in CNNs yet, as they are generally difGLYPH<28>cult to handle. This study presented a starting point for effectively using singular vectors with essential information in deep learning. Although the proposed SVP has a simple form, it is very useful because it is about 36% more robust to adversarial attacks than GAP and produces 1.69% more improvement in knowledge distillation than the naGLYPH<239>ve solution [12]. Future work should be done to improve the performance of CNNs further by devising ways of more effectively using the information of singular vectors.\n\nHighlights the importance of SVD in deep learning and introduces a new method (SVP) as a starting point for effectively utilizing singular vectors, emphasizing its robustness and potential for improvement in CNNs.",
    "original_text": "SVDisone of the most important techniques in many areas of high-level data manipulation. Therefore, if SVD can be used in deep learning methods such as CNN, it could be useful for various purposes. However, singular vectors decomposed by SVD have not been widely used in CNNs yet, as they are generally difGLYPH<28>cult to handle. This study presented a starting point for effectively using singular vectors with essential information in deep learning. Although the proposed SVP has a simple form, it is very useful because it is about 36% more robust to adversarial attacks than GAP and produces 1.69% more improvement in knowledge distillation than the naGLYPH<239>ve solution [12]. Future work should be done to improve the performance of CNNs further by devising ways of more effectively using the information of singular vectors.",
    "context": "Highlights the importance of SVD in deep learning and introduces a new method (SVP) as a starting point for effectively utilizing singular vectors, emphasizing its robustness and potential for improvement in CNNs.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      9
    ],
    "id": "273675854b71eb18e7ee710ed2c3006c09a73416d18c5410cc47473b7721e415"
  },
  {
    "text": "- [1] O. Alter, P. O. Brown, and D. Botstein, ''Singular value decomposition for genome-wide expression data processing and modeling,'' Proc. Nat. Acad. Sci. USA , vol. 97, no. 18, pp. 10101GLYPH<21>10106, Aug. 2000.\n- [2] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus, ''Exploiting linear structure within convolutional networks for efGLYPH<28>cient evaluation,'' in Proc. Adv. Neural Inf. Process. Syst. , 2014, pp. 1269GLYPH<21>1277.\n- [3] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl, ''Incremental singular value decomposition algorithms for highly scalable recommender systems,'' in Proc. 5th Int. Conf. Comput. Inf. Sci. , 2002, pp. 27GLYPH<21>28.\n- [4] C. Ionescu, O. Vantzos, and C. Sminchisescu, ''Matrix backpropagation for deep networks with structured layers,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , Dec. 2015, pp. 2965GLYPH<21>2973.\n- [5] Z. Huang, J. Wu, and L. Van Gool, ''Building deep networks on Grassmann manifolds,'' 2016, arXiv:1611.05742 . [Online]. Available: http://arxiv.org/abs/1611.05742\n- [6] R. Ranftl and V. Koltun, ''Deep fundamental matrix estimation,'' in Proc. Eur. Conf. Comput. Vis. (ECCV) , Sep. 2018, pp. 284GLYPH<21>299.\n\nThis section introduces foundational research on singular value decomposition (SVD) and its applications, including its use in genomics, efficient convolutional network evaluation, and scalable recommender systems, as well as exploring its potential within deep learning frameworks like Grassmann manifolds.",
    "original_text": "- [1] O. Alter, P. O. Brown, and D. Botstein, ''Singular value decomposition for genome-wide expression data processing and modeling,'' Proc. Nat. Acad. Sci. USA , vol. 97, no. 18, pp. 10101GLYPH<21>10106, Aug. 2000.\n- [2] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus, ''Exploiting linear structure within convolutional networks for efGLYPH<28>cient evaluation,'' in Proc. Adv. Neural Inf. Process. Syst. , 2014, pp. 1269GLYPH<21>1277.\n- [3] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl, ''Incremental singular value decomposition algorithms for highly scalable recommender systems,'' in Proc. 5th Int. Conf. Comput. Inf. Sci. , 2002, pp. 27GLYPH<21>28.\n- [4] C. Ionescu, O. Vantzos, and C. Sminchisescu, ''Matrix backpropagation for deep networks with structured layers,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , Dec. 2015, pp. 2965GLYPH<21>2973.\n- [5] Z. Huang, J. Wu, and L. Van Gool, ''Building deep networks on Grassmann manifolds,'' 2016, arXiv:1611.05742 . [Online]. Available: http://arxiv.org/abs/1611.05742\n- [6] R. Ranftl and V. Koltun, ''Deep fundamental matrix estimation,'' in Proc. Eur. Conf. Comput. Vis. (ECCV) , Sep. 2018, pp. 284GLYPH<21>299.",
    "context": "This section introduces foundational research on singular value decomposition (SVD) and its applications, including its use in genomics, efficient convolutional network evaluation, and scalable recommender systems, as well as exploring its potential within deep learning frameworks like Grassmann manifolds.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      9
    ],
    "id": "a0fa410fc42efa47853e5134f94eeba979773ebca2504106a2bfa2fa22f2e0e4"
  },
  {
    "text": "- [7] S. Suwajanakorn, N. Snavely, J. J. Tompson, and M. Norouzi, ''Discovery of latent 3d keypoints via end-to-end geometric reasoning,'' in Proc. Adv. Neural Inf. Process. Syst. , 2018, pp. 2059GLYPH<21>2070.\n- [8] P. Li, J. Xie, Q. Wang, and Z. Gao, ''Towards faster training of global covariance pooling networks by iterative matrix square root normalization,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , Jun. 2018, pp. 947GLYPH<21>955.\n- [9] Q. Wang, P. Li, and L. Zhang, ''G2DeNet: Global Gaussian distribution embedding network and its application to visual recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jul. 2017, pp. 2730GLYPH<21>2739.\n- [10] X. Wei, Y. Zhang, Y. Gong, J. Zhang, and N. Zheng, ''Grassmann pooling as compact homogeneous bilinear pooling for GLYPH<28>ne-grained visual classiGLYPH<28>cation,'' in Proc. Eur. Conf. Comput. Vis. (ECCV) , Sep. 2018, pp. 355GLYPH<21>370.\n- [11] R. Bro, E. Acar, and T. G. Kolda, ''Resolving the sign ambiguity in the singular value decomposition,'' J. Chemometrics, A J. Chemometrics Soc. , vol. 22, no. 2, pp. 135GLYPH<21>140, 2008.\n- [12] S. H. Lee, D. H. Kim, and B. C. Song, ''Self-supervised knowledge distillation using singular value decomposition,'' in Proc. Eur. Conf. Comput. Vis. Springer, 2018, pp. 339GLYPH<21>354.\n- [13] L. Song, B. Du, L. Zhang, L. Zhang, J. Wu, and X. Li, ''Nonlocal patch based T-SVD for image inpainting: Algorithm and error analysis,'' in Proc. 32nd AAAI Conf. Artif. Intell. , 2018, pp. 2419GLYPH<21>2426.\n- [14] T.-Y. Lin and S. Maji, ''Improved bilinear pooling with CNNs,'' 2017, arXiv:1707.06772 . [Online]. Available: http://arxiv.org/abs/1707.06772\n- [15] Z. Gao, J. Xie, Q. Wang, and P. Li, ''Global second-order pooling convolutional networks,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2019, pp. 3024GLYPH<21>3033.\n- [16] M. Lin, Q. Chen, and S. Yan, ''Network in network,'' 2013, arXiv:1312.4400 . [Online]. Available: http://arxiv.org/abs/1312.4400\n- [17] X. Zhang and X. Zhang, ''Global learnable pooling with enhancing distinctive feature for image classiGLYPH<28>cation,'' IEEE Access , vol. 8, pp. 98539GLYPH<21>98547, 2020.\n- [18] H. Azizpour, A. S. Razavian, J. Sullivan, A. Maki, and S. Carlsson, ''From generic to speciGLYPH<28>c deep representations for visual recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW) , Jun. 2015, pp. 36GLYPH<21>45.\n- [19] B. Zhang, Q. Zhao, W. Feng, and S. Lyu, ''AlphaMEX: A smarter global pooling method for convolutional neural networks,'' Neurocomputing , vol. 321, pp. 36GLYPH<21>48, Dec. 2018.\n- [20] T.-Y. Lin, A. RoyChowdhury, and S. Maji, ''Bilinear CNN models for GLYPH<28>negrained visual recognition,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , Dec. 2015, pp. 1449GLYPH<21>1457.\n- [21] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, ''Bm3D image denoising with shape-adaptive principal component analysis,'' Tech. Rep., 2009.\n- [22] L. Hazelhoff, J. Han, and P. H. N. de With, ''Video-based fall detection in the home using principal component analysis,'' in Proc. Int. Conf. Adv. Concepts Intell. Vis. Syst. , Springer, 2008, pp. 298GLYPH<21>309.\n- [23] L. E. Blumenson, ''A derivation of n-dimensional spherical coordinates,'' Amer. Math. Monthly , vol. 67, no. 1, pp. 63GLYPH<21>66, Jan. 1960. [Online]. Available: http://www.jstor.org/stable/2308932\n- [24] S. Ioffe and C. Szegedy, ''Batch normalization: Accelerating deep network training by reducing internal covariate shift,'' in Int. Conf. Mach. Learn. , 2015, pp. 448GLYPH<21>456.\n- [25] S. Kullback and R. A. Leibler, ''On information and sufGLYPH<28>ciency,'' Ann. Math. Statist. , vol. 22, no. 1, pp. 79GLYPH<21>86, 1951.\n- [26] A. Krizhevsky and G. Hinton, ''Learning multiple layers of features from tiny images,'' Citeseer, Tech. Rep., 2009.\n- [27] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ''ImageNet classiGLYPH<28>cation with deep convolutional neural networks,'' in Proc. Adv. Neural Inf. Process. Syst. , 2012, pp. 1097GLYPH<21>1105.\n- [28] M. Abadi et al. , ''TensorFlow: A system for large-scale machine learning,'' in Proc. OSDI , vol. 16. 2016, pp. 265GLYPH<21>283.\n- [29] V. Nair and G. E. Hinton, ''RectiGLYPH<28>ed linear units improve restricted Boltzmann machines,'' in Proc. 27th Int. Conf. Mach. Learn. (ICML) , 2010, pp. 807GLYPH<21>814.\n- [30] K. He, X. Zhang, S. Ren, and J. Sun, ''Deep residual learning for image recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2016, pp. 770GLYPH<21>778.\n- [31] J. Kiefer and J. Wolfowitz, ''Stochastic estimation of the maximum of a regression function,'' Ann. Math. Statist. , vol. 23, no. 3, pp. 462GLYPH<21>466, Sep. 1952.\n- [32] Y. Nesterov, ''A method for unconstrained convex minimization problem with the rate of convergence o (1/k ^ 2),'' in Proc. Doklady AN USSR , vol. 269, 1983, pp. 543GLYPH<21>547.\n- [33] P. J. Rousseeuw, ''Silhouettes: A graphical aid to the interpretation and validation of cluster analysis,'' J. Comput. Appl. Math. , vol. 20, pp. 53GLYPH<21>65, Nov. 1987.\n\nFocuses on techniques for efficient deep learning, specifically exploring methods for pooling, dimensionality reduction, and representation learning within convolutional neural networks.",
    "original_text": "- [7] S. Suwajanakorn, N. Snavely, J. J. Tompson, and M. Norouzi, ''Discovery of latent 3d keypoints via end-to-end geometric reasoning,'' in Proc. Adv. Neural Inf. Process. Syst. , 2018, pp. 2059GLYPH<21>2070.\n- [8] P. Li, J. Xie, Q. Wang, and Z. Gao, ''Towards faster training of global covariance pooling networks by iterative matrix square root normalization,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , Jun. 2018, pp. 947GLYPH<21>955.\n- [9] Q. Wang, P. Li, and L. Zhang, ''G2DeNet: Global Gaussian distribution embedding network and its application to visual recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jul. 2017, pp. 2730GLYPH<21>2739.\n- [10] X. Wei, Y. Zhang, Y. Gong, J. Zhang, and N. Zheng, ''Grassmann pooling as compact homogeneous bilinear pooling for GLYPH<28>ne-grained visual classiGLYPH<28>cation,'' in Proc. Eur. Conf. Comput. Vis. (ECCV) , Sep. 2018, pp. 355GLYPH<21>370.\n- [11] R. Bro, E. Acar, and T. G. Kolda, ''Resolving the sign ambiguity in the singular value decomposition,'' J. Chemometrics, A J. Chemometrics Soc. , vol. 22, no. 2, pp. 135GLYPH<21>140, 2008.\n- [12] S. H. Lee, D. H. Kim, and B. C. Song, ''Self-supervised knowledge distillation using singular value decomposition,'' in Proc. Eur. Conf. Comput. Vis. Springer, 2018, pp. 339GLYPH<21>354.\n- [13] L. Song, B. Du, L. Zhang, L. Zhang, J. Wu, and X. Li, ''Nonlocal patch based T-SVD for image inpainting: Algorithm and error analysis,'' in Proc. 32nd AAAI Conf. Artif. Intell. , 2018, pp. 2419GLYPH<21>2426.\n- [14] T.-Y. Lin and S. Maji, ''Improved bilinear pooling with CNNs,'' 2017, arXiv:1707.06772 . [Online]. Available: http://arxiv.org/abs/1707.06772\n- [15] Z. Gao, J. Xie, Q. Wang, and P. Li, ''Global second-order pooling convolutional networks,'' in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2019, pp. 3024GLYPH<21>3033.\n- [16] M. Lin, Q. Chen, and S. Yan, ''Network in network,'' 2013, arXiv:1312.4400 . [Online]. Available: http://arxiv.org/abs/1312.4400\n- [17] X. Zhang and X. Zhang, ''Global learnable pooling with enhancing distinctive feature for image classiGLYPH<28>cation,'' IEEE Access , vol. 8, pp. 98539GLYPH<21>98547, 2020.\n- [18] H. Azizpour, A. S. Razavian, J. Sullivan, A. Maki, and S. Carlsson, ''From generic to speciGLYPH<28>c deep representations for visual recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW) , Jun. 2015, pp. 36GLYPH<21>45.\n- [19] B. Zhang, Q. Zhao, W. Feng, and S. Lyu, ''AlphaMEX: A smarter global pooling method for convolutional neural networks,'' Neurocomputing , vol. 321, pp. 36GLYPH<21>48, Dec. 2018.\n- [20] T.-Y. Lin, A. RoyChowdhury, and S. Maji, ''Bilinear CNN models for GLYPH<28>negrained visual recognition,'' in Proc. IEEE Int. Conf. Comput. Vis. (ICCV) , Dec. 2015, pp. 1449GLYPH<21>1457.\n- [21] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, ''Bm3D image denoising with shape-adaptive principal component analysis,'' Tech. Rep., 2009.\n- [22] L. Hazelhoff, J. Han, and P. H. N. de With, ''Video-based fall detection in the home using principal component analysis,'' in Proc. Int. Conf. Adv. Concepts Intell. Vis. Syst. , Springer, 2008, pp. 298GLYPH<21>309.\n- [23] L. E. Blumenson, ''A derivation of n-dimensional spherical coordinates,'' Amer. Math. Monthly , vol. 67, no. 1, pp. 63GLYPH<21>66, Jan. 1960. [Online]. Available: http://www.jstor.org/stable/2308932\n- [24] S. Ioffe and C. Szegedy, ''Batch normalization: Accelerating deep network training by reducing internal covariate shift,'' in Int. Conf. Mach. Learn. , 2015, pp. 448GLYPH<21>456.\n- [25] S. Kullback and R. A. Leibler, ''On information and sufGLYPH<28>ciency,'' Ann. Math. Statist. , vol. 22, no. 1, pp. 79GLYPH<21>86, 1951.\n- [26] A. Krizhevsky and G. Hinton, ''Learning multiple layers of features from tiny images,'' Citeseer, Tech. Rep., 2009.\n- [27] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ''ImageNet classiGLYPH<28>cation with deep convolutional neural networks,'' in Proc. Adv. Neural Inf. Process. Syst. , 2012, pp. 1097GLYPH<21>1105.\n- [28] M. Abadi et al. , ''TensorFlow: A system for large-scale machine learning,'' in Proc. OSDI , vol. 16. 2016, pp. 265GLYPH<21>283.\n- [29] V. Nair and G. E. Hinton, ''RectiGLYPH<28>ed linear units improve restricted Boltzmann machines,'' in Proc. 27th Int. Conf. Mach. Learn. (ICML) , 2010, pp. 807GLYPH<21>814.\n- [30] K. He, X. Zhang, S. Ren, and J. Sun, ''Deep residual learning for image recognition,'' in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2016, pp. 770GLYPH<21>778.\n- [31] J. Kiefer and J. Wolfowitz, ''Stochastic estimation of the maximum of a regression function,'' Ann. Math. Statist. , vol. 23, no. 3, pp. 462GLYPH<21>466, Sep. 1952.\n- [32] Y. Nesterov, ''A method for unconstrained convex minimization problem with the rate of convergence o (1/k ^ 2),'' in Proc. Doklady AN USSR , vol. 269, 1983, pp. 543GLYPH<21>547.\n- [33] P. J. Rousseeuw, ''Silhouettes: A graphical aid to the interpretation and validation of cluster analysis,'' J. Comput. Appl. Math. , vol. 20, pp. 53GLYPH<21>65, Nov. 1987.",
    "context": "Focuses on techniques for efficient deep learning, specifically exploring methods for pooling, dimensionality reduction, and representation learning within convolutional neural networks.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      10
    ],
    "id": "c10f7d05286c2b9c2497493b90acaaac51d81b878e9df7360703bc83457849d2"
  },
  {
    "text": "- [34] I. J. Goodfellow, J. Shlens, and C. Szegedy, ''Explaining and harnessing adversarial examples,'' 2014, arXiv:1412.6572 . [Online]. Available: http://arxiv.org/abs/1412.6572\n- [35] A. Kurakin, I. Goodfellow, and S. Bengio, ''Adversarial examples in the physical world,'' 2016, arXiv:1607.02533 . [Online]. Available: http://arxiv.org/abs/1607.02533\n- [36] K. Simonyan and A. Zisserman, ''Very deep convolutional networks for large-scale image recognition,'' 2014, arXiv:1409.1556 . [Online]. Available: http://arxiv.org/abs/1409.1556\nSEUNGHYUN LEE (Associate Member, IEEE) received the B.S. degree in electronic engineering from Inha University, Incheon, South Korea, in 2017, where he is currently pursuing the Ph.D. degree. His research interests include computer vision and machine learning.\nBYUNG CHEOL SONG (Senior Member, IEEE) received the B.S., M.S., and Ph.D. degrees in electrical engineering from the Korea Advanced Institute of Science and Technology, Daejeon, South Korea, in 1994, 1996, and 2001, respectively. From 2001 to 2008, he was a Senior Engineer with the Digital Media Research and Development Center, Samsung Electronics Company Ltd., Suwon, South Korea. He joined the Department of Electronic Engineering, Inha University, Incheon,\nSouth Korea, in 2008, where he is currently a Professor. His research interests include general areas of image processing and computer vision.\n\nProvides citations to foundational research on adversarial examples and deep convolutional networks.",
    "original_text": "- [34] I. J. Goodfellow, J. Shlens, and C. Szegedy, ''Explaining and harnessing adversarial examples,'' 2014, arXiv:1412.6572 . [Online]. Available: http://arxiv.org/abs/1412.6572\n- [35] A. Kurakin, I. Goodfellow, and S. Bengio, ''Adversarial examples in the physical world,'' 2016, arXiv:1607.02533 . [Online]. Available: http://arxiv.org/abs/1607.02533\n- [36] K. Simonyan and A. Zisserman, ''Very deep convolutional networks for large-scale image recognition,'' 2014, arXiv:1409.1556 . [Online]. Available: http://arxiv.org/abs/1409.1556\nSEUNGHYUN LEE (Associate Member, IEEE) received the B.S. degree in electronic engineering from Inha University, Incheon, South Korea, in 2017, where he is currently pursuing the Ph.D. degree. His research interests include computer vision and machine learning.\nBYUNG CHEOL SONG (Senior Member, IEEE) received the B.S., M.S., and Ph.D. degrees in electrical engineering from the Korea Advanced Institute of Science and Technology, Daejeon, South Korea, in 1994, 1996, and 2001, respectively. From 2001 to 2008, he was a Senior Engineer with the Digital Media Research and Development Center, Samsung Electronics Company Ltd., Suwon, South Korea. He joined the Department of Electronic Engineering, Inha University, Incheon,\nSouth Korea, in 2008, where he is currently a Professor. His research interests include general areas of image processing and computer vision.",
    "context": "Provides citations to foundational research on adversarial examples and deep convolutional networks.",
    "document": "Transformation_of_Non-Euclidean_Space_to_Euclidean_Space_for_Efficient_Learning_of_Singular_Vectors.pdf",
    "pages": [
      10
    ],
    "id": "ea24210c3bc8284895b1be69dfd57772134a6182003235bf06121d1002e11a62"
  },
  {
    "text": "Daewoon Seong , Deokmin Jeon , Ruchire Eranga Wijesinghe , Kibeom Park , Hyeree Kim , Euimin Lee , Mansik Jeon , Member, IEEE , and Jeehyun Kim , Member, IEEE\nAbstract -The primary optimization of the imaging speed of optical coherence tomography (OCT) has been keenly studied. In order to overcome the major speed limitation of spectral-domain OCT (SD-OCT), we developed an ultrahigh-speed SD-OCT system, with an A-scan rate of up to 1 MHz, using the method of space-time-division multiplexing (STDM). Multicameras comprising a single spectrometer were implemented in the developed ultrahigh-speed STDM method to eliminate the dead time of operation, whereas STDM was simultaneously employed to enable wide-range scanning measurements at a high speed. By successfully integrating the developed STDM method with GPU parallel processing, 8 vol/s for an image range of 250 × 250 × 2048 pixels (9 × 4.5 × 5 mm) was achieved, with an adjustable volume rate according to the required scanning speed and range. The examined STDM-OCT results of the customized optical thin film confirmed its feasibility for various fields that require rapid and wide-field scanning.\nstructures [1], [2]. Due to the method's potential resolution merits, OCT has been widely employed in various applications, such as ophthalmology [3], dentistry [4], [5], otolaryngology [6], [7], dermatology [8], [9], and even industrial fields [10], [11]. In terms of imaging speed, the development of high-speed real-time OCT has been required in order to observe morphological time variation of biological tissues with minimal motion artifacts [12], [13] and to fit a limited inspection time in industrial applications [14], [15].\nIndex Terms -Fourier optical signal processing, parallel processing, space-time-division multiplexing (STDM), spectraldomain optical coherence tomography (OCT), ultrahigh-speed imaging system.\n\nDeveloped an ultrahigh-speed SD-OCT system achieving 1 MHz A-scan rate using STDM and multicameras to overcome SD-OCT speed limitations, demonstrating feasibility for rapid scanning of optical thin films.",
    "original_text": "Daewoon Seong , Deokmin Jeon , Ruchire Eranga Wijesinghe , Kibeom Park , Hyeree Kim , Euimin Lee , Mansik Jeon , Member, IEEE , and Jeehyun Kim , Member, IEEE\nAbstract -The primary optimization of the imaging speed of optical coherence tomography (OCT) has been keenly studied. In order to overcome the major speed limitation of spectral-domain OCT (SD-OCT), we developed an ultrahigh-speed SD-OCT system, with an A-scan rate of up to 1 MHz, using the method of space-time-division multiplexing (STDM). Multicameras comprising a single spectrometer were implemented in the developed ultrahigh-speed STDM method to eliminate the dead time of operation, whereas STDM was simultaneously employed to enable wide-range scanning measurements at a high speed. By successfully integrating the developed STDM method with GPU parallel processing, 8 vol/s for an image range of 250 × 250 × 2048 pixels (9 × 4.5 × 5 mm) was achieved, with an adjustable volume rate according to the required scanning speed and range. The examined STDM-OCT results of the customized optical thin film confirmed its feasibility for various fields that require rapid and wide-field scanning.\nstructures [1], [2]. Due to the method's potential resolution merits, OCT has been widely employed in various applications, such as ophthalmology [3], dentistry [4], [5], otolaryngology [6], [7], dermatology [8], [9], and even industrial fields [10], [11]. In terms of imaging speed, the development of high-speed real-time OCT has been required in order to observe morphological time variation of biological tissues with minimal motion artifacts [12], [13] and to fit a limited inspection time in industrial applications [14], [15].\nIndex Terms -Fourier optical signal processing, parallel processing, space-time-division multiplexing (STDM), spectraldomain optical coherence tomography (OCT), ultrahigh-speed imaging system.",
    "context": "Developed an ultrahigh-speed SD-OCT system achieving 1 MHz A-scan rate using STDM and multicameras to overcome SD-OCT speed limitations, demonstrating feasibility for rapid scanning of optical thin films.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      1
    ],
    "id": "4ea2f6384dd79e950f4f62a1dd74f65ac9807fad9e8607a908444bbba6eec2eb"
  },
  {
    "text": "O PTICAL coherence tomography (OCT) is a noninvasive and high-resolution imaging technique that enables 2-D imaging and 3-D imaging to measure morphological\nManuscript received January 27, 2021; revised March 27, 2021; accepted April 6, 2021. Date of publication April 16, 2021; date of current version May 4, 2021. This work was supported in part by the Bio & Medical Technology Development Program of the NRF funded by the Korean government (the Ministry of Science, ICT and Future Planning) under Grant 2017M3A9E2065282 and in part by the Korea Medical Device Development Fund grant funded by the Korea government (the Ministry of Science and ICT, the Ministry of Trade, Industry and Energy, the Ministry of Health & Welfare, the Ministry of Food and Drug Safety) under Grant KMDF-PR202009010055, 202011C13. The Associate Editor coordinating the review process was Jing Lei. (Daewoon Seong and Deokmin Jeon contributed equally to this work.) (Corresponding authors: Mansik Jeon; Jeehyun Kim.)\nDaewoon Seong, Deokmin Jeon, Hyeree Kim, Euimin Lee, Mansik Jeon, and Jeehyun Kim are with the School of Electronic and Electrical Engineering, College of IT Engineering, Kyungpook National University, Daegu 41566, South Korea (e-mail: smc7095@knu.ac.kr; dmjeon@knu.ac.kr; hleeworld@gmail.com; augustmini@knu.ac.kr; msjeon@knu.ac.kr; jeehk@ knu.ac.kr).\nRuchire Eranga Wijesinghe is with the Department of Materials and Mechanical Technology, Faculty of Technology, University of Sri Jayewardenepura, Homagama 10200, Sri Lanka (e-mail: erangawijesinghe@sjp.ac.lk). Kibeom Park is with the Department of Biomedical Engineering, Ulsan National Institute of Science and Technology, Ulsan 44919, South Korea (e-mail: pepl116@unist.ac.kr).\nThis article has supplementary downloadable material available at https://doi.org/10.1109/TIM.2021.3073701, provided by the authors.\nDigital Object Identifier 10.1109/TIM.2021.3073701\nThe imaging speed of the initially developed time-domain OCT (TD-OCT) was limited due to the motor-based scanning mechanism. To overcome this drawback, the Fourier-domain OCT (FD-OCT) was developed to directly obtain depthresolved spectra without unnecessary movement of mirror and with an improved system sensitivity [16], [17]. Swept-source OCT (SS-OCT) is one of the FD-OCT methods, which has a sufficient imaging speed and detects the spectrum of each wavelength by a photodetector that is mainly influenced by the sweeping rate of the wavelength-tunable light source [18], [19], whereas SS-OCT systems with an MHz high A-scan rate have been presented in previous reports [20]-[22]. Although SS-OCT provides the reduced sensitivity roll-off and high-speed A-scan rate [16], [23], axial resolution is slightly degraded, because of the longer wavelength of the source, and the cost of development is expansive compared to spectral-domain OCT (SD-OCT) [24], [25].\nSD-OCT is a widely applied and comparatively economical imaging technique with an exceptional resolution. However, the low speed of the detector has a direct negative impact on the system speed [26]. Thus, multiple attempts were made to enhance the system speed, where those studies are mainly focused on shortening the integration time of the line-scan camera, which needs to sacrifice the detectable maximum number of detector pixels [27]. To further improve the A-line rate, multiple line-scan cameras were used, and their operating times were conversely controlled to use the dead time for capturing [26], [27]. Based on this method, a 1-MHz A-line rate SD-OCT system was developed using four separate cameras [28]. However, the conventional multicamerabased system requires multiple independent spectrometers as much as a number of line-scan cameras, causing inevitable errors of spectrum difference at each spectrometer [26]-[28]. In addition, optical demultiplexer-based FD-OCT, which is used as a spectral analyzer, was introduced to further improve\nthe scanning speed up to multi-MHz [29], [30]. Nevertheless, the interference spectrum is diffracted by the limited channels of the demultiplexer, which attenuates the signal and decreases the resolution of frequency interval [29]. Moreover, multiple data acquisition (DAQ) boards and digitizers are required to improve the speed, leading to an increase in the total system prices of optical demultiplexer-based OCT [30].\nAs an alternative approach to enhancing the speed, the MHz streak-mode OCT, which utilizes an area-scan camera with a resonant scanner in the spectrometer to extend the integration time, was proposed. However, the signal-to-noise ratio (SNR) is much degraded compared with the conventional OCT, whereas nonuniformed exposure of the camera reduces the utilization of the duty cycle as well [31]. In addition, a parallel-OCT-based high-speed system, which improves the illumination power by applying a line focus to the sample, was reported in [32] and [33]. However, the limitations of sensitivity and imaging depth were nonnegligible [32], [34]. Moreover, as an alternative method for improving the scanning rate, multispatial scanning method-based OCT was developed, which utilized wavelength filters to split the beam of the sample arm; however, the axial resolution was reduced because of a higher number of scanning channel [35]. In addition, the space-division multiplexing (SDM) OCT, which illuminates the sample with multiple imaging beams with different optical delays, was also demonstrated [36], [37].\nIn this study, we developed ultrahigh-speed SD-OCT by optimizing the A-line rate up to 1 MHz, which was achieved by space-time-division multiplexing (STDM). To precisely synchronize the operating time and improve the processing speed, a control software platform was developed using C ++ with the Compute Unified Device Architecture (CUDA). The system performance, as an aspect of SDM, was quantitatively evaluated by sensitivity roll-off of each camera, and cross-sectional images were obtained. In addition, the time-division multiplexing (TDM) method, which improves the operating speed and minimizes the alignment error, was verified by the A-scan profiling results of the image-merging process. Moreover, to verify the capability of high-speed STDM-OCT for industrial applications, laboratory-customized subsurface structures of multilayered optical thin films (OTFs) were examined. Therefore, the demonstrated high-speed SD-OCT system has a potential for various applications, where high-speed scanning is essential, such as clinical and biomedical research areas.\n\nIntroduces the development of ultrahigh-speed SD-OCT using space-time-division multiplexing (STDM) to achieve a 1 MHz A-line rate, focusing on overcoming limitations of previous OCT systems.",
    "original_text": "O PTICAL coherence tomography (OCT) is a noninvasive and high-resolution imaging technique that enables 2-D imaging and 3-D imaging to measure morphological\nManuscript received January 27, 2021; revised March 27, 2021; accepted April 6, 2021. Date of publication April 16, 2021; date of current version May 4, 2021. This work was supported in part by the Bio & Medical Technology Development Program of the NRF funded by the Korean government (the Ministry of Science, ICT and Future Planning) under Grant 2017M3A9E2065282 and in part by the Korea Medical Device Development Fund grant funded by the Korea government (the Ministry of Science and ICT, the Ministry of Trade, Industry and Energy, the Ministry of Health & Welfare, the Ministry of Food and Drug Safety) under Grant KMDF-PR202009010055, 202011C13. The Associate Editor coordinating the review process was Jing Lei. (Daewoon Seong and Deokmin Jeon contributed equally to this work.) (Corresponding authors: Mansik Jeon; Jeehyun Kim.)\nDaewoon Seong, Deokmin Jeon, Hyeree Kim, Euimin Lee, Mansik Jeon, and Jeehyun Kim are with the School of Electronic and Electrical Engineering, College of IT Engineering, Kyungpook National University, Daegu 41566, South Korea (e-mail: smc7095@knu.ac.kr; dmjeon@knu.ac.kr; hleeworld@gmail.com; augustmini@knu.ac.kr; msjeon@knu.ac.kr; jeehk@ knu.ac.kr).\nRuchire Eranga Wijesinghe is with the Department of Materials and Mechanical Technology, Faculty of Technology, University of Sri Jayewardenepura, Homagama 10200, Sri Lanka (e-mail: erangawijesinghe@sjp.ac.lk). Kibeom Park is with the Department of Biomedical Engineering, Ulsan National Institute of Science and Technology, Ulsan 44919, South Korea (e-mail: pepl116@unist.ac.kr).\nThis article has supplementary downloadable material available at https://doi.org/10.1109/TIM.2021.3073701, provided by the authors.\nDigital Object Identifier 10.1109/TIM.2021.3073701\nThe imaging speed of the initially developed time-domain OCT (TD-OCT) was limited due to the motor-based scanning mechanism. To overcome this drawback, the Fourier-domain OCT (FD-OCT) was developed to directly obtain depthresolved spectra without unnecessary movement of mirror and with an improved system sensitivity [16], [17]. Swept-source OCT (SS-OCT) is one of the FD-OCT methods, which has a sufficient imaging speed and detects the spectrum of each wavelength by a photodetector that is mainly influenced by the sweeping rate of the wavelength-tunable light source [18], [19], whereas SS-OCT systems with an MHz high A-scan rate have been presented in previous reports [20]-[22]. Although SS-OCT provides the reduced sensitivity roll-off and high-speed A-scan rate [16], [23], axial resolution is slightly degraded, because of the longer wavelength of the source, and the cost of development is expansive compared to spectral-domain OCT (SD-OCT) [24], [25].\nSD-OCT is a widely applied and comparatively economical imaging technique with an exceptional resolution. However, the low speed of the detector has a direct negative impact on the system speed [26]. Thus, multiple attempts were made to enhance the system speed, where those studies are mainly focused on shortening the integration time of the line-scan camera, which needs to sacrifice the detectable maximum number of detector pixels [27]. To further improve the A-line rate, multiple line-scan cameras were used, and their operating times were conversely controlled to use the dead time for capturing [26], [27]. Based on this method, a 1-MHz A-line rate SD-OCT system was developed using four separate cameras [28]. However, the conventional multicamerabased system requires multiple independent spectrometers as much as a number of line-scan cameras, causing inevitable errors of spectrum difference at each spectrometer [26]-[28]. In addition, optical demultiplexer-based FD-OCT, which is used as a spectral analyzer, was introduced to further improve\nthe scanning speed up to multi-MHz [29], [30]. Nevertheless, the interference spectrum is diffracted by the limited channels of the demultiplexer, which attenuates the signal and decreases the resolution of frequency interval [29]. Moreover, multiple data acquisition (DAQ) boards and digitizers are required to improve the speed, leading to an increase in the total system prices of optical demultiplexer-based OCT [30].\nAs an alternative approach to enhancing the speed, the MHz streak-mode OCT, which utilizes an area-scan camera with a resonant scanner in the spectrometer to extend the integration time, was proposed. However, the signal-to-noise ratio (SNR) is much degraded compared with the conventional OCT, whereas nonuniformed exposure of the camera reduces the utilization of the duty cycle as well [31]. In addition, a parallel-OCT-based high-speed system, which improves the illumination power by applying a line focus to the sample, was reported in [32] and [33]. However, the limitations of sensitivity and imaging depth were nonnegligible [32], [34]. Moreover, as an alternative method for improving the scanning rate, multispatial scanning method-based OCT was developed, which utilized wavelength filters to split the beam of the sample arm; however, the axial resolution was reduced because of a higher number of scanning channel [35]. In addition, the space-division multiplexing (SDM) OCT, which illuminates the sample with multiple imaging beams with different optical delays, was also demonstrated [36], [37].\nIn this study, we developed ultrahigh-speed SD-OCT by optimizing the A-line rate up to 1 MHz, which was achieved by space-time-division multiplexing (STDM). To precisely synchronize the operating time and improve the processing speed, a control software platform was developed using C ++ with the Compute Unified Device Architecture (CUDA). The system performance, as an aspect of SDM, was quantitatively evaluated by sensitivity roll-off of each camera, and cross-sectional images were obtained. In addition, the time-division multiplexing (TDM) method, which improves the operating speed and minimizes the alignment error, was verified by the A-scan profiling results of the image-merging process. Moreover, to verify the capability of high-speed STDM-OCT for industrial applications, laboratory-customized subsurface structures of multilayered optical thin films (OTFs) were examined. Therefore, the demonstrated high-speed SD-OCT system has a potential for various applications, where high-speed scanning is essential, such as clinical and biomedical research areas.",
    "context": "Introduces the development of ultrahigh-speed SD-OCT using space-time-division multiplexing (STDM) to achieve a 1 MHz A-line rate, focusing on overcoming limitations of previous OCT systems.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      1,
      2
    ],
    "id": "6991b79b0cb635b6784e1ade00e5d6c098f38b7976f778723783d5e3ea51cac8"
  },
  {
    "text": "The optical configuration of the STDM-OCT system is illustrated in Fig. 1(a). The light from the broadband light source (SLD-371-HP3, Superlum, Ireland), with a center wavelength of 838 nm, a full-width at half-maximum of 81 nm, and optical power of 27.2 mW, was transmitted to a 50:50 fiber coupler (TW850R5A2, Thorlabs, USA) and evenly distributed to each interferometer. Each interferometer is identically comprised of a 75:25 fiber coupler (TW850R3A2, Thorlabs, USA), a reference arm, and a sample arm. The reference arm consisted of the collimator (F780APC-850, Thorlabs, USA),\nFig. 1. (a) Optical configuration of the developed STDM-OCT system. (b) Photograph of multiscanners for the implementation of SDM. (c) Photograph of multicameras comprising the single spectrometer for TDM. BS: beam splitter; C: collimator; DG: diffraction grating; FC: fiber coupler; GVS: galvanometer scanner; L: lens; LSC: line-scan camera; LMS: linear-motor stage; M: mirror; S: sample; and SLD: superluminescent diode.\nlens (AC508-100-B, Thorlabs, USA), and mirror (PF10-03P01, Thorlabs, USA). Both the sample and reference arm optics were maintained equivalently, whereas 2-D and 3-D scans were obtained using a galvanometer scanner (GVS002, Thorlabs, USA). In the case of scanning OTF sample, a linear-motor stage (TS-P, YAMAHA, Japan) was used instead of a y -axis galvo scanner to achieve an accurate translation speed for 3-D inspection. To yield an ultrafast A-scan rate, the high-precision SDM method was used, whereas an adequate physical optical path difference between the fiber couplers of each interferometer was accurately regulated. The space-divided coherence signal was generated and properly transmitted to the spectrometer without any distortion. The utilized spectrometer was customized for STDM-OCT, where the conventional spectrometer described in [38] was optically modulated with the addition of beam splitter and additional line-scan camera enabling ultrahigh-speed A-scan rate up to 1 MHz. The diffracted interference signal by diffraction grating (WP-HD1800/840-50.8, Wasatch Photonics, USA) was split up by a beam splitter (BS032, Thorlabs, USA) and separately passed into two line-scan cameras with a resolution of 2048 pixels (e2v OCTOPLUS, TELEDYNE e2v, U.K.), which are the principal components for the application of the TDM technique described in Section II-B. Two frame grabbers (APX-3326A, AVAL DATA, Japan) and a DAQ board (NI PCIe-6363, National Instrument Corporation, USA) were utilized to precisely control the hardware compositions. The photograph of the multiscanners for the implementation of the SDM is presented in Fig. 1(b). In addition, the photograph of\nFig. 2. (a) Software flowchart of STDM-OCT. (b) Timing diagram of synchronized trigger signal sequences. API: application program interface; GPU: graphics processing unit.\na single spectrometer comprised of multicameras, as an aspect of TDM, is presented in Fig. 1(c).\n\nDescribes the optical configuration and hardware components of the developed STDM-OCT system.",
    "original_text": "The optical configuration of the STDM-OCT system is illustrated in Fig. 1(a). The light from the broadband light source (SLD-371-HP3, Superlum, Ireland), with a center wavelength of 838 nm, a full-width at half-maximum of 81 nm, and optical power of 27.2 mW, was transmitted to a 50:50 fiber coupler (TW850R5A2, Thorlabs, USA) and evenly distributed to each interferometer. Each interferometer is identically comprised of a 75:25 fiber coupler (TW850R3A2, Thorlabs, USA), a reference arm, and a sample arm. The reference arm consisted of the collimator (F780APC-850, Thorlabs, USA),\nFig. 1. (a) Optical configuration of the developed STDM-OCT system. (b) Photograph of multiscanners for the implementation of SDM. (c) Photograph of multicameras comprising the single spectrometer for TDM. BS: beam splitter; C: collimator; DG: diffraction grating; FC: fiber coupler; GVS: galvanometer scanner; L: lens; LSC: line-scan camera; LMS: linear-motor stage; M: mirror; S: sample; and SLD: superluminescent diode.\nlens (AC508-100-B, Thorlabs, USA), and mirror (PF10-03P01, Thorlabs, USA). Both the sample and reference arm optics were maintained equivalently, whereas 2-D and 3-D scans were obtained using a galvanometer scanner (GVS002, Thorlabs, USA). In the case of scanning OTF sample, a linear-motor stage (TS-P, YAMAHA, Japan) was used instead of a y -axis galvo scanner to achieve an accurate translation speed for 3-D inspection. To yield an ultrafast A-scan rate, the high-precision SDM method was used, whereas an adequate physical optical path difference between the fiber couplers of each interferometer was accurately regulated. The space-divided coherence signal was generated and properly transmitted to the spectrometer without any distortion. The utilized spectrometer was customized for STDM-OCT, where the conventional spectrometer described in [38] was optically modulated with the addition of beam splitter and additional line-scan camera enabling ultrahigh-speed A-scan rate up to 1 MHz. The diffracted interference signal by diffraction grating (WP-HD1800/840-50.8, Wasatch Photonics, USA) was split up by a beam splitter (BS032, Thorlabs, USA) and separately passed into two line-scan cameras with a resolution of 2048 pixels (e2v OCTOPLUS, TELEDYNE e2v, U.K.), which are the principal components for the application of the TDM technique described in Section II-B. Two frame grabbers (APX-3326A, AVAL DATA, Japan) and a DAQ board (NI PCIe-6363, National Instrument Corporation, USA) were utilized to precisely control the hardware compositions. The photograph of the multiscanners for the implementation of the SDM is presented in Fig. 1(b). In addition, the photograph of\nFig. 2. (a) Software flowchart of STDM-OCT. (b) Timing diagram of synchronized trigger signal sequences. API: application program interface; GPU: graphics processing unit.\na single spectrometer comprised of multicameras, as an aspect of TDM, is presented in Fig. 1(c).",
    "context": "Describes the optical configuration and hardware components of the developed STDM-OCT system.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      2,
      3
    ],
    "id": "cf9c4932434016213bdc8d8bcb5ea35d96ebf5fdef8bf065a4f5a41733cab90c"
  },
  {
    "text": "The developed software algorithm used for STDM-OCT is presented in Fig. 2(a). The developed custom control programming platform is built using C ++ , CUDA, and Qt. Before the initiation of optical scanning, parameter setting and initialization process, in which the scanning mode was selected and the range was matched to the properties of the sample, were performed. In addition, memories are automatically allocated according to previously initiated values. After the initialization stage, the STDM-OCT engine operation is initiated simultaneously with optical scanning and camera acquisition. The acquired raw signal is transferred to the graphics processing unit (GPU) (from host memory to GPU memory) and processed via the CUDA-based signal processing stage, which includes k-domain linearization, the fast Fourier transform, and log scaling. Through the GPU acceleration, data transmission, processing, and displaying process are properly operated in real-time without buffer overflow. After completing the signal processing of the obtained data in one cycle of x-galvo on GPU, the status of the engine (B-mode completed) is transferred and updated to the user interface (UI) via the callback function in the application programming interface (API). When the UI receives the B-mode completed signal, it starts the 'Get image' process by utilizing a callback function in the API for a memory copy of processed data in the OCT engine and displays the image. According to the API-based source code design, precise status synchronization and highly efficient source code management were achieved by loose coupling between the UI and OCT engine.\nTo control the hardware compositions in order to successfully achieve a 1-MHz A-line rate, synchronization with precise triggering is essentially required. The working sequence is represented by the main trigger, signals of two scanners, and two cameras, as presented in Fig. 2(b). To initiate the starting signal from UI, the DAQ board generates the main trigger, which ensures synchronized timing of the start of implementation. The operations of both scanners and frame grabbers are started simultaneously at the rising edge of the main trigger.\nTo enhance the speed of STDM-OCT, the SDM method, which simultaneously operates the multiscanners, was implemented. Although two sample arms were composed separately, the x -axis and y -axis galvo scanners of each sample arm were controlled similarly with an equal timing by triangular and square waves. Since the raster scanning method was applied to STDM-OCT, the x -axis scanner bidirectionally scans, while the y -axis scanner moves one step in synchronization at the end of each direction (shown in Fig. 2(b) as forward scanning and backward scanning). The path lengths of each scanner were precisely controlled to multiplex the dual scanning data in a single imaging depth. In the case of frame grabbers, identical but reversed trigger sequences of 250 kHz with 50% duty cycle were transferred to each camera to fully utilize the duty cycle without a dead time, which is called TDM in the proposed scheme. Therefore, two continuous A-lines were captured in one period of camera trigger; it guarantees 500 kHz of the effective imaging rate, which is twice faster than the maximum sampling rate of the line-scan camera. These captured A-lines of each camera were alternately composed of B-scan images. Therefore, the speed of STDM-OCT was further increased to 1 MHz by successfully implementing the integrated STDM method.\n\nDescribes the software algorithm and control platform for the STDM-OCT system, including the use of C++, CUDA, and Qt. Details the synchronization process with triggers, scanners, and cameras to achieve a 1 MHz A-line rate through the SDM method.",
    "original_text": "The developed software algorithm used for STDM-OCT is presented in Fig. 2(a). The developed custom control programming platform is built using C ++ , CUDA, and Qt. Before the initiation of optical scanning, parameter setting and initialization process, in which the scanning mode was selected and the range was matched to the properties of the sample, were performed. In addition, memories are automatically allocated according to previously initiated values. After the initialization stage, the STDM-OCT engine operation is initiated simultaneously with optical scanning and camera acquisition. The acquired raw signal is transferred to the graphics processing unit (GPU) (from host memory to GPU memory) and processed via the CUDA-based signal processing stage, which includes k-domain linearization, the fast Fourier transform, and log scaling. Through the GPU acceleration, data transmission, processing, and displaying process are properly operated in real-time without buffer overflow. After completing the signal processing of the obtained data in one cycle of x-galvo on GPU, the status of the engine (B-mode completed) is transferred and updated to the user interface (UI) via the callback function in the application programming interface (API). When the UI receives the B-mode completed signal, it starts the 'Get image' process by utilizing a callback function in the API for a memory copy of processed data in the OCT engine and displays the image. According to the API-based source code design, precise status synchronization and highly efficient source code management were achieved by loose coupling between the UI and OCT engine.\nTo control the hardware compositions in order to successfully achieve a 1-MHz A-line rate, synchronization with precise triggering is essentially required. The working sequence is represented by the main trigger, signals of two scanners, and two cameras, as presented in Fig. 2(b). To initiate the starting signal from UI, the DAQ board generates the main trigger, which ensures synchronized timing of the start of implementation. The operations of both scanners and frame grabbers are started simultaneously at the rising edge of the main trigger.\nTo enhance the speed of STDM-OCT, the SDM method, which simultaneously operates the multiscanners, was implemented. Although two sample arms were composed separately, the x -axis and y -axis galvo scanners of each sample arm were controlled similarly with an equal timing by triangular and square waves. Since the raster scanning method was applied to STDM-OCT, the x -axis scanner bidirectionally scans, while the y -axis scanner moves one step in synchronization at the end of each direction (shown in Fig. 2(b) as forward scanning and backward scanning). The path lengths of each scanner were precisely controlled to multiplex the dual scanning data in a single imaging depth. In the case of frame grabbers, identical but reversed trigger sequences of 250 kHz with 50% duty cycle were transferred to each camera to fully utilize the duty cycle without a dead time, which is called TDM in the proposed scheme. Therefore, two continuous A-lines were captured in one period of camera trigger; it guarantees 500 kHz of the effective imaging rate, which is twice faster than the maximum sampling rate of the line-scan camera. These captured A-lines of each camera were alternately composed of B-scan images. Therefore, the speed of STDM-OCT was further increased to 1 MHz by successfully implementing the integrated STDM method.",
    "context": "Describes the software algorithm and control platform for the STDM-OCT system, including the use of C++, CUDA, and Qt. Details the synchronization process with triggers, scanners, and cameras to achieve a 1 MHz A-line rate through the SDM method.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      3
    ],
    "id": "15b1bae1f79839dad95fb6f3e1ec83694bced71c46e7ee2597f48321b93b4d38"
  },
  {
    "text": "The image merging process of STDM-OCT with description and generalized configuration of each step is shown in Fig. 3. Since we implemented raster scanning (forward scanning and backward scanning described in Fig. 2 in detail) to fully utilize the duty cycle, the image merging process to obtain the combined B-scan image of each direction is essentially required. As shown in Fig. 3(a) and (b), the B-scan data of both cameras were comprised with both back-and-forth scanning data. We use black for camera #1 and red for camera #2 to distinguish the data of each camera and select representative lines (L1-L4). Fig. 3(c) and (d) demonstrates the merged image of both directions (L1 and L3 for forward and L2 and L4 for backward), and the positions of each line are determined by the description. According to the case of forward scanning presented in Fig. 3(c), the odd-numbered A-lines of merged B-scan images were captured by camera #1 and the even-numbered A-lines by camera #2. As a case of backward scanning shown in Fig. 3(d), a flipping process is additionally required to compensate for the reversed scanning direction of the x -axis scanner after performing the identical merging process of forward scanning. Therefore, the flipping step is applied to Fig. 3(d) and has obtained completed B-scan data, as shown in Fig. 3(e). As an aspect of time-consuming, image merging proceeds in real-time with scanning, data processing, and displaying.\n\nDescribes the image merging process of STDM-OCT, highlighting the need for forward/backward scanning and the flipping step to combine data from the two cameras.",
    "original_text": "The image merging process of STDM-OCT with description and generalized configuration of each step is shown in Fig. 3. Since we implemented raster scanning (forward scanning and backward scanning described in Fig. 2 in detail) to fully utilize the duty cycle, the image merging process to obtain the combined B-scan image of each direction is essentially required. As shown in Fig. 3(a) and (b), the B-scan data of both cameras were comprised with both back-and-forth scanning data. We use black for camera #1 and red for camera #2 to distinguish the data of each camera and select representative lines (L1-L4). Fig. 3(c) and (d) demonstrates the merged image of both directions (L1 and L3 for forward and L2 and L4 for backward), and the positions of each line are determined by the description. According to the case of forward scanning presented in Fig. 3(c), the odd-numbered A-lines of merged B-scan images were captured by camera #1 and the even-numbered A-lines by camera #2. As a case of backward scanning shown in Fig. 3(d), a flipping process is additionally required to compensate for the reversed scanning direction of the x -axis scanner after performing the identical merging process of forward scanning. Therefore, the flipping step is applied to Fig. 3(d) and has obtained completed B-scan data, as shown in Fig. 3(e). As an aspect of time-consuming, image merging proceeds in real-time with scanning, data processing, and displaying.",
    "context": "Describes the image merging process of STDM-OCT, highlighting the need for forward/backward scanning and the flipping step to combine data from the two cameras.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      3
    ],
    "id": "d3c74186766da8fc2f192329f10adfad3900c8f52ab3085782f6d19a2bfaa15e"
  },
  {
    "text": "Fig. 4 demonstrates the UI of the presented STDM-OCT system. UI was largely divided into two parts: signal displaying [see Fig. 4(a)-(e)] and parameter setting [see Fig. 4(f)]. Fig. 4(a) and (b) shows the real-time cross-sectional OCT image of cameras #1 and #2, respectively. Since SDM was\nFig. 3. Detailed description and generalized configuration of flowchart for imaging merging process of STDM-OCT. (a) and (b) Before merging state. (c) and (d) Merged state of (a) and (b). (e) Flipped state of (d).\napplied in the presented system, Fig. 4(a) and (b) is composed of the dual signals obtained from two different scanners. Fig. 4(c) and (d) provides the extracted en face information of two scanners in real-time measured at different positions according to the preset region of interest in the depth direction. In addition, Fig. 4(e) simultaneously shows the OCT raw signals obtained from both cameras. The operational parameters of the STMD-OCT are demonstrated in Fig. 4(f), which are consisted of the scanning range of both axes, the scanning mode, and the region of interest. The actual operating video of the developed software is shown in Video 1 the Supplementary Material (two-axis galvanometer scanner) and Video 2 in the Supplementary Material (one-axis galvanometer scanner with the linear motor stage).\nFig. 4. UI of the STDM-OCT system. (a) OCT cross-sectional view of camera #1. (b) OCT cross-sectional view of camera #2. (c) Extracted en face view of scanner #1. (d) Extracted en face view of scanner #2. (e) Processed raw signals of both cameras. (f) Parameter setting part for STMD-OCT acquisition.\n\nDescribes the STDM-OCT system’s user interface, specifically outlining the display of cross-sectional OCT images from two cameras, extracted en face views from each scanner, processed raw signals, and parameter settings.",
    "original_text": "Fig. 4 demonstrates the UI of the presented STDM-OCT system. UI was largely divided into two parts: signal displaying [see Fig. 4(a)-(e)] and parameter setting [see Fig. 4(f)]. Fig. 4(a) and (b) shows the real-time cross-sectional OCT image of cameras #1 and #2, respectively. Since SDM was\nFig. 3. Detailed description and generalized configuration of flowchart for imaging merging process of STDM-OCT. (a) and (b) Before merging state. (c) and (d) Merged state of (a) and (b). (e) Flipped state of (d).\napplied in the presented system, Fig. 4(a) and (b) is composed of the dual signals obtained from two different scanners. Fig. 4(c) and (d) provides the extracted en face information of two scanners in real-time measured at different positions according to the preset region of interest in the depth direction. In addition, Fig. 4(e) simultaneously shows the OCT raw signals obtained from both cameras. The operational parameters of the STMD-OCT are demonstrated in Fig. 4(f), which are consisted of the scanning range of both axes, the scanning mode, and the region of interest. The actual operating video of the developed software is shown in Video 1 the Supplementary Material (two-axis galvanometer scanner) and Video 2 in the Supplementary Material (one-axis galvanometer scanner with the linear motor stage).\nFig. 4. UI of the STDM-OCT system. (a) OCT cross-sectional view of camera #1. (b) OCT cross-sectional view of camera #2. (c) Extracted en face view of scanner #1. (d) Extracted en face view of scanner #2. (e) Processed raw signals of both cameras. (f) Parameter setting part for STMD-OCT acquisition.",
    "context": "Describes the STDM-OCT system’s user interface, specifically outlining the display of cross-sectional OCT images from two cameras, extracted en face views from each scanner, processed raw signals, and parameter settings.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      3,
      4
    ],
    "id": "c37432cb7ec48ac732498b94125ef511665e7c9c2f641d955e48ca15a5f60eb3"
  },
  {
    "text": "To confirm the feasibility of STDM-OCT for industrial inspection applications, an OTF sample was fabricated. The internal structure of the OTF is largely divided into four layers: a protective film, a transparent film, a deco film, and a base film. The measured thicknesses of each layer were 100, 250, 150, and 100 µ m, respectively. Moreover, the measured total thickness of OTF was 700 µ m, including the vacuum gaps between each layer, which are 40, 20, and 40 µ m. Since the refractive indices of the sublayers in the OTF were different, customized OTF as an industrial sample is appropriate for evaluating the applicability of a high-speed STDM-OCT system. Moreover, the measured total surface area of the sample was 68 × 140 mm, which is suitable for the inspection of the internal structure using wide-field STDM-OCT.\n\nConfirms STDM-OCT feasibility for industrial inspection by evaluating an OTF sample; details the OTF's layered structure, measured thicknesses, and total thickness, highlighting its suitability for wide-field STDM-OCT and its appropriate surface area for internal structure inspection.",
    "original_text": "To confirm the feasibility of STDM-OCT for industrial inspection applications, an OTF sample was fabricated. The internal structure of the OTF is largely divided into four layers: a protective film, a transparent film, a deco film, and a base film. The measured thicknesses of each layer were 100, 250, 150, and 100 µ m, respectively. Moreover, the measured total thickness of OTF was 700 µ m, including the vacuum gaps between each layer, which are 40, 20, and 40 µ m. Since the refractive indices of the sublayers in the OTF were different, customized OTF as an industrial sample is appropriate for evaluating the applicability of a high-speed STDM-OCT system. Moreover, the measured total surface area of the sample was 68 × 140 mm, which is suitable for the inspection of the internal structure using wide-field STDM-OCT.",
    "context": "Confirms STDM-OCT feasibility for industrial inspection by evaluating an OTF sample; details the OTF's layered structure, measured thicknesses, and total thickness, highlighting its suitability for wide-field STDM-OCT and its appropriate surface area for internal structure inspection.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      4
    ],
    "id": "d8512fd8bf3c05b2b8d3a55c87a1f69f76d16350ef631e687088a4d32668dc5b"
  },
  {
    "text": "To quantitatively analyze the performance of STDM-OCT, we evaluated the presented system as an aspect of SDM and TDM. First, the sensitivity of each camera at a 1-MHz A-line rate was measured at every 100 pixels (0.44-mm interval) in the range of 100th-800th, as presented in Fig. 5(a) and (b). In addition, each value of measured sensitivity was shown in Table I as well. Since the developed single spectrometer consists of two line-scan cameras using a beam splitter to identically divide the coherent signal for each detector, the measured sensitivity roll-off values of the two independently detected signals confirmed the almost equivalent performance. The averaged sensitivity difference between the two cameras is 2.5 dB, and the measured peak sensitivities (at the 100th pixel) were 139 and 137 dB, respectively. To demonstrate the degree of image quality degradation in th depth direction, B-scan images of IR-card were obtained, which are shown in Fig. 5(c) and (d) while varying depth position from the zero-path delay to 2.6 mm. The measured pixel range of B-scan images in the depth direction was distinguished by edge colors (red and blue). Following the edge colors, Fig. 5(c) and (d), which were STDM-OCT images of IR-card, was measured at 0-300 and 300-600, respectively. In addition, we obtained the combined\nFig. 5. Quantitative analysis of STDM-OCT performance. (a) Sensitivity fall-off graph of camera #1. (b) Sensitivity fall-off graph of camera #2. (c) and (d) Cross-sectional images of IR-card obtained at different depth positions according to the edge color to evaluate the performance of the SDM. (e) Combined A-scan profile corresponding to the yellow dashed line of Fig. 5(c) and (d).\nTABLE I MEASURED SENSITIVITY ROLL-OFF OF EACH CAMERA\n\n100, Measured sensitivity [dB.Camera #1 = 139. 100, Measured sensitivity [dB.Camera #2 = 137. 200, Measured sensitivity [dB.Camera #1 = 135. 200, Measured sensitivity [dB.Camera #2 = 132. 300, Measured sensitivity [dB.Camera #1 = 129. 300, Measured sensitivity [dB.Camera #2 = 126. 400, Measured sensitivity [dB.Camera #1 = 118. 400, Measured sensitivity [dB.Camera #2 = 120. 500, Measured sensitivity [dB.Camera #1 = 111. 500, Measured sensitivity [dB.Camera #2 = 108. 600, Measured sensitivity [dB.Camera #1 = 108. 600, Measured sensitivity [dB.Camera #2 = 105. 700, Measured sensitivity [dB.Camera #1 = 99. 700, Measured sensitivity [dB.Camera #2 = 96. 800, Measured sensitivity [dB.Camera #1 = 91. 800, Measured sensitivity [dB.Camera #2 = 86\ndepth-dependent A-scan profile [as presented in Fig. 5(e)], which was extracted from the depth information indicated in the yellow dashed line in Fig. 5(c) and (d). As emphasized in Fig. 5(e), four intensity peaks corresponding to each B-scan image (indicated in red and blue (1)-(4) intensity peaks) were extracted from the yellow dashed line in Fig. 5(c) and (d) for an intensity comparison of cross sections. The averaged intensity of the four blue intensity peaks [depth intensity corresponds to Fig. 5(d)] is 10.6%, which decreased compared with the averaged intensity of the four red intensity peaks [depth intensity corresponds to Fig. 5(c)]. However, the internal sublayers of the IR card were obviously observed in both graphs. These results indicated that the SDM method simultaneously utilizes a full detectable range of camera pixels, which can be applied to the STDM-OCT. Moreover, Table II demonstrates the result of quantitative performance evaluation of the developed STMD-OCT for various categories, including scanning speed, resolution, and imaging depth.\nFurthermore, to quantitatively assess the TDM method applied to STDM-OCT, we independently obtained two B-scan images from each camera (before merging) presented in Fig. 6(a) and (b), which are consisted of bidirectional images (demonstrated in Fig. 2(b) as forward scanning and backward scanning). To accurately obtain the scanned data with TDM, the image-merging process described in\nFig. 6. Verification of the result of image merging process of STMD-OCT, as described in Section II-C. (a) and (b) Cross-sectional IR detection card images of cameras #1 and #2. (c) and (d) Merged images of (a) and (b). (e) Flipped image of (d). (f)-(i) A-scan profiling results of L1 (and L1') to L4 (and L4') indicated in (a)-(d).\n\nA-scan rate, Obtained value = MHz. Scanning range (representative), Obtained value = 250 250 x 2048 pixels x 4.5 x 5 mm). 3D volume rate (representative), Obtained value = 8 volls. Sensitivity, Obtained value = 139 dB. 10 dB roll-off depth, Obtained value = 1.3 mm. Lateral resolution, Obtained value = . Axial resolution (in air), Obtained value = . Imaging depth (in air), Obtained value = 5 mm\nTABLE II OBTAINED QUANTITATIVE PERFORMANCE OF STDM-OCT\nSection II-C was essentially required. In order to clearly elucidate the merging process of TDM, four lines (L1-L4) in Fig. 6(a) and (b) were representatively selected. L1 and L3 are the 300th A-lines of the B-scan image, which is part of forward scanning [first to 500th in Fig. 6(a) and (b)] obtained by each camera. L2 and L4 are the 800th A-lines, which were included in backward scanning [500th to 1000th in Fig. 6(a) and (b)]. L1 and L3 (300th) and L2 and L4 (800th) are the same A-line positions of B-scan images, but the obtained structural information is different because cameras were alternatively operated, whereas the galvanometer scanner performed continuous scanning, which can be simultaneously verified by the A-scan profiling results in Fig. 6(e)-(h). Following the image merging process described\nFig. 7. (a) Top view of the OTF indicated by five yellow dashed lines (L1-L5). (b)-(f) Corresponding cross-sectional OCT images of the OTF samples sequentially measured at five representative lines presented in Fig. 7(a) as L1-L5. Both the red and blue bars, as presented in (b)-(f), represent the space-divided pixel range of each scanner, which covers 0-300 and 300-600, respectively.\nin Section II-C, L1 and L3, which are both 300th lines of B-scan images (forward scanning), were matched to L1' (599th) and L3' (600th) in Fig. 6(c), respectively. In addition, the finally obtained B-scan image of backward direction is shown in Fig. 6(e), which is the result of applying the flipping step to Fig. 6(d). In this way, L2 and L4, which are both 800th lines, corresponded to L2' (400th) and L4' (399th) in Fig. 6(d). Hence, these results indicate that the TDM utilizing sequentially operated multicameras in a single spectrometer was accurately implemented to STDM-OCT in order to achieve a 1-MHz A-line sampling rate.\n\nEvaluates STDM-OCT performance through SDM and TDM, measuring camera sensitivity and depth image quality using IR-card samples.",
    "original_text": "To quantitatively analyze the performance of STDM-OCT, we evaluated the presented system as an aspect of SDM and TDM. First, the sensitivity of each camera at a 1-MHz A-line rate was measured at every 100 pixels (0.44-mm interval) in the range of 100th-800th, as presented in Fig. 5(a) and (b). In addition, each value of measured sensitivity was shown in Table I as well. Since the developed single spectrometer consists of two line-scan cameras using a beam splitter to identically divide the coherent signal for each detector, the measured sensitivity roll-off values of the two independently detected signals confirmed the almost equivalent performance. The averaged sensitivity difference between the two cameras is 2.5 dB, and the measured peak sensitivities (at the 100th pixel) were 139 and 137 dB, respectively. To demonstrate the degree of image quality degradation in th depth direction, B-scan images of IR-card were obtained, which are shown in Fig. 5(c) and (d) while varying depth position from the zero-path delay to 2.6 mm. The measured pixel range of B-scan images in the depth direction was distinguished by edge colors (red and blue). Following the edge colors, Fig. 5(c) and (d), which were STDM-OCT images of IR-card, was measured at 0-300 and 300-600, respectively. In addition, we obtained the combined\nFig. 5. Quantitative analysis of STDM-OCT performance. (a) Sensitivity fall-off graph of camera #1. (b) Sensitivity fall-off graph of camera #2. (c) and (d) Cross-sectional images of IR-card obtained at different depth positions according to the edge color to evaluate the performance of the SDM. (e) Combined A-scan profile corresponding to the yellow dashed line of Fig. 5(c) and (d).\nTABLE I MEASURED SENSITIVITY ROLL-OFF OF EACH CAMERA\n\n100, Measured sensitivity [dB.Camera #1 = 139. 100, Measured sensitivity [dB.Camera #2 = 137. 200, Measured sensitivity [dB.Camera #1 = 135. 200, Measured sensitivity [dB.Camera #2 = 132. 300, Measured sensitivity [dB.Camera #1 = 129. 300, Measured sensitivity [dB.Camera #2 = 126. 400, Measured sensitivity [dB.Camera #1 = 118. 400, Measured sensitivity [dB.Camera #2 = 120. 500, Measured sensitivity [dB.Camera #1 = 111. 500, Measured sensitivity [dB.Camera #2 = 108. 600, Measured sensitivity [dB.Camera #1 = 108. 600, Measured sensitivity [dB.Camera #2 = 105. 700, Measured sensitivity [dB.Camera #1 = 99. 700, Measured sensitivity [dB.Camera #2 = 96. 800, Measured sensitivity [dB.Camera #1 = 91. 800, Measured sensitivity [dB.Camera #2 = 86\ndepth-dependent A-scan profile [as presented in Fig. 5(e)], which was extracted from the depth information indicated in the yellow dashed line in Fig. 5(c) and (d). As emphasized in Fig. 5(e), four intensity peaks corresponding to each B-scan image (indicated in red and blue (1)-(4) intensity peaks) were extracted from the yellow dashed line in Fig. 5(c) and (d) for an intensity comparison of cross sections. The averaged intensity of the four blue intensity peaks [depth intensity corresponds to Fig. 5(d)] is 10.6%, which decreased compared with the averaged intensity of the four red intensity peaks [depth intensity corresponds to Fig. 5(c)]. However, the internal sublayers of the IR card were obviously observed in both graphs. These results indicated that the SDM method simultaneously utilizes a full detectable range of camera pixels, which can be applied to the STDM-OCT. Moreover, Table II demonstrates the result of quantitative performance evaluation of the developed STMD-OCT for various categories, including scanning speed, resolution, and imaging depth.\nFurthermore, to quantitatively assess the TDM method applied to STDM-OCT, we independently obtained two B-scan images from each camera (before merging) presented in Fig. 6(a) and (b), which are consisted of bidirectional images (demonstrated in Fig. 2(b) as forward scanning and backward scanning). To accurately obtain the scanned data with TDM, the image-merging process described in\nFig. 6. Verification of the result of image merging process of STMD-OCT, as described in Section II-C. (a) and (b) Cross-sectional IR detection card images of cameras #1 and #2. (c) and (d) Merged images of (a) and (b). (e) Flipped image of (d). (f)-(i) A-scan profiling results of L1 (and L1') to L4 (and L4') indicated in (a)-(d).\n\nA-scan rate, Obtained value = MHz. Scanning range (representative), Obtained value = 250 250 x 2048 pixels x 4.5 x 5 mm). 3D volume rate (representative), Obtained value = 8 volls. Sensitivity, Obtained value = 139 dB. 10 dB roll-off depth, Obtained value = 1.3 mm. Lateral resolution, Obtained value = . Axial resolution (in air), Obtained value = . Imaging depth (in air), Obtained value = 5 mm\nTABLE II OBTAINED QUANTITATIVE PERFORMANCE OF STDM-OCT\nSection II-C was essentially required. In order to clearly elucidate the merging process of TDM, four lines (L1-L4) in Fig. 6(a) and (b) were representatively selected. L1 and L3 are the 300th A-lines of the B-scan image, which is part of forward scanning [first to 500th in Fig. 6(a) and (b)] obtained by each camera. L2 and L4 are the 800th A-lines, which were included in backward scanning [500th to 1000th in Fig. 6(a) and (b)]. L1 and L3 (300th) and L2 and L4 (800th) are the same A-line positions of B-scan images, but the obtained structural information is different because cameras were alternatively operated, whereas the galvanometer scanner performed continuous scanning, which can be simultaneously verified by the A-scan profiling results in Fig. 6(e)-(h). Following the image merging process described\nFig. 7. (a) Top view of the OTF indicated by five yellow dashed lines (L1-L5). (b)-(f) Corresponding cross-sectional OCT images of the OTF samples sequentially measured at five representative lines presented in Fig. 7(a) as L1-L5. Both the red and blue bars, as presented in (b)-(f), represent the space-divided pixel range of each scanner, which covers 0-300 and 300-600, respectively.\nin Section II-C, L1 and L3, which are both 300th lines of B-scan images (forward scanning), were matched to L1' (599th) and L3' (600th) in Fig. 6(c), respectively. In addition, the finally obtained B-scan image of backward direction is shown in Fig. 6(e), which is the result of applying the flipping step to Fig. 6(d). In this way, L2 and L4, which are both 800th lines, corresponded to L2' (400th) and L4' (399th) in Fig. 6(d). Hence, these results indicate that the TDM utilizing sequentially operated multicameras in a single spectrometer was accurately implemented to STDM-OCT in order to achieve a 1-MHz A-line sampling rate.",
    "context": "Evaluates STDM-OCT performance through SDM and TDM, measuring camera sensitivity and depth image quality using IR-card samples.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      4,
      5,
      6
    ],
    "id": "84309a28efbe4a24cc08b12ceff3fd4ca0939a5e3cfd896d1850b36e57153299"
  },
  {
    "text": "To demonstrate the applicability of STDM-OCT for industrial applications, laboratory-customized OTF was initially evaluated as a representative sample of industrial applications. Fabricated OTF sample was placed on a linear-motor stage, and we continued with the 2000 frames of B-scan with 70µ m intervals in the OTF inspection (totally, 2000 × 2000 × 2048 pixels), which consumes 8 s for the whole-range scanning of the sample. The obtained results of the OTF inspection through STDM-OCT are presented in Fig. 7. A photograph of the OTF is presented in Fig. 7(a), indicating five representative scanning positions (L1-L5). Moreover, the cross section extracted locations were 140th, 500th, 1000th, 1500th, and 1900th lines, which were measured at 0.56, 2, 4, 6, and 7.6 s, respectively. Cross-sectional images of the OTF measured by STDM-OCT-based inspection system from L1 to L5 are sequentially presented in Fig. 7(b)-(f). In addition, the scanning ranges of each scanner used for SDM are indicated by edge color and color bar (red and blue) in Fig. 7(a). As presented in Fig. 7(b)-(f), the layer structure of the OTF is well distinguished at every measuring position (L1-L5) during the whole-range scanning of the sample. Therefore, these results reveal that the STDM-OCT with a 1-MHz A-line rate is feasible for industrial applications, ensuring high accuracy and reliable quality with a minimum inspection time.\n\nDemonstrates the feasibility of STDM-OCT for industrial applications by evaluating a customized OTF sample, showcasing its ability to distinguish the OTF's layer structure and achieve high accuracy with minimal inspection time.",
    "original_text": "To demonstrate the applicability of STDM-OCT for industrial applications, laboratory-customized OTF was initially evaluated as a representative sample of industrial applications. Fabricated OTF sample was placed on a linear-motor stage, and we continued with the 2000 frames of B-scan with 70µ m intervals in the OTF inspection (totally, 2000 × 2000 × 2048 pixels), which consumes 8 s for the whole-range scanning of the sample. The obtained results of the OTF inspection through STDM-OCT are presented in Fig. 7. A photograph of the OTF is presented in Fig. 7(a), indicating five representative scanning positions (L1-L5). Moreover, the cross section extracted locations were 140th, 500th, 1000th, 1500th, and 1900th lines, which were measured at 0.56, 2, 4, 6, and 7.6 s, respectively. Cross-sectional images of the OTF measured by STDM-OCT-based inspection system from L1 to L5 are sequentially presented in Fig. 7(b)-(f). In addition, the scanning ranges of each scanner used for SDM are indicated by edge color and color bar (red and blue) in Fig. 7(a). As presented in Fig. 7(b)-(f), the layer structure of the OTF is well distinguished at every measuring position (L1-L5) during the whole-range scanning of the sample. Therefore, these results reveal that the STDM-OCT with a 1-MHz A-line rate is feasible for industrial applications, ensuring high accuracy and reliable quality with a minimum inspection time.",
    "context": "Demonstrates the feasibility of STDM-OCT for industrial applications by evaluating a customized OTF sample, showcasing its ability to distinguish the OTF's layer structure and achieve high accuracy with minimal inspection time.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      6
    ],
    "id": "9b5994418ebd01657e49306db316087747fb9205564c6a3bb2fca0c2e31e174d"
  },
  {
    "text": "Here, we demonstrated the SD-OCT of a 1-MHz A-line rate and introduced a single-spectrometer-based STDM method. Various approaches to enhance imaging speed have been keenly studied and demonstrated. Compared with the conventional method of using a multiple-camera-based system [28], the proposed STDM-OCT of the current research not only resolves the alignment difference error of each spectrometer but also minimizes power loss and effectively economizes the total cost of the system (single-spectrometer-based system configuration). Furthermore, STDM-OCT partially provides enhanced SNR, sensitivity (more than 130 dB), and imaging depth (more than 4 mm) with cost-effective development in comparison with the optical demultiplexer-based method [29], [30], the streak-mode OCT [31], and the parallel OCT [32], [34]. In addition, as an aspect of system price, which is a factor to broaden applications of OCT, SD-OCT has obvious merit compared to SS-OCT. Based on this fact, the developed STDM-OCT provides a 1-MHz A-scan rate, which reduces the inspection time, with comparably low cost to develop compared to SS-OCT. Furthermore, SS-OCT provides a high-speed A-scan rate; however, to the best of our knowledge, the maximum sweeping rate of the commercial swept source is up to 400 kHz, which can be sufficiently obtained by the developed STDM-OCT.\nIn the case of the SDM method applied to STDM-OCT, two scanners were separately used, where the path lengths were differently maintained to ensure a wide scanning range of the OTF sample. However, single-scanner-based conventional SDM can also be applied to match the sample properties. As an aspect of 3-D volumetric imaging by integrating GPU parallel processing with the STDM method, 8 vol/s for 250 × 250 × 2048 pixels (9 × 4.5 × 5 mm) was achieved, which covers a much wider scanning range maintaining a high speed compared with the conventional real-time SD-OCT systems [39]-[41]. Furthermore, the volume rate of STDM-OCT can be freely adjusted according to experimental conditions, such as 2 vol/s for 16 × 8 × 5 mm and 0.5 vol/s for 30 × 16 × 5 mm. In addition, the scanning speed and range can be further increased by applying a multiple-STDM method, which employs additional scanners and cameras. Since we applied GPU acceleration, signal processing and displaying (including cross-sectional images and extracted each scanner's en face images) are processed in real time. Only when the image save process is proceeded, displaying B-scan images are updated with preset interval, as shown in Video 2 in the Supplementary Material, while signal processing and saving are processed in real time. In addition, each scanner's spliced B-scan images can be obtained by applying our extracting method described in [42].\nHere, a customized OTF was chosen as a sample to confirm the feasibility of the presented ultrahigh-speed OCT system\nin industrial applications for the measurement of the final product to inspect the cracks, which causes huge losses and damage to the product. As an aspect of the thickness of the sample, the path length of the reference arm is adjustable to match the focal point without overlapping each scanner's pixel position. Based on the obtained results, STDM-OCT demonstrates the possibility of applying to various fields, which requires high-speed systems, including industrial product manufacturing and inspecting, such as ultrathin glass ( ∼ 100 µ m), polarizer ( ∼ 300 µ m), glass substrate ( ∼ 500 µ m), OTF ( ∼ 700 µ m), and light guide plate ( ∼ 1 mm), which are within an imaging range of the proposed system (4.4 mm) and even biomedical applications. Furthermore, the successful utilization of higher-speed data-transferring techniques and processing techniques and faster line-scan cameras can further enhance the speed of the system beyond the developed 1-MHz\nA-scan rate.\n\nHighlights the feasibility of the ultrahigh-speed STDM-OCT system for industrial applications, emphasizing its cost-effectiveness and advantages over conventional multi-camera systems.",
    "original_text": "Here, we demonstrated the SD-OCT of a 1-MHz A-line rate and introduced a single-spectrometer-based STDM method. Various approaches to enhance imaging speed have been keenly studied and demonstrated. Compared with the conventional method of using a multiple-camera-based system [28], the proposed STDM-OCT of the current research not only resolves the alignment difference error of each spectrometer but also minimizes power loss and effectively economizes the total cost of the system (single-spectrometer-based system configuration). Furthermore, STDM-OCT partially provides enhanced SNR, sensitivity (more than 130 dB), and imaging depth (more than 4 mm) with cost-effective development in comparison with the optical demultiplexer-based method [29], [30], the streak-mode OCT [31], and the parallel OCT [32], [34]. In addition, as an aspect of system price, which is a factor to broaden applications of OCT, SD-OCT has obvious merit compared to SS-OCT. Based on this fact, the developed STDM-OCT provides a 1-MHz A-scan rate, which reduces the inspection time, with comparably low cost to develop compared to SS-OCT. Furthermore, SS-OCT provides a high-speed A-scan rate; however, to the best of our knowledge, the maximum sweeping rate of the commercial swept source is up to 400 kHz, which can be sufficiently obtained by the developed STDM-OCT.\nIn the case of the SDM method applied to STDM-OCT, two scanners were separately used, where the path lengths were differently maintained to ensure a wide scanning range of the OTF sample. However, single-scanner-based conventional SDM can also be applied to match the sample properties. As an aspect of 3-D volumetric imaging by integrating GPU parallel processing with the STDM method, 8 vol/s for 250 × 250 × 2048 pixels (9 × 4.5 × 5 mm) was achieved, which covers a much wider scanning range maintaining a high speed compared with the conventional real-time SD-OCT systems [39]-[41]. Furthermore, the volume rate of STDM-OCT can be freely adjusted according to experimental conditions, such as 2 vol/s for 16 × 8 × 5 mm and 0.5 vol/s for 30 × 16 × 5 mm. In addition, the scanning speed and range can be further increased by applying a multiple-STDM method, which employs additional scanners and cameras. Since we applied GPU acceleration, signal processing and displaying (including cross-sectional images and extracted each scanner's en face images) are processed in real time. Only when the image save process is proceeded, displaying B-scan images are updated with preset interval, as shown in Video 2 in the Supplementary Material, while signal processing and saving are processed in real time. In addition, each scanner's spliced B-scan images can be obtained by applying our extracting method described in [42].\nHere, a customized OTF was chosen as a sample to confirm the feasibility of the presented ultrahigh-speed OCT system\nin industrial applications for the measurement of the final product to inspect the cracks, which causes huge losses and damage to the product. As an aspect of the thickness of the sample, the path length of the reference arm is adjustable to match the focal point without overlapping each scanner's pixel position. Based on the obtained results, STDM-OCT demonstrates the possibility of applying to various fields, which requires high-speed systems, including industrial product manufacturing and inspecting, such as ultrathin glass ( ∼ 100 µ m), polarizer ( ∼ 300 µ m), glass substrate ( ∼ 500 µ m), OTF ( ∼ 700 µ m), and light guide plate ( ∼ 1 mm), which are within an imaging range of the proposed system (4.4 mm) and even biomedical applications. Furthermore, the successful utilization of higher-speed data-transferring techniques and processing techniques and faster line-scan cameras can further enhance the speed of the system beyond the developed 1-MHz\nA-scan rate.",
    "context": "Highlights the feasibility of the ultrahigh-speed STDM-OCT system for industrial applications, emphasizing its cost-effectiveness and advantages over conventional multi-camera systems.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      6,
      7
    ],
    "id": "f440435aeaf72d17f912c1200fd3bbf254c3cb4d5807b9d56c88583ac35bfb15"
  },
  {
    "text": "In conclusion, the ultrahigh-speed SD-OCT system up to a 1-MHz A-scan rate was achieved by implementing the developed STDM method. We used multicameras in single spectrometer to resolve the existing limitations of the TDM method, such as alignment errors of each spectrometer, power loss of detected interference signal caused by the use of multiple couplers, and sacrificing the spectral resolution of the camera. In addition, multiscanners were simultaneously used to implement the SDM to cover a wide scanning range. To accurately and rapidly control the hardware operation timing while employing scanners and cameras with high performance, a C ++ -based software platform with CUDA and Qt was developed and utilized to improve the data processing speed. The quantitatively analyzed results successfully demonstrate the effective implementation of the STDM method for achieving a 1-MHz A-scan rate. In addition, we inspected the OTF as a sample to confirm the feasibility of the developed STDM-OCT system for industrial applications. The obtained results of the implemented STDM method demonstrate the feasibility of the presented system for both real-time volumetric imaging and wide-field scanning at a fast speed according to the required conditions. Hence, the proposed ultrahighspeed STDM-OCT shows promising results encouraging the application of SD-OCT to various fields, including biomedical and industrial measurement fields, such as the noninvasive quality test of the final product and defect inspection, essentially requiring fast scanning speed, while maintaining the advantages of the conventional SD-OCT system.\n\nSummarizes the successful implementation of the STDM-OCT system, highlighting its achievement of a 1-MHz A-scan rate through the use of multiple cameras and a custom software platform, and confirms its feasibility for industrial applications like quality testing and defect inspection.",
    "original_text": "In conclusion, the ultrahigh-speed SD-OCT system up to a 1-MHz A-scan rate was achieved by implementing the developed STDM method. We used multicameras in single spectrometer to resolve the existing limitations of the TDM method, such as alignment errors of each spectrometer, power loss of detected interference signal caused by the use of multiple couplers, and sacrificing the spectral resolution of the camera. In addition, multiscanners were simultaneously used to implement the SDM to cover a wide scanning range. To accurately and rapidly control the hardware operation timing while employing scanners and cameras with high performance, a C ++ -based software platform with CUDA and Qt was developed and utilized to improve the data processing speed. The quantitatively analyzed results successfully demonstrate the effective implementation of the STDM method for achieving a 1-MHz A-scan rate. In addition, we inspected the OTF as a sample to confirm the feasibility of the developed STDM-OCT system for industrial applications. The obtained results of the implemented STDM method demonstrate the feasibility of the presented system for both real-time volumetric imaging and wide-field scanning at a fast speed according to the required conditions. Hence, the proposed ultrahighspeed STDM-OCT shows promising results encouraging the application of SD-OCT to various fields, including biomedical and industrial measurement fields, such as the noninvasive quality test of the final product and defect inspection, essentially requiring fast scanning speed, while maintaining the advantages of the conventional SD-OCT system.",
    "context": "Summarizes the successful implementation of the STDM-OCT system, highlighting its achievement of a 1-MHz A-scan rate through the use of multiple cameras and a custom software platform, and confirms its feasibility for industrial applications like quality testing and defect inspection.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      7
    ],
    "id": "1698910a20c6de3626ade4bc5134e2509393b6534482b8b3eaa71924680f3903"
  },
  {
    "text": "- [1] D. Huang et al. , 'Optical coherence tomography,' Science , vol. 254, no. 5035, pp. 1178-1181, 1991.\n- [2] J. M. Schmitt, 'Optical coherence tomography (OCT): A review,' IEEE J. Sel. Topics Quantum Electron. , vol. 5, no. 4, pp. 1205-1215, Jul. 1999.\n- [3] W. Drexler, U. Morgner, R. K. Ghanta, F. X. Kärtner, J. S. Schuman, and J. G. Fujimoto, 'Ultrahigh-resolution ophthalmic optical coherence tomography,' Nature Med. , vol. 7, no. 4, pp. 502-507, 2001.\n- [4] Y. S. Hsieh et al. , 'Dental optical coherence tomography,' Sensors , vol. 13, no. 7, pp. 8928-8949, Jul. 2013.\n- [5] J. Lee et al. , 'Decalcification using ethylenediaminetetraacetic acid for clear microstructure imaging of cochlea through optical coherence tomography,' J. Biomed. Opt. , vol. 21, no. 8, Mar. 2016, Art. no. 081204.\n- [6] S. Van der Jeught, J. J. J. Dirckx, J. R. M. Aerts, A. Bradu, A. G. Podoleanu, and J. A. N. Buytaert, 'Full-field thickness distribution of human tympanic membrane obtained with optical coherence tomography,' J. Assoc. Res. Otolaryngol. , vol. 14, no. 4, pp. 483-494, Aug. 2013.\n- [7] D. Seong et al. , ' In situ characterization of micro-vibration in natural latex membrane resembling tympanic membrane functionally using optical Doppler tomography,' Sensors , vol. 20, no. 1, p. 64, Dec. 2019.\n- [8] F. Liu et al. , 'A flexible touch-based fingerprint acquisition device and a benchmark database using optical coherence tomography,' IEEE Trans. Instrum. Meas. , vol. 69, no. 9, pp. 6518-6529, Sep. 2020.\n- [9] H. Sun et al. , 'Synchronous fingerprint acquisition system based on total internal reflection and optical coherence tomography,' IEEE Trans. Instrum. Meas. , vol. 69, no. 10, pp. 8452-8465, Oct. 2020.\n- [10] M. D. Duncan, M. Bashkansky, and J. Reintjes, 'Subsurface defect detection in materials using optical coherence tomography,' Opt. Exp. , vol. 2, pp. 540-545, Jun. 1998.\n- [11] Z. Chen, Y. Shen, W. Bao, P. Li, X. Wang, and Z. Ding, 'Identification of surface defects on glass by parallel spectral domain optical coherence tomography,' Opt. Exp. , vol. 23, no. 18, pp. 23634-23646, 2015.\n- [12] A. F. Low, G. J. Tearney, B. E. Bouma, and I.-K. Jang, 'Technology insight: Optical coherence tomography-Current status and future development,' Nature Clin. Pract. Cardiovascular Med. , vol. 3, no. 3, pp. 154-162, Mar. 2006.\n- [13] M. Gargesha, M. W. Jenkins, D. L. Wilson, and A. M. Rollins, 'High temporal resolution OCT using image-based retrospective gating,' Opt. Exp. , vol. 17, pp. 10786-10799, Jun. 2009.\n- [14] M. Bashkansky, M. Duncan, M. Kahn, D. Lewis, and J. Reintjes, 'Subsurface defect detection in ceramics by high-speed high-resolution optical coherent tomography,' Opt. Lett. , vol. 22, pp. 61-63, Jan. 1997.\n- [15] N. H. Cho, K. Park, J.-Y. Kim, Y. Jung, and J. Kim, 'Quantitative assessment of touch-screen panel by nondestructive inspection with three-dimensional real-time display optical coherence tomography,' Opt. Lasers Eng. , vol. 68, pp. 50-57, May 2015.\n- [16] M. A. Choma, M. V. Sarunic, C. Yang, and J. A. Izatt, 'Sensitivity advantage of swept source and Fourier domain optical coherence tomography,' Opt. Exp. , vol. 11, no. 18, pp. 2183-2189, 2003.\n- [17] R. Leitgeb, C. K. Hitzenberger, and A. F. Fercher, 'Performance of Fourier domain vs. time domain optical coherence tomography,' Opt. Exp. , vol. 11, no. 8, pp. 889-894, Apr. 2003.\n- [18] S. Chinn, E. Swanson, and J. Fujimoto, 'Optical coherence tomography using a frequency-tunable optical source,' Opt. Lett. , vol. 22, pp. 340-342, Mar. 1997.\n- [19] T. Klein and R. Huber, 'High-speed OCT light sources and systems,' Biomed. Opt. Exp. , vol. 8, pp. 828-859, Feb. 2017.\n- [20] T. Klein, W. Wieser, C. M. Eigenwillig, B. R. Biedermann, and R. Huber, 'Megahertz OCT for ultrawide-field retinal imaging with a 1050 nm Fourier domain mode-locked laser,' Opt. Exp. , vol. 19, pp. 3044-3062, Feb. 2011.\n- [21] T. Klein, W. Wieser, L. Reznicek, A. Neubauer, A. Kampik, and R. Huber, 'Multi-MHz retinal OCT,' Biomed. Opt. Exp. , vol. 4, pp. 1890-1908, Oct. 2013.\n- [22] J. P. Kolb, T. Pfeiffer, M. Eibl, H. Hakert, and R. Huber, 'Highresolution retinal swept source optical coherence tomography with an ultra-wideband Fourier-domain mode-locked laser at MHz A-scan rates,' Biomed. Opt. Exp. , vol. 9, pp. 120-130, Jan. 2018.\n- [23] F. Lavinsky and D. Lavinsky, 'Novel perspectives on swept-source optical coherence tomography,' Int. J. Retina Vitreous , vol. 2, no. 1, pp. 1-11, Dec. 2016.\n- [24] B. Potsaid et al. , 'Ultrahigh speed 1050nm swept source/Fourier domain OCT retinal and anterior segment imaging at 100,000 to 400,000 axial scans per second,' Opt. Exp. , vol. 18, pp. 20029-20048, Sep. 2010.\n- [25] A. Y. Alibhai, C. Or, and A. J. Witkin, 'Swept source optical coherence tomography: A review,' Current Ophthalmol. Rep. , vol. 6, pp. 7-16, Mar. 2018.\n- [26] L. An, G. Guan, and R. K. Wang, 'High-speed 1310 nm-band spectral domain optical coherence tomography at 184,000 lines per second,' J. Biomed. Opt. , vol. 16, no. 6, 2011, Art. no. 060506.\n- [27] L. An, P. Li, T. T. Shen, and R. Wang, 'High speed spectral domain optical coherence tomography for retinal imaging at 500,000 A-lines per second,' Biomed. Opt. Exp. , vol. 2, pp. 2770-2783, Oct. 2011.\n- [28] O. P. Kocaoglu, T. L. Turner, Z. Liu, and D. T. Miller, 'Adaptive optics optical coherence tomography at 1 MHz,' Biomed. Opt. Exp. , vol. 5, pp. 4186-4200, Dec. 2014.\n\nProvides a historical overview of optical coherence tomography, including early reviews, advancements in scanning speed, and applications in diverse fields like ophthalmology, dentistry, and industrial defect detection.",
    "original_text": "- [1] D. Huang et al. , 'Optical coherence tomography,' Science , vol. 254, no. 5035, pp. 1178-1181, 1991.\n- [2] J. M. Schmitt, 'Optical coherence tomography (OCT): A review,' IEEE J. Sel. Topics Quantum Electron. , vol. 5, no. 4, pp. 1205-1215, Jul. 1999.\n- [3] W. Drexler, U. Morgner, R. K. Ghanta, F. X. Kärtner, J. S. Schuman, and J. G. Fujimoto, 'Ultrahigh-resolution ophthalmic optical coherence tomography,' Nature Med. , vol. 7, no. 4, pp. 502-507, 2001.\n- [4] Y. S. Hsieh et al. , 'Dental optical coherence tomography,' Sensors , vol. 13, no. 7, pp. 8928-8949, Jul. 2013.\n- [5] J. Lee et al. , 'Decalcification using ethylenediaminetetraacetic acid for clear microstructure imaging of cochlea through optical coherence tomography,' J. Biomed. Opt. , vol. 21, no. 8, Mar. 2016, Art. no. 081204.\n- [6] S. Van der Jeught, J. J. J. Dirckx, J. R. M. Aerts, A. Bradu, A. G. Podoleanu, and J. A. N. Buytaert, 'Full-field thickness distribution of human tympanic membrane obtained with optical coherence tomography,' J. Assoc. Res. Otolaryngol. , vol. 14, no. 4, pp. 483-494, Aug. 2013.\n- [7] D. Seong et al. , ' In situ characterization of micro-vibration in natural latex membrane resembling tympanic membrane functionally using optical Doppler tomography,' Sensors , vol. 20, no. 1, p. 64, Dec. 2019.\n- [8] F. Liu et al. , 'A flexible touch-based fingerprint acquisition device and a benchmark database using optical coherence tomography,' IEEE Trans. Instrum. Meas. , vol. 69, no. 9, pp. 6518-6529, Sep. 2020.\n- [9] H. Sun et al. , 'Synchronous fingerprint acquisition system based on total internal reflection and optical coherence tomography,' IEEE Trans. Instrum. Meas. , vol. 69, no. 10, pp. 8452-8465, Oct. 2020.\n- [10] M. D. Duncan, M. Bashkansky, and J. Reintjes, 'Subsurface defect detection in materials using optical coherence tomography,' Opt. Exp. , vol. 2, pp. 540-545, Jun. 1998.\n- [11] Z. Chen, Y. Shen, W. Bao, P. Li, X. Wang, and Z. Ding, 'Identification of surface defects on glass by parallel spectral domain optical coherence tomography,' Opt. Exp. , vol. 23, no. 18, pp. 23634-23646, 2015.\n- [12] A. F. Low, G. J. Tearney, B. E. Bouma, and I.-K. Jang, 'Technology insight: Optical coherence tomography-Current status and future development,' Nature Clin. Pract. Cardiovascular Med. , vol. 3, no. 3, pp. 154-162, Mar. 2006.\n- [13] M. Gargesha, M. W. Jenkins, D. L. Wilson, and A. M. Rollins, 'High temporal resolution OCT using image-based retrospective gating,' Opt. Exp. , vol. 17, pp. 10786-10799, Jun. 2009.\n- [14] M. Bashkansky, M. Duncan, M. Kahn, D. Lewis, and J. Reintjes, 'Subsurface defect detection in ceramics by high-speed high-resolution optical coherent tomography,' Opt. Lett. , vol. 22, pp. 61-63, Jan. 1997.\n- [15] N. H. Cho, K. Park, J.-Y. Kim, Y. Jung, and J. Kim, 'Quantitative assessment of touch-screen panel by nondestructive inspection with three-dimensional real-time display optical coherence tomography,' Opt. Lasers Eng. , vol. 68, pp. 50-57, May 2015.\n- [16] M. A. Choma, M. V. Sarunic, C. Yang, and J. A. Izatt, 'Sensitivity advantage of swept source and Fourier domain optical coherence tomography,' Opt. Exp. , vol. 11, no. 18, pp. 2183-2189, 2003.\n- [17] R. Leitgeb, C. K. Hitzenberger, and A. F. Fercher, 'Performance of Fourier domain vs. time domain optical coherence tomography,' Opt. Exp. , vol. 11, no. 8, pp. 889-894, Apr. 2003.\n- [18] S. Chinn, E. Swanson, and J. Fujimoto, 'Optical coherence tomography using a frequency-tunable optical source,' Opt. Lett. , vol. 22, pp. 340-342, Mar. 1997.\n- [19] T. Klein and R. Huber, 'High-speed OCT light sources and systems,' Biomed. Opt. Exp. , vol. 8, pp. 828-859, Feb. 2017.\n- [20] T. Klein, W. Wieser, C. M. Eigenwillig, B. R. Biedermann, and R. Huber, 'Megahertz OCT for ultrawide-field retinal imaging with a 1050 nm Fourier domain mode-locked laser,' Opt. Exp. , vol. 19, pp. 3044-3062, Feb. 2011.\n- [21] T. Klein, W. Wieser, L. Reznicek, A. Neubauer, A. Kampik, and R. Huber, 'Multi-MHz retinal OCT,' Biomed. Opt. Exp. , vol. 4, pp. 1890-1908, Oct. 2013.\n- [22] J. P. Kolb, T. Pfeiffer, M. Eibl, H. Hakert, and R. Huber, 'Highresolution retinal swept source optical coherence tomography with an ultra-wideband Fourier-domain mode-locked laser at MHz A-scan rates,' Biomed. Opt. Exp. , vol. 9, pp. 120-130, Jan. 2018.\n- [23] F. Lavinsky and D. Lavinsky, 'Novel perspectives on swept-source optical coherence tomography,' Int. J. Retina Vitreous , vol. 2, no. 1, pp. 1-11, Dec. 2016.\n- [24] B. Potsaid et al. , 'Ultrahigh speed 1050nm swept source/Fourier domain OCT retinal and anterior segment imaging at 100,000 to 400,000 axial scans per second,' Opt. Exp. , vol. 18, pp. 20029-20048, Sep. 2010.\n- [25] A. Y. Alibhai, C. Or, and A. J. Witkin, 'Swept source optical coherence tomography: A review,' Current Ophthalmol. Rep. , vol. 6, pp. 7-16, Mar. 2018.\n- [26] L. An, G. Guan, and R. K. Wang, 'High-speed 1310 nm-band spectral domain optical coherence tomography at 184,000 lines per second,' J. Biomed. Opt. , vol. 16, no. 6, 2011, Art. no. 060506.\n- [27] L. An, P. Li, T. T. Shen, and R. Wang, 'High speed spectral domain optical coherence tomography for retinal imaging at 500,000 A-lines per second,' Biomed. Opt. Exp. , vol. 2, pp. 2770-2783, Oct. 2011.\n- [28] O. P. Kocaoglu, T. L. Turner, Z. Liu, and D. T. Miller, 'Adaptive optics optical coherence tomography at 1 MHz,' Biomed. Opt. Exp. , vol. 5, pp. 4186-4200, Dec. 2014.",
    "context": "Provides a historical overview of optical coherence tomography, including early reviews, advancements in scanning speed, and applications in diverse fields like ophthalmology, dentistry, and industrial defect detection.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      7
    ],
    "id": "dfbf8fb189bc44596caa9c63a166ca6e4ed73c970daf4f81de569feb6cca43d0"
  },
  {
    "text": "[29] K. Ohbayashi et al. , '60 MHz A-line rate ultra-high speed Fourierdomain optical coherence tomography,' in Proc. 12th Coherence Domain Opt. Methods Opt. Coherence Tomogr. Biomed. , Feb. 2008, Art. no. 68470M.\n[30] D.-H. Choi, H. Hiro-Oka, K. Shimizu, and K. Ohbayashi, 'Spectral domain optical coherence tomography of multi-MHz A-scan rates at 1310 nm range and real-time 4D-display up to 41 volumes/second,' Biomed. Opt. Exp. , vol. 3, pp. 3067-3086, Dec. 2012.\n[31] R. Wang, J. X. Yun, X. Yuan, R. Goodwin, R. R. Markwald, and B. Z. Gao, 'Megahertz streak-mode Fourier domain optical coherence tomography,' J. Biomed. Opt. , vol. 16, no. 6, 2011, Art. no. 066016.\n[32] J. Barrick, A. Doblas, M. R. Gardner, P. R. Sears, L. E. Ostrowski, and A. L. Oldenburg, 'High-speed and high-sensitivity parallel spectraldomain optical coherence tomography using a supercontinuum light source,' Opt. Lett. , vol. 41, pp. 5620-5623, Dec. 2016.\n[33] B. Fang et al. , 'Full-range line-field optical coherence tomography for high-accuracy measurements of optical lens,' IEEE Trans. Instrum. Meas. , vol. 69, no. 9, pp. 7180-7190, Sep. 2020.\n[34] K.-S. Lee et al. , 'High speed parallel spectral-domain OCT using spectrally encoded line-field illumination,' Appl. Phys. Lett. , vol. 112, no. 4, Jan. 2018, Art. no. 041102.\n[35] T. Mekonnen, A. Kourmatzis, J. Amatoury, and S. Cheng, 'Simultaneous multi-spatial scanning optical coherence tomography (OCT) based on spectrum-slicing of a broadband source,' Meas. Sci. Technol. , vol. 30, no. 4, Feb. 2019, Art. no. 045203.\n[36] C. Zhou, A. Alex, J. Rasakanthan, and Y. Ma, 'Space-division multiplexing optical coherence tomography,' Opt. Exp. , vol. 21, pp. 19219-19227, Aug. 2013.\n[37] Y. Huang, M. Badar, A. Nitkowski, A. Weinroth, N. Tansu, and C. Zhou, 'Wide-field high-speed space-division multiplexing optical coherence tomography using an integrated photonic device,' Biomed. Opt. Exp. , vol. 8, pp. 3856-3867, Aug. 2017.\n[38] D. Seong et al. , 'Dynamic compensation of path length difference in optical coherence tomography by an automatic temperature control system of optical fiber,' IEEE Access , vol. 8, pp. 77501-77510, 2020.\n[39] M. Sylwestrzak, M. Szkulmowski, D. Szlag, and P. Targowski, 'Realtime imaging for spectral optical coherence tomography with massively parallel data processing,' Photon. Lett. Poland , vol. 2, no. 3, pp. 137-139, Oct. 2010.\n[40] K. Zhang and J. U. Kang, 'Real-time 4D signal processing and visualization using graphics processing unit on a regular nonlinear-k Fourierdomain OCT system,' Opt. Exp. , vol. 18, no. 11, pp. 11772-11784, 2010.\n[41] K. Zhang and J. U. Kang, 'Real-time intraoperative 4D full-range FD-OCT based on the dual graphics processing units architecture for microsurgery guidance,' Biomed. Opt. Exp. , vol. 2, pp. 764-770, Apr. 2011.\n[42] S. A. Saleah et al. , 'Integrated quad-scanner strategy-based optical coherence tomography for the whole-directional volumetric imaging of a sample,' Sensors , vol. 21, no. 4, p. 1305, Feb. 2021.\nDaewoon Seong received the B.E. degree from the School of Electronics Engineering, Kyungpook National University, Daegu, Republic of Korea, in 2020.\nHe is currently a MD Researcher with the School of Electronic and Electrical Engineering, Kyungpook National University. His research focuses on developing high-speed and high-resolution optical imaging systems, photoacoustic microscopy, and optical coherence tomography applied to medical fields and industrial applications.\nDeokmin Jeon received the Ph.D. degree in electronics engineering from Kyungpook National University, Daegu, South Korea, in 2020.\nHe is currently with Samsung Electronics Company Ltd., Republic of Korea. His research interests include the development of biomedical and industrial imaging techniques, including optical coherence tomography, parallel computing, and 3-D vision technology.\nRuchire Eranga Wijesinghe received the B.Sc. and Ph.D. degrees in electronics engineering from Kyungpook National University, Daegu, South Korea, in 2012 and 2018, respectively.\nHe is currently a Senior Lecturer with the Department of Materials and Mechanical Technology, University of Sri Jayewardenepura, Homagama, Sri Lanka. His research interests are in the development of high-resolution novel biological and biomedical imaging techniques, including optical coherence tomography and microscopy for clinical utility.\nKibeom Park received the B.Sc. and Ph.D. degrees in electronics engineering from Kyungpook National University, Daegu, South Korea, in 2012 and 2018, respectively.\nHe is currently a Post-Doctoral Researcher with the Department of Department of Biomedical Engineering, Ulsan National Institute of Science and Technology, Ulsan, South Korea. His research interests are in the development of high-throughput optical imaging techniques in biomedical and industrial fields.\nHyeree Kim received the B.E. degree in avionic electronics engineering from Kyungwoon University, Gumi, Republic of Korea, in 2018.\nShe is currently an MS Researcher with the Electronics Engineering Department, Kyungpook National University, Daegu, South Korea. Her research interests are the development of biomedical imaging systems, including optical coherence tomography and optical instrument optimization design.\nEuimin Lee received the B.E. degree from the School of Electronic and Electrical Engineering, Kyungpook National University, Daegu, Republic of Korea, in 2020.\nHe is currently a MD Researcher with the School of Electronics Engineering, Kyungpook National University. His research focuses on development of imaging processing technique and convolutional neural networks using optical coherence tomography applied to industrial and agricultural application.\nMansik Jeon (Member, IEEE) received the Ph.D. degree in electronics engineering from Kyungpook National University, Daegu, Republic of Korea, in 2011.\nHe is currently an Associate Professor with the School of Electronics Engineering, Kyungpook National University. His research interests are in the development of nonionizing and noninvasive novel biomedical imaging techniques, including photoacoustic tomography, photoacoustic microscopy, optical coherence tomography, ultrasonic imaging, handheld scanner, and their clinical applications.\nJeehyun Kim (Member, IEEE) received the Ph.D. degree in biomedical engineering from The University of Texas at Austin, Austin, TX, USA, in 2004.\nHe has worked as a Post-Doctoral Researcher at the Beckman Laser Institute, University of California at Irvine, Irvine, CA, USA. He is currently an Associate Professor with Kyungpook National University, Daegu, Republic of Korea. His research interests are in biomedical imaging and sensing, neuroscience studies using multiphoton microscopy, photoacoustic imaging, and other novel applications of sensors.\n\nThe chunk details various advancements in optical coherence tomography, including high-speed scanning, spectral domain techniques, and parallel processing, showcasing a progression towards faster and more sophisticated imaging systems.",
    "original_text": "[29] K. Ohbayashi et al. , '60 MHz A-line rate ultra-high speed Fourierdomain optical coherence tomography,' in Proc. 12th Coherence Domain Opt. Methods Opt. Coherence Tomogr. Biomed. , Feb. 2008, Art. no. 68470M.\n[30] D.-H. Choi, H. Hiro-Oka, K. Shimizu, and K. Ohbayashi, 'Spectral domain optical coherence tomography of multi-MHz A-scan rates at 1310 nm range and real-time 4D-display up to 41 volumes/second,' Biomed. Opt. Exp. , vol. 3, pp. 3067-3086, Dec. 2012.\n[31] R. Wang, J. X. Yun, X. Yuan, R. Goodwin, R. R. Markwald, and B. Z. Gao, 'Megahertz streak-mode Fourier domain optical coherence tomography,' J. Biomed. Opt. , vol. 16, no. 6, 2011, Art. no. 066016.\n[32] J. Barrick, A. Doblas, M. R. Gardner, P. R. Sears, L. E. Ostrowski, and A. L. Oldenburg, 'High-speed and high-sensitivity parallel spectraldomain optical coherence tomography using a supercontinuum light source,' Opt. Lett. , vol. 41, pp. 5620-5623, Dec. 2016.\n[33] B. Fang et al. , 'Full-range line-field optical coherence tomography for high-accuracy measurements of optical lens,' IEEE Trans. Instrum. Meas. , vol. 69, no. 9, pp. 7180-7190, Sep. 2020.\n[34] K.-S. Lee et al. , 'High speed parallel spectral-domain OCT using spectrally encoded line-field illumination,' Appl. Phys. Lett. , vol. 112, no. 4, Jan. 2018, Art. no. 041102.\n[35] T. Mekonnen, A. Kourmatzis, J. Amatoury, and S. Cheng, 'Simultaneous multi-spatial scanning optical coherence tomography (OCT) based on spectrum-slicing of a broadband source,' Meas. Sci. Technol. , vol. 30, no. 4, Feb. 2019, Art. no. 045203.\n[36] C. Zhou, A. Alex, J. Rasakanthan, and Y. Ma, 'Space-division multiplexing optical coherence tomography,' Opt. Exp. , vol. 21, pp. 19219-19227, Aug. 2013.\n[37] Y. Huang, M. Badar, A. Nitkowski, A. Weinroth, N. Tansu, and C. Zhou, 'Wide-field high-speed space-division multiplexing optical coherence tomography using an integrated photonic device,' Biomed. Opt. Exp. , vol. 8, pp. 3856-3867, Aug. 2017.\n[38] D. Seong et al. , 'Dynamic compensation of path length difference in optical coherence tomography by an automatic temperature control system of optical fiber,' IEEE Access , vol. 8, pp. 77501-77510, 2020.\n[39] M. Sylwestrzak, M. Szkulmowski, D. Szlag, and P. Targowski, 'Realtime imaging for spectral optical coherence tomography with massively parallel data processing,' Photon. Lett. Poland , vol. 2, no. 3, pp. 137-139, Oct. 2010.\n[40] K. Zhang and J. U. Kang, 'Real-time 4D signal processing and visualization using graphics processing unit on a regular nonlinear-k Fourierdomain OCT system,' Opt. Exp. , vol. 18, no. 11, pp. 11772-11784, 2010.\n[41] K. Zhang and J. U. Kang, 'Real-time intraoperative 4D full-range FD-OCT based on the dual graphics processing units architecture for microsurgery guidance,' Biomed. Opt. Exp. , vol. 2, pp. 764-770, Apr. 2011.\n[42] S. A. Saleah et al. , 'Integrated quad-scanner strategy-based optical coherence tomography for the whole-directional volumetric imaging of a sample,' Sensors , vol. 21, no. 4, p. 1305, Feb. 2021.\nDaewoon Seong received the B.E. degree from the School of Electronics Engineering, Kyungpook National University, Daegu, Republic of Korea, in 2020.\nHe is currently a MD Researcher with the School of Electronic and Electrical Engineering, Kyungpook National University. His research focuses on developing high-speed and high-resolution optical imaging systems, photoacoustic microscopy, and optical coherence tomography applied to medical fields and industrial applications.\nDeokmin Jeon received the Ph.D. degree in electronics engineering from Kyungpook National University, Daegu, South Korea, in 2020.\nHe is currently with Samsung Electronics Company Ltd., Republic of Korea. His research interests include the development of biomedical and industrial imaging techniques, including optical coherence tomography, parallel computing, and 3-D vision technology.\nRuchire Eranga Wijesinghe received the B.Sc. and Ph.D. degrees in electronics engineering from Kyungpook National University, Daegu, South Korea, in 2012 and 2018, respectively.\nHe is currently a Senior Lecturer with the Department of Materials and Mechanical Technology, University of Sri Jayewardenepura, Homagama, Sri Lanka. His research interests are in the development of high-resolution novel biological and biomedical imaging techniques, including optical coherence tomography and microscopy for clinical utility.\nKibeom Park received the B.Sc. and Ph.D. degrees in electronics engineering from Kyungpook National University, Daegu, South Korea, in 2012 and 2018, respectively.\nHe is currently a Post-Doctoral Researcher with the Department of Department of Biomedical Engineering, Ulsan National Institute of Science and Technology, Ulsan, South Korea. His research interests are in the development of high-throughput optical imaging techniques in biomedical and industrial fields.\nHyeree Kim received the B.E. degree in avionic electronics engineering from Kyungwoon University, Gumi, Republic of Korea, in 2018.\nShe is currently an MS Researcher with the Electronics Engineering Department, Kyungpook National University, Daegu, South Korea. Her research interests are the development of biomedical imaging systems, including optical coherence tomography and optical instrument optimization design.\nEuimin Lee received the B.E. degree from the School of Electronic and Electrical Engineering, Kyungpook National University, Daegu, Republic of Korea, in 2020.\nHe is currently a MD Researcher with the School of Electronics Engineering, Kyungpook National University. His research focuses on development of imaging processing technique and convolutional neural networks using optical coherence tomography applied to industrial and agricultural application.\nMansik Jeon (Member, IEEE) received the Ph.D. degree in electronics engineering from Kyungpook National University, Daegu, Republic of Korea, in 2011.\nHe is currently an Associate Professor with the School of Electronics Engineering, Kyungpook National University. His research interests are in the development of nonionizing and noninvasive novel biomedical imaging techniques, including photoacoustic tomography, photoacoustic microscopy, optical coherence tomography, ultrasonic imaging, handheld scanner, and their clinical applications.\nJeehyun Kim (Member, IEEE) received the Ph.D. degree in biomedical engineering from The University of Texas at Austin, Austin, TX, USA, in 2004.\nHe has worked as a Post-Doctoral Researcher at the Beckman Laser Institute, University of California at Irvine, Irvine, CA, USA. He is currently an Associate Professor with Kyungpook National University, Daegu, Republic of Korea. His research interests are in biomedical imaging and sensing, neuroscience studies using multiphoton microscopy, photoacoustic imaging, and other novel applications of sensors.",
    "context": "The chunk details various advancements in optical coherence tomography, including high-speed scanning, spectral domain techniques, and parallel processing, showcasing a progression towards faster and more sophisticated imaging systems.",
    "document": "Ultrahigh-Speed_Spectral-Domain_Optical_Coherence_Tomography_up_to_1-MHz_A-Scan_Rate_Using_SpaceTime-Division_Multiplexing.pdf",
    "pages": [
      8
    ],
    "id": "7c76d1d0daab13a0dfc3dee3b3ee08bc7270633448afa5c55fd51e697a16e4e0"
  },
  {
    "text": "Received 22 January 2023, accepted 11 February 2023, date of publication 22 February 2023, date of current version 27 February 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3246486\n\nIntroduces the paper's publication details and funding sources.",
    "original_text": "Received 22 January 2023, accepted 11 February 2023, date of publication 22 February 2023, date of current version 27 February 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3246486",
    "context": "Introduces the paper's publication details and funding sources.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      1
    ],
    "id": "549a022816caabb6ce69bfb8fd22635e67f0eff447d1e5e2ce0d923347a691f1"
  },
  {
    "text": "TYLER PHILLIPS 1 , (Member, IEEE), LAURENTIU D. MARINOVICI 2 , (Member, IEEE), CRAIG RIEGER 1 , (Senior Member, IEEE), AND ALICE ORRELL 2\nIdaho National Laboratory, Idaho Falls, ID 83415, USA\n1\n2 Pacific Northwest National Laboratory, Richland, WA 99354, USA\nCorresponding author: Laurentiu D. Marinovici (Laurentiu.Marinovici@pnnl.gov)\nThis work was supported in part by the U.S. Department of Energy Wind Energy Technologies Office, in part by the Pacific Northwest National Laboratory under Contract DE-AC05-76RL01830, and in part by the Idaho National Laboratory through the U.S. DOE Idaho Operations Office under Contract DE-AC07-05ID14517.\nABSTRACT The ability to robustly characterize transmission and distribution grid resilience requires the ability to perform time-scale analysis that interweaves communications, control, and power contributions. This consideration is important to ensuring an understanding of how each individual aspect can affect the resulting systemic resilience. The combination of co-simulation of these time-based characteristics and a resilience-specific metric provides a likely method to inform both design planning and implementation/operational goals to ensure resilience in power systems. The Microgrids, Infrastructure Resilience, and Advanced Controls Launchpad co-simulation platform (MIRACL-CSP) has been developed to allow the modular integration of power grid models with control and metrics applications. This paper introduces MIRACL-CSPasafundamental platform to study and improve the resilient operation of a microgrid. It offers a holistic investigation environment for systemically comparing the cyber-physical resilience to natural and manmade events. We emphasize the importance/advantage of intertwining the distribution system simulator GridLAB-D with the Power Distribution Designing for Resilience (PowDDeR) application to analyze the resilience of the St. Mary's microgrid in Alaska. The resilience is evaluated in both short-term (frequency stability) and long-term (energy constrained) metrics. The results of the analysis of the St. Mary's microgrid show that there is a trade-off between the two. As inertia-based generation assets are taken off-line, shortterm resilience drops. However, the long-term resilience is retained longer as less fuel is being used.\nINDEX TERMS Resilience, co-simulation, microgrid, distributed wind, cyber-physical.\n\nIntroduces a co-simulation platform (MIRACL-CSP) for studying microgrid resilience, integrating GridLAB-D and PowDDeR to analyze the St. Mary's microgrid's short-term (frequency stability) and long-term (energy constrained) resilience, highlighting a trade-off between the two metrics.",
    "original_text": "TYLER PHILLIPS 1 , (Member, IEEE), LAURENTIU D. MARINOVICI 2 , (Member, IEEE), CRAIG RIEGER 1 , (Senior Member, IEEE), AND ALICE ORRELL 2\nIdaho National Laboratory, Idaho Falls, ID 83415, USA\n1\n2 Pacific Northwest National Laboratory, Richland, WA 99354, USA\nCorresponding author: Laurentiu D. Marinovici (Laurentiu.Marinovici@pnnl.gov)\nThis work was supported in part by the U.S. Department of Energy Wind Energy Technologies Office, in part by the Pacific Northwest National Laboratory under Contract DE-AC05-76RL01830, and in part by the Idaho National Laboratory through the U.S. DOE Idaho Operations Office under Contract DE-AC07-05ID14517.\nABSTRACT The ability to robustly characterize transmission and distribution grid resilience requires the ability to perform time-scale analysis that interweaves communications, control, and power contributions. This consideration is important to ensuring an understanding of how each individual aspect can affect the resulting systemic resilience. The combination of co-simulation of these time-based characteristics and a resilience-specific metric provides a likely method to inform both design planning and implementation/operational goals to ensure resilience in power systems. The Microgrids, Infrastructure Resilience, and Advanced Controls Launchpad co-simulation platform (MIRACL-CSP) has been developed to allow the modular integration of power grid models with control and metrics applications. This paper introduces MIRACL-CSPasafundamental platform to study and improve the resilient operation of a microgrid. It offers a holistic investigation environment for systemically comparing the cyber-physical resilience to natural and manmade events. We emphasize the importance/advantage of intertwining the distribution system simulator GridLAB-D with the Power Distribution Designing for Resilience (PowDDeR) application to analyze the resilience of the St. Mary's microgrid in Alaska. The resilience is evaluated in both short-term (frequency stability) and long-term (energy constrained) metrics. The results of the analysis of the St. Mary's microgrid show that there is a trade-off between the two. As inertia-based generation assets are taken off-line, shortterm resilience drops. However, the long-term resilience is retained longer as less fuel is being used.\nINDEX TERMS Resilience, co-simulation, microgrid, distributed wind, cyber-physical.",
    "context": "Introduces a co-simulation platform (MIRACL-CSP) for studying microgrid resilience, integrating GridLAB-D and PowDDeR to analyze the St. Mary's microgrid's short-term (frequency stability) and long-term (energy constrained) resilience, highlighting a trade-off between the two metrics.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      1
    ],
    "id": "b0a9d71205dcdd4cbabf1a8542ffb0b4f5c590fd758112a4c108d931f365e2b1"
  },
  {
    "text": "To assess options in the advancement of modern distribution system (MDS), a set of quantifying metrics are necessary to correlate a value proposition for industry. In alignment with the recently released National Electric Grid Security and Resilience Action Plan, 1 a framework will correlate the overall resilience of MDS options to performance degrading impacts from threats.\nThe associate editor coordinating the review of this manuscript and approving it for publication was R.K. Saket .\n1 https://obamawhitehouse.archives.gov/sites/whitehouse.gov/files/ images/National_Electric_Grid_Action_Plan_06Dec2016.pdf\nWithin the power system community, quantitative metrics have been proposed as mathematical formalism to objectively measure the resilience of a system. Based on how the power system and its associated controls perform, these resilience metrics quantify the impact in direct measures associated with loss of generation, ability to maintain critical functionality, and often an integration of the cyber-physical system characteristics. State-of-the-art resilience assessment and quantification methods are reviewed in [1], [2], and [3]. While [1] focuses on frameworks, resilience curves, and quantitative metrics, [2] presents the challenges faced by researchers and power utilities due to nonstandard frameworks and tries to release that burden by categorizing them.\nIn [3] the authors evaluate and compare common metrics for short- and long-term resilience assessments focusing on microgrids' potential for power system resilience improvement. The review of all proposed resilience metrics for electrical power systems, or any other complex system, is not within the scope of this study. However, an established resilience metric that allows for systemic comparison of distributed systems will be leveraged, as will be differentiated in what follows. The ability to recover from an attack, provided the attack is discovered within a fixed time interval, is quantified through the metric proposed in [4]. Metrics to assess the two stages of smart grid operation, that is the duration before a failure occurs and the recovery time after the event, are presented in [5]. The metrics in [6] are directed towards the hardness and asset health as measures of stress, and towards capacity and efficacy as measures of strain. The works of [7] and [8] introduce a quantitative metrics basis to integrate the cognitive, cyber-physical aspects, which should all be considered when defining solutions for resilience. This approach is applied together with considering the uncertainties of solar and hydro renewables in [9], [10], and [11]. In these studies the performance index that provides the basis for resilience is the system adaptive capacity and its inertia. It considers several MDS design variables including generation or demand response delivery capacity, reactive power, power network topology, and control system architecture.\nResilience of complex systems is not a short-term or long-term measure. It encompasses many time scales from prior to an event to potentially days or weeks after. This is shown notionally by the disturbance and impact resilience evaluation (DIRE) curve in Fig. 1. It can be seen that the resilience is broken into five different time scales, known as the ''R's'' of resilience: recon, resist, respond, recover, and restore. To account for this time-dependent behavior, a dynamic resilience study approach is explored in this paper. In the context of the electrical power grid, resilience depends on supportive and responsive relationships between all the components at the transmission and distribution levels. Mastering a set of capabilities that could help the system respond and adapt to adversity at each individual aspect in a timely and appropriately healthy manner is of utmost importance. For the quantitative metrics to robustly characterize the transmission and distribution grid resilience, there is need for them to capture the interactions of grid components in response to adversity. Therefore, it is imperative to be able to perform time-scale analysis of the intertwined communication, control, and power systems.\nThe study in [12] proposes co-simulation as means at different design stages to analyze the resilience of complex and multi-disciplinary cyber-physical systems. Co-simulation of domain-specific simulators is involved in [13] to study the resilience of microgrids including smart grid technologies connected through cyber networks. Similarly, [14] presents co-simulation as the approach when studying integrated energy systems resilience. Though studying system resilience is among the goals of the co-simulation, no metrics are\nFIGURE 1. The Disturbance and Impact Resilience (DIRE) curve showing ''R's'' or time scales of resilience. Image taken from [11].\nintegrated to evaluate it for multiple scenarios and at different time scales.\nIn this study, co-simulation considers the temporal nature of integrating disparate models to achieve relevant results. The benefit of having the resilience metric in [9], [10], and [11] in parallel is to be able to address in real time short-term and long-term resilience concerns when adaptive capacity and inertia problems arise. The need to integrate disparate models has been the subject of a number of architectures, including the standardized high level architecture, some decades ago. However, the ability to achieve this integration requires that the time scale of the integrated simulation recognizes the artifacts necessary to characterize a judgment, which for resilience considers the man-made and natural threats.\nUnder the Microgrids, Infrastructure Resilience, and Advanced Controls Launchpad (MIRACL) project 2 at PNNL, the MIRACL co-simulation platform (CSP) in [15] has been developed to provide a real-life-like utility operation system incorporating data monitoring and control systems. In this work, MIRACL-CSP is leveraged to convey distribution system simulation measurements to the resilience metrics at run time. Through the holistic approach of cosimulation, the platform developed in this paper offers an environment that allows the study of complex systems under the specific conditions of dynamic use cases, and, moreover, scaling not only the size of the analyzed distribution system, but also the number of adversaries that could affect it.\nThe paper is organized as follows. Section II introduces the particular components of the MIRACL-CSP built for this application. Section III details the use cases, while Section IV discusses the results of the current study. Conclusions end the manuscript in Section V.\n\nEstablishes the need for quantifiable metrics to assess modern distribution system resilience, aligning with the National Electric Grid Security and Resilience Action Plan and leveraging co-simulation techniques.",
    "original_text": "To assess options in the advancement of modern distribution system (MDS), a set of quantifying metrics are necessary to correlate a value proposition for industry. In alignment with the recently released National Electric Grid Security and Resilience Action Plan, 1 a framework will correlate the overall resilience of MDS options to performance degrading impacts from threats.\nThe associate editor coordinating the review of this manuscript and approving it for publication was R.K. Saket .\n1 https://obamawhitehouse.archives.gov/sites/whitehouse.gov/files/ images/National_Electric_Grid_Action_Plan_06Dec2016.pdf\nWithin the power system community, quantitative metrics have been proposed as mathematical formalism to objectively measure the resilience of a system. Based on how the power system and its associated controls perform, these resilience metrics quantify the impact in direct measures associated with loss of generation, ability to maintain critical functionality, and often an integration of the cyber-physical system characteristics. State-of-the-art resilience assessment and quantification methods are reviewed in [1], [2], and [3]. While [1] focuses on frameworks, resilience curves, and quantitative metrics, [2] presents the challenges faced by researchers and power utilities due to nonstandard frameworks and tries to release that burden by categorizing them.\nIn [3] the authors evaluate and compare common metrics for short- and long-term resilience assessments focusing on microgrids' potential for power system resilience improvement. The review of all proposed resilience metrics for electrical power systems, or any other complex system, is not within the scope of this study. However, an established resilience metric that allows for systemic comparison of distributed systems will be leveraged, as will be differentiated in what follows. The ability to recover from an attack, provided the attack is discovered within a fixed time interval, is quantified through the metric proposed in [4]. Metrics to assess the two stages of smart grid operation, that is the duration before a failure occurs and the recovery time after the event, are presented in [5]. The metrics in [6] are directed towards the hardness and asset health as measures of stress, and towards capacity and efficacy as measures of strain. The works of [7] and [8] introduce a quantitative metrics basis to integrate the cognitive, cyber-physical aspects, which should all be considered when defining solutions for resilience. This approach is applied together with considering the uncertainties of solar and hydro renewables in [9], [10], and [11]. In these studies the performance index that provides the basis for resilience is the system adaptive capacity and its inertia. It considers several MDS design variables including generation or demand response delivery capacity, reactive power, power network topology, and control system architecture.\nResilience of complex systems is not a short-term or long-term measure. It encompasses many time scales from prior to an event to potentially days or weeks after. This is shown notionally by the disturbance and impact resilience evaluation (DIRE) curve in Fig. 1. It can be seen that the resilience is broken into five different time scales, known as the ''R's'' of resilience: recon, resist, respond, recover, and restore. To account for this time-dependent behavior, a dynamic resilience study approach is explored in this paper. In the context of the electrical power grid, resilience depends on supportive and responsive relationships between all the components at the transmission and distribution levels. Mastering a set of capabilities that could help the system respond and adapt to adversity at each individual aspect in a timely and appropriately healthy manner is of utmost importance. For the quantitative metrics to robustly characterize the transmission and distribution grid resilience, there is need for them to capture the interactions of grid components in response to adversity. Therefore, it is imperative to be able to perform time-scale analysis of the intertwined communication, control, and power systems.\nThe study in [12] proposes co-simulation as means at different design stages to analyze the resilience of complex and multi-disciplinary cyber-physical systems. Co-simulation of domain-specific simulators is involved in [13] to study the resilience of microgrids including smart grid technologies connected through cyber networks. Similarly, [14] presents co-simulation as the approach when studying integrated energy systems resilience. Though studying system resilience is among the goals of the co-simulation, no metrics are\nFIGURE 1. The Disturbance and Impact Resilience (DIRE) curve showing ''R's'' or time scales of resilience. Image taken from [11].\nintegrated to evaluate it for multiple scenarios and at different time scales.\nIn this study, co-simulation considers the temporal nature of integrating disparate models to achieve relevant results. The benefit of having the resilience metric in [9], [10], and [11] in parallel is to be able to address in real time short-term and long-term resilience concerns when adaptive capacity and inertia problems arise. The need to integrate disparate models has been the subject of a number of architectures, including the standardized high level architecture, some decades ago. However, the ability to achieve this integration requires that the time scale of the integrated simulation recognizes the artifacts necessary to characterize a judgment, which for resilience considers the man-made and natural threats.\nUnder the Microgrids, Infrastructure Resilience, and Advanced Controls Launchpad (MIRACL) project 2 at PNNL, the MIRACL co-simulation platform (CSP) in [15] has been developed to provide a real-life-like utility operation system incorporating data monitoring and control systems. In this work, MIRACL-CSP is leveraged to convey distribution system simulation measurements to the resilience metrics at run time. Through the holistic approach of cosimulation, the platform developed in this paper offers an environment that allows the study of complex systems under the specific conditions of dynamic use cases, and, moreover, scaling not only the size of the analyzed distribution system, but also the number of adversaries that could affect it.\nThe paper is organized as follows. Section II introduces the particular components of the MIRACL-CSP built for this application. Section III details the use cases, while Section IV discusses the results of the current study. Conclusions end the manuscript in Section V.",
    "context": "Establishes the need for quantifiable metrics to assess modern distribution system resilience, aligning with the National Electric Grid Security and Resilience Action Plan and leveraging co-simulation techniques.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      1,
      2
    ],
    "id": "b66bb4a9f4cc49c2872133a43629b241c79547537059621dd1df5da93d97e43b"
  },
  {
    "text": "The Microgrids, Infrastructure Resilience, and Advanced Controls Launchpad co-simulation platform (MIRACL-CSP) [15] was developed to allow operational coordination between distribution system models and distribution system applications and technologies. This is particularly important\nfor cases where more realistic and dynamic data exchange is required while all simulation instances run in a synchronized manner regardless of their time periods, either continuous or discrete.\nMIRACL-CSP offers a modular integration of distribution systems modeled in GridLAB-D [16] and custom-built distribution system monitoring and control applications modeled in Python ™ through the Hierarchical Engine for Large Infrastructure Co-Simulation (HELICS) [17], [18] environment. Fig. 2 illustrates the integration of the power distribution system, control, and resilience modeling aspects for our cosimulation.\nThe software comprising the MIRACL-CSP has been packaged in a Docker image. It ensures rapid application development and testing. Moreover, it offers portability, deployment and execution on different platforms. The MIRACL-CSP depicted in Fig. 2 currently includes :\n- Ubuntu 3 20.4 as the base operating system,\n- HELICS 4 version 3 as the co-simulation environment,\n- GridLAB-D 5 as the distribution system modeling and simulation environment,\n- Python 6 3.9, with appropriate modules, as the wrapper around the utility control center (UCC) applications for monitoring, optimization-based control, and system resiliency metrics calculation with Power Distribution Designing for Resilience 7 (PowDDeR).\n\nIntroduces the MIRACL-CSP platform and its modular integration of GridLAB-D and Python for co-simulation.",
    "original_text": "The Microgrids, Infrastructure Resilience, and Advanced Controls Launchpad co-simulation platform (MIRACL-CSP) [15] was developed to allow operational coordination between distribution system models and distribution system applications and technologies. This is particularly important\nfor cases where more realistic and dynamic data exchange is required while all simulation instances run in a synchronized manner regardless of their time periods, either continuous or discrete.\nMIRACL-CSP offers a modular integration of distribution systems modeled in GridLAB-D [16] and custom-built distribution system monitoring and control applications modeled in Python ™ through the Hierarchical Engine for Large Infrastructure Co-Simulation (HELICS) [17], [18] environment. Fig. 2 illustrates the integration of the power distribution system, control, and resilience modeling aspects for our cosimulation.\nThe software comprising the MIRACL-CSP has been packaged in a Docker image. It ensures rapid application development and testing. Moreover, it offers portability, deployment and execution on different platforms. The MIRACL-CSP depicted in Fig. 2 currently includes :\n- Ubuntu 3 20.4 as the base operating system,\n- HELICS 4 version 3 as the co-simulation environment,\n- GridLAB-D 5 as the distribution system modeling and simulation environment,\n- Python 6 3.9, with appropriate modules, as the wrapper around the utility control center (UCC) applications for monitoring, optimization-based control, and system resiliency metrics calculation with Power Distribution Designing for Resilience 7 (PowDDeR).",
    "context": "Introduces the MIRACL-CSP platform and its modular integration of GridLAB-D and Python for co-simulation.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      2,
      3
    ],
    "id": "9a8ebcd7292f3abc39991b3b8742971b3c42cc58a18bb9c0e787f5f38f56cf15"
  },
  {
    "text": "At the core of the MIRACL-CSP is HELICS, an open-source co-simulation platform that coordinates off-the-shelf simulators and applications, including electric transmission systems, electric distribution systems, communication systems, market models, and end-use loads [17], [18]. HELICS performs the two main functions of a co-simulation, that is time management and synchronization of simulators, in particular, GridLAB-D and Python federates, as well as data exchanges between them.\n\nCo-simulation platform MIRACL-CSP integrates GridLAB-D and Python for resilience analysis, specifically focusing on the St. Mary’s microgrid and its trade-off between short-term and long-term resilience metrics.",
    "original_text": "At the core of the MIRACL-CSP is HELICS, an open-source co-simulation platform that coordinates off-the-shelf simulators and applications, including electric transmission systems, electric distribution systems, communication systems, market models, and end-use loads [17], [18]. HELICS performs the two main functions of a co-simulation, that is time management and synchronization of simulators, in particular, GridLAB-D and Python federates, as well as data exchanges between them.",
    "context": "Co-simulation platform MIRACL-CSP integrates GridLAB-D and Python for resilience analysis, specifically focusing on the St. Mary’s microgrid and its trade-off between short-term and long-term resilience metrics.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      3
    ],
    "id": "a828295e05adb81c4fff2f5e36468a2ca64fa3d92f61a305de9bab4dedb7c3ca"
  },
  {
    "text": "GridLAB-D is the power system simulator integrated in the MIRACL-CSP. It models and performs power flow calculations of the distribution system. In GridLAB-D, the distribution system is modeled as a three-phase, unbalanced system and can be simulated in either quasi-steady-state or dynamic modes [16], [19]. Moreover, distributed energy resources (DERs), including diesel and wind turbine generators are also modeled in GridLAB-D.\n3 https://ubuntu.com/\n4 https://www.helics.org/\n5 https://www.gridlabd.org/\n6 https://www.python.org/\n7 https://github.com/IdahoLabUnsupported/PowDDeR\n\nGridLAB-D’s integration within the MIRACL-CSP enables power flow simulations, including modeling of DERs and dynamic system behavior.",
    "original_text": "GridLAB-D is the power system simulator integrated in the MIRACL-CSP. It models and performs power flow calculations of the distribution system. In GridLAB-D, the distribution system is modeled as a three-phase, unbalanced system and can be simulated in either quasi-steady-state or dynamic modes [16], [19]. Moreover, distributed energy resources (DERs), including diesel and wind turbine generators are also modeled in GridLAB-D.\n3 https://ubuntu.com/\n4 https://www.helics.org/\n5 https://www.gridlabd.org/\n6 https://www.python.org/\n7 https://github.com/IdahoLabUnsupported/PowDDeR",
    "context": "GridLAB-D’s integration within the MIRACL-CSP enables power flow simulations, including modeling of DERs and dynamic system behavior.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      3
    ],
    "id": "da05f1bd5e8cdcaa0d838304fb065208f86334d4064c493031219cdd2157c4b5"
  },
  {
    "text": "An application-specific utility monitoring and decisionsupport system is developed in Python [20] as the UCC in Fig. 2. The UCC incorporates an asset data (e.g., DERs, switches, sensors, loads) monitoring and analysis procedure necessary to run the resilience study with the PowDDeR application [21]. It also extracts different wind power generation profiles from measurement data files and dispatches them to the wind turbine model in GridLAB-D.\n\nThe UCC monitors and processes asset data, including wind generation profiles, for the St. Mary’s microgrid, feeding these profiles into GridLAB-D to simulate realistic scenarios for resilience analysis.",
    "original_text": "An application-specific utility monitoring and decisionsupport system is developed in Python [20] as the UCC in Fig. 2. The UCC incorporates an asset data (e.g., DERs, switches, sensors, loads) monitoring and analysis procedure necessary to run the resilience study with the PowDDeR application [21]. It also extracts different wind power generation profiles from measurement data files and dispatches them to the wind turbine model in GridLAB-D.",
    "context": "The UCC monitors and processes asset data, including wind generation profiles, for the St. Mary’s microgrid, feeding these profiles into GridLAB-D to simulate realistic scenarios for resilience analysis.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      3
    ],
    "id": "05127258b3cbe6c89fc8aec4154949bc81ece0943c07a33f3cb9472fbe7bbfce"
  },
  {
    "text": "PowDDeR is a software application developed to provide a specific resilience metric for a power system. It is based on the systems adaptive capacity and inertia. Given a set of generation assets in the system, PowDDeR captures the systems real-time inertia and the available adaptive capacity in real and reactive power looking forward in time. It gives a measure of a power system's ability to respond to disturbances, either natural, such as weather related, or human, such as cyberphysical attacks.\nThe adaptive capacity of a generation asset is bound by its operational generation limits based on its current operation point and the speed it can ramp up and down its output power. The operational limit is the maximum real and reactive power output capability at any power factor angle θ . The real and reactive power components at any power factor are given by\n<!-- formula-not-decoded -->\nand\n<!-- formula-not-decoded -->\nrespectively. Here S is the apparent power limit calculated with the nameplate real and reactive capacity, given as\n<!-- formula-not-decoded -->\nThe operational limit of the asset must be translated based on its real-time generation in real power, P 0, and reactive power, Q 0, resulting in components given by\n<!-- formula-not-decoded -->\nfor the real power and\n<!-- formula-not-decoded -->\nfor the reactive component. The temporal limits of real/reactive power outputs are defined by the latency λ and ramp rates, as mathematically expressed in equations (6), (7),\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere t is the future time, dP / dt is the real power ramping rate, and dQ / dt is the reactive power ramping rate.\nFIGURE 2. MIRACL-CSP architecture.\nThe adaptive capacity is therefore bound by the real and reactive power component limits given in equations (4) and (5) and the temporal limits defined in equations (6) and (7). This results in a ''manifold'' that represents the adaptive capacity of an asset. The manifold and a more detailed derivation, which covers the aggregation of assets can be found in [9]. Further derivations including uncertainty in solar PV and battery assets have been shown in [10], as well as hydropower generation assets in [11].\nThe inertia of power systems comes from generation units that have rotating masses or stored kinetic energy. The kinetic energy slows the rate of frequency response to disturbances on the system, given as\n<!-- formula-not-decoded -->\nwhere 1 P is the disturbance or difference between generation and load, and H is the inertial constant. A large amount of inertia on the system allows for additional time for generation units to ramp up or down output to arrest the frequency before it reaches the point of under frequency load shed (UFLS) or over frequency generator tripping. The kinetic energy of a generator is given as\n<!-- formula-not-decoded -->\nwhere f 0 , Np , and J are the real-time frequency, number of poles, and the mass moment of inertia, respectively. The kinetic energy can also be evaluated at the maximum and minimum frequency limits of the system to give the available kinetic energy. In the case of UFLS, this is given as\n<!-- formula-not-decoded -->\nThe available kinetic energy is used to find the amount of time it takes for a disturbance to result in a limiting frequency being reached, given as\n<!-- formula-not-decoded -->\nwhere Pd is the size of the power disturbance.\nThe short-term resilience in this paper is based on the ability of the system to arrest frequency prior to an UFLS event. This occurs when a disturbance results in load being greater than the generation. In this scenario, the frequency begins to drop and generation assets begin to increase their output power to re-balance the load. Therefore, the short-term resilience is the maximum size of disturbance the system can withstand and is calculated using the aggregated adaptive capacity in the positive real-power direction and the inertia or time it takes to reach the UFLS after the disturbance. The short-term resilience in regards to the DIRE curve (Fig. 1) relates to the 'resist' phase and its derivation can be seen in more detail in [22].\nThe long-term resilience is defined by the positive real power adaptive capacity over a large time span. It relates to the energy left in the system and gives a measure of the time the system can maintain generation. With regard to the DIRE curve, the long-term resilience is in the time frames of the 'respond' and 'recover' phases. In the St. Mary's microgrid it relates to the amount of time a generator can run based on its remaining fuel, its fuel burn rate, and the generators power output. The simulations ran in this study were over a short time duration of 350 seconds. In order to demonstrate the long-term resilience, the time axis is changed from seconds to hours. The longer time horizon allows a measurable reduction in the amount of fuel remaining, resulting in a change to the long-term resilience.\nIn this study framework, PowDDeR becomes part of the UCC. Through the HELICS Python APIs, the UCC monitors and processes required GridLAB-D asset information. Data is then loaded through the PowDDeR APIs to calculate\nthe adaptive capacity of each asset and of the overall system, and the results are saved into a Hierarchical Data Format Version 5 (HDF5) file for post-processing.\n\nPowDDeR calculates a system’s resilience metric by capturing real-time inertia and adaptive capacity in real and reactive power, providing a measure of the system’s ability to respond to disturbances. This metric is based on generation asset operational limits, ramping rates, and temporal constraints, resulting in a “manifold” representing the asset’s adaptive capacity.",
    "original_text": "PowDDeR is a software application developed to provide a specific resilience metric for a power system. It is based on the systems adaptive capacity and inertia. Given a set of generation assets in the system, PowDDeR captures the systems real-time inertia and the available adaptive capacity in real and reactive power looking forward in time. It gives a measure of a power system's ability to respond to disturbances, either natural, such as weather related, or human, such as cyberphysical attacks.\nThe adaptive capacity of a generation asset is bound by its operational generation limits based on its current operation point and the speed it can ramp up and down its output power. The operational limit is the maximum real and reactive power output capability at any power factor angle θ . The real and reactive power components at any power factor are given by\n<!-- formula-not-decoded -->\nand\n<!-- formula-not-decoded -->\nrespectively. Here S is the apparent power limit calculated with the nameplate real and reactive capacity, given as\n<!-- formula-not-decoded -->\nThe operational limit of the asset must be translated based on its real-time generation in real power, P 0, and reactive power, Q 0, resulting in components given by\n<!-- formula-not-decoded -->\nfor the real power and\n<!-- formula-not-decoded -->\nfor the reactive component. The temporal limits of real/reactive power outputs are defined by the latency λ and ramp rates, as mathematically expressed in equations (6), (7),\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere t is the future time, dP / dt is the real power ramping rate, and dQ / dt is the reactive power ramping rate.\nFIGURE 2. MIRACL-CSP architecture.\nThe adaptive capacity is therefore bound by the real and reactive power component limits given in equations (4) and (5) and the temporal limits defined in equations (6) and (7). This results in a ''manifold'' that represents the adaptive capacity of an asset. The manifold and a more detailed derivation, which covers the aggregation of assets can be found in [9]. Further derivations including uncertainty in solar PV and battery assets have been shown in [10], as well as hydropower generation assets in [11].\nThe inertia of power systems comes from generation units that have rotating masses or stored kinetic energy. The kinetic energy slows the rate of frequency response to disturbances on the system, given as\n<!-- formula-not-decoded -->\nwhere 1 P is the disturbance or difference between generation and load, and H is the inertial constant. A large amount of inertia on the system allows for additional time for generation units to ramp up or down output to arrest the frequency before it reaches the point of under frequency load shed (UFLS) or over frequency generator tripping. The kinetic energy of a generator is given as\n<!-- formula-not-decoded -->\nwhere f 0 , Np , and J are the real-time frequency, number of poles, and the mass moment of inertia, respectively. The kinetic energy can also be evaluated at the maximum and minimum frequency limits of the system to give the available kinetic energy. In the case of UFLS, this is given as\n<!-- formula-not-decoded -->\nThe available kinetic energy is used to find the amount of time it takes for a disturbance to result in a limiting frequency being reached, given as\n<!-- formula-not-decoded -->\nwhere Pd is the size of the power disturbance.\nThe short-term resilience in this paper is based on the ability of the system to arrest frequency prior to an UFLS event. This occurs when a disturbance results in load being greater than the generation. In this scenario, the frequency begins to drop and generation assets begin to increase their output power to re-balance the load. Therefore, the short-term resilience is the maximum size of disturbance the system can withstand and is calculated using the aggregated adaptive capacity in the positive real-power direction and the inertia or time it takes to reach the UFLS after the disturbance. The short-term resilience in regards to the DIRE curve (Fig. 1) relates to the 'resist' phase and its derivation can be seen in more detail in [22].\nThe long-term resilience is defined by the positive real power adaptive capacity over a large time span. It relates to the energy left in the system and gives a measure of the time the system can maintain generation. With regard to the DIRE curve, the long-term resilience is in the time frames of the 'respond' and 'recover' phases. In the St. Mary's microgrid it relates to the amount of time a generator can run based on its remaining fuel, its fuel burn rate, and the generators power output. The simulations ran in this study were over a short time duration of 350 seconds. In order to demonstrate the long-term resilience, the time axis is changed from seconds to hours. The longer time horizon allows a measurable reduction in the amount of fuel remaining, resulting in a change to the long-term resilience.\nIn this study framework, PowDDeR becomes part of the UCC. Through the HELICS Python APIs, the UCC monitors and processes required GridLAB-D asset information. Data is then loaded through the PowDDeR APIs to calculate\nthe adaptive capacity of each asset and of the overall system, and the results are saved into a Hierarchical Data Format Version 5 (HDF5) file for post-processing.",
    "context": "PowDDeR calculates a system’s resilience metric by capturing real-time inertia and adaptive capacity in real and reactive power, providing a measure of the system’s ability to respond to disturbances. This metric is based on generation asset operational limits, ramping rates, and temporal constraints, resulting in a “manifold” representing the asset’s adaptive capacity.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      3,
      4,
      5
    ],
    "id": "368ce34089706851f334e4b8e727aac6b1cf630b06fa35fb8821b788f412045b"
  },
  {
    "text": "In what follows, the testing methods and design will be presented for an infrastructure based upon an Alaskan renewables integration in the St. Mary's microgrid.\n\nThe document outlines a co-simulation framework for evaluating the resilience of the St. Mary’s microgrid, focusing on integrating renewable energy sources like wind power and assessing the impact of fuel availability challenges.",
    "original_text": "In what follows, the testing methods and design will be presented for an infrastructure based upon an Alaskan renewables integration in the St. Mary's microgrid.",
    "context": "The document outlines a co-simulation framework for evaluating the resilience of the St. Mary’s microgrid, focusing on integrating renewable energy sources like wind power and assessing the impact of fuel availability challenges.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      5
    ],
    "id": "05fff9ce2ca703392bb2eaf580dafaa72d14814ea64038b0be88a37dd3eda23e"
  },
  {
    "text": "St. Mary's is a remote rural community in western Alaska, located on the Yukon River and served by its own isolated electrical power grid. The St. Mary's power distribution feeder system is a 12.47 kV and 400 V system. The total load of the system can reach a peak of approximately 600 kW, with a minimum load hitting approximately 150 kW. Because it is situated in such a remote location with the associated challenges to supply energy to consumers, the price of energy is rather high, more than double the average U.S. household in 2022 according to [23] and [24]. The power demand of the St. Mary's community is served by three diesel generators listed in Table 1. The generators are assumed to have an inertia constant of 2, have ramping capability of reaching full output in 10 seconds, and their rate of burning fuel for this study are constant.\nTABLE 1. St. Mary's gensets.\nBy January 5, 2019, the Alaska Village Electric Cooperative (A VEC) had installed a 900 kW, 52-meter rotor diameter type IV pitch-controlled wind turbine generator manufactured by Emergya Wind Technologies and started producing power. Schematically illustrated as a single-line diagram in Fig. 3, the St. Mary's power distribution feeder details can be found in [25] and [26].\nLocated in a remote geographical area falling within the transitional climate zone with seasons changing from long, cold winters to shorter, warmer summers, the St. Mary's microgrid is predisposed to seasonal and operational disturbances, such as, failures due to diesel fuel delivery, high wind speeds, or cyber-physical security attacks. Therefore, it is imperative for the owner operators to have access to real-time information about how much uncertainty the system can sustain during its operation given the current operation points and disturbances.\nThe MIRACL-CSP has been designed to build use-case scenarios to evaluate the ability of a distribution system, specifically the St. Mary's microgrid, to resist, adapt, and recover from possible disturbances by measuring its resilience metric at simulation time. This operational\nFIGURE 3. St. Mary's distribution feeder single-line diagram.\nTABLE 2. St. Mary's gensets running capacity.\nresilience metric depends on monitoring the system inertia and aggregating all the generation assets adaptive capacity in real and reactive power domains. According to [9], given the nameplate rated capacity, latency, ramp rates, and energy constraints, the adaptive capacity of an asset can be explored by calculating its control domain in real and reactive power from the current point of operation.\nThe St. Mary's power distribution system in Fig. 3 has been modeled in GridLAB-D [16] as an isolated grid with the diesel generators being represented by the synchronous machine model. The unbalanced operation of three-phase synchronous machines is modeled with a simplified fundamental frequency model in phasor representation. 8\nThe wind turbine generator is modeled as an inverterinterfaced resource operating as a constant real and reactive power generator. 9 This allows the application to emulate the real wind turbine behavior, by dispatching actual measured wind generation profiles in different scenarios.\nThe loads in the St. Mary's microgrid are modeled as constant power loads 10 and have no voltage dependency. They all sum up to approximately 501 kW of real power demand on all three phases.\nIf the wind turbine is inactive, given the current configuration, to supply all loads and cover the line losses, the GridLAB-D powerflow solver calculates that the diesel generators would run according to the data in Table 2.\n8 http://gridlab-d.shoutwiki.com/wiki/Generators_Module_Guide# Diesel_DG_Model\n9 http://gridlab-d.shoutwiki.com/wiki/Inverter\n10 http://gridlab-d.shoutwiki.com/wiki/ZIPload\nFIGURE 4. HELICS integration architecture.\nGridLAB-D models for the diesel generator dynamics do not include parameters to specifically control generator efficiency and priority. Rather, their generation will vary following the bus frequency deviation due to an imbalance in the system supply and demand, which in this case is assumed to be constant.\nFrom [27], it can be concluded that the primary resilience challenge for the St. Mary's microgrid is fuel availability for the diesel generators. Fuel gets delivered by boat on the Yukon River, which is impassable from August through April due to the long and very cold winters. Therefore, life threatening situations could arise during winter as consequences to diesel fuel depletion. The addition of the distributed wind (DW) turbine to the grid reduces the community's dependence on diesel fuel.\nAVEC, as the electric utility serving the city of St. Mary's, has provided wind speed (m/s) and power generation (kW) measurement data for the wind turbine for years 2019, 2020, and 2021.\nWithin MIRACL-CSP, the UCC loads similar profiles and dispatches them to the GridLAB-D inverter-based wind turbine model to follow the generation profile, thus creating more realistic use-case scenarios inside the co-simulation environment.\nTo perform a quantitative analysis of the value of DW, MIRACL-CSP integrates the GridLAB-D model of St. Mary's microgrid with PowDDeR leveraging its capabilities to establish performance under uncertainties. The integration is realized through the HELICS API 11 [28]. In particular, for this co-simulation integration, the GridLAB-D and Python federates are treated as message federates 12 and data exchange is configured using JSON config files 13 by defining\n11 https://docs.helics.org/en/latest/references/api-reference/C_API.html\n12 https://docs.helics.org/en/latest/user-guide/fundamental_topics/ message_federates.html\n13 https://docs.helics.org/en/latest/user-guide/fundamental_topics/ interface_configuration.html#json-configuration\ncorresponding endpoints. 14 Fig. 4 exemplifies how the connection between simulators/federates is realized. Specifically, the monitor endpoint of the UCC federate communicates the wind turbine generation profile loaded from real-life acquired measurements. The inv_WTG_Pref endpoint of the GridLAB-D federate subscribing to those values captures them and feeds them in the distribution system power flow calculations. Similarly, endpoints, such as meter_DG1_real , publish the measured generation of the diesel generators to the HELICS environment to be picked by the UCC federate and loaded into the PowDDeR application for processing and metrics calculation.\n\nDescribes the St. Mary’s microgrid, highlighting its remote location, reliance on diesel generators, and the need for real-time information about fuel availability and system resilience.",
    "original_text": "St. Mary's is a remote rural community in western Alaska, located on the Yukon River and served by its own isolated electrical power grid. The St. Mary's power distribution feeder system is a 12.47 kV and 400 V system. The total load of the system can reach a peak of approximately 600 kW, with a minimum load hitting approximately 150 kW. Because it is situated in such a remote location with the associated challenges to supply energy to consumers, the price of energy is rather high, more than double the average U.S. household in 2022 according to [23] and [24]. The power demand of the St. Mary's community is served by three diesel generators listed in Table 1. The generators are assumed to have an inertia constant of 2, have ramping capability of reaching full output in 10 seconds, and their rate of burning fuel for this study are constant.\nTABLE 1. St. Mary's gensets.\nBy January 5, 2019, the Alaska Village Electric Cooperative (A VEC) had installed a 900 kW, 52-meter rotor diameter type IV pitch-controlled wind turbine generator manufactured by Emergya Wind Technologies and started producing power. Schematically illustrated as a single-line diagram in Fig. 3, the St. Mary's power distribution feeder details can be found in [25] and [26].\nLocated in a remote geographical area falling within the transitional climate zone with seasons changing from long, cold winters to shorter, warmer summers, the St. Mary's microgrid is predisposed to seasonal and operational disturbances, such as, failures due to diesel fuel delivery, high wind speeds, or cyber-physical security attacks. Therefore, it is imperative for the owner operators to have access to real-time information about how much uncertainty the system can sustain during its operation given the current operation points and disturbances.\nThe MIRACL-CSP has been designed to build use-case scenarios to evaluate the ability of a distribution system, specifically the St. Mary's microgrid, to resist, adapt, and recover from possible disturbances by measuring its resilience metric at simulation time. This operational\nFIGURE 3. St. Mary's distribution feeder single-line diagram.\nTABLE 2. St. Mary's gensets running capacity.\nresilience metric depends on monitoring the system inertia and aggregating all the generation assets adaptive capacity in real and reactive power domains. According to [9], given the nameplate rated capacity, latency, ramp rates, and energy constraints, the adaptive capacity of an asset can be explored by calculating its control domain in real and reactive power from the current point of operation.\nThe St. Mary's power distribution system in Fig. 3 has been modeled in GridLAB-D [16] as an isolated grid with the diesel generators being represented by the synchronous machine model. The unbalanced operation of three-phase synchronous machines is modeled with a simplified fundamental frequency model in phasor representation. 8\nThe wind turbine generator is modeled as an inverterinterfaced resource operating as a constant real and reactive power generator. 9 This allows the application to emulate the real wind turbine behavior, by dispatching actual measured wind generation profiles in different scenarios.\nThe loads in the St. Mary's microgrid are modeled as constant power loads 10 and have no voltage dependency. They all sum up to approximately 501 kW of real power demand on all three phases.\nIf the wind turbine is inactive, given the current configuration, to supply all loads and cover the line losses, the GridLAB-D powerflow solver calculates that the diesel generators would run according to the data in Table 2.\n8 http://gridlab-d.shoutwiki.com/wiki/Generators_Module_Guide# Diesel_DG_Model\n9 http://gridlab-d.shoutwiki.com/wiki/Inverter\n10 http://gridlab-d.shoutwiki.com/wiki/ZIPload\nFIGURE 4. HELICS integration architecture.\nGridLAB-D models for the diesel generator dynamics do not include parameters to specifically control generator efficiency and priority. Rather, their generation will vary following the bus frequency deviation due to an imbalance in the system supply and demand, which in this case is assumed to be constant.\nFrom [27], it can be concluded that the primary resilience challenge for the St. Mary's microgrid is fuel availability for the diesel generators. Fuel gets delivered by boat on the Yukon River, which is impassable from August through April due to the long and very cold winters. Therefore, life threatening situations could arise during winter as consequences to diesel fuel depletion. The addition of the distributed wind (DW) turbine to the grid reduces the community's dependence on diesel fuel.\nAVEC, as the electric utility serving the city of St. Mary's, has provided wind speed (m/s) and power generation (kW) measurement data for the wind turbine for years 2019, 2020, and 2021.\nWithin MIRACL-CSP, the UCC loads similar profiles and dispatches them to the GridLAB-D inverter-based wind turbine model to follow the generation profile, thus creating more realistic use-case scenarios inside the co-simulation environment.\nTo perform a quantitative analysis of the value of DW, MIRACL-CSP integrates the GridLAB-D model of St. Mary's microgrid with PowDDeR leveraging its capabilities to establish performance under uncertainties. The integration is realized through the HELICS API 11 [28]. In particular, for this co-simulation integration, the GridLAB-D and Python federates are treated as message federates 12 and data exchange is configured using JSON config files 13 by defining\n11 https://docs.helics.org/en/latest/references/api-reference/C_API.html\n12 https://docs.helics.org/en/latest/user-guide/fundamental_topics/ message_federates.html\n13 https://docs.helics.org/en/latest/user-guide/fundamental_topics/ interface_configuration.html#json-configuration\ncorresponding endpoints. 14 Fig. 4 exemplifies how the connection between simulators/federates is realized. Specifically, the monitor endpoint of the UCC federate communicates the wind turbine generation profile loaded from real-life acquired measurements. The inv_WTG_Pref endpoint of the GridLAB-D federate subscribing to those values captures them and feeds them in the distribution system power flow calculations. Similarly, endpoints, such as meter_DG1_real , publish the measured generation of the diesel generators to the HELICS environment to be picked by the UCC federate and loaded into the PowDDeR application for processing and metrics calculation.",
    "context": "Describes the St. Mary’s microgrid, highlighting its remote location, reliance on diesel generators, and the need for real-time information about fuel availability and system resilience.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      5,
      6
    ],
    "id": "fde12340d082042724b65b95d13a5fb9fdacd32170699d66f5ba8570c850048c"
  },
  {
    "text": "With the St. Mary's GridLAB-D model running with a 501 kW peak load demand, the diesel generators need to run at the capacities listed in Table 2. When a DW resource is added to the system, its dynamics are modified according to the variability of wind power generation. Using different wind profiles provided by AVEC, the St. Mary's microgrid can be simulated under various wind conditions to study its resilience in the presence of variable generation and uncertainties. MIRACL-CSP offers the platform to easily create these scenarios and supply the simulated data directly to PowDDeR for resilience metrics calculation during runtime. Two scenarios have been considered, with wind generation profile selected from the A VEC data presenting a decreasing trend over the simulated time, natural uncertainties that could affect the grid, and fuel depletion:\n- S.1: When the wind speed varies from higher to lower speeds (max 11 . 17 m/s, min 4 . 7 m/s, mean 7 . 74 m/s, and standard deviation 1 . 65), analyzing the system resilience through its assets' adaptive capacity can\n14 https://docs.helics.org/en/latest/user-guide/fundamental_topics/ message_federates.html#message-federate-endpoints\nanswer questions related to possible needs for diesel generation curtailment or load shed.\n- S.2: This scenario assumes the same wind profile as S.1: with the addition of large disturbances. These system dynamics force a drastic change in generation to meet the load. The resilience metric is very important in this case. The disturbances here are based on loss of diesel generators due to lack of fuel. However, the loss of these generators could occur from physical degradation, storms, or cyber threats.\n\nDescribes the simulation scenarios and the use of MIRACL-CSP to assess the St. Mary’s microgrid’s resilience under varying wind conditions and potential generator failures.",
    "original_text": "With the St. Mary's GridLAB-D model running with a 501 kW peak load demand, the diesel generators need to run at the capacities listed in Table 2. When a DW resource is added to the system, its dynamics are modified according to the variability of wind power generation. Using different wind profiles provided by AVEC, the St. Mary's microgrid can be simulated under various wind conditions to study its resilience in the presence of variable generation and uncertainties. MIRACL-CSP offers the platform to easily create these scenarios and supply the simulated data directly to PowDDeR for resilience metrics calculation during runtime. Two scenarios have been considered, with wind generation profile selected from the A VEC data presenting a decreasing trend over the simulated time, natural uncertainties that could affect the grid, and fuel depletion:\n- S.1: When the wind speed varies from higher to lower speeds (max 11 . 17 m/s, min 4 . 7 m/s, mean 7 . 74 m/s, and standard deviation 1 . 65), analyzing the system resilience through its assets' adaptive capacity can\n14 https://docs.helics.org/en/latest/user-guide/fundamental_topics/ message_federates.html#message-federate-endpoints\nanswer questions related to possible needs for diesel generation curtailment or load shed.\n- S.2: This scenario assumes the same wind profile as S.1: with the addition of large disturbances. These system dynamics force a drastic change in generation to meet the load. The resilience metric is very important in this case. The disturbances here are based on loss of diesel generators due to lack of fuel. However, the loss of these generators could occur from physical degradation, storms, or cyber threats.",
    "context": "Describes the simulation scenarios and the use of MIRACL-CSP to assess the St. Mary’s microgrid’s resilience under varying wind conditions and potential generator failures.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      6,
      7
    ],
    "id": "2074ae87dfb749985b54dd06c7c2efddd3b1136a83d01ae141abfd420d2d45ca"
  },
  {
    "text": "This section provides the testing results following the use case breakdowns previously discussed. During the co-simulation runtime of the St. Mary's microgrid, PowDDeR gathers the current states of the considered system assets, that is diesel generators and the wind turbine, constructs the adaptive capacity manifolds, gets the system inertia, and calculates the short-term and long-term resilience.\n\nThis section details the simulation results and resilience metrics calculated during the co-simulation of the St. Mary’s microgrid, focusing on the impact of diesel generator failures and wind variability on system stability and fuel consumption.",
    "original_text": "This section provides the testing results following the use case breakdowns previously discussed. During the co-simulation runtime of the St. Mary's microgrid, PowDDeR gathers the current states of the considered system assets, that is diesel generators and the wind turbine, constructs the adaptive capacity manifolds, gets the system inertia, and calculates the short-term and long-term resilience.",
    "context": "This section details the simulation results and resilience metrics calculated during the co-simulation of the St. Mary’s microgrid, focusing on the impact of diesel generator failures and wind variability on system stability and fuel consumption.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      7
    ],
    "id": "3fda79053d87c5eb265bfa2282b371eea64b1128e3e520d7a1d4222bd2631ce6"
  },
  {
    "text": "The simulated wind and diesel generation output and frequency at the bus from GridLAB-D are shown in Figs. 5a and 5b, respectively. It can be seen that the wind turbine generator (WTG) and diesel generator 1 (DG1) support the majority of the load. The loss of wind generation over the simulation is compensated by the ramping of output of generator 1. However, diesel generators 2 and 3 are both online over the duration of the simulation and can be seen by the inertia plot in Fig. 5c. Here, each generator is supplying inertia to the system. It is assumed that each generator has an inertia constant of 2. Since each generator is running, they are burning fuel at the rate given in Table 1 and the resulting fuel remaining is shown in Fig. 5d.\nThe resulting short-term resilience, which is a measure of the size of disturbance the system can withstand without dropping below a frequency limit of 58Hz, is shown in Fig. 5e. Here, it can be seen that the short-term resilience has small variation and it follows the same profile as the system frequency. As the system frequency falls, the short-term resilience also falls. The long-term resilience is shown in Fig. 5f. Here, it can be seen that the system resilience is the aggregation of each generator and is continually reduced over time. It is based on the available fuel left, the fuel consumption of the generators, and their maximum outputs.\n\nThis section details the simulation results, specifically focusing on the output and frequency data from GridLAB-D, illustrating how the wind turbine and diesel generators support the load and the subsequent impact on fuel levels, short-term resilience, and long-term resilience.",
    "original_text": "The simulated wind and diesel generation output and frequency at the bus from GridLAB-D are shown in Figs. 5a and 5b, respectively. It can be seen that the wind turbine generator (WTG) and diesel generator 1 (DG1) support the majority of the load. The loss of wind generation over the simulation is compensated by the ramping of output of generator 1. However, diesel generators 2 and 3 are both online over the duration of the simulation and can be seen by the inertia plot in Fig. 5c. Here, each generator is supplying inertia to the system. It is assumed that each generator has an inertia constant of 2. Since each generator is running, they are burning fuel at the rate given in Table 1 and the resulting fuel remaining is shown in Fig. 5d.\nThe resulting short-term resilience, which is a measure of the size of disturbance the system can withstand without dropping below a frequency limit of 58Hz, is shown in Fig. 5e. Here, it can be seen that the short-term resilience has small variation and it follows the same profile as the system frequency. As the system frequency falls, the short-term resilience also falls. The long-term resilience is shown in Fig. 5f. Here, it can be seen that the system resilience is the aggregation of each generator and is continually reduced over time. It is based on the available fuel left, the fuel consumption of the generators, and their maximum outputs.",
    "context": "This section details the simulation results, specifically focusing on the output and frequency data from GridLAB-D, illustrating how the wind turbine and diesel generators support the load and the subsequent impact on fuel levels, short-term resilience, and long-term resilience.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      7
    ],
    "id": "9999a0ba7dd603beba1cad1d62ec225e7536148c5c753b65961eb869705fbf63"
  },
  {
    "text": "The power generation and frequency from the GridLab-D simulation for this scenario are shown in Figs. 6a and 6b, respectively. Again, it can be seen that the wind generation and diesel generator 1 support the majority of the load. However, in this scenario, generator 1 and 3 run out of fuel and are taken off-line at 250 and 75 seconds, respectively. The loss of wind generation is initially compensated by ramping of generator 1. When this generator runs out of fuel, generator 2\nFIGURE 5. Scenario 1, without any large disturbances, there is little change in the short-term resilience while the long-term resilience is continually reduced as fuel is depleted.\nis ramped up to balance the load demand. The loss of generation units has a direct impact to the system inertia, shown in\nLuc\nDowDDeR Iono_term recilience\nFIGURE 6. Scenario 2, the loss of a generator from running out of fuel has a large negative impact on the short-term resilience. However, the rate of reduction of the long-term resilience is slowed.\nFIGURE 7. Short-term and long-term resilience for each scenario. The impact of losing or taking a generator off-line has a reduction of short-term resilience but increases the long-term resilience.\nFig. 6c. When they are taken off-line, they no longer add any inertia to the system.\nIn this case, the resulting short-term resilience is shown in Fig. 6e. Here, it can be seen that the short-term resilience not only follows the frequency of the system, but, more importantly, it takes a large step reduction when a generator is taken off-line. This results in the system not being able to withstand large disturbances before frequency limits are reached. The long-term resilience is shown in Fig. 6f. Here, the long-term resilience contribution of each generator is shown along with system's resilience. The system resilience is actually reduced at a slower rate after each generator is taken off-line. It should be noted that this is only possible as there is adequate generation still online to support the load demand.\n\nHighlights the scenario's impact on short-term and long-term resilience, specifically noting a significant drop in short-term resilience when generators are taken offline due to fuel depletion.",
    "original_text": "The power generation and frequency from the GridLab-D simulation for this scenario are shown in Figs. 6a and 6b, respectively. Again, it can be seen that the wind generation and diesel generator 1 support the majority of the load. However, in this scenario, generator 1 and 3 run out of fuel and are taken off-line at 250 and 75 seconds, respectively. The loss of wind generation is initially compensated by ramping of generator 1. When this generator runs out of fuel, generator 2\nFIGURE 5. Scenario 1, without any large disturbances, there is little change in the short-term resilience while the long-term resilience is continually reduced as fuel is depleted.\nis ramped up to balance the load demand. The loss of generation units has a direct impact to the system inertia, shown in\nLuc\nDowDDeR Iono_term recilience\nFIGURE 6. Scenario 2, the loss of a generator from running out of fuel has a large negative impact on the short-term resilience. However, the rate of reduction of the long-term resilience is slowed.\nFIGURE 7. Short-term and long-term resilience for each scenario. The impact of losing or taking a generator off-line has a reduction of short-term resilience but increases the long-term resilience.\nFig. 6c. When they are taken off-line, they no longer add any inertia to the system.\nIn this case, the resulting short-term resilience is shown in Fig. 6e. Here, it can be seen that the short-term resilience not only follows the frequency of the system, but, more importantly, it takes a large step reduction when a generator is taken off-line. This results in the system not being able to withstand large disturbances before frequency limits are reached. The long-term resilience is shown in Fig. 6f. Here, the long-term resilience contribution of each generator is shown along with system's resilience. The system resilience is actually reduced at a slower rate after each generator is taken off-line. It should be noted that this is only possible as there is adequate generation still online to support the load demand.",
    "context": "Highlights the scenario's impact on short-term and long-term resilience, specifically noting a significant drop in short-term resilience when generators are taken offline due to fuel depletion.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      8,
      7
    ],
    "id": "857842a177a8ea7162d106c3d4d562a8339207dcff726eec25e7ce6a981a2bbc"
  },
  {
    "text": "The results of the short-term and long-term resilience for each scenario are shown in Fig. 7. The key takeaway that can be seen is the trade-off between short-term and longterm resilience. Examining scenario 2, you can see that when a generator goes off-line it has a large negative impact to the short-term resilience because of the reduced inertia and generation ramping capability. However, it has a positive effect on the long-term resilience. This is due to the system no longer burning as much fuel to support the load. It should be noted that this is only possible because the generators are not being run near their maximum generation capability, therefore they can be ramped up to support the load demand.\nAnother key takeaway is that the short-term and long-term resilience metrics do not directly show the impact of wind generation. In the case of the short-term resilience, wind does not directly add inertia to the system and it is ran at its maximum output. In order to add to the short-term resilience, the wind turbine generator can be ran below its maximum capability. The impact to resilience based on the dynamics of different generation assets has been demonstrated in [22]. Here, it was shown that the quick ramping capability of inverter-based generation can have a large impact to maintain frequency stability. For the long-term resilience, wind\ngeneration allows for diesel generators to be taken off-line, conserving the fuel, and therefore increasing the long-term resilience.\n\nHighlights the trade-off between short-term and long-term resilience, specifically noting that losing a generator negatively impacts short-term resilience due to reduced inertia and ramping capability, but positively affects long-term resilience by conserving fuel.",
    "original_text": "The results of the short-term and long-term resilience for each scenario are shown in Fig. 7. The key takeaway that can be seen is the trade-off between short-term and longterm resilience. Examining scenario 2, you can see that when a generator goes off-line it has a large negative impact to the short-term resilience because of the reduced inertia and generation ramping capability. However, it has a positive effect on the long-term resilience. This is due to the system no longer burning as much fuel to support the load. It should be noted that this is only possible because the generators are not being run near their maximum generation capability, therefore they can be ramped up to support the load demand.\nAnother key takeaway is that the short-term and long-term resilience metrics do not directly show the impact of wind generation. In the case of the short-term resilience, wind does not directly add inertia to the system and it is ran at its maximum output. In order to add to the short-term resilience, the wind turbine generator can be ran below its maximum capability. The impact to resilience based on the dynamics of different generation assets has been demonstrated in [22]. Here, it was shown that the quick ramping capability of inverter-based generation can have a large impact to maintain frequency stability. For the long-term resilience, wind\ngeneration allows for diesel generators to be taken off-line, conserving the fuel, and therefore increasing the long-term resilience.",
    "context": "Highlights the trade-off between short-term and long-term resilience, specifically noting that losing a generator negatively impacts short-term resilience due to reduced inertia and ramping capability, but positively affects long-term resilience by conserving fuel.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      8,
      9
    ],
    "id": "22a8217c790b081d22c67a9b62d6a51d314ffa43b6e25102b6cd841042de2692"
  },
  {
    "text": "In this paper, to account for the time-dependent behavior of a power distribution system under uncertainties and dynamically study its resilience, a co-simulation platform, also known as MIRACL-CSP, has been developed and detailed. The intended purpose of this platform was to integrate the dynamics of the distribution system with the resilience assessment and quantification metric. As a use case for the proposed platform to quantitatively study the resilience of a microgrid in a holistic manner, we analyzed the resilience of the real world St. Mary's microgrid incorporating a 900 kW wind turbine generator under different scenarios dictated by real-life uncertainties using PowDDeR, a software application providing resilience metrics for power systems based on their adaptive capacity and inertia. The integration of PowDDeR with GridLAB-D, the St. Mary's microgrid power flow simulator, is facilitated by MIRACL-CSP so that the resilience application could monitor and analyze distribution system measurements in real time. The co-simulation environment allowed us to collect system dynamics data to characterize its resilience at different time scales.\nThe resilience results of the St. Mary's microgrid show a trade-off between short-term and long-term resilience. Having diesel generators online add to the short-term resilience as there is more inertia in the system and a quicker ramping capability if a disturbance occurs. However, this results in faster reduction of fuel and therefore a quicker reduction in long-term resilience. The contribution of wind generation has the ability to increase either the short-term or long-term resilience depending on how it is utilized. If it is run at maximum output, diesel generation can be taken off-line. If it is run below its maximum output, the fast ramping capability of inverters allows for increased short-term resilience of the system. Future work includes leveraging MIRACL-CSP as the practical framework for asset owners and operators, and original equipment providers to understand benefit versus risk through co-optimization of resilience and economic considerations for applications. With DW as a resilience enhancement, the resilience metrics can be used as control decisions to benefit the overall system efficiency, and the cost of operation in particular.\nMoreover, MIRACL-CSP serves as a prototype for complex systems integration with dynamic resilience quantitative metrics, and could be adapted and/or extended for studies pertaining to other domains, such as chemical and water infrastructure systems.\n\nDetails the development and application of a co-simulation platform (MIRACL-CSP) for analyzing the resilience of the St. Mary’s microgrid, including its use with PowDDeR to characterize resilience at different time scales and highlighting a trade-off between short-term and long-term resilience due to the interplay of wind and diesel generation.",
    "original_text": "In this paper, to account for the time-dependent behavior of a power distribution system under uncertainties and dynamically study its resilience, a co-simulation platform, also known as MIRACL-CSP, has been developed and detailed. The intended purpose of this platform was to integrate the dynamics of the distribution system with the resilience assessment and quantification metric. As a use case for the proposed platform to quantitatively study the resilience of a microgrid in a holistic manner, we analyzed the resilience of the real world St. Mary's microgrid incorporating a 900 kW wind turbine generator under different scenarios dictated by real-life uncertainties using PowDDeR, a software application providing resilience metrics for power systems based on their adaptive capacity and inertia. The integration of PowDDeR with GridLAB-D, the St. Mary's microgrid power flow simulator, is facilitated by MIRACL-CSP so that the resilience application could monitor and analyze distribution system measurements in real time. The co-simulation environment allowed us to collect system dynamics data to characterize its resilience at different time scales.\nThe resilience results of the St. Mary's microgrid show a trade-off between short-term and long-term resilience. Having diesel generators online add to the short-term resilience as there is more inertia in the system and a quicker ramping capability if a disturbance occurs. However, this results in faster reduction of fuel and therefore a quicker reduction in long-term resilience. The contribution of wind generation has the ability to increase either the short-term or long-term resilience depending on how it is utilized. If it is run at maximum output, diesel generation can be taken off-line. If it is run below its maximum output, the fast ramping capability of inverters allows for increased short-term resilience of the system. Future work includes leveraging MIRACL-CSP as the practical framework for asset owners and operators, and original equipment providers to understand benefit versus risk through co-optimization of resilience and economic considerations for applications. With DW as a resilience enhancement, the resilience metrics can be used as control decisions to benefit the overall system efficiency, and the cost of operation in particular.\nMoreover, MIRACL-CSP serves as a prototype for complex systems integration with dynamic resilience quantitative metrics, and could be adapted and/or extended for studies pertaining to other domains, such as chemical and water infrastructure systems.",
    "context": "Details the development and application of a co-simulation platform (MIRACL-CSP) for analyzing the resilience of the St. Mary’s microgrid, including its use with PowDDeR to characterize resilience at different time scales and highlighting a trade-off between short-term and long-term resilience due to the interplay of wind and diesel generation.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      9
    ],
    "id": "4e5d0a69a818b29d6b9573d65a66eb648709e9b5212833c5ccfdf1074b084ca7"
  },
  {
    "text": "- [1] S. Afzal, H. Mokhlis, H. A. Illias, N. N. Mansor, and H. Shareef, ''State-of-the-art review on power system resilience and assessment techniques,'' IET Gener., Transmiss. Distribution , vol. 14, no. 25, pp. 6107-6121, Dec. 2020. [Online]. Available: https://ietresearch. onlinelibrary.wiley.com/doi/abs/10.1049/iet-gtd.2020.0531\n- [2] A. Umunnakwe, H. Huang, K. Oikonomou, and K. Davis, ''Quantitative analysis of power systems resilience: Standardization, categorizations, and challenges,'' Renew. Sustain. Energy Rev. , vol. 149, Oct. 2021, Art. no. 111252. [Online]. Available: https://www.sciencedirect.com/ science/article/pii/S1364032121005396\n- [3] A. Younesi, H. Shayeghi, Z. Wang, P. Siano, A. Mehrizi-Sani, and A. Safari, ''Trends in modern power systems resilience: State-ofthe-art review,'' Renew. Sustain. Energy Rev. , vol. 162, Jul. 2022, Art. no. 112397. [Online]. Available: https://www.sciencedirect.com/ science/article/pii/S1364032122003070\n- [4] D. Wei and K. Ji, ''Resilient industrial control system (RICS): Concepts, formulation, metrics, and insights,'' in Proc. 3rd Int. Symp. Resilient Control Syst. , Aug. 2010, pp. 15-22.\n- [5] A. Clark and S. Zonouz, ''Cyber-physical resilience: Definition and assessment metric,'' IEEE Trans. Smart Grid , vol. 10, no. 2, pp. 1671-1684, Mar. 2018.\n- [6] J. D. Taft, ''Electric grid resilience and reliability for grid architecture,'' Pacific Northwest Nat. Lab., Richland, WA, USA, Tech. Rep. PNNL26623, 2017.\n- [7] T. R. McJunkin and C. G. Rieger, ''Electricity distribution system resilient control system metrics,'' in Proc. Resilience Week (RWS) , Sep. 2017, pp. 103-112.\n- [8] C. G. Rieger, ''Resilient control systems practical metrics basis for defining mission impact,'' in Proc. 7th Int. Symp. Resilient Control Syst. (ISRCS) , Aug. 2014, pp. 1-10.\n- [9] T. Phillips, T. McJunkin, C. Rieger, J. Gardner, and H. Mehrpouyan, ''An operational resilience metric for modern power distribution systems,'' in Proc. IEEE 20th Int. Conf. Softw. Qual., Rel. Secur. Companion (QRS-C) , Dec. 2020, pp. 334-342.\n- [10] T. Phillips, T. McJunkin, C. Rieger, J. Gardner, and H. Mehrpouyan, ''A framework for evaluating the resilience contribution of solar PV and battery storage on the grid,'' in Proc. Resilience Week (RWS) , Oct. 2020, pp. 133-139.\n- [11] T. Phillips, V. Chalishazar, T. McJunkin, M. Maharjan, S. M. Shafiul Alam, T. Mosier, and A. Somani, ''A metric framework for evaluating the resilience contribution of hydropower to the grid,'' in Proc. Resilience Week (RWS) , Oct. 2020, pp. 78-85.\n- [12] M. Jackson and J. S. Fitzgerald, ''Towards resilience-explicit modelling and co-simulation of cyber-physical systems,'' in Software Engineering and Formal Methods , A. Cerone and M. Roveri, Eds. Berlin, Germany: Springer, 2018, pp. 361-376.\n- [13] P. T. Mana, K. P. Schneider, W. Du, M. Mukherjee, T. Hardy, and F. K. Tuffner, ''Study of microgrid resilience through co-simulation of power system dynamics and communication systems,'' IEEE Trans. Ind. Informat. , vol. 17, no. 3, pp. 1905-1915, Mar. 2021.\n- [14] K. Hoth, T. Steffen, B. Wiegel, A. Youssfi, D. Babazadeh, M. Venzke, C. Becker, K. Fischer, and V. Turau, ''Holistic simulation approach for optimal operation of smart integrated energy systems under consideration of resilience, economics and sustainability,'' Infrastructures , vol. 6, no. 11, p. 150, Oct. 2021. [Online]. Available: https://www.mdpi.com/24123811/6/11/150\n- [15] B. Bhattarai, L. Marinovici, P. S. Sarker, and A. Orrell, ''MIRACL co-simulation platform for control and operation of distributed wind in microgrid,'' IET Smart Grid , vol. 5, no. 2, pp. 90-100, Apr. 2022. [Online]. Available: https://ietresearch.onlinelibrary.wiley.com/ doi/abs/10.1049/stg2.12054\n- [16] PNNL. (2022). GridLAB-D . [Online]. Available: https://www.gridlabd. org/\n- [17] LNNL. (2022). Hierarchical Engine for Large-Scale Infrastructure CoSimulation . [Online]. Available: https://www.helics.org/\n- [18] B. Palmintier, D. Krishnamurthy, P. Top, S. Smith, J. Daily, and J. Fuller, ''Design of the HELICS high-performance transmissiondistribution-communication-market co-simulation framework,'' in Proc. Workshop Model. Simul. Cyber-Phys. Energy Syst. (MSCPES) , Apr. 2017, pp. 1-6.\n- [19] D. P. Chassin, K. Schneider, and C. Gerkensmeyer, ''GridLAB-D: An open-source power systems modeling and simulation environment,'' in Proc. IEEE/PES Transmiss. Distrib. Conf. Expo. , Apr. 2008, pp. 1-5.\n- [20] (2022). Python . [Online]. Available: https://www.python.org/\n- [21] INL. (2019). PowDDeR . [Online]. Available: https://github.com/ IdahoLabUnsupported/PowDDeR\n\nProvides a review of power system resilience assessment techniques and introduces a co-simulation platform for studying microgrid resilience.",
    "original_text": "- [1] S. Afzal, H. Mokhlis, H. A. Illias, N. N. Mansor, and H. Shareef, ''State-of-the-art review on power system resilience and assessment techniques,'' IET Gener., Transmiss. Distribution , vol. 14, no. 25, pp. 6107-6121, Dec. 2020. [Online]. Available: https://ietresearch. onlinelibrary.wiley.com/doi/abs/10.1049/iet-gtd.2020.0531\n- [2] A. Umunnakwe, H. Huang, K. Oikonomou, and K. Davis, ''Quantitative analysis of power systems resilience: Standardization, categorizations, and challenges,'' Renew. Sustain. Energy Rev. , vol. 149, Oct. 2021, Art. no. 111252. [Online]. Available: https://www.sciencedirect.com/ science/article/pii/S1364032121005396\n- [3] A. Younesi, H. Shayeghi, Z. Wang, P. Siano, A. Mehrizi-Sani, and A. Safari, ''Trends in modern power systems resilience: State-ofthe-art review,'' Renew. Sustain. Energy Rev. , vol. 162, Jul. 2022, Art. no. 112397. [Online]. Available: https://www.sciencedirect.com/ science/article/pii/S1364032122003070\n- [4] D. Wei and K. Ji, ''Resilient industrial control system (RICS): Concepts, formulation, metrics, and insights,'' in Proc. 3rd Int. Symp. Resilient Control Syst. , Aug. 2010, pp. 15-22.\n- [5] A. Clark and S. Zonouz, ''Cyber-physical resilience: Definition and assessment metric,'' IEEE Trans. Smart Grid , vol. 10, no. 2, pp. 1671-1684, Mar. 2018.\n- [6] J. D. Taft, ''Electric grid resilience and reliability for grid architecture,'' Pacific Northwest Nat. Lab., Richland, WA, USA, Tech. Rep. PNNL26623, 2017.\n- [7] T. R. McJunkin and C. G. Rieger, ''Electricity distribution system resilient control system metrics,'' in Proc. Resilience Week (RWS) , Sep. 2017, pp. 103-112.\n- [8] C. G. Rieger, ''Resilient control systems practical metrics basis for defining mission impact,'' in Proc. 7th Int. Symp. Resilient Control Syst. (ISRCS) , Aug. 2014, pp. 1-10.\n- [9] T. Phillips, T. McJunkin, C. Rieger, J. Gardner, and H. Mehrpouyan, ''An operational resilience metric for modern power distribution systems,'' in Proc. IEEE 20th Int. Conf. Softw. Qual., Rel. Secur. Companion (QRS-C) , Dec. 2020, pp. 334-342.\n- [10] T. Phillips, T. McJunkin, C. Rieger, J. Gardner, and H. Mehrpouyan, ''A framework for evaluating the resilience contribution of solar PV and battery storage on the grid,'' in Proc. Resilience Week (RWS) , Oct. 2020, pp. 133-139.\n- [11] T. Phillips, V. Chalishazar, T. McJunkin, M. Maharjan, S. M. Shafiul Alam, T. Mosier, and A. Somani, ''A metric framework for evaluating the resilience contribution of hydropower to the grid,'' in Proc. Resilience Week (RWS) , Oct. 2020, pp. 78-85.\n- [12] M. Jackson and J. S. Fitzgerald, ''Towards resilience-explicit modelling and co-simulation of cyber-physical systems,'' in Software Engineering and Formal Methods , A. Cerone and M. Roveri, Eds. Berlin, Germany: Springer, 2018, pp. 361-376.\n- [13] P. T. Mana, K. P. Schneider, W. Du, M. Mukherjee, T. Hardy, and F. K. Tuffner, ''Study of microgrid resilience through co-simulation of power system dynamics and communication systems,'' IEEE Trans. Ind. Informat. , vol. 17, no. 3, pp. 1905-1915, Mar. 2021.\n- [14] K. Hoth, T. Steffen, B. Wiegel, A. Youssfi, D. Babazadeh, M. Venzke, C. Becker, K. Fischer, and V. Turau, ''Holistic simulation approach for optimal operation of smart integrated energy systems under consideration of resilience, economics and sustainability,'' Infrastructures , vol. 6, no. 11, p. 150, Oct. 2021. [Online]. Available: https://www.mdpi.com/24123811/6/11/150\n- [15] B. Bhattarai, L. Marinovici, P. S. Sarker, and A. Orrell, ''MIRACL co-simulation platform for control and operation of distributed wind in microgrid,'' IET Smart Grid , vol. 5, no. 2, pp. 90-100, Apr. 2022. [Online]. Available: https://ietresearch.onlinelibrary.wiley.com/ doi/abs/10.1049/stg2.12054\n- [16] PNNL. (2022). GridLAB-D . [Online]. Available: https://www.gridlabd. org/\n- [17] LNNL. (2022). Hierarchical Engine for Large-Scale Infrastructure CoSimulation . [Online]. Available: https://www.helics.org/\n- [18] B. Palmintier, D. Krishnamurthy, P. Top, S. Smith, J. Daily, and J. Fuller, ''Design of the HELICS high-performance transmissiondistribution-communication-market co-simulation framework,'' in Proc. Workshop Model. Simul. Cyber-Phys. Energy Syst. (MSCPES) , Apr. 2017, pp. 1-6.\n- [19] D. P. Chassin, K. Schneider, and C. Gerkensmeyer, ''GridLAB-D: An open-source power systems modeling and simulation environment,'' in Proc. IEEE/PES Transmiss. Distrib. Conf. Expo. , Apr. 2008, pp. 1-5.\n- [20] (2022). Python . [Online]. Available: https://www.python.org/\n- [21] INL. (2019). PowDDeR . [Online]. Available: https://github.com/ IdahoLabUnsupported/PowDDeR",
    "context": "Provides a review of power system resilience assessment techniques and introduces a co-simulation platform for studying microgrid resilience.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      9
    ],
    "id": "f42b5f0168169a1d5f62d038143ec06f50e9ecb2cfec229d3cb0b8280093bdd0"
  },
  {
    "text": "- [22] T. Phillips, T. McJunkin, S. M. S. Alam, B. Poudel, and T. Mosier, ''An operational resilience metric to evaluate inertia and inverter-based generation on the grid,'' in Proc. IEEE Power Energy Soc. Gen. Meeting (PESGM) , Jul. 2022, pp. 1-5.\n- [23] Utilities Local. (2022). Residential Electricity Rates in Saint Mary's . [Online]. Available: https://utilitieslocal.com/states/alaska/saint-marys/ #electricity\n- [24] US EIA. (Nov. 2021). State Electricity Profiles . [Online]. Available: https://www.eia.gov/electricity/state/\n- [25] D. Vaught, ''Saint Mary's, Alaska REF 8 wind-diesel project analysis,'' V3 Energy LLC, Eagle River, AK, USA, Tech. Rep., 2014.\n- [26] J. Flicker, J. Hernandez-Alvidrez, M. Shirazi, J. Vandermeer, and W. Thomson, ''Grid forming inverters for spinning reserve in hybrid diesel microgrids,'' in Proc. IEEE Power Energy Soc. Gen. Meeting (PESGM) , Aug. 2020, pp. 1-5.\n- [27] J. D. Flicker. (Dec. 2019). Grid-Bridging Inverter Application at St. Mary's/Mountain Village Microgrid Systems . [Online]. Available: https://www.osti.gov/biblio/1646326\n- [28] LNNL. (2022). Hierarchical Engine for Large-scale Infrastructure CoSimulation-C API Reference . [Online]. Available: https://docs.helics.org/ en/latest/references/api-reference/C_API.html\nTYLER PHILLIPS (Member, IEEE) received the B.S. and M.S. degrees in mechanical engineering and the Ph.D. degree in computing, computational mathematics, science, and engineering from Boise State University, in 2009, 2014, and 2020, respectively. He is currently a Postdoctoral Researcher with the Idaho National Laboratory, Energy and Environmental Science and Technology Group. His research interests include power and energy system simulations, numerical comput- ing, and data analysis in the areas of resilient control, resilience metrics, integration of renewable generation, microgrids, and dynamic line ratings. He is also involved with the interaction of human factors in power systems. Since 2015, he has been with the IEEE Power and Energy Society.\nLAURENTIU D. MARINOVICI (Member, IEEE) received the B.Eng. degree in computer engineering and the M.Sc. degree in automatic control from the Gheorghe Asachi Technical University of Iaşi, Iaşi, Romania, in 2000 and 2001, respectively, and the M.Sc. and Ph.D. degrees in electrical and computer engineering from Louisiana State University, in 2007 and 2011, respectively. He is currently a Research Engineer with the Pacific Northwest National Laboratory, Optimization and Control\nGroup. His research interests lie at the confluence of software engineering and development, and model-based control, simulation and data analysis. He has been involved with designing, developing, and testing co-simulation platforms to implement and validate control algorithms for applications at different levels of the electrical power grid. Since 2011, he has been with the IEEE Power and Energy Society and IEEE Control Systems Society.\nCRAIG RIEGER (Senior Member, IEEE) received the B.S. and M.S. degrees in chemical engineering from Montana State University, in 1983 and 1985, respectively, and the Ph.D. degree in engineering and applied science from Idaho State University, in 2008. His Ph.D. coursework and dissertation focused on measurements and control, with specific application to intelligent, supervisory ventilation controls for critical infrastructure. He is currently a Professional Engineer. He is the Chief\nControl Systems Research Engineer and a Directorate Fellow with the Idaho National Laboratory (INL), pioneering interdisciplinary research in next generation resilient control systems. The grand challenge provided an integrated research strategy to address the cognitive, cyber-physical challenges of complex control systems into self-aware, trust-confirming, and threatresilient architectures. In addition, he has organized and chaired 14 cosponsored symposia and one National Science Foundation workshop in this new research area and authored more than 75 peer-reviewed publications. He has 20 years of software and hardware design experience for process control system upgrades and new installations. He has also been a supervisor and the technical lead for control systems engineering groups having design, configuration management, and security responsibilities for several INL nuclear facilities and various control system architectures.\nALICE ORRELL received the B.S. degree in mechanical engineering from the University of Vermont and the M.B.A. degree from the University of Washington. She is currently a Professional Engineer and manages the Pacific Northwest National Laboratory's distributed wind research portfolio, which included the MIRACL Project. She is also the Lead Author of the U.S. Department of Energy's annual ''Distributed Wind Market Report.''\n\nProvides a detailed analysis of the St. Mary’s, Alaska wind-diesel project and related grid-inverter applications, highlighting research related to microgrids and resilience metrics.",
    "original_text": "- [22] T. Phillips, T. McJunkin, S. M. S. Alam, B. Poudel, and T. Mosier, ''An operational resilience metric to evaluate inertia and inverter-based generation on the grid,'' in Proc. IEEE Power Energy Soc. Gen. Meeting (PESGM) , Jul. 2022, pp. 1-5.\n- [23] Utilities Local. (2022). Residential Electricity Rates in Saint Mary's . [Online]. Available: https://utilitieslocal.com/states/alaska/saint-marys/ #electricity\n- [24] US EIA. (Nov. 2021). State Electricity Profiles . [Online]. Available: https://www.eia.gov/electricity/state/\n- [25] D. Vaught, ''Saint Mary's, Alaska REF 8 wind-diesel project analysis,'' V3 Energy LLC, Eagle River, AK, USA, Tech. Rep., 2014.\n- [26] J. Flicker, J. Hernandez-Alvidrez, M. Shirazi, J. Vandermeer, and W. Thomson, ''Grid forming inverters for spinning reserve in hybrid diesel microgrids,'' in Proc. IEEE Power Energy Soc. Gen. Meeting (PESGM) , Aug. 2020, pp. 1-5.\n- [27] J. D. Flicker. (Dec. 2019). Grid-Bridging Inverter Application at St. Mary's/Mountain Village Microgrid Systems . [Online]. Available: https://www.osti.gov/biblio/1646326\n- [28] LNNL. (2022). Hierarchical Engine for Large-scale Infrastructure CoSimulation-C API Reference . [Online]. Available: https://docs.helics.org/ en/latest/references/api-reference/C_API.html\nTYLER PHILLIPS (Member, IEEE) received the B.S. and M.S. degrees in mechanical engineering and the Ph.D. degree in computing, computational mathematics, science, and engineering from Boise State University, in 2009, 2014, and 2020, respectively. He is currently a Postdoctoral Researcher with the Idaho National Laboratory, Energy and Environmental Science and Technology Group. His research interests include power and energy system simulations, numerical comput- ing, and data analysis in the areas of resilient control, resilience metrics, integration of renewable generation, microgrids, and dynamic line ratings. He is also involved with the interaction of human factors in power systems. Since 2015, he has been with the IEEE Power and Energy Society.\nLAURENTIU D. MARINOVICI (Member, IEEE) received the B.Eng. degree in computer engineering and the M.Sc. degree in automatic control from the Gheorghe Asachi Technical University of Iaşi, Iaşi, Romania, in 2000 and 2001, respectively, and the M.Sc. and Ph.D. degrees in electrical and computer engineering from Louisiana State University, in 2007 and 2011, respectively. He is currently a Research Engineer with the Pacific Northwest National Laboratory, Optimization and Control\nGroup. His research interests lie at the confluence of software engineering and development, and model-based control, simulation and data analysis. He has been involved with designing, developing, and testing co-simulation platforms to implement and validate control algorithms for applications at different levels of the electrical power grid. Since 2011, he has been with the IEEE Power and Energy Society and IEEE Control Systems Society.\nCRAIG RIEGER (Senior Member, IEEE) received the B.S. and M.S. degrees in chemical engineering from Montana State University, in 1983 and 1985, respectively, and the Ph.D. degree in engineering and applied science from Idaho State University, in 2008. His Ph.D. coursework and dissertation focused on measurements and control, with specific application to intelligent, supervisory ventilation controls for critical infrastructure. He is currently a Professional Engineer. He is the Chief\nControl Systems Research Engineer and a Directorate Fellow with the Idaho National Laboratory (INL), pioneering interdisciplinary research in next generation resilient control systems. The grand challenge provided an integrated research strategy to address the cognitive, cyber-physical challenges of complex control systems into self-aware, trust-confirming, and threatresilient architectures. In addition, he has organized and chaired 14 cosponsored symposia and one National Science Foundation workshop in this new research area and authored more than 75 peer-reviewed publications. He has 20 years of software and hardware design experience for process control system upgrades and new installations. He has also been a supervisor and the technical lead for control systems engineering groups having design, configuration management, and security responsibilities for several INL nuclear facilities and various control system architectures.\nALICE ORRELL received the B.S. degree in mechanical engineering from the University of Vermont and the M.B.A. degree from the University of Washington. She is currently a Professional Engineer and manages the Pacific Northwest National Laboratory's distributed wind research portfolio, which included the MIRACL Project. She is also the Lead Author of the U.S. Department of Energy's annual ''Distributed Wind Market Report.''",
    "context": "Provides a detailed analysis of the St. Mary’s, Alaska wind-diesel project and related grid-inverter applications, highlighting research related to microgrids and resilience metrics.",
    "document": "Scalable_Resilience_Analysis_Through_Power_Systems_Co-Simulation.pdf",
    "pages": [
      10
    ],
    "id": "1b896171821ce33182bc990497b020c0d231e722080ba98e3d0471c448bddf3c"
  },
  {
    "text": "Tomasz Ryczkowski, Agata Froncz ak & Piotr Fronczak\nIn this paper, we analyse the gravity model in the global passenger air-transport network. We show that in the standard form, the model is inadequate for correctly describing the relationship between passenger flows and typical geo-economic variables that characterize connected countries. We propose a model for transfer flights that allows exploitation of these discrepancies in order to discover hidden subflows in the network. We illustrate its usefulness by retrieving the distance coefficient in the gravity model, which is one of the determinants of the globalization process. Finally, we discuss the correctness of the presented approach by comparing the distance coefficient to several well-known economic events.\nFor many decades, gravity models have been successfully applied in many different contexts for analysing socio-economic flows of varying types. The well-known examples include migration 1-3 , consumer spatial behaviour 4 , inter-city telephone communication flows 5 , hospital-patient flow systems 6 , and international trade 7-12 .\nAll these models predict or describe certain behaviours that mimic gravitational interaction, as described in Isaac Newton's law of gravity. They assume that a flow between two places is directly proportional to their importance (expressed in, e.g., population size, gross domestic product (GDP), or some attractiveness index) and is inversely proportional to the physical distance between them. Thus, the simplest form of the gravity equation, written, for example, for the bilateral trade volume, is given by\n<!-- formula-not-decoded -->\nwhere f ij is the trade volume between country i and country j ; xi xj is the product of their GDPs; r ij is the geographic distance between them; and G is a constant. Gravity models (GM) work particularly well in systems where all the places are directly connected (i.e., where the underlying structure is a complete graph). International trade network is a typical example of such a system. The value f ij of products or services exported from country i to country j does not affect (at least not directly) the other flows in the network.\nUnlike in the above example, most transport networks involve a series of intermediate stops, which are, themselves, generators of originating and terminating traffic (see e.g. Chapter 7 in ref. 13). In such networks, especially for large distances, no direct connection may be present from location i to location j . In these cases, the potential flow, f ij ( g ) , which might be described by Eq. (1), is realized by the increase in subsequent flows ... -f f f f , , , , ib b b b b b j n n n 1 1 2 1 .\nObviously, this scenario will lead to an observed flow, that differs from the expected one:\n<!-- formula-not-decoded -->\nIt means that, in the case of airline networks, the standard gravity model cannot be directly used to estimate weights of the existing connection flights.\nContrary to appearances, the divergence of the gravity model with actual data may prove useful for obtaining deeper insight into the details of the traffic patterns in transportation networks. In this paper, we demonstrate how one can exploit these discrepancies to discover statistical paths i -b 1 -… -bn -j underlying the observed flows, f ij , in the network.\nUsually, traffic data are collected in two ways. First, the data are obtained by counting objects (e.g., people, vehicles, or information packets) that pass any available link in the network. Such a counting provides information about local traffic intensity, but it says nothing about the places or the objects that started the travel or where they plan to finish. Second, the data are obtained by gathering information about the origin and destination of\nFaculty of Physics, Warsaw University of Technology, Koszykowa 75, PL-00-662, Warsaw, Poland. Correspondence and requests for materials should be addressed to P.F. (email: fronczak@if.pw.edu.pl )\nReceived: 30 March 2017\nAccepted: 7 June 2017\nPublished: xx xx xxxx\neach object (e.g., from survey data or from travel tickets) without knowledge about the detailed path each object follows.\nFor this study, we had at our disposal the data of the first type relating to international flights. We have checked that regardless of the choice of xi (GDP, population, size etc.) in the standard gravity model, the flows f ij are not correctly described by Eq. (1). Careful data analysis shows that the observed inconsistency is due to transfer flights, which allow passengers to travel from (or to) less developed regions even though the network is rare. The so-called'transfer passengers' contribute to reducing flight costs and enhance the frequency of flights, which is profitable especially for large airports. They also have a positive impact on the development of small airports. Thus, the understanding of how people choose between different intermediate airports has great practical potential. In this paper, we make a small contribution toward this goal.\nWe propose a simple model of connecting flights, which is confirmed by real data. The main assumption of the model is that the potential flows between two countries, f ij g ( ) , which includes all the passengers who start the journey in country i and end it in country j , regardless of the transfer flights, is given by the gravity law, Eq. (1), with xi xj referring to the product of GDPs (the case of population size is discussed in Supplementary Material). Although the mentioned assumption cannot be directly verified, it is well supported by the common observation that the gravity relationship arises from almost any microscopic economic model that includes costs that increase with distance 7 . The last condition is certainly true in most types of transportation networks.\nThe final subject of this paper is the discussion of the distance coefficient α in Eq. (1). Its behaviour over time is strictly related to the globalization process, which can be conceptualized as a continuous reduction of the effective distance in the world. Unexpectedly, most studies about gravity models in econometrics clearly show that, since the distance coefficient increases in time, the role of the distance grows simultaneously 14-17 . This counter-intuitive result is currently known as the missing globalization puzzle. Here, by recovering the gravity relationship in the flight network, we are able to analyse the time dependence of the distance coefficient in a typical transportation network.\nThe outline of the paper is as follows. First, we provide a version of the gravity model adapted to the flight network. Then, we introducthe model of connecting flights. Finally, we present the obtained results and discuss the behaviour of the distance coefficient. The data used in this study are described in the Methods section.\n\nIntroduces the central thesis about climate policy reform.",
    "original_text": "Tomasz Ryczkowski, Agata Froncz ak & Piotr Fronczak\nIn this paper, we analyse the gravity model in the global passenger air-transport network. We show that in the standard form, the model is inadequate for correctly describing the relationship between passenger flows and typical geo-economic variables that characterize connected countries. We propose a model for transfer flights that allows exploitation of these discrepancies in order to discover hidden subflows in the network. We illustrate its usefulness by retrieving the distance coefficient in the gravity model, which is one of the determinants of the globalization process. Finally, we discuss the correctness of the presented approach by comparing the distance coefficient to several well-known economic events.\nFor many decades, gravity models have been successfully applied in many different contexts for analysing socio-economic flows of varying types. The well-known examples include migration 1-3 , consumer spatial behaviour 4 , inter-city telephone communication flows 5 , hospital-patient flow systems 6 , and international trade 7-12 .\nAll these models predict or describe certain behaviours that mimic gravitational interaction, as described in Isaac Newton's law of gravity. They assume that a flow between two places is directly proportional to their importance (expressed in, e.g., population size, gross domestic product (GDP), or some attractiveness index) and is inversely proportional to the physical distance between them. Thus, the simplest form of the gravity equation, written, for example, for the bilateral trade volume, is given by\n<!-- formula-not-decoded -->\nwhere f ij is the trade volume between country i and country j ; xi xj is the product of their GDPs; r ij is the geographic distance between them; and G is a constant. Gravity models (GM) work particularly well in systems where all the places are directly connected (i.e., where the underlying structure is a complete graph). International trade network is a typical example of such a system. The value f ij of products or services exported from country i to country j does not affect (at least not directly) the other flows in the network.\nUnlike in the above example, most transport networks involve a series of intermediate stops, which are, themselves, generators of originating and terminating traffic (see e.g. Chapter 7 in ref. 13). In such networks, especially for large distances, no direct connection may be present from location i to location j . In these cases, the potential flow, f ij ( g ) , which might be described by Eq. (1), is realized by the increase in subsequent flows ... -f f f f , , , , ib b b b b b j n n n 1 1 2 1 .\nObviously, this scenario will lead to an observed flow, that differs from the expected one:\n<!-- formula-not-decoded -->\nIt means that, in the case of airline networks, the standard gravity model cannot be directly used to estimate weights of the existing connection flights.\nContrary to appearances, the divergence of the gravity model with actual data may prove useful for obtaining deeper insight into the details of the traffic patterns in transportation networks. In this paper, we demonstrate how one can exploit these discrepancies to discover statistical paths i -b 1 -… -bn -j underlying the observed flows, f ij , in the network.\nUsually, traffic data are collected in two ways. First, the data are obtained by counting objects (e.g., people, vehicles, or information packets) that pass any available link in the network. Such a counting provides information about local traffic intensity, but it says nothing about the places or the objects that started the travel or where they plan to finish. Second, the data are obtained by gathering information about the origin and destination of\nFaculty of Physics, Warsaw University of Technology, Koszykowa 75, PL-00-662, Warsaw, Poland. Correspondence and requests for materials should be addressed to P.F. (email: fronczak@if.pw.edu.pl )\nReceived: 30 March 2017\nAccepted: 7 June 2017\nPublished: xx xx xxxx\neach object (e.g., from survey data or from travel tickets) without knowledge about the detailed path each object follows.\nFor this study, we had at our disposal the data of the first type relating to international flights. We have checked that regardless of the choice of xi (GDP, population, size etc.) in the standard gravity model, the flows f ij are not correctly described by Eq. (1). Careful data analysis shows that the observed inconsistency is due to transfer flights, which allow passengers to travel from (or to) less developed regions even though the network is rare. The so-called'transfer passengers' contribute to reducing flight costs and enhance the frequency of flights, which is profitable especially for large airports. They also have a positive impact on the development of small airports. Thus, the understanding of how people choose between different intermediate airports has great practical potential. In this paper, we make a small contribution toward this goal.\nWe propose a simple model of connecting flights, which is confirmed by real data. The main assumption of the model is that the potential flows between two countries, f ij g ( ) , which includes all the passengers who start the journey in country i and end it in country j , regardless of the transfer flights, is given by the gravity law, Eq. (1), with xi xj referring to the product of GDPs (the case of population size is discussed in Supplementary Material). Although the mentioned assumption cannot be directly verified, it is well supported by the common observation that the gravity relationship arises from almost any microscopic economic model that includes costs that increase with distance 7 . The last condition is certainly true in most types of transportation networks.\nThe final subject of this paper is the discussion of the distance coefficient α in Eq. (1). Its behaviour over time is strictly related to the globalization process, which can be conceptualized as a continuous reduction of the effective distance in the world. Unexpectedly, most studies about gravity models in econometrics clearly show that, since the distance coefficient increases in time, the role of the distance grows simultaneously 14-17 . This counter-intuitive result is currently known as the missing globalization puzzle. Here, by recovering the gravity relationship in the flight network, we are able to analyse the time dependence of the distance coefficient in a typical transportation network.\nThe outline of the paper is as follows. First, we provide a version of the gravity model adapted to the flight network. Then, we introducthe model of connecting flights. Finally, we present the obtained results and discuss the behaviour of the distance coefficient. The data used in this study are described in the Methods section.",
    "context": "Introduces the central thesis about climate policy reform.",
    "document": "s41598-017-06108-z.pdf",
    "pages": [
      1,
      2
    ],
    "id": "06de918a758aef03483e0d83eb79e32712720e4741f6164a8c14aa405cd0774b"
  },
  {
    "text": "Simple gravity model. Before we can verify if the gravity model can reproduce the weights of flight connections, we need to determine the value of the constant G in Eq. (1). To do this, one has to keep in mind that, in Eq. (1), in addition to G , there is another free parameter, namely the distance coefficient α . This coefficient is usually found from the slope of the linear relation (see, e.g., Fig. 1 in ref. 17)\n<!-- formula-not-decoded -->\nWe will discuss the distance coefficient in the next subsection. At the moment, let us assume, that it its value is known.\nIn the systems, such as the international trade network, where the flow between i and j only depends on the importance of trading countries, the constant G can be simply obtained from Eq. (1),\n<!-- formula-not-decoded -->\nafter summing over all pairs of countries, i.e.\n<!-- formula-not-decoded -->\nwhere X is the total world GDP, and the left side of Eq. (5) is related to a distance-averaged value of a typical trade channel. This shows that for a fixed value of α , the parameter G can be calculated directly from real data. Unfortunately, this is not the case of the airline network.\nIn the air-transport network, besides the main contribution to the flow f ij coming from the'direct passengers' traveling from i to j , the value f ij also contains those travellers, for which the flight i -j is only an intermediate link in a longer chain of flights. In other words, the total number of occupied seats, i.e., the sum of all the elements f ij of the matrix F ( t ),\n<!-- formula-not-decoded -->\nis larger than the total number of traveling people. In particular, people traveling from i to j with one change occur in this sum twice. Correspondingly, those who travel with two changes (i.e., with three connecting flights) are taken three times. Therefore, the global traffic T can be estimated as follows:\n<!-- formula-not-decoded -->\nwhere the summation runs over all pairs of countries ( i , j ), such that the shortest path between them, in terms of the number of links, is dij , and the expected flow f ij g ( ) is given by the gravity equation (1),\nFigure 1. The observed weights of connections in the airline network, f ij , vs. their expected values, f ij g ( ) . Plots in the same row correspond to the same year: 1996 (top row) and 2004 (bottom row). Values of the distance coefficient α are indicated in the plots. All data are logarithmically binned (black squares). In panel (b), we have also shown raw data for comparison (grey squares).\n<!-- formula-not-decoded -->\nwith xi xj standing for the product of GDPs of the connected countries. This means that the constant G can be estimated from the following relation\n<!-- formula-not-decoded -->\nHaving the constant G estimated, one can plot the observed flows, f ij , versus these expected, f ij q ( ) . In Fig. 1, we present the data for two different years, 1996 and 2004, and for three different values of the distance parameter, α = 0, 1, and 3. The straight line demonstrating the expected flows f ij g ( ) , resulting from Eq. (8), is also drawn for better comparison. Let us note that the noise, which is inherent to the raw data, makes it difficult to clearly estimate the plotted relation (see Fig. 1b). To overcome this problem, in all the figures, we present logarithmically binned data only.\nIt is obvious that the direct applicability of the gravity model to the flight network is at least questionable. The best fit is obtained for α ≈ 1 (panels b) and e) in Fig. 1), which coincides with the results obtained by other studies of the distance coefficient in econometric data 17 . However, even if one agrees with such a choice of the distance coefficient, the fit is correct only for the right part of each plot. Over a span of at least three decades, the expected, f ij g ( ) , and the observed flows, f ij , differ even by several decades. It seems that there are important factors at play other than economic ones that increase the passenger flow between some countries. In the next section, we will show that the connecting flights from country i to j , which do not depend of the economic conditions, xi xj , of these two countries, can radically change the total flow f ij , and we explain the discrepancies between the gravity model and real data presented above.\nModel of connecting flights. We claim that the passenger flow, f ij , from country i to country j , that is observed in the data, is composed of two components:\nFigure 2. Graphical presentation of the summations in Eq. (10).\n- f ij g ( ) - the number of passengers traveling directly from the origin of a trip in the country i to the final destination in country j , which, we assume, is given by Eq. (8),\n- and the number of passengers, f ij transit ( ) , who use the connection i → j as a part of their longer journey.\nFor simplicity, we assume that these longer journeys consist of two direct flights only, i.e., we neglect travels with two or more intermediate stops. This assumption seems to be quite strong. For example, for 2004, we have flight data for 151 countries and 22650 possible connections between them. Only 2308 (10%) of them are direct. There are also 12749 (56%) shortest paths with length equal to 2. It means that we take into consideration only 66% of all possible connections between the countries. However, it is reasonable to expect that the number of passengers traveling with two or more stops is much lesser than the lacking 34% of the global traffic. One of the possible reasons for this is that too many transfers complicate the chance for a convenient schedule, which costs valuable time. Then, it is usually better to choose other kind of transportation to reach a destination. We will come back to this issue later when we discuss the obtained results.\nThe number of passengers f ij transit ( ) can be estimated as follows:\n<!-- formula-not-decoded -->\nwhere the first (second) summation is over such nodes k (respectively l ), that there is no direct connection from i to k (from l to j ). The term p ( i → j → k ) describes the probability that one takes a direct flight from i to j during indirect travel from i to k . Contributions of both summations to the total transit passenger flow f ij transit ( ) are graphically depicted in Fig. 2.\nThe choice of a particular connecting flight from i through j to k (which is expressed by the probability p ( i → j → k )) should depend, in the first approximation, on the distance r ij between i and j , and the distance r jk between j and k . Thus, we omit the other factors such as the convenient flight schedules and type or level of airline service or airport quality, that could influence actual passenger behaviour 18 . Therefore,\n<!-- formula-not-decoded -->\nwhere C is a normalization constant, which is given by\n<!-- formula-not-decoded -->\nand the function f ( r ij , r jk ) should reflect the tendency of the passengers to choose the shortest, and therefore, the cheapest or the fastest connections. Among many possible choices, we have chosen the following form for this function\n<!-- formula-not-decoded -->\nalthough the other possible forms, e.g.\n<!-- formula-not-decoded -->\nlead to similar quantitative results (see Supplementary Material for details).\nNow, having the model defined, one can estimate the total passenger flow between any two countries as follows:\n<!-- formula-not-decoded -->\nwhose components are correspondingly given by Eqs (8) and (10)-(13).\nFigure 3. Performance of the model of connected flights (black squares) against real data (open circles) for two years: 1996 and 2004. Straight lines correspond to the standard gravity model.\n\nDetermines the constant G in the gravity model, which is linked to the distance coefficient α.  The best fit for α is approximately 1, aligning with previous studies.",
    "original_text": "Simple gravity model. Before we can verify if the gravity model can reproduce the weights of flight connections, we need to determine the value of the constant G in Eq. (1). To do this, one has to keep in mind that, in Eq. (1), in addition to G , there is another free parameter, namely the distance coefficient α . This coefficient is usually found from the slope of the linear relation (see, e.g., Fig. 1 in ref. 17)\n<!-- formula-not-decoded -->\nWe will discuss the distance coefficient in the next subsection. At the moment, let us assume, that it its value is known.\nIn the systems, such as the international trade network, where the flow between i and j only depends on the importance of trading countries, the constant G can be simply obtained from Eq. (1),\n<!-- formula-not-decoded -->\nafter summing over all pairs of countries, i.e.\n<!-- formula-not-decoded -->\nwhere X is the total world GDP, and the left side of Eq. (5) is related to a distance-averaged value of a typical trade channel. This shows that for a fixed value of α , the parameter G can be calculated directly from real data. Unfortunately, this is not the case of the airline network.\nIn the air-transport network, besides the main contribution to the flow f ij coming from the'direct passengers' traveling from i to j , the value f ij also contains those travellers, for which the flight i -j is only an intermediate link in a longer chain of flights. In other words, the total number of occupied seats, i.e., the sum of all the elements f ij of the matrix F ( t ),\n<!-- formula-not-decoded -->\nis larger than the total number of traveling people. In particular, people traveling from i to j with one change occur in this sum twice. Correspondingly, those who travel with two changes (i.e., with three connecting flights) are taken three times. Therefore, the global traffic T can be estimated as follows:\n<!-- formula-not-decoded -->\nwhere the summation runs over all pairs of countries ( i , j ), such that the shortest path between them, in terms of the number of links, is dij , and the expected flow f ij g ( ) is given by the gravity equation (1),\nFigure 1. The observed weights of connections in the airline network, f ij , vs. their expected values, f ij g ( ) . Plots in the same row correspond to the same year: 1996 (top row) and 2004 (bottom row). Values of the distance coefficient α are indicated in the plots. All data are logarithmically binned (black squares). In panel (b), we have also shown raw data for comparison (grey squares).\n<!-- formula-not-decoded -->\nwith xi xj standing for the product of GDPs of the connected countries. This means that the constant G can be estimated from the following relation\n<!-- formula-not-decoded -->\nHaving the constant G estimated, one can plot the observed flows, f ij , versus these expected, f ij q ( ) . In Fig. 1, we present the data for two different years, 1996 and 2004, and for three different values of the distance parameter, α = 0, 1, and 3. The straight line demonstrating the expected flows f ij g ( ) , resulting from Eq. (8), is also drawn for better comparison. Let us note that the noise, which is inherent to the raw data, makes it difficult to clearly estimate the plotted relation (see Fig. 1b). To overcome this problem, in all the figures, we present logarithmically binned data only.\nIt is obvious that the direct applicability of the gravity model to the flight network is at least questionable. The best fit is obtained for α ≈ 1 (panels b) and e) in Fig. 1), which coincides with the results obtained by other studies of the distance coefficient in econometric data 17 . However, even if one agrees with such a choice of the distance coefficient, the fit is correct only for the right part of each plot. Over a span of at least three decades, the expected, f ij g ( ) , and the observed flows, f ij , differ even by several decades. It seems that there are important factors at play other than economic ones that increase the passenger flow between some countries. In the next section, we will show that the connecting flights from country i to j , which do not depend of the economic conditions, xi xj , of these two countries, can radically change the total flow f ij , and we explain the discrepancies between the gravity model and real data presented above.\nModel of connecting flights. We claim that the passenger flow, f ij , from country i to country j , that is observed in the data, is composed of two components:\nFigure 2. Graphical presentation of the summations in Eq. (10).\n- f ij g ( ) - the number of passengers traveling directly from the origin of a trip in the country i to the final destination in country j , which, we assume, is given by Eq. (8),\n- and the number of passengers, f ij transit ( ) , who use the connection i → j as a part of their longer journey.\nFor simplicity, we assume that these longer journeys consist of two direct flights only, i.e., we neglect travels with two or more intermediate stops. This assumption seems to be quite strong. For example, for 2004, we have flight data for 151 countries and 22650 possible connections between them. Only 2308 (10%) of them are direct. There are also 12749 (56%) shortest paths with length equal to 2. It means that we take into consideration only 66% of all possible connections between the countries. However, it is reasonable to expect that the number of passengers traveling with two or more stops is much lesser than the lacking 34% of the global traffic. One of the possible reasons for this is that too many transfers complicate the chance for a convenient schedule, which costs valuable time. Then, it is usually better to choose other kind of transportation to reach a destination. We will come back to this issue later when we discuss the obtained results.\nThe number of passengers f ij transit ( ) can be estimated as follows:\n<!-- formula-not-decoded -->\nwhere the first (second) summation is over such nodes k (respectively l ), that there is no direct connection from i to k (from l to j ). The term p ( i → j → k ) describes the probability that one takes a direct flight from i to j during indirect travel from i to k . Contributions of both summations to the total transit passenger flow f ij transit ( ) are graphically depicted in Fig. 2.\nThe choice of a particular connecting flight from i through j to k (which is expressed by the probability p ( i → j → k )) should depend, in the first approximation, on the distance r ij between i and j , and the distance r jk between j and k . Thus, we omit the other factors such as the convenient flight schedules and type or level of airline service or airport quality, that could influence actual passenger behaviour 18 . Therefore,\n<!-- formula-not-decoded -->\nwhere C is a normalization constant, which is given by\n<!-- formula-not-decoded -->\nand the function f ( r ij , r jk ) should reflect the tendency of the passengers to choose the shortest, and therefore, the cheapest or the fastest connections. Among many possible choices, we have chosen the following form for this function\n<!-- formula-not-decoded -->\nalthough the other possible forms, e.g.\n<!-- formula-not-decoded -->\nlead to similar quantitative results (see Supplementary Material for details).\nNow, having the model defined, one can estimate the total passenger flow between any two countries as follows:\n<!-- formula-not-decoded -->\nwhose components are correspondingly given by Eqs (8) and (10)-(13).\nFigure 3. Performance of the model of connected flights (black squares) against real data (open circles) for two years: 1996 and 2004. Straight lines correspond to the standard gravity model.",
    "context": "Determines the constant G in the gravity model, which is linked to the distance coefficient α.  The best fit for α is approximately 1, aligning with previous studies.",
    "document": "s41598-017-06108-z.pdf",
    "pages": [
      2,
      3,
      4,
      5
    ],
    "id": "034ccb16d7cd3e5313fdd2315eb5796c83c491ecd405e9c35b580f7aaabcfd38"
  },
  {
    "text": "In Fig. 3, we compare results obtained from our model of connected flights with real data for two different years, 1996 and 2004. We also plot there the straight lines corresponding to the classical GM, Eq. (8), to demonstrate a significant improvement in performance of the expanded model over GM alone. The largest discrepancies visible in the left part of the plots occur for the long-distance countries with low GDPs, i.e., for large (small) values of the denominator (nominator) in the horizontal axis in Fig. 3. We have checked that these countries are usually island-based (African, Caribbean and Pacific states), and therefore, the travel between them requires multiple transfers - the feature that is not included in our one-stop model. Moreover, a lack of transport alternatives in these countries makes air travel channels more preferred than in the typical continental states. Although it is possible to extend the model to include two-stop connections, we think it is not worth the price, i.e., the significantly increased complexity of the model, especially since its present form correctly predicts more than 98% of the total passenger flow in the world.\nThe numerical results for f ij mcf shown in Fig. 3 have been obtained for the particular values of the distance coefficient α (the reason why we have chosen α = 1.5 and α = 1.6 for years 1996 and 2004, respectively, will become clear shortly). One has to keep in mind that the other values of this quantity can lead to the different results and to better or worse agreement between the model and real data. We can use this observation to select the most probable value of α and to analyse the behaviour of the distance coefficient in time. As mentioned in the introduction, this behaviour can be strictly related with the progress of the globalization process in the context of transportation networks. Thus, analysing changes of the distance coefficient would provide another indicator of the rate of the global integration.\nFor every year in the analysed period 1990-2011, we have created the histograms of empirical and modelled flows, P ( f ij ) and P ( f ij mcf )( α ), respectively, in m = 15 logarithmically spaced bins. The examples of such normalized histograms for year 1996 are presented in Fig. 4b. As one can see, the histograms P ( f ij mcf )( α ) created for different values of the α parameter differ in agreement with the histogram of empirical flows (marked by the shaded grey area). To measure this agreement, ∆ ( α ), we use a simple RMS formula\n<!-- formula-not-decoded -->\nIn Fig. 4a, we show how this quality measure, ∆ ( α ), depends on the parameter α in the year 1996. The clearly visible minimum at α = 1.5 indicates the correct value of the distance coefficient in this year.\nFigure 5 demonstrates the behaviour of the distance coefficient for the years 1990-2011 retrieved by this method. The general conclusion that follows from the figure is that the distance effect in air transportation network is constant over time and the globalization process, which is reflected in the distance coefficient has been stabilized in the XXI century. This conclusion confirms the other results (presented by the grey circles in Fig. 5) obtained in ref. 17, where the authors estimated the distance coefficient for the international trade network.\nNow, let us shortly analyse the major fluctuations around this constant distance coefficient. In Fig. 5, we have marked three historical events that could influence the behaviour of the distance coefficient in the same way as they had an impact on the whole aviation industry. Attacks in New York and Washington D.C. in September of 2001 started a chain of events such as SARS epidemic, additional terrorist attempts, wars, and rising oil prices, that cost the airline industry three years of growth. Airline revenues and traffic surpassed 2000 levels only in 2004 19 . The 2008 global financial crisis costed another several years of growth. The effect was further enhanced by the eruption of the Eyjafjallajökull volcano in Iceland in 2010 that caused the closure of airspace over many countries. The correlation between the distance coefficient and all these events, shown in Fig. 5, confirms that they have a negative impact not only on airline revenues or air traffic but on the whole globalization process.\nFigure 4. ( a ) Example of the agreement measure ∆ ( α ) calculated for different values of parameter α in the year 1996. The arrows show the values for which three histograms P ( f ij mcf )( α ) are shown in panel ( b ). Grey shaded area represents the histogram P ( f ij ) characterizing real data.\nFigure 5. The year-by-year values of the distance coefficient α for the air transportation network resulting from the minimalization of the measure ∆ ( α ) (black squares) and for the world trade network taken from a previous paper 17 (grey circles).\nIt should be noted that the globalization process is sometimes conceptualized as a continuous reduction in the effective distance in the world 20 , which means that the distance coefficient should vanish in time. However, the observed temporary decrease in the distance coefficient is evidently negatively correlated with the progress of globalization. It confirms the recent observations that the distance coefficient is rather associated with the fractal dimension of the considered system and a decrease in this coefficient is the effect of decreasing number and weight of air transport connections, which reduce dimensionality of the system 17 .\nConcluding remarks. The presented model of connecting flights allowed us to retrieve, from the observed flow between any two countries, the terms corresponding to direct and transfer passengers utilizing this connection. Although we neglected many aspects that influence the choice of intermediate airports by travellers, the model allows to correctly predict more than 98% of the total passenger flow in the world. The only assumption we had to take into account was that the gravity model is applicable to the case of air transport network. The\ncorrectness of the above assumption was confirmed by the time behaviour of the retrieved distance coefficient that reflects several historical events with known strong economic impact.\nThere are still many possible research directions that may be worth exploring in this area. First, the most promising of these seems to be derivation of the so-called fluctuation-response relations 21 that would allow predictions of the changes in the flows f ij on the basis of changes in GDPs of the connected countries. Now, when we can determine direct and indirect contributions to the particular flow, this should be possible by the analogy to the similar approach done for international trade network 12 . Next, it would be challenging but also rewarding to extend the model taking into account, e.g., time schedules that strongly determine the passenger preference to select a particular intermediate airport. This would generally allow modelling of the microscopic time-dependent flows in the network. Analysing more detailed level of the air transportation network, in which the nodes represent rather single cities or even airports 22 than the whole countries, can also be interesting for strategic planning in the airport industry.\n\nCompares results from a model of connecting flights with real data for 1996 and 2004, highlighting discrepancies for long-distance countries with low GDPs. The model, using a distance coefficient, accurately predicts over 98% of the total passenger flow and demonstrates a stable distance coefficient over time, influenced by historical events like the 2001 attacks and the 2008 financial crisis.",
    "original_text": "In Fig. 3, we compare results obtained from our model of connected flights with real data for two different years, 1996 and 2004. We also plot there the straight lines corresponding to the classical GM, Eq. (8), to demonstrate a significant improvement in performance of the expanded model over GM alone. The largest discrepancies visible in the left part of the plots occur for the long-distance countries with low GDPs, i.e., for large (small) values of the denominator (nominator) in the horizontal axis in Fig. 3. We have checked that these countries are usually island-based (African, Caribbean and Pacific states), and therefore, the travel between them requires multiple transfers - the feature that is not included in our one-stop model. Moreover, a lack of transport alternatives in these countries makes air travel channels more preferred than in the typical continental states. Although it is possible to extend the model to include two-stop connections, we think it is not worth the price, i.e., the significantly increased complexity of the model, especially since its present form correctly predicts more than 98% of the total passenger flow in the world.\nThe numerical results for f ij mcf shown in Fig. 3 have been obtained for the particular values of the distance coefficient α (the reason why we have chosen α = 1.5 and α = 1.6 for years 1996 and 2004, respectively, will become clear shortly). One has to keep in mind that the other values of this quantity can lead to the different results and to better or worse agreement between the model and real data. We can use this observation to select the most probable value of α and to analyse the behaviour of the distance coefficient in time. As mentioned in the introduction, this behaviour can be strictly related with the progress of the globalization process in the context of transportation networks. Thus, analysing changes of the distance coefficient would provide another indicator of the rate of the global integration.\nFor every year in the analysed period 1990-2011, we have created the histograms of empirical and modelled flows, P ( f ij ) and P ( f ij mcf )( α ), respectively, in m = 15 logarithmically spaced bins. The examples of such normalized histograms for year 1996 are presented in Fig. 4b. As one can see, the histograms P ( f ij mcf )( α ) created for different values of the α parameter differ in agreement with the histogram of empirical flows (marked by the shaded grey area). To measure this agreement, ∆ ( α ), we use a simple RMS formula\n<!-- formula-not-decoded -->\nIn Fig. 4a, we show how this quality measure, ∆ ( α ), depends on the parameter α in the year 1996. The clearly visible minimum at α = 1.5 indicates the correct value of the distance coefficient in this year.\nFigure 5 demonstrates the behaviour of the distance coefficient for the years 1990-2011 retrieved by this method. The general conclusion that follows from the figure is that the distance effect in air transportation network is constant over time and the globalization process, which is reflected in the distance coefficient has been stabilized in the XXI century. This conclusion confirms the other results (presented by the grey circles in Fig. 5) obtained in ref. 17, where the authors estimated the distance coefficient for the international trade network.\nNow, let us shortly analyse the major fluctuations around this constant distance coefficient. In Fig. 5, we have marked three historical events that could influence the behaviour of the distance coefficient in the same way as they had an impact on the whole aviation industry. Attacks in New York and Washington D.C. in September of 2001 started a chain of events such as SARS epidemic, additional terrorist attempts, wars, and rising oil prices, that cost the airline industry three years of growth. Airline revenues and traffic surpassed 2000 levels only in 2004 19 . The 2008 global financial crisis costed another several years of growth. The effect was further enhanced by the eruption of the Eyjafjallajökull volcano in Iceland in 2010 that caused the closure of airspace over many countries. The correlation between the distance coefficient and all these events, shown in Fig. 5, confirms that they have a negative impact not only on airline revenues or air traffic but on the whole globalization process.\nFigure 4. ( a ) Example of the agreement measure ∆ ( α ) calculated for different values of parameter α in the year 1996. The arrows show the values for which three histograms P ( f ij mcf )( α ) are shown in panel ( b ). Grey shaded area represents the histogram P ( f ij ) characterizing real data.\nFigure 5. The year-by-year values of the distance coefficient α for the air transportation network resulting from the minimalization of the measure ∆ ( α ) (black squares) and for the world trade network taken from a previous paper 17 (grey circles).\nIt should be noted that the globalization process is sometimes conceptualized as a continuous reduction in the effective distance in the world 20 , which means that the distance coefficient should vanish in time. However, the observed temporary decrease in the distance coefficient is evidently negatively correlated with the progress of globalization. It confirms the recent observations that the distance coefficient is rather associated with the fractal dimension of the considered system and a decrease in this coefficient is the effect of decreasing number and weight of air transport connections, which reduce dimensionality of the system 17 .\nConcluding remarks. The presented model of connecting flights allowed us to retrieve, from the observed flow between any two countries, the terms corresponding to direct and transfer passengers utilizing this connection. Although we neglected many aspects that influence the choice of intermediate airports by travellers, the model allows to correctly predict more than 98% of the total passenger flow in the world. The only assumption we had to take into account was that the gravity model is applicable to the case of air transport network. The\ncorrectness of the above assumption was confirmed by the time behaviour of the retrieved distance coefficient that reflects several historical events with known strong economic impact.\nThere are still many possible research directions that may be worth exploring in this area. First, the most promising of these seems to be derivation of the so-called fluctuation-response relations 21 that would allow predictions of the changes in the flows f ij on the basis of changes in GDPs of the connected countries. Now, when we can determine direct and indirect contributions to the particular flow, this should be possible by the analogy to the similar approach done for international trade network 12 . Next, it would be challenging but also rewarding to extend the model taking into account, e.g., time schedules that strongly determine the passenger preference to select a particular intermediate airport. This would generally allow modelling of the microscopic time-dependent flows in the network. Analysing more detailed level of the air transportation network, in which the nodes represent rather single cities or even airports 22 than the whole countries, can also be interesting for strategic planning in the airport industry.",
    "context": "Compares results from a model of connecting flights with real data for 1996 and 2004, highlighting discrepancies for long-distance countries with low GDPs. The model, using a distance coefficient, accurately predicts over 98% of the total passenger flow and demonstrates a stable distance coefficient over time, influenced by historical events like the 2001 attacks and the 2008 financial crisis.",
    "document": "s41598-017-06108-z.pdf",
    "pages": [
      5,
      6,
      7
    ],
    "id": "eab3c6bf7e74fde417c54b6cadd3491fbeb105ca5c6c6ed118561b3f31eb74de"
  },
  {
    "text": "Results reported in this paper are based on data provided by International Civil Aviation Organization (ICAO). They contain 'annual traffic on-board aircraft on individual flight stages of international scheduled services' 23 . As a flight stage or a direct flight, we understand 'the operation of an aircraft from take-off to landing' 24 . It means that if a particular flight consists of two (or more) flight stages, we consider it as two (or more) separated direct flights.\nAmong the many attributes the data contain, such as aircraft type used, the number of flights operated, the aircraft capacity offered, and the traffic (passengers, freight and mail) carried, in our analyses, we use only the number of passengers traveling between countries. The data are employed to build a sequence of weighted directed networks, F ( t ), in the consecutive years t = 1990, … , 2011. In each network, each country is represented by a node and the weight of a link f ij ( t ) refers to the number of passengers traveling from i to j in year t . The flows f ij ( t ) may vary from a few persons (e.g., 6 people travelled for Togo to Uruguay in 2004) to several millions of passengers (e.g., 9532303 people travelled from Great Britain to USA in 2000).\nApart from traffic data, we also use econometric data from Penn World Table 8.1 25 . To characterize the economic performance of a country we use real GDP at constant 2005 national prices value xi ( t ) (in mil. 2005US$). The distance between countries is based on CEPII data 26 . Geodesic distances therein are calculated following the great circle formula, which uses latitudes and longitudes of the most important cities/agglomerations (in terms of population).\n\nDetails of the data sources and methodology used for the analysis, including the specific datasets (ICAO traffic data, Penn World Table, CEPII distances) and the construction of weighted directed networks.",
    "original_text": "Results reported in this paper are based on data provided by International Civil Aviation Organization (ICAO). They contain 'annual traffic on-board aircraft on individual flight stages of international scheduled services' 23 . As a flight stage or a direct flight, we understand 'the operation of an aircraft from take-off to landing' 24 . It means that if a particular flight consists of two (or more) flight stages, we consider it as two (or more) separated direct flights.\nAmong the many attributes the data contain, such as aircraft type used, the number of flights operated, the aircraft capacity offered, and the traffic (passengers, freight and mail) carried, in our analyses, we use only the number of passengers traveling between countries. The data are employed to build a sequence of weighted directed networks, F ( t ), in the consecutive years t = 1990, … , 2011. In each network, each country is represented by a node and the weight of a link f ij ( t ) refers to the number of passengers traveling from i to j in year t . The flows f ij ( t ) may vary from a few persons (e.g., 6 people travelled for Togo to Uruguay in 2004) to several millions of passengers (e.g., 9532303 people travelled from Great Britain to USA in 2000).\nApart from traffic data, we also use econometric data from Penn World Table 8.1 25 . To characterize the economic performance of a country we use real GDP at constant 2005 national prices value xi ( t ) (in mil. 2005US$). The distance between countries is based on CEPII data 26 . Geodesic distances therein are calculated following the great circle formula, which uses latitudes and longitudes of the most important cities/agglomerations (in terms of population).",
    "context": "Details of the data sources and methodology used for the analysis, including the specific datasets (ICAO traffic data, Penn World Table, CEPII distances) and the construction of weighted directed networks.",
    "document": "s41598-017-06108-z.pdf",
    "pages": [
      7
    ],
    "id": "ff85f0095d1028eb9297c9344bebab762a13a9d171b812b8af6ef85d31185b1d"
  },
  {
    "text": "1.  Lewer, J. J. & den Berg, H. V . A gravity model of immigration. Economics Letter 99 , 164-167 (2008).\n2.  Karemera, D., Oguledo, V . I. & Davis, B. A gravity model analysis of international migration to north america. Applied Economics 32 , 1745-1755 (2000).\n3.  Ravenstein, E. The laws of migration. J. Royal. Stat. Soc. 52 , 241-310 (1885).\n4.  Huff, D. L. A probabilistic analysis of shopping center trade areas. Land Economics 39 , 81-90 (1963).\n5.  Krings, G., Calabrese, F., Ratti, C. & Blondel, V . D. A gravity model for inter-city telephone communication networks. J. Stat. Mech. 7 , L7003 (2009).\n6.  Lowe, J. M. & Sen, A. Gravity model applications in health planning: analysis of an urban hospital market. J. Regional Sci 36 , 437-461 (1996).\n7.  Deardorff, A. V . vol. 69, chap. Determinants of Bilateral Trade: Does Gravity Work in a Neoclassical World ? 106 (University of Chicago Press, Chicago, 1998).\n8.  Anderson, J. E. A theoretical foundation for the gravity equation. Amer. Econ. Rev. 69 , 106-116 (1979).\n9.  Bergstrand, J. H. The gravity equation in international trade: some microscopic foundations and empirial evidence. Rev. Econ. Stat. 67 , 474-481 (1985).\n10.  Bhattacharya, K., Mukherjee, G., Saramaki, J., Kaski, K. & Manna, S. S. The international trade network: weighted network analysis and modelling. J. Stat. Mech. 02 , P0 (2002).\n11.  Duenas, M. & Fagiolo, G. Modeling the international-trade network: a gravity approach. J. Econ. Inter. Coord. 8 , 155-178 (2013).\n12.  Fronczak, A. & Fronczak, P . Statistical mechanics of the international trade network. Phys. Rev. E 85 , 056113 (2012).\n13.  T affee, E. J., Gauthier, H. L. & O'Kelly, M. E. Geography of Transportation (Upper Saddle River, NJ: Prentice Hall, 1996), second edn.\n14.  Coe, D. T., Subramanian, A. & Tamirisa, N. T. Missing globalization puzzle. IMF Staff Papers 54 , 34 (2007).\n15.  Brun, J.-F ., Carrere, C., Guillaumont, P . & de Melo, J. Has distance died? evidence from a panel gravity model. World Bark Econ. Rev. 19 , 1 (2005).\n16.  Disdier, A.-C. & Head, K. The puzzling persistence of the distance effect on bilateral trade. Rev. Econ. Stat. 90 , 37 (2008).\n17.  Karpiarz, M., Fronczak, A. & Fronczak, P . International trade network: fractal properties and globalization puzzle. Phys. Rev. Lett. 113 , 248701 (2014).\n18.  Johnson, D., Hess, S. & Matthews, B. Understanding air travellers' trade-offs between connecting flights and surface access characteristics. J. Air Transp. Manag. 34 , 70-77 (2014).\n19.  International Air Transport Association (2012). The impact of september 11 2001 on aviation. Geneva, Switzerland. http://www.iata. org/pressroom/Documents/impact-9-11-aviation.pdf (Date of access: 01/06/2017).\n20.  Cairncross, F. The Death of Distance: How the Communications Revolution Is Changing Our Lives (Harvard Business School Press, Cambridge, 1997), 1 edn.\n21.  Fronczak, A., Fronczak, P . & Holyst, J. Fluctuation-dissipation relations in complex networks. Phys. Rev. E 73 , 016108 (2016).\n22.  Grosche, T., Rothlauf, F. & Heinzl, A. Gravity models for airline passenger volume estimation. J. Air Transp. Manag. 13 , 175-183 (2007).\n23.  I nternational Civil Aviation Organization (2016). ICAO Data Plus, Montréal, Canada. https://www4.icao.int/NewDataPlus/Tools (Date of access: 01/06/2017).\n24.  US Government Electronic Code of Federal Regulations. Government Printing Office, 14 C.F.R§ 241.03 (2017).\n25.  Heston A., Summers R. & Aten B. Penn World Table Version 8.1, C enter for International Comparisons of Production , Income and Prices at the University of Pennsylvania (2011).\n26.  Mayer, T. & Zignago, S. Notes on CEPII's distances measures. MPRA Paper 26469, University Library of Munich, Germany (2006).\n\nProvides a comprehensive review of various gravity models applied to immigration, international trade, airline passenger volume, and transportation networks.",
    "original_text": "1.  Lewer, J. J. & den Berg, H. V . A gravity model of immigration. Economics Letter 99 , 164-167 (2008).\n2.  Karemera, D., Oguledo, V . I. & Davis, B. A gravity model analysis of international migration to north america. Applied Economics 32 , 1745-1755 (2000).\n3.  Ravenstein, E. The laws of migration. J. Royal. Stat. Soc. 52 , 241-310 (1885).\n4.  Huff, D. L. A probabilistic analysis of shopping center trade areas. Land Economics 39 , 81-90 (1963).\n5.  Krings, G., Calabrese, F., Ratti, C. & Blondel, V . D. A gravity model for inter-city telephone communication networks. J. Stat. Mech. 7 , L7003 (2009).\n6.  Lowe, J. M. & Sen, A. Gravity model applications in health planning: analysis of an urban hospital market. J. Regional Sci 36 , 437-461 (1996).\n7.  Deardorff, A. V . vol. 69, chap. Determinants of Bilateral Trade: Does Gravity Work in a Neoclassical World ? 106 (University of Chicago Press, Chicago, 1998).\n8.  Anderson, J. E. A theoretical foundation for the gravity equation. Amer. Econ. Rev. 69 , 106-116 (1979).\n9.  Bergstrand, J. H. The gravity equation in international trade: some microscopic foundations and empirial evidence. Rev. Econ. Stat. 67 , 474-481 (1985).\n10.  Bhattacharya, K., Mukherjee, G., Saramaki, J., Kaski, K. & Manna, S. S. The international trade network: weighted network analysis and modelling. J. Stat. Mech. 02 , P0 (2002).\n11.  Duenas, M. & Fagiolo, G. Modeling the international-trade network: a gravity approach. J. Econ. Inter. Coord. 8 , 155-178 (2013).\n12.  Fronczak, A. & Fronczak, P . Statistical mechanics of the international trade network. Phys. Rev. E 85 , 056113 (2012).\n13.  T affee, E. J., Gauthier, H. L. & O'Kelly, M. E. Geography of Transportation (Upper Saddle River, NJ: Prentice Hall, 1996), second edn.\n14.  Coe, D. T., Subramanian, A. & Tamirisa, N. T. Missing globalization puzzle. IMF Staff Papers 54 , 34 (2007).\n15.  Brun, J.-F ., Carrere, C., Guillaumont, P . & de Melo, J. Has distance died? evidence from a panel gravity model. World Bark Econ. Rev. 19 , 1 (2005).\n16.  Disdier, A.-C. & Head, K. The puzzling persistence of the distance effect on bilateral trade. Rev. Econ. Stat. 90 , 37 (2008).\n17.  Karpiarz, M., Fronczak, A. & Fronczak, P . International trade network: fractal properties and globalization puzzle. Phys. Rev. Lett. 113 , 248701 (2014).\n18.  Johnson, D., Hess, S. & Matthews, B. Understanding air travellers' trade-offs between connecting flights and surface access characteristics. J. Air Transp. Manag. 34 , 70-77 (2014).\n19.  International Air Transport Association (2012). The impact of september 11 2001 on aviation. Geneva, Switzerland. http://www.iata. org/pressroom/Documents/impact-9-11-aviation.pdf (Date of access: 01/06/2017).\n20.  Cairncross, F. The Death of Distance: How the Communications Revolution Is Changing Our Lives (Harvard Business School Press, Cambridge, 1997), 1 edn.\n21.  Fronczak, A., Fronczak, P . & Holyst, J. Fluctuation-dissipation relations in complex networks. Phys. Rev. E 73 , 016108 (2016).\n22.  Grosche, T., Rothlauf, F. & Heinzl, A. Gravity models for airline passenger volume estimation. J. Air Transp. Manag. 13 , 175-183 (2007).\n23.  I nternational Civil Aviation Organization (2016). ICAO Data Plus, Montréal, Canada. https://www4.icao.int/NewDataPlus/Tools (Date of access: 01/06/2017).\n24.  US Government Electronic Code of Federal Regulations. Government Printing Office, 14 C.F.R§ 241.03 (2017).\n25.  Heston A., Summers R. & Aten B. Penn World Table Version 8.1, C enter for International Comparisons of Production , Income and Prices at the University of Pennsylvania (2011).\n26.  Mayer, T. & Zignago, S. Notes on CEPII's distances measures. MPRA Paper 26469, University Library of Munich, Germany (2006).",
    "context": "Provides a comprehensive review of various gravity models applied to immigration, international trade, airline passenger volume, and transportation networks.",
    "document": "s41598-017-06108-z.pdf",
    "pages": [
      7
    ],
    "id": "0df78c5e6669bb2364e1e7dbb5bca087b2161154b62f6d5539d991c0e11a4be1"
  },
  {
    "text": "The work has been supported from the National Science Centre in Poland (grant no. 2012/05/E/ST2/02300).\n\nThis section details funding sources and acknowledgements, specifically noting support from the National Science Centre in Poland and referencing relevant grant information.",
    "original_text": "The work has been supported from the National Science Centre in Poland (grant no. 2012/05/E/ST2/02300).",
    "context": "This section details funding sources and acknowledgements, specifically noting support from the National Science Centre in Poland and referencing relevant grant information.",
    "document": "s41598-017-06108-z.pdf",
    "pages": [
      8
    ],
    "id": "07b81941e0df6e3491dd893be6c0b6be6276e91fe1fe64b66e0d91930eba8aac"
  },
  {
    "text": "T.R., P .F . and A.F. wrote the main text, analyzed the data, and discussed the results.\n\nDetails the authorship and contributions to the manuscript.",
    "original_text": "T.R., P .F . and A.F. wrote the main text, analyzed the data, and discussed the results.",
    "context": "Details the authorship and contributions to the manuscript.",
    "document": "s41598-017-06108-z.pdf",
    "pages": [
      8
    ],
    "id": "6f8a980f08ae5af32a69d7d1f76eee3685cb382cfdc7216b1b9f49cffef28178"
  },
  {
    "text": "Supplementary information accompanies this paper at doi:10.1038/s41598-017-06108-z\nCompeting Interests: The authors declare that they have no competing interests.\nPublisher's note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2017\n\nReplicates standard legal disclaimers and licensing information associated with the publication.",
    "original_text": "Supplementary information accompanies this paper at doi:10.1038/s41598-017-06108-z\nCompeting Interests: The authors declare that they have no competing interests.\nPublisher's note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2017",
    "context": "Replicates standard legal disclaimers and licensing information associated with the publication.",
    "document": "s41598-017-06108-z.pdf",
    "pages": [
      8
    ],
    "id": "d294dfaa3d71f881260de3a54ba2582fc01ebeb675a5e1a9c846322d6f5d9285"
  },
  {
    "text": "1234567890():,;\n\nIntroduces an indicator for detecting market instabilities by quantifying self-organizing processes arising from stock returns co-movements.",
    "original_text": "1234567890():,;",
    "context": "Introduces an indicator for detecting market instabilities by quantifying self-organizing processes arising from stock returns co-movements.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      1
    ],
    "id": "46a2bc46175cdc109a96d97b08f8146e1c0c18c8da9462c0a5f6c0851427f11c"
  },
  {
    "text": "https://doi.org/10.1038/s41467-020-15356-z\nOPEN\n\nIntroduces an indicator for detecting market instabilities by quantifying self-organizing processes arising from stock returns co-movements. Highlights the emergence of a sub-graph of stock returns as a signal of an out-of-equilibrium transition.\n",
    "original_text": "https://doi.org/10.1038/s41467-020-15356-z\nOPEN",
    "context": "Introduces an indicator for detecting market instabilities by quantifying self-organizing processes arising from stock returns co-movements. Highlights the emergence of a sub-graph of stock returns as a signal of an out-of-equilibrium transition.\n",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      1
    ],
    "id": "107af56134d4c13f9c550aa2b5bb14ba6adc75253e20e6a3b89fbf7654aad9d2"
  },
  {
    "text": "Alessandro Spelta 1,2 ✉ , Andrea Flori 3 , Nicolò Pecora 4 , Sergey Buldyrev 5 & Fabio Pammolli 2,3\nWe introduce an indicator that aims to detect the emergence of market instabilities by quantifying the intensity of self-organizing processes arising from stock returns ' comovements. In /uniFB01 nancial markets, phenomena like imitation, herding and positive feedbacks characterize the emergence of endogenous instabilities, which can modify the qualitative and quantitative behavior of the underlying system. The impossibility to formalize ex-ante the dynamic laws that rule the evolution of /uniFB01 nancial systems motivates the use of a parsimonious synthetic indicator to detect the disruption of an existing equilibrium con /uniFB01 guration. Here we show that the emergence of an interconnected sub-graph of stock returns co-movements from a broader market index is a signal of an out-of-equilibrium transition of the underlying system. To test the validity of our approach, we propose a model-free application that builds on the identi /uniFB01 cation of up and down market phases.\n1 Department of Economics and Management, University of Pavia, Via San Felice 7, 27100 Pavia, Italy. 2 CADS, Joint Center for Analysis, Decisions and Society, Human Technopole, Milan, Italy. 3 Department of Management, Economics and Industrial Engineering, Politecnico di Milano, Via Lambruschini, 4/B, 20156 Milan, Italy. 4 Department of Economics and Social Sciences, Catholic University, Via Emilia Parmense 84, 29122 Piacenza, Italy. 5 Department of Physics, Yeshiva University, 500 West 185th Street, Belfer Hall, New York City, NY, USA. ✉ email: alessandro.spelta@unipv.it\nT his paper tackles the issue of detecting long-range memory and co-movements across /uniFB01 nancial time series as informative signals of market instability and of upcoming changes in the dynamic laws governing the evolution of the system. A precise mathematical description of the underlying system through dynamic equations is, in fact, not feasible during the transition between equilibria. An in-depth inspection of the system is thus instrumental to uncover how the evolving relationships between market participants induce distinguishable variations in the set of /uniFB01 nancial variables, which may lead to instability 1 -5 .\nImitation, herding behaviors, and positive feedbacks among market participants have been recognized as phenomena leading to endogenous instabilities 6,7 . Herding behaviors spread when the knowledge about other investors ' allocation decisions in /uniFB02 uences personal strategies, meaning that investors tend to use similar investment practices to those applied by other market participants even when this is not justi /uniFB01 ed by their own information set 8 -10 , while positive feedbacks can induce the underlying system to accumulate instabilities that lead to new con /uniFB01 gurations as a selfful /uniFB01 lling mechanism 11 -14 . Hence, a strengthening of interactions among asset prices may emerge due to market euphoria, which drives prices to a sharp rise or, by contrast, to phenomena of /uniFB01 nancial turmoil, which induce /uniFB01 re sales and stock market crashes.\nSeveral techniques have been applied in the literature to study how cross-market linkages, co-movements, and interdependencies between stocks contribute to in /uniFB02 uence the sustainability conditions of /uniFB01 nancial markets and, possibly, the mechanisms behind shock transmission 15 -19 . Here, against this background, we focus on the intensity of self-organizing processes arising from stock returns ' co-movements and self-similarities.\nInspired by H.A. Simon ' s near decomposability condition 20 to represent a stable system con /uniFB01 guration 21 -23 , we hypothesize that, during instability phases, a sub-graph of stocks displays increasing co-movements and self-similarity patterns, which we propose to quantify by means of the Pearson ' s correlation coef /uniFB01 cient (PCC) and the autocovariance (AC) of stock returns (see Supplementary information, Section 3.1). We refer to this sub-graph of stocks as the leading temporal module (LTM) of the system, whose dynamics is anticipatory for the whole underlying system. In particular, when the system is approaching a change in its equilibrium con /uniFB01 guration, we observe that the absolute value of the average PCC is increasing within the set of stocks composing the LTM sub-graph, but decreasing between stocks belonging to the LTM and stocks outside the LTM group, while the average AC of stocks within the LTM is increasing.\nA rigorous investigation of the properties of the LTM, based on its temporal and spatial dimensions, allows us to build a synthetic and /uniFB02 exible indicator, which we use to detect the emergence of signi /uniFB01 cant changes in the underlying /uniFB01 nancial market. We propose a parsimonious aggregate indicator based on the mean absolute value of the AC of the stocks belonging to the LTM ( < j AC LTM t j > ) and on the ratio between the correlations of stocks within the LTM ( < j PCC LTM t j > ) and the correlations of stocks outside the leading module ( < j PCC f LTM t j > ). We relate the /uniFB01 rst component to the existence of positive feedbacks in the market 24,25 , while the second component reveals the presence of herding behaviors among investors 26,27 . The corresponding synthetic indicator is de /uniFB01 ned accordingly as:\n<!-- formula-not-decoded -->\nTo identify those stocks that have a higher potential triggering, before applying the LTM procedure, we use the detrended\n/uniFB02 uctuation analysis (DFA) 28 -30 on the time series of the original returns and on data obtained by independent time permutation. We focus on stocks that show DFA exponents signi /uniFB01 cantly different from 1/2, which is the expected value for a memoryless signal. Hence, the LTM identi /uniFB01 cation is performed within the set of stocks for which the DFA indicates the presence of long-range memory. For comparison purposes, we also verify the predictive properties of both the set of stocks that have a statistically signi /uniFB01 cant DFA, but are not in the LTM sub-graph (namely, DFA -) and the ones not selected by neither the DFA nor the LTM procedures (indicated as Rest).\nTo mimic the possible system dynamics far and near a transition point, we also employ a Lotka -Volterra model of stocks dynamics (see Supplementary information, Section 3.3). Speci /uniFB01 -cally, we simulate the system with different values of the bifurcation parameter and then we compute the statistical components of our proposed indicator. We note that while far from transition the time series exhibit small correlations and relatively low ACs, close to the bifurcation point the series exhibit both higher ACs and stronger correlation values.\nFinally, we implement an illustrative investment strategy that builds on the identi /uniFB01 cation of the emergence of up and down market phases 31,32 to show the functioning of our approach. Our analysis thus contributes to the understanding of /uniFB01 nancial markets by studying how the effects of linkages at the micro-level turn out to be relevant at the macro-level in the corresponding aggregate system. In fact, at the micro-level investors interact through heterogeneous allocation strategies, adapting their behavior in response to the performance of their investments, the arrival of new information, and the interplay of social interactions and observations, which generate, at the macro-level, non-trivial aggregate patterns of the corresponding /uniFB01 nancial system 13,33 -40 . Related to our work is, therefore, the approach of employing community detection methodologies 41 -43 to understand the properties of the dynamic processes taking place in a correlation network, from which the detection of the LTM is inspired.\nMoreover, we can establish a link between our approach and what is observed in natural sciences, since variations in asset prices can be seen as the social equivalent of nucleation phenomena near the limit of stability in a thermodynamic system, such as a superheated liquid or supercooled gas 44 . In our approach, the LTM can be viewed as analogous to the nucleus of the new phase for /uniFB01 nancial markets. We can say the indicator I LTM t plays a role similar to compressibility in thermodynamic systems, that is, the macroscopic thermodynamic quantity referring to the increasing instability near the spinodal lines.\n\nDetects the emergence of market instabilities by quantifying self-organizing processes in stock returns.",
    "original_text": "Alessandro Spelta 1,2 ✉ , Andrea Flori 3 , Nicolò Pecora 4 , Sergey Buldyrev 5 & Fabio Pammolli 2,3\nWe introduce an indicator that aims to detect the emergence of market instabilities by quantifying the intensity of self-organizing processes arising from stock returns ' comovements. In /uniFB01 nancial markets, phenomena like imitation, herding and positive feedbacks characterize the emergence of endogenous instabilities, which can modify the qualitative and quantitative behavior of the underlying system. The impossibility to formalize ex-ante the dynamic laws that rule the evolution of /uniFB01 nancial systems motivates the use of a parsimonious synthetic indicator to detect the disruption of an existing equilibrium con /uniFB01 guration. Here we show that the emergence of an interconnected sub-graph of stock returns co-movements from a broader market index is a signal of an out-of-equilibrium transition of the underlying system. To test the validity of our approach, we propose a model-free application that builds on the identi /uniFB01 cation of up and down market phases.\n1 Department of Economics and Management, University of Pavia, Via San Felice 7, 27100 Pavia, Italy. 2 CADS, Joint Center for Analysis, Decisions and Society, Human Technopole, Milan, Italy. 3 Department of Management, Economics and Industrial Engineering, Politecnico di Milano, Via Lambruschini, 4/B, 20156 Milan, Italy. 4 Department of Economics and Social Sciences, Catholic University, Via Emilia Parmense 84, 29122 Piacenza, Italy. 5 Department of Physics, Yeshiva University, 500 West 185th Street, Belfer Hall, New York City, NY, USA. ✉ email: alessandro.spelta@unipv.it\nT his paper tackles the issue of detecting long-range memory and co-movements across /uniFB01 nancial time series as informative signals of market instability and of upcoming changes in the dynamic laws governing the evolution of the system. A precise mathematical description of the underlying system through dynamic equations is, in fact, not feasible during the transition between equilibria. An in-depth inspection of the system is thus instrumental to uncover how the evolving relationships between market participants induce distinguishable variations in the set of /uniFB01 nancial variables, which may lead to instability 1 -5 .\nImitation, herding behaviors, and positive feedbacks among market participants have been recognized as phenomena leading to endogenous instabilities 6,7 . Herding behaviors spread when the knowledge about other investors ' allocation decisions in /uniFB02 uences personal strategies, meaning that investors tend to use similar investment practices to those applied by other market participants even when this is not justi /uniFB01 ed by their own information set 8 -10 , while positive feedbacks can induce the underlying system to accumulate instabilities that lead to new con /uniFB01 gurations as a selfful /uniFB01 lling mechanism 11 -14 . Hence, a strengthening of interactions among asset prices may emerge due to market euphoria, which drives prices to a sharp rise or, by contrast, to phenomena of /uniFB01 nancial turmoil, which induce /uniFB01 re sales and stock market crashes.\nSeveral techniques have been applied in the literature to study how cross-market linkages, co-movements, and interdependencies between stocks contribute to in /uniFB02 uence the sustainability conditions of /uniFB01 nancial markets and, possibly, the mechanisms behind shock transmission 15 -19 . Here, against this background, we focus on the intensity of self-organizing processes arising from stock returns ' co-movements and self-similarities.\nInspired by H.A. Simon ' s near decomposability condition 20 to represent a stable system con /uniFB01 guration 21 -23 , we hypothesize that, during instability phases, a sub-graph of stocks displays increasing co-movements and self-similarity patterns, which we propose to quantify by means of the Pearson ' s correlation coef /uniFB01 cient (PCC) and the autocovariance (AC) of stock returns (see Supplementary information, Section 3.1). We refer to this sub-graph of stocks as the leading temporal module (LTM) of the system, whose dynamics is anticipatory for the whole underlying system. In particular, when the system is approaching a change in its equilibrium con /uniFB01 guration, we observe that the absolute value of the average PCC is increasing within the set of stocks composing the LTM sub-graph, but decreasing between stocks belonging to the LTM and stocks outside the LTM group, while the average AC of stocks within the LTM is increasing.\nA rigorous investigation of the properties of the LTM, based on its temporal and spatial dimensions, allows us to build a synthetic and /uniFB02 exible indicator, which we use to detect the emergence of signi /uniFB01 cant changes in the underlying /uniFB01 nancial market. We propose a parsimonious aggregate indicator based on the mean absolute value of the AC of the stocks belonging to the LTM ( < j AC LTM t j > ) and on the ratio between the correlations of stocks within the LTM ( < j PCC LTM t j > ) and the correlations of stocks outside the leading module ( < j PCC f LTM t j > ). We relate the /uniFB01 rst component to the existence of positive feedbacks in the market 24,25 , while the second component reveals the presence of herding behaviors among investors 26,27 . The corresponding synthetic indicator is de /uniFB01 ned accordingly as:\n<!-- formula-not-decoded -->\nTo identify those stocks that have a higher potential triggering, before applying the LTM procedure, we use the detrended\n/uniFB02 uctuation analysis (DFA) 28 -30 on the time series of the original returns and on data obtained by independent time permutation. We focus on stocks that show DFA exponents signi /uniFB01 cantly different from 1/2, which is the expected value for a memoryless signal. Hence, the LTM identi /uniFB01 cation is performed within the set of stocks for which the DFA indicates the presence of long-range memory. For comparison purposes, we also verify the predictive properties of both the set of stocks that have a statistically signi /uniFB01 cant DFA, but are not in the LTM sub-graph (namely, DFA -) and the ones not selected by neither the DFA nor the LTM procedures (indicated as Rest).\nTo mimic the possible system dynamics far and near a transition point, we also employ a Lotka -Volterra model of stocks dynamics (see Supplementary information, Section 3.3). Speci /uniFB01 -cally, we simulate the system with different values of the bifurcation parameter and then we compute the statistical components of our proposed indicator. We note that while far from transition the time series exhibit small correlations and relatively low ACs, close to the bifurcation point the series exhibit both higher ACs and stronger correlation values.\nFinally, we implement an illustrative investment strategy that builds on the identi /uniFB01 cation of the emergence of up and down market phases 31,32 to show the functioning of our approach. Our analysis thus contributes to the understanding of /uniFB01 nancial markets by studying how the effects of linkages at the micro-level turn out to be relevant at the macro-level in the corresponding aggregate system. In fact, at the micro-level investors interact through heterogeneous allocation strategies, adapting their behavior in response to the performance of their investments, the arrival of new information, and the interplay of social interactions and observations, which generate, at the macro-level, non-trivial aggregate patterns of the corresponding /uniFB01 nancial system 13,33 -40 . Related to our work is, therefore, the approach of employing community detection methodologies 41 -43 to understand the properties of the dynamic processes taking place in a correlation network, from which the detection of the LTM is inspired.\nMoreover, we can establish a link between our approach and what is observed in natural sciences, since variations in asset prices can be seen as the social equivalent of nucleation phenomena near the limit of stability in a thermodynamic system, such as a superheated liquid or supercooled gas 44 . In our approach, the LTM can be viewed as analogous to the nucleus of the new phase for /uniFB01 nancial markets. We can say the indicator I LTM t plays a role similar to compressibility in thermodynamic systems, that is, the macroscopic thermodynamic quantity referring to the increasing instability near the spinodal lines.",
    "context": "Detects the emergence of market instabilities by quantifying self-organizing processes in stock returns.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      1,
      2
    ],
    "id": "086f5b9ed15ae21446b3e3fe64a197abad0e67dae102e2e64dace7d9b95c5282"
  },
  {
    "text": "The LTM indicator . We analyze the stocks referring to the STOXX Asia/Paci /uniFB01 c 600 Index, which is a broad and liquid subset of the STOXX Global 1800 Index. We investigate the dynamics of the aggregate index starting from the micro-level represented by the stocks that approximately constitute it. We employ daily closure prices along the period 2006 -2017 to compute the corresponding returns at the ground of the analysis. During the period of our analysis, the Asian stock market experienced unstable dynamics, with large booms and bursts. These up and down swings re /uniFB02 ect the 2008 global /uniFB01 nancial crisis /uniFB01 rstly, and, more recently, the real estate bubble and the /uniFB02 ood of debt by municipal governments and local enterprises designed to fund infrastructure investments 45 -48 . We also provide additional evidence on stocks constituting the STOXX North America 600 Index (see Supplementary information, Section 3.7).\nThe dynamics of the LTM sub-graph identi /uniFB01 es market phases characterized by the strengthening of price co-movements\nFig. 1 The leading temporal module (LTM) sub-graph in different market phases together with the indicator I LTM t reported against the market\ndynamics. a , b show the absolute value of the correlation matrices (PCC) derived from stocks returns, emphasizing the LTM sub-graph with a red square, together with the absolute average auto-covariance (AAC) computed on both its members and on the rest of the system. Correlation matrices and autocovariance are displayed for two different market phases, centered around 08-06-2009 in a and around 21-04-2010 in b , which stand for an unstable and a business as usual phase, respectively. c illustrates the sets of stocks within the system: stocks composing the LTM, stocks that have a statistically signi /uniFB01 cant DFA but are not in the LTM sub-graph (DFA -), and the rest of the stocks not selected by neither the LTM algorithm nor the DFA (Rest). d reports the leading indicator (right axis) computed on the LTM members (blue line), on DFA -members (red line), and on the Rest of the stocks (yellow line). These indicators have been smoothed based on a Lowess (locally weighted scatter-plot smoothing) /uniFB01 lter and compared with the dynamics of the underlying reference index displayed in gray (left axis). Vertical bars correspond to crisis events affecting the /uniFB01 nancial market such as the banking sector ratings downgrades of 2007, the failure of Lehman Brothers in September 2008, the American Recovery and Reinvestment Act of 2009, the European Debt crisis of 2011, and the Chinese stock market crisis of 2015 -2016. Error bounds are computed by performing 500 bootstrapping re-sampling of stocks' returns from the empirical distribution of the observed data and computing for each run the LTM indicator. Shaded areas represent the 5 -95% con /uniFB01 dence intervals. Two-sample Kolmogorov -Smirnov (KS) test provides evidence about the statistical difference between I LTM and the indicators computed on DFA -and Rest. The pairwise KS statistics of I LTM vs. the indicators for DFA -and Rest are 0.95 and 0.99 at 1% signi /uniFB01 cance level, respectively, thus suggesting that they are from different continuous distributions. e -g show the dynamics of the components of the leading indicator; from the left to the right: the absolute auto-covariance of stocks' returns ( e ), the within cluster absolute Pearson ' s correlation ( f ) and the between clusters absolute Pearson ' s correlation ( g ). All computations are made using a moving window of 200 days.\nresponsible for the transition of the whole underlying market away from its current con /uniFB01 guration. Upper panels of Fig. 1 show the absolute values of both the correlation and the AC for the LTM members and for the other stocks not included in the LTM. Figure 1a refers to an unstable period (centered around 08-06-2009), while Fig. 1b refers to a stable phase (centered around 21-04-2010). Figure 1c shows the schematic diagram of the sets of stocks composing the system: the LTM group (labeled as LTM), the stocks that have a statistically signi /uniFB01 cant DFA, but that are not in the LTM sub-graph (labeled as DFA -), and the rest of the stocks not selected by neither the LTM algorithm nor the DFA (labeled as Rest). Figure 1d shows the dynamics of the underlying index (in gray), the pattern of I LTM t referring to the LTM members (blue line) and the analogous indicators computed on stocks belonging to the DFA -group (red line) and on the Rest group (yellow line). Figure 1 shows how, during unstable phases, the LTM emerges in the correlation matrix, displaying also relatively high values of the ACs (Fig. 1a). On the\ncontrary, the module is indistinguishable from the remaining part of the system during ' business as usual ' phases (Fig. 1b). In a nutshell, I LTM t increases and assumes higher values around periods of market instability than during a tranquil period. For instance, during the 2008 global /uniFB01 nancial crisis, I LTM t starts to increase prior to the outbreak of the market and it reaches a local maximum approximately in correspondence of the onset of the crisis. Figure 1d also points out that I LTM t shows an increasing dynamics in correspondence of major events affecting the market, such as the banking sector ratings downgrades of 2007, the failure of Lehman Brothers in September 2008, the American Recovery and Reinvestment Act of 2009, the European Debt crisis of 2011, and the Chinese stock market crisis of 2015 -2016. Moreover, I LTM t shows a sharp increase also for transitions occurring during positive market trends, as for instance in the recovery period after the global /uniFB01 nancial crisis and at the end of the sample period. By contrast, the dynamics of both DFA -and the Rest groups seem less informative in distinguishing phases of instability in the market as reported in Fig. 1d. Error bounds are computed by performing 500 bootstraps re-sampling by randomly permuting stocks ' returns. Every bootstrap sample allows acquiring an estimate of I LTM t , which is used to compute the distribution of the indicator and to estimate the error bound as the 5 -95th percentiles of such distribution.\nOur analysis of the system at different points in time is able to identify stages of accumulation of market instability by detecting qualitative changes in the structure of the interactions among market participants.\nThe dynamics of the LTM mimics some behavioral attitudes of market participants, such as positive feedbacks and herding behaviors that reverberate in the path of stock prices. The former empirically translates into an increased AC of stock returns, while the latter empirically drives an increase of the correlation of such returns. The lower panels of Fig. 1 (panels e, f, and g) show the time dynamics of the components of I LTM t as described by Eq. (1). From the left to the right: the absolute AC of stocks ' returns (panel e), the within-group absolute Pearson ' s correlation (panel f), and the between groups absolute Pearson ' s correlation (panel g). These components jointly contribute to detect the emergence of phases of cumulative market instabilities. In particular, the AC signals the presence of positive feedbacks around the outbreak of the global /uniFB01 nancial crisis and of its rebound, while high correlation values between the LTM members indicate the presence of a bunch of stocks having strong synchronized patterns, which deviate from the behavior of the rest of the system. Notice that when these components are evaluated separately, they do not provide a clear interpretation of market conditions, while only once jointly considered they convey a meaningful signal.\n\nDetects the emergence of market instabilities by quantifying the intensity of self-organizing processes arising from stock returns co-movements.",
    "original_text": "The LTM indicator . We analyze the stocks referring to the STOXX Asia/Paci /uniFB01 c 600 Index, which is a broad and liquid subset of the STOXX Global 1800 Index. We investigate the dynamics of the aggregate index starting from the micro-level represented by the stocks that approximately constitute it. We employ daily closure prices along the period 2006 -2017 to compute the corresponding returns at the ground of the analysis. During the period of our analysis, the Asian stock market experienced unstable dynamics, with large booms and bursts. These up and down swings re /uniFB02 ect the 2008 global /uniFB01 nancial crisis /uniFB01 rstly, and, more recently, the real estate bubble and the /uniFB02 ood of debt by municipal governments and local enterprises designed to fund infrastructure investments 45 -48 . We also provide additional evidence on stocks constituting the STOXX North America 600 Index (see Supplementary information, Section 3.7).\nThe dynamics of the LTM sub-graph identi /uniFB01 es market phases characterized by the strengthening of price co-movements\nFig. 1 The leading temporal module (LTM) sub-graph in different market phases together with the indicator I LTM t reported against the market\ndynamics. a , b show the absolute value of the correlation matrices (PCC) derived from stocks returns, emphasizing the LTM sub-graph with a red square, together with the absolute average auto-covariance (AAC) computed on both its members and on the rest of the system. Correlation matrices and autocovariance are displayed for two different market phases, centered around 08-06-2009 in a and around 21-04-2010 in b , which stand for an unstable and a business as usual phase, respectively. c illustrates the sets of stocks within the system: stocks composing the LTM, stocks that have a statistically signi /uniFB01 cant DFA but are not in the LTM sub-graph (DFA -), and the rest of the stocks not selected by neither the LTM algorithm nor the DFA (Rest). d reports the leading indicator (right axis) computed on the LTM members (blue line), on DFA -members (red line), and on the Rest of the stocks (yellow line). These indicators have been smoothed based on a Lowess (locally weighted scatter-plot smoothing) /uniFB01 lter and compared with the dynamics of the underlying reference index displayed in gray (left axis). Vertical bars correspond to crisis events affecting the /uniFB01 nancial market such as the banking sector ratings downgrades of 2007, the failure of Lehman Brothers in September 2008, the American Recovery and Reinvestment Act of 2009, the European Debt crisis of 2011, and the Chinese stock market crisis of 2015 -2016. Error bounds are computed by performing 500 bootstrapping re-sampling of stocks' returns from the empirical distribution of the observed data and computing for each run the LTM indicator. Shaded areas represent the 5 -95% con /uniFB01 dence intervals. Two-sample Kolmogorov -Smirnov (KS) test provides evidence about the statistical difference between I LTM and the indicators computed on DFA -and Rest. The pairwise KS statistics of I LTM vs. the indicators for DFA -and Rest are 0.95 and 0.99 at 1% signi /uniFB01 cance level, respectively, thus suggesting that they are from different continuous distributions. e -g show the dynamics of the components of the leading indicator; from the left to the right: the absolute auto-covariance of stocks' returns ( e ), the within cluster absolute Pearson ' s correlation ( f ) and the between clusters absolute Pearson ' s correlation ( g ). All computations are made using a moving window of 200 days.\nresponsible for the transition of the whole underlying market away from its current con /uniFB01 guration. Upper panels of Fig. 1 show the absolute values of both the correlation and the AC for the LTM members and for the other stocks not included in the LTM. Figure 1a refers to an unstable period (centered around 08-06-2009), while Fig. 1b refers to a stable phase (centered around 21-04-2010). Figure 1c shows the schematic diagram of the sets of stocks composing the system: the LTM group (labeled as LTM), the stocks that have a statistically signi /uniFB01 cant DFA, but that are not in the LTM sub-graph (labeled as DFA -), and the rest of the stocks not selected by neither the LTM algorithm nor the DFA (labeled as Rest). Figure 1d shows the dynamics of the underlying index (in gray), the pattern of I LTM t referring to the LTM members (blue line) and the analogous indicators computed on stocks belonging to the DFA -group (red line) and on the Rest group (yellow line). Figure 1 shows how, during unstable phases, the LTM emerges in the correlation matrix, displaying also relatively high values of the ACs (Fig. 1a). On the\ncontrary, the module is indistinguishable from the remaining part of the system during ' business as usual ' phases (Fig. 1b). In a nutshell, I LTM t increases and assumes higher values around periods of market instability than during a tranquil period. For instance, during the 2008 global /uniFB01 nancial crisis, I LTM t starts to increase prior to the outbreak of the market and it reaches a local maximum approximately in correspondence of the onset of the crisis. Figure 1d also points out that I LTM t shows an increasing dynamics in correspondence of major events affecting the market, such as the banking sector ratings downgrades of 2007, the failure of Lehman Brothers in September 2008, the American Recovery and Reinvestment Act of 2009, the European Debt crisis of 2011, and the Chinese stock market crisis of 2015 -2016. Moreover, I LTM t shows a sharp increase also for transitions occurring during positive market trends, as for instance in the recovery period after the global /uniFB01 nancial crisis and at the end of the sample period. By contrast, the dynamics of both DFA -and the Rest groups seem less informative in distinguishing phases of instability in the market as reported in Fig. 1d. Error bounds are computed by performing 500 bootstraps re-sampling by randomly permuting stocks ' returns. Every bootstrap sample allows acquiring an estimate of I LTM t , which is used to compute the distribution of the indicator and to estimate the error bound as the 5 -95th percentiles of such distribution.\nOur analysis of the system at different points in time is able to identify stages of accumulation of market instability by detecting qualitative changes in the structure of the interactions among market participants.\nThe dynamics of the LTM mimics some behavioral attitudes of market participants, such as positive feedbacks and herding behaviors that reverberate in the path of stock prices. The former empirically translates into an increased AC of stock returns, while the latter empirically drives an increase of the correlation of such returns. The lower panels of Fig. 1 (panels e, f, and g) show the time dynamics of the components of I LTM t as described by Eq. (1). From the left to the right: the absolute AC of stocks ' returns (panel e), the within-group absolute Pearson ' s correlation (panel f), and the between groups absolute Pearson ' s correlation (panel g). These components jointly contribute to detect the emergence of phases of cumulative market instabilities. In particular, the AC signals the presence of positive feedbacks around the outbreak of the global /uniFB01 nancial crisis and of its rebound, while high correlation values between the LTM members indicate the presence of a bunch of stocks having strong synchronized patterns, which deviate from the behavior of the rest of the system. Notice that when these components are evaluated separately, they do not provide a clear interpretation of market conditions, while only once jointly considered they convey a meaningful signal.",
    "context": "Detects the emergence of market instabilities by quantifying the intensity of self-organizing processes arising from stock returns co-movements.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      2,
      3,
      4
    ],
    "id": "90e893fea07e47b45edd23db4bf43f1c8da47db3d97c7dd12228369b697f7e10"
  },
  {
    "text": "I LTM t is a dynamic indicator whose members may vary in time. Changes in the LTM composition are important to identify the drivers of the upcoming period of instability. In order to investigate the composition of the LTM sub-graph, its size and the entry -exit dynamics of the stocks in the module, we report, in Fig. 2a, the stability coef /uniFB01 cient of the LTM computed as the portion of stocks that belong to the module during two consecutive days (green line). We also report the size of the LTM (in red) and that of the group DFA -(in blue), expressed as percentages to the total number of stocks composing the reference index. In the lower part of Fig. 2a, we also report the correlation between the number of stocks selected by the DFA procedure and the average correlation of these stocks ' returns. This helps us to verify whether a rise in the number of stocks with a signi /uniFB01 cant DFA exponent is related to the growth of the average correlation of the returns associated with these stocks, and thus to a higher likelihood of being LTM members. When most of the stocks with a signi /uniFB01 cant DFA exponent belong to the LTM (see red line), we observe a stable dynamics of the leading module (see green line) or, to put it differently, a low turnover of the stocks inside the LTM. On the contrary, the LTM stability drastically decreases when there exists a considerable amount of stocks with a signi /uniFB01 cant DFA exponent that are not part of the leading module (see blue line). Indeed, we observe positive and high values of the correlation when most of the stocks selected by the DFA also belong to the LTM as, for instance, during the 2008 global /uniFB01 nancial crises and during the last semester of 2015, after the Renmimbi devaluation, while low values of the correlation are associated with periods of substantial changes of the LTM members. The negative Pearson ' s correlation ( -0.19) between the LTM stability coef /uniFB01 cient and the size of the subset of stocks composing the DFA -group (i.e., not included in the leading module) indicates that new leading modules are likely to emerge in those periods in which there are stocks with signi /uniFB01 cant DFA exponents but with poorly correlated returns. In the Section 3.4 of\nFig. 2 LTM membership stability. a Shows in green the percentage of stocks that belong to the LTM for two consecutive trading days (LTM stability), while the red line stands for the percentage of stocks belonging to the LTM. The blue line emphasizes the percentage of DFA -stocks. The black line shows the value of the correlation (Corr) between the number of stocks selected by the DFA and the average correlation of these stocks' returns, for those having a DFA exponent signi /uniFB01 cantly larger than 1/2 (AC DFA up-tail), while the gray line refers to stocks with a DFA exponent signi /uniFB01 cantly lower than 1/2 (AC DFA low-tail). Finally, the dynamics of the series are compared with the market index (reported in orange). b Shows the distributions of pairs of stocks such that both stocks have Hurst exponent outside the interval 0.2 -0.8 (in black), of pairs such that none of them is outside the interval 0.2 -0.8 (in green) and those with only one stock belonging to such interval (in red).\nFig. 3 The buy/sell signals provided by the dynamics of I LTM t and of its members ' returns together with the obtained Pro /uniFB01 ts and Losses (P&L). The /uniFB01 gure reports, in a , the forecast dynamics of the benchmark index (green-red colors stand for buy and sell signals, respectively) together with the P&L of the investment strategy based on I LTM t (blue line). The P&L of an investment strategy based on the value at risk (VaR), that is, the maximum potential loss computed on a daily time horizon with an interval of con /uniFB01 dence of 0.975 is also reported (black line) as a comparative measure. The true positives, false positives, false negatives, and true negatives obtained by investing following I LTM t are 53%, 47%, 49%, and 51%, respectively, while for the strategy based on the VaR, we obtain 49%, 51%, 52%, and 48%. Finally, the brown line refers to the P&L evolution obtained by considering an investment strategy based on the indicator proposed by ref. 58 , while the cyan line shows the P&L obtained when considering only the average correlation among stocks' returns. In b , we report the accuracy and precision measures of the proposed investment strategy conditionally on the forecast of the market index returns larger than a certain percentile of their distribution in absolute terms.\nthe Supplementary information, we also report that, on average, stocks stay continuously in the leading module for about 1.5 months. Figure 2b shows the distributions of the correlations between pairs of stocks such that both stocks have Hurst exponent outside the interval 0.2 -0.8 (in black) in comparison with the pairs such that none of them is outside that interval (in green) or such that only one of the stock in the pair belongs to that interval (in red). The shifting to the right of the distribution for the stocks with Hurst exponent outside the interval 0.2 -0.8 suggests that the DFA selects assets with high correlated returns. In other words, an increase of the number of stocks with a signi /uniFB01 cant DFA exponent implies an increase of the average correlation of the returns associated with these stocks and subsequently to a higher probability of entering the LTM.\nmarket dynamics. In other words, whenever I LTM t falls in the right tail of its empirical distribution, a switch between a long or short (or vice versa) investment position is possible depending on the average returns of the LTM members. More speci /uniFB01 cally, we refer to values of the I LTM t larger than the 95th percentile of its empirical distribution as the trigger for switching the investment exposure: if the average return of the stocks composing the LTM sub-graph is positive at time t , then we opt for a buy signal in that day; otherwise, the strategy goes short.\nThe predictive performance . We assess the predictive performance of I LTM t by testing an investment strategy that consists of two steps: /uniFB01 rst, the detection of a cumulative process leading to a phase of instability and, second, the identi /uniFB01 cation of the market direction. As for the /uniFB01 rst point, a thresholding approach for extracting signals from I LTM t appears not suitable, since it would require the prior knowledge of the out-of-sample distribution. Therefore, the most recent value of the indicator is compared against its empirical distribution computed over the previous three trading weeks (15 working days). If this value belongs to the right tail of the distribution, then the LTM is interpreted as signaling a cumulative process leading to market instability. Instead, to detect the direction of the market trend, we exploit the most recent returns of the LTM members, averaging among such values at the day corresponding to the investment decision. If the average value of the returns is positive, then the signal conveyed by the LTM indicates the arrival of a shift towards a bullish equilibrium; otherwise, it stands for a declining and bearish\n\nDetects the emergence of market instabilities by quantifying self-organizing processes arising from stock returns co-movements. Identifies a leading temporal module (LTM) within the market, which signals an out-of-equilibrium transition. The LTM is defined by increasing co-movements and self-similarity patterns, quantified through Pearson correlation and autocovariance, and serves as an indicator of market instability.",
    "original_text": "I LTM t is a dynamic indicator whose members may vary in time. Changes in the LTM composition are important to identify the drivers of the upcoming period of instability. In order to investigate the composition of the LTM sub-graph, its size and the entry -exit dynamics of the stocks in the module, we report, in Fig. 2a, the stability coef /uniFB01 cient of the LTM computed as the portion of stocks that belong to the module during two consecutive days (green line). We also report the size of the LTM (in red) and that of the group DFA -(in blue), expressed as percentages to the total number of stocks composing the reference index. In the lower part of Fig. 2a, we also report the correlation between the number of stocks selected by the DFA procedure and the average correlation of these stocks ' returns. This helps us to verify whether a rise in the number of stocks with a signi /uniFB01 cant DFA exponent is related to the growth of the average correlation of the returns associated with these stocks, and thus to a higher likelihood of being LTM members. When most of the stocks with a signi /uniFB01 cant DFA exponent belong to the LTM (see red line), we observe a stable dynamics of the leading module (see green line) or, to put it differently, a low turnover of the stocks inside the LTM. On the contrary, the LTM stability drastically decreases when there exists a considerable amount of stocks with a signi /uniFB01 cant DFA exponent that are not part of the leading module (see blue line). Indeed, we observe positive and high values of the correlation when most of the stocks selected by the DFA also belong to the LTM as, for instance, during the 2008 global /uniFB01 nancial crises and during the last semester of 2015, after the Renmimbi devaluation, while low values of the correlation are associated with periods of substantial changes of the LTM members. The negative Pearson ' s correlation ( -0.19) between the LTM stability coef /uniFB01 cient and the size of the subset of stocks composing the DFA -group (i.e., not included in the leading module) indicates that new leading modules are likely to emerge in those periods in which there are stocks with signi /uniFB01 cant DFA exponents but with poorly correlated returns. In the Section 3.4 of\nFig. 2 LTM membership stability. a Shows in green the percentage of stocks that belong to the LTM for two consecutive trading days (LTM stability), while the red line stands for the percentage of stocks belonging to the LTM. The blue line emphasizes the percentage of DFA -stocks. The black line shows the value of the correlation (Corr) between the number of stocks selected by the DFA and the average correlation of these stocks' returns, for those having a DFA exponent signi /uniFB01 cantly larger than 1/2 (AC DFA up-tail), while the gray line refers to stocks with a DFA exponent signi /uniFB01 cantly lower than 1/2 (AC DFA low-tail). Finally, the dynamics of the series are compared with the market index (reported in orange). b Shows the distributions of pairs of stocks such that both stocks have Hurst exponent outside the interval 0.2 -0.8 (in black), of pairs such that none of them is outside the interval 0.2 -0.8 (in green) and those with only one stock belonging to such interval (in red).\nFig. 3 The buy/sell signals provided by the dynamics of I LTM t and of its members ' returns together with the obtained Pro /uniFB01 ts and Losses (P&L). The /uniFB01 gure reports, in a , the forecast dynamics of the benchmark index (green-red colors stand for buy and sell signals, respectively) together with the P&L of the investment strategy based on I LTM t (blue line). The P&L of an investment strategy based on the value at risk (VaR), that is, the maximum potential loss computed on a daily time horizon with an interval of con /uniFB01 dence of 0.975 is also reported (black line) as a comparative measure. The true positives, false positives, false negatives, and true negatives obtained by investing following I LTM t are 53%, 47%, 49%, and 51%, respectively, while for the strategy based on the VaR, we obtain 49%, 51%, 52%, and 48%. Finally, the brown line refers to the P&L evolution obtained by considering an investment strategy based on the indicator proposed by ref. 58 , while the cyan line shows the P&L obtained when considering only the average correlation among stocks' returns. In b , we report the accuracy and precision measures of the proposed investment strategy conditionally on the forecast of the market index returns larger than a certain percentile of their distribution in absolute terms.\nthe Supplementary information, we also report that, on average, stocks stay continuously in the leading module for about 1.5 months. Figure 2b shows the distributions of the correlations between pairs of stocks such that both stocks have Hurst exponent outside the interval 0.2 -0.8 (in black) in comparison with the pairs such that none of them is outside that interval (in green) or such that only one of the stock in the pair belongs to that interval (in red). The shifting to the right of the distribution for the stocks with Hurst exponent outside the interval 0.2 -0.8 suggests that the DFA selects assets with high correlated returns. In other words, an increase of the number of stocks with a signi /uniFB01 cant DFA exponent implies an increase of the average correlation of the returns associated with these stocks and subsequently to a higher probability of entering the LTM.\nmarket dynamics. In other words, whenever I LTM t falls in the right tail of its empirical distribution, a switch between a long or short (or vice versa) investment position is possible depending on the average returns of the LTM members. More speci /uniFB01 cally, we refer to values of the I LTM t larger than the 95th percentile of its empirical distribution as the trigger for switching the investment exposure: if the average return of the stocks composing the LTM sub-graph is positive at time t , then we opt for a buy signal in that day; otherwise, the strategy goes short.\nThe predictive performance . We assess the predictive performance of I LTM t by testing an investment strategy that consists of two steps: /uniFB01 rst, the detection of a cumulative process leading to a phase of instability and, second, the identi /uniFB01 cation of the market direction. As for the /uniFB01 rst point, a thresholding approach for extracting signals from I LTM t appears not suitable, since it would require the prior knowledge of the out-of-sample distribution. Therefore, the most recent value of the indicator is compared against its empirical distribution computed over the previous three trading weeks (15 working days). If this value belongs to the right tail of the distribution, then the LTM is interpreted as signaling a cumulative process leading to market instability. Instead, to detect the direction of the market trend, we exploit the most recent returns of the LTM members, averaging among such values at the day corresponding to the investment decision. If the average value of the returns is positive, then the signal conveyed by the LTM indicates the arrival of a shift towards a bullish equilibrium; otherwise, it stands for a declining and bearish",
    "context": "Detects the emergence of market instabilities by quantifying self-organizing processes arising from stock returns co-movements. Identifies a leading temporal module (LTM) within the market, which signals an out-of-equilibrium transition. The LTM is defined by increasing co-movements and self-similarity patterns, quantified through Pearson correlation and autocovariance, and serves as an indicator of market instability.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      4,
      5
    ],
    "id": "cc2ee39298dbe0071528f0606346397cae6f8d901adc4aea8329d5872ac53053"
  },
  {
    "text": "Figure 3a reports the behavior of the underlying market index in which price forecasts are emphasized by green (i.e., buy) and red (i.e., sell) colors. Notice how, when there is a declining dynamics of the market index, our strategy mostly signals a short position, while in ascending price phases the green color prevails, indicating a long portfolio exposure. In particular, prior to the global /uniFB01 nancial of 2008, the indicator is able to correctly anticipate the price downturn, while at the onset of the crisis the wave of /uniFB01 nancial turbulence prevents a clear market trend detection. However, the subsequent rebound is timely identi /uniFB01 ed. Figure 3a also shows the Pro /uniFB01 t and Loss (P&L) of the strategy based on I LTM t (in blue) to disentangle the phases in which updown movements allow to obtain positive portfolio performances. In fact, the proposed strategy is able to generate a positive cumulative performance along the sample period. In Fig. 3a, we also compare the P&L of an investment strategy based on a wellknown measure of risk such as the value at risk (VaR) 49,50 , which estimates the maximum amount of expected loss over a speci /uniFB01 ed time horizon at a given con /uniFB01 dence level (see Supplementary information, Section 3.5). High values of VaR, that is, values higher than the 95th percentile of its empirical distribution, suggest a phase of instability, and, accordingly, the strategy takes a short position; otherwise, it goes long. Notice how, while I LTM recognizes changes in market trajectories in a timely way, the investment strategy based on VaR (in black), on the other hand, is\n\nTable 1 Pro /uniFB01 ts and Losses (P&L) performances.\n\nProvides a predictive indicator for detecting market instability based on co-movements and self-organizing processes.",
    "original_text": "Figure 3a reports the behavior of the underlying market index in which price forecasts are emphasized by green (i.e., buy) and red (i.e., sell) colors. Notice how, when there is a declining dynamics of the market index, our strategy mostly signals a short position, while in ascending price phases the green color prevails, indicating a long portfolio exposure. In particular, prior to the global /uniFB01 nancial of 2008, the indicator is able to correctly anticipate the price downturn, while at the onset of the crisis the wave of /uniFB01 nancial turbulence prevents a clear market trend detection. However, the subsequent rebound is timely identi /uniFB01 ed. Figure 3a also shows the Pro /uniFB01 t and Loss (P&L) of the strategy based on I LTM t (in blue) to disentangle the phases in which updown movements allow to obtain positive portfolio performances. In fact, the proposed strategy is able to generate a positive cumulative performance along the sample period. In Fig. 3a, we also compare the P&L of an investment strategy based on a wellknown measure of risk such as the value at risk (VaR) 49,50 , which estimates the maximum amount of expected loss over a speci /uniFB01 ed time horizon at a given con /uniFB01 dence level (see Supplementary information, Section 3.5). High values of VaR, that is, values higher than the 95th percentile of its empirical distribution, suggest a phase of instability, and, accordingly, the strategy takes a short position; otherwise, it goes long. Notice how, while I LTM recognizes changes in market trajectories in a timely way, the investment strategy based on VaR (in black), on the other hand, is\n\nTable 1 Pro /uniFB01 ts and Losses (P&L) performances.",
    "context": "Provides a predictive indicator for detecting market instability based on co-movements and self-organizing processes.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      5,
      6
    ],
    "id": "b2c5e4fa343cb150c160cf5b480ebbfa5ec35ec75f8caa00b487917ffeae8813"
  },
  {
    "text": "MV = 10; PRCTILE = 90, 2006 = 3.15. MV = 10; PRCTILE = 90, 2007 = - 12.80. MV = 10; PRCTILE = 90, 2008 = 7.90. MV = 10; PRCTILE = 90, 2009 = 11.96. MV = 10; PRCTILE = 90, 2010 = 11.64. MV = 10; PRCTILE = 90, 2011 = - 2.04. MV = 10; PRCTILE = 90, 2012 = 2.48. MV = 10; PRCTILE = 90, 2013 = 2.96. MV = 10; PRCTILE = 90, 2014 = 3.78. MV = 10; PRCTILE = 90, 2015 = 19.78. MV = 10; PRCTILE = 90, 2016 = 2.50. MV = 10; PRCTILE = 90, 2017 = 4.76. MV = 10; PRCTILE = 90, 2006 - 17 = 56.05. MV = 10; PRCTILE = 95, 2006 = 2.27. MV = 10; PRCTILE = 95, 2007 = 6.42. MV = 10; PRCTILE = 95, 2008 = - 8.48. MV = 10; PRCTILE = 95, 2009 = 20.26. MV = 10; PRCTILE = 95, 2010 = 3.76. MV = 10; PRCTILE = 95, 2011 = - 7.51. MV = 10; PRCTILE = 95, 2012 = 19.82. MV = 10; PRCTILE = 95, 2013 = 15.40. MV = 10; PRCTILE = 95, 2014 = 1.32. MV = 10; PRCTILE = 95, 2015 = 29.54. MV = 10; PRCTILE = 95, 2016 = 25.48. MV = 10; PRCTILE = 95, 2017 = 6.16. MV = 10; PRCTILE = 95, 2006 - 17 = 114.44. MV = 10; PRCTILE = 98, 2006 = 2.27. MV = 10; PRCTILE = 98, 2007 = 6.42. MV = 10; PRCTILE = 98, 2008 = - 8.48. MV = 10; PRCTILE = 98, 2009 = 20.26. MV = 10; PRCTILE = 98, 2010 = 3.76. MV = 10; PRCTILE = 98, 2011 = - 7.51. MV = 10; PRCTILE = 98, 2012 = 19.82. MV = 10; PRCTILE = 98, 2013 = 15.40. MV = 10; PRCTILE = 98, 2014 = 1.32. MV = 10; PRCTILE = 98, 2015 = 29.54. MV = 10; PRCTILE = 98, 2016 = 25.48. MV = 10; PRCTILE = 98, 2017 = 6.16. MV = 10; PRCTILE = 98, 2006 - 17 = 114.44. MV = 15; PRCTILE = 90, 2006 = 6.86. MV = 15; PRCTILE = 90, 2007 = - 16.37. MV = 15; PRCTILE = 90, 2008 = 24.59. MV = 15; PRCTILE = 90, 2009 = 14.00. MV = 15; PRCTILE = 90, 2010 = 25.96. MV = 15; PRCTILE = 90, 2011 = - 3.23. MV = 15; PRCTILE = 90, 2012 = 11.21. MV = 15; PRCTILE = 90, 2013 = - 17.76. MV = 15; PRCTILE = 90, 2014 = - 6.19. MV = 15; PRCTILE = 90, 2015 = 19.22. MV = 15; PRCTILE = 90, 2016 = 25.85. MV = 15; PRCTILE = 90, 2017 = - 0.63. MV = 15; PRCTILE = 90, 2006 - 17 = 83.51. MV = 15; PRCTILE = 95, 2006 = - 6.18. MV = 15; PRCTILE = 95, 2007 = 10.18. MV = 15; PRCTILE = 95, 2008 = 4.04. MV = 15; PRCTILE = 95, 2009 = 17.42. MV = 15; PRCTILE = 95, 2010 = 23.18. MV = 15; PRCTILE = 95, 2011 = 3.08. MV = 15; PRCTILE = 95, 2012 = 12.40. MV = 15; PRCTILE = 95, 2013 = - 9.82. MV = 15; PRCTILE = 95, 2014 = 0.78. MV = 15; PRCTILE = 95, 2015 = 30.75. MV = 15; PRCTILE = 95, 2016 = 31.05. MV = 15; PRCTILE = 95, 2017 = 6.16. MV = 15; PRCTILE = 95, 2006 - 17 = 123.04. MV = 15; =, 2006 = - 1.94. MV = 15; =, 2007 = 10.18. MV = 15; =, 2008 = 4.04. MV = 15; =, 2009 = 17.42. MV = 15; =, 2010 = 0.00. MV = 15; =, 2011 = - 0.92. MV = 15; =, 2012 = 18.13. MV = 15; =, 2013 = - 6.58. MV = 15; =, 2014 = 0.27. MV = 15; =, 2015 = 30.75. MV = 15; =, 2016 = 25.67. MV = 15; =, 2017 = 6.16. MV = 15; =, 2006 - 17 = 103.18. MV = 20; PRCTILE = 90, 2006 = 8.62. MV = 20; PRCTILE = 90, 2007 = - 0.27. MV = 20; PRCTILE = 90, 2008 = - 0.39. MV = 20; PRCTILE = 90, 2009 = 19.01. MV = 20; PRCTILE = 90, 2010 = 25.34. MV = 20; PRCTILE = 90, 2011 = - 3.99. MV = 20; PRCTILE = 90, 2012 = - 4.03. MV = 20; PRCTILE = 90, 2013 = - 3.76. MV = 20; PRCTILE = 90, 2014 = - 11.80. MV = 20; PRCTILE = 90, 2015 = 19.35. MV = 20; PRCTILE = 90, 2016 = 16.89. MV = 20; PRCTILE = 90, 2017 = 0.43. MV = 20; PRCTILE = 90, 2006 - 17 = 65.39. MV = 20; PRCTILE = 95, 2006 = 3.54. MV = 20; PRCTILE = 95, 2007 = 7.26. MV = 20; PRCTILE = 95, 2008 = 18.99. MV = 20; PRCTILE = 95, 2009 = 41.03. MV = 20; PRCTILE = 95, 2010 = 19.21. MV = 20; PRCTILE = 95, 2011 = - 0.92. MV = 20; PRCTILE = 95, 2012 = 10.60. MV = 20; PRCTILE = 95, 2013 = - 16.14. MV = 20; PRCTILE = 95, 2014 = - 21.74. MV = 20; PRCTILE = 95, 2015 = 18.56. MV = 20; PRCTILE = 95, 2016 = 25.10. MV = 20; PRCTILE = 95, 2017 = 4.76. MV = 20; PRCTILE = 95, 2006 - 17 = 110.25. MV = 20; PRCTILE = 98, 2006 = 2.69. MV = 20; PRCTILE = 98, 2007 = 13.57. MV = 20; PRCTILE = 98, 2008 = 2.61. MV = 20; PRCTILE = 98, 2009 = 43.15. MV = 20; PRCTILE = 98, 2010 = - 1.83. MV = 20; PRCTILE = 98, 2011 = - 4.60. MV = 20; PRCTILE = 98, 2012 = 20.25. MV = 20; PRCTILE = 98, 2013 = - 7.21. MV = 20; PRCTILE = 98, 2014 = - 22.25. MV = 20; PRCTILE = 98, 2015 = 28.73. MV = 20; PRCTILE = 98, 2016 = 25.10. MV = 20; PRCTILE = 98, 2017 = 6.16. MV = 20; PRCTILE = 98, 2006 - 17 = 106.38. MV = 25; PRCTILE = 90, 2006 = 12.79. MV = 25; PRCTILE = 90, 2007 = - 3.50. MV = 25; PRCTILE = 90, 2008 = 14.23. MV = 25; PRCTILE = 90, 2009 = 13.44. MV = 25; PRCTILE = 90, 2010 = 21.30. MV = 25; PRCTILE = 90, 2011 = 6.74. MV = 25; PRCTILE = 90, 2012 = - 20.85. MV = 25; PRCTILE = 90, 2013 = - 3.52. MV = 25; PRCTILE = 90, 2014 = - 15.07. MV = 25; PRCTILE = 90, 2015 = 19.81. MV = 25; PRCTILE = 90, 2016 = 12.38. MV = 25; PRCTILE = 90, 2017 = - 2.12. MV = 25; PRCTILE = 90, 2006 - 17 = 55.63. MV = 25; PRCTILE = 95, 2006 = 8.94. MV = 25; PRCTILE = 95, 2007 = 9.23. MV = 25;\n\nIntroduces the leading temporal module, a key indicator of market instability.",
    "original_text": "MV = 10; PRCTILE = 90, 2006 = 3.15. MV = 10; PRCTILE = 90, 2007 = - 12.80. MV = 10; PRCTILE = 90, 2008 = 7.90. MV = 10; PRCTILE = 90, 2009 = 11.96. MV = 10; PRCTILE = 90, 2010 = 11.64. MV = 10; PRCTILE = 90, 2011 = - 2.04. MV = 10; PRCTILE = 90, 2012 = 2.48. MV = 10; PRCTILE = 90, 2013 = 2.96. MV = 10; PRCTILE = 90, 2014 = 3.78. MV = 10; PRCTILE = 90, 2015 = 19.78. MV = 10; PRCTILE = 90, 2016 = 2.50. MV = 10; PRCTILE = 90, 2017 = 4.76. MV = 10; PRCTILE = 90, 2006 - 17 = 56.05. MV = 10; PRCTILE = 95, 2006 = 2.27. MV = 10; PRCTILE = 95, 2007 = 6.42. MV = 10; PRCTILE = 95, 2008 = - 8.48. MV = 10; PRCTILE = 95, 2009 = 20.26. MV = 10; PRCTILE = 95, 2010 = 3.76. MV = 10; PRCTILE = 95, 2011 = - 7.51. MV = 10; PRCTILE = 95, 2012 = 19.82. MV = 10; PRCTILE = 95, 2013 = 15.40. MV = 10; PRCTILE = 95, 2014 = 1.32. MV = 10; PRCTILE = 95, 2015 = 29.54. MV = 10; PRCTILE = 95, 2016 = 25.48. MV = 10; PRCTILE = 95, 2017 = 6.16. MV = 10; PRCTILE = 95, 2006 - 17 = 114.44. MV = 10; PRCTILE = 98, 2006 = 2.27. MV = 10; PRCTILE = 98, 2007 = 6.42. MV = 10; PRCTILE = 98, 2008 = - 8.48. MV = 10; PRCTILE = 98, 2009 = 20.26. MV = 10; PRCTILE = 98, 2010 = 3.76. MV = 10; PRCTILE = 98, 2011 = - 7.51. MV = 10; PRCTILE = 98, 2012 = 19.82. MV = 10; PRCTILE = 98, 2013 = 15.40. MV = 10; PRCTILE = 98, 2014 = 1.32. MV = 10; PRCTILE = 98, 2015 = 29.54. MV = 10; PRCTILE = 98, 2016 = 25.48. MV = 10; PRCTILE = 98, 2017 = 6.16. MV = 10; PRCTILE = 98, 2006 - 17 = 114.44. MV = 15; PRCTILE = 90, 2006 = 6.86. MV = 15; PRCTILE = 90, 2007 = - 16.37. MV = 15; PRCTILE = 90, 2008 = 24.59. MV = 15; PRCTILE = 90, 2009 = 14.00. MV = 15; PRCTILE = 90, 2010 = 25.96. MV = 15; PRCTILE = 90, 2011 = - 3.23. MV = 15; PRCTILE = 90, 2012 = 11.21. MV = 15; PRCTILE = 90, 2013 = - 17.76. MV = 15; PRCTILE = 90, 2014 = - 6.19. MV = 15; PRCTILE = 90, 2015 = 19.22. MV = 15; PRCTILE = 90, 2016 = 25.85. MV = 15; PRCTILE = 90, 2017 = - 0.63. MV = 15; PRCTILE = 90, 2006 - 17 = 83.51. MV = 15; PRCTILE = 95, 2006 = - 6.18. MV = 15; PRCTILE = 95, 2007 = 10.18. MV = 15; PRCTILE = 95, 2008 = 4.04. MV = 15; PRCTILE = 95, 2009 = 17.42. MV = 15; PRCTILE = 95, 2010 = 23.18. MV = 15; PRCTILE = 95, 2011 = 3.08. MV = 15; PRCTILE = 95, 2012 = 12.40. MV = 15; PRCTILE = 95, 2013 = - 9.82. MV = 15; PRCTILE = 95, 2014 = 0.78. MV = 15; PRCTILE = 95, 2015 = 30.75. MV = 15; PRCTILE = 95, 2016 = 31.05. MV = 15; PRCTILE = 95, 2017 = 6.16. MV = 15; PRCTILE = 95, 2006 - 17 = 123.04. MV = 15; =, 2006 = - 1.94. MV = 15; =, 2007 = 10.18. MV = 15; =, 2008 = 4.04. MV = 15; =, 2009 = 17.42. MV = 15; =, 2010 = 0.00. MV = 15; =, 2011 = - 0.92. MV = 15; =, 2012 = 18.13. MV = 15; =, 2013 = - 6.58. MV = 15; =, 2014 = 0.27. MV = 15; =, 2015 = 30.75. MV = 15; =, 2016 = 25.67. MV = 15; =, 2017 = 6.16. MV = 15; =, 2006 - 17 = 103.18. MV = 20; PRCTILE = 90, 2006 = 8.62. MV = 20; PRCTILE = 90, 2007 = - 0.27. MV = 20; PRCTILE = 90, 2008 = - 0.39. MV = 20; PRCTILE = 90, 2009 = 19.01. MV = 20; PRCTILE = 90, 2010 = 25.34. MV = 20; PRCTILE = 90, 2011 = - 3.99. MV = 20; PRCTILE = 90, 2012 = - 4.03. MV = 20; PRCTILE = 90, 2013 = - 3.76. MV = 20; PRCTILE = 90, 2014 = - 11.80. MV = 20; PRCTILE = 90, 2015 = 19.35. MV = 20; PRCTILE = 90, 2016 = 16.89. MV = 20; PRCTILE = 90, 2017 = 0.43. MV = 20; PRCTILE = 90, 2006 - 17 = 65.39. MV = 20; PRCTILE = 95, 2006 = 3.54. MV = 20; PRCTILE = 95, 2007 = 7.26. MV = 20; PRCTILE = 95, 2008 = 18.99. MV = 20; PRCTILE = 95, 2009 = 41.03. MV = 20; PRCTILE = 95, 2010 = 19.21. MV = 20; PRCTILE = 95, 2011 = - 0.92. MV = 20; PRCTILE = 95, 2012 = 10.60. MV = 20; PRCTILE = 95, 2013 = - 16.14. MV = 20; PRCTILE = 95, 2014 = - 21.74. MV = 20; PRCTILE = 95, 2015 = 18.56. MV = 20; PRCTILE = 95, 2016 = 25.10. MV = 20; PRCTILE = 95, 2017 = 4.76. MV = 20; PRCTILE = 95, 2006 - 17 = 110.25. MV = 20; PRCTILE = 98, 2006 = 2.69. MV = 20; PRCTILE = 98, 2007 = 13.57. MV = 20; PRCTILE = 98, 2008 = 2.61. MV = 20; PRCTILE = 98, 2009 = 43.15. MV = 20; PRCTILE = 98, 2010 = - 1.83. MV = 20; PRCTILE = 98, 2011 = - 4.60. MV = 20; PRCTILE = 98, 2012 = 20.25. MV = 20; PRCTILE = 98, 2013 = - 7.21. MV = 20; PRCTILE = 98, 2014 = - 22.25. MV = 20; PRCTILE = 98, 2015 = 28.73. MV = 20; PRCTILE = 98, 2016 = 25.10. MV = 20; PRCTILE = 98, 2017 = 6.16. MV = 20; PRCTILE = 98, 2006 - 17 = 106.38. MV = 25; PRCTILE = 90, 2006 = 12.79. MV = 25; PRCTILE = 90, 2007 = - 3.50. MV = 25; PRCTILE = 90, 2008 = 14.23. MV = 25; PRCTILE = 90, 2009 = 13.44. MV = 25; PRCTILE = 90, 2010 = 21.30. MV = 25; PRCTILE = 90, 2011 = 6.74. MV = 25; PRCTILE = 90, 2012 = - 20.85. MV = 25; PRCTILE = 90, 2013 = - 3.52. MV = 25; PRCTILE = 90, 2014 = - 15.07. MV = 25; PRCTILE = 90, 2015 = 19.81. MV = 25; PRCTILE = 90, 2016 = 12.38. MV = 25; PRCTILE = 90, 2017 = - 2.12. MV = 25; PRCTILE = 90, 2006 - 17 = 55.63. MV = 25; PRCTILE = 95, 2006 = 8.94. MV = 25; PRCTILE = 95, 2007 = 9.23. MV = 25;",
    "context": "Introduces the leading temporal module, a key indicator of market instability.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      6
    ],
    "id": "7cde64e58a49dd6f5a4d1b6abbd0bee2195dccc62f23d5067545f10038260212"
  },
  {
    "text": "PRCTILE = 95, 2008 = 7.07. MV = 25; PRCTILE = 95, 2009 = 30.87. MV = 25; PRCTILE = 95, 2010 = 17.87. MV = 25; PRCTILE = 95, 2011 = - 3.47. MV = 25; PRCTILE = 95, 2012 = - 9.95. MV = 25; PRCTILE = 95, 2013 = - 21.56. MV = 25; PRCTILE = 95, 2014 = - 17.47. MV = 25; PRCTILE = 95, 2015 = 33.01. MV = 25; PRCTILE = 95, 2016 = 20.57. MV = 25; PRCTILE = 95, 2017 = - 0.63. MV = 25; PRCTILE = 95, 2006 - 17 = 74.49. MV = 25; PRCTILE = 98, 2006 = 4.97. MV = 25; PRCTILE = 98, 2007 = 13.57. MV = 25; PRCTILE = 98, 2008 = - 11.09. MV = 25; PRCTILE = 98, 2009 = 38.22. MV = 25; PRCTILE = 98, 2010 = - 4.14. MV = 25; PRCTILE = 98, 2011 = - 2.87. MV = 25; PRCTILE = 98, 2012 = 6.73. MV = 25; PRCTILE = 98, 2013 = - 10.08. MV = 25; PRCTILE = 98, 2014 = - 13.73. MV = 25; PRCTILE = 98, 2015 = 20.21. MV = 25; PRCTILE = 98, 2016 = 23.28. MV = 25; PRCTILE = 98, 2017 = 6.16. MV = 25; PRCTILE = 98, 2006 - 17 = 71.24. Buy&Hold, 2006 = - 5.67. Buy&Hold, 2007 = - 5.95. Buy&Hold, 2008 = - 42.25. Buy&Hold, 2009 = 15.24. Buy&Hold, 2010 = 18.83. Buy&Hold, 2011 = - 13.83. Buy&Hold, 2012 = 9.09. Buy&Hold, 2013 = 9.58. Buy&Hold, 2014 = 8.24. Buy&Hold, 2015 = 12.42. Buy&Hold, 2016 = 4.62. Buy&Hold, 2017 = 3.11. Buy&Hold, 2006 - 17 = 13.42\nThe table shows the P&L per year obtained by following either I LTM t or the Buy and Hold strategy. Rows also indicate the sensitivity of the investment strategy to different values of the moving window (MV) and of the threshold of the empirical distribution (PRCTILE) used to identify phases of unstable market co-movements. Last column represents the P&L over the entire sample.\nmuch less reactive as it can be seen; for instance, after the market rebound of the second half of 2009 and in the /uniFB01 rst part of 2016. Finally, lower performances with respect to our proposed indicator clearly emerge if we observe the P&L obtained by applying a strategy relying on the indicator introduced by ref. 49 (in brown) and the P&L derived from a strategy based only on the average correlation of stocks ' returns (in cyan). All in all, this suggests that the behavioral features of market participants that we propose to capture through the use of the AC and correlation values are together instrumental for anticipating the dynamics of the underlying /uniFB01 nancial system.\naccuracy measures for each percentile (see Supplementary information, Section 3.8). From Fig. 3b, it clearly emerges that the capability of the proposed investment strategy in discriminating between positive and negative market movements increases as long as we select larger absolute values of market returns. This means that the trading strategy based on the LTM indicator correctly anticipates future changes in the aggregate stock price indices, especially around large market movement.\nWe also report in Table 1 the annual P&L achieved by following either our proposed investment strategy or a simply Buy&Hold strategy (last row). We investigate the robustness of our /uniFB01 ndings to changes in the length of the moving window adopted to compute the empirical distribution of I LTM t and in the threshold employed to de /uniFB01 ne the extreme values for this indicator. We observe that our strategy over-performs the simple Buy&Hold strategy over the entire sample period (last column). Beside the fact that the proposed investment strategy does not produce positive P&L for all the periods and for all parameters con /uniFB01 gurations, results in Table 1 still support the predictive performance of our approach. While during market up-trends the signal produces P&L in lines with the Buy&Hold strategy, the timely identi /uniFB01 cation of downward phases limits severe losses that, on the contrary, impact on the naive Buy&Hold strategy (see also Supplementary information, Section 3.6).\nTo quantitatively assess the performance of the proposed investment strategy, we employ a non-parametric approach. We proceed by /uniFB01 rst computing the true-positive, true-negative, falsepositive, and false-negative calls of our investment strategy conditionally on some pre-determined percentiles of the distribution of the absolute values of the market returns. We consider returns larger than an α percentile, with α varying from 10% to 90%. Then, we compute the associated precision and\nFinally, in the Section 3.6 of the Supplementary information, the P&L obtained from this strategy is compared against other investment alternatives such as strategies based on the DFA signals alone. We show that our approach outperforms the other strategies even when we do consider transaction costs. In fact, by assuming transaction costs of 10 basis-points for each portfolio rebalance, we still get a positive P&L of about 5.5% per year.\n\nSummary of the chunk’s relevance: The table presents annual P&L results from following either the proposed I LTM t indicator or a Buy & Hold strategy, demonstrating the indicator’s performance across different moving window sizes (MV) and thresholds (PRCTILE). The results consistently show the indicator outperforming the simple Buy & Hold strategy, particularly during market downturns.",
    "original_text": "PRCTILE = 95, 2008 = 7.07. MV = 25; PRCTILE = 95, 2009 = 30.87. MV = 25; PRCTILE = 95, 2010 = 17.87. MV = 25; PRCTILE = 95, 2011 = - 3.47. MV = 25; PRCTILE = 95, 2012 = - 9.95. MV = 25; PRCTILE = 95, 2013 = - 21.56. MV = 25; PRCTILE = 95, 2014 = - 17.47. MV = 25; PRCTILE = 95, 2015 = 33.01. MV = 25; PRCTILE = 95, 2016 = 20.57. MV = 25; PRCTILE = 95, 2017 = - 0.63. MV = 25; PRCTILE = 95, 2006 - 17 = 74.49. MV = 25; PRCTILE = 98, 2006 = 4.97. MV = 25; PRCTILE = 98, 2007 = 13.57. MV = 25; PRCTILE = 98, 2008 = - 11.09. MV = 25; PRCTILE = 98, 2009 = 38.22. MV = 25; PRCTILE = 98, 2010 = - 4.14. MV = 25; PRCTILE = 98, 2011 = - 2.87. MV = 25; PRCTILE = 98, 2012 = 6.73. MV = 25; PRCTILE = 98, 2013 = - 10.08. MV = 25; PRCTILE = 98, 2014 = - 13.73. MV = 25; PRCTILE = 98, 2015 = 20.21. MV = 25; PRCTILE = 98, 2016 = 23.28. MV = 25; PRCTILE = 98, 2017 = 6.16. MV = 25; PRCTILE = 98, 2006 - 17 = 71.24. Buy&Hold, 2006 = - 5.67. Buy&Hold, 2007 = - 5.95. Buy&Hold, 2008 = - 42.25. Buy&Hold, 2009 = 15.24. Buy&Hold, 2010 = 18.83. Buy&Hold, 2011 = - 13.83. Buy&Hold, 2012 = 9.09. Buy&Hold, 2013 = 9.58. Buy&Hold, 2014 = 8.24. Buy&Hold, 2015 = 12.42. Buy&Hold, 2016 = 4.62. Buy&Hold, 2017 = 3.11. Buy&Hold, 2006 - 17 = 13.42\nThe table shows the P&L per year obtained by following either I LTM t or the Buy and Hold strategy. Rows also indicate the sensitivity of the investment strategy to different values of the moving window (MV) and of the threshold of the empirical distribution (PRCTILE) used to identify phases of unstable market co-movements. Last column represents the P&L over the entire sample.\nmuch less reactive as it can be seen; for instance, after the market rebound of the second half of 2009 and in the /uniFB01 rst part of 2016. Finally, lower performances with respect to our proposed indicator clearly emerge if we observe the P&L obtained by applying a strategy relying on the indicator introduced by ref. 49 (in brown) and the P&L derived from a strategy based only on the average correlation of stocks ' returns (in cyan). All in all, this suggests that the behavioral features of market participants that we propose to capture through the use of the AC and correlation values are together instrumental for anticipating the dynamics of the underlying /uniFB01 nancial system.\naccuracy measures for each percentile (see Supplementary information, Section 3.8). From Fig. 3b, it clearly emerges that the capability of the proposed investment strategy in discriminating between positive and negative market movements increases as long as we select larger absolute values of market returns. This means that the trading strategy based on the LTM indicator correctly anticipates future changes in the aggregate stock price indices, especially around large market movement.\nWe also report in Table 1 the annual P&L achieved by following either our proposed investment strategy or a simply Buy&Hold strategy (last row). We investigate the robustness of our /uniFB01 ndings to changes in the length of the moving window adopted to compute the empirical distribution of I LTM t and in the threshold employed to de /uniFB01 ne the extreme values for this indicator. We observe that our strategy over-performs the simple Buy&Hold strategy over the entire sample period (last column). Beside the fact that the proposed investment strategy does not produce positive P&L for all the periods and for all parameters con /uniFB01 gurations, results in Table 1 still support the predictive performance of our approach. While during market up-trends the signal produces P&L in lines with the Buy&Hold strategy, the timely identi /uniFB01 cation of downward phases limits severe losses that, on the contrary, impact on the naive Buy&Hold strategy (see also Supplementary information, Section 3.6).\nTo quantitatively assess the performance of the proposed investment strategy, we employ a non-parametric approach. We proceed by /uniFB01 rst computing the true-positive, true-negative, falsepositive, and false-negative calls of our investment strategy conditionally on some pre-determined percentiles of the distribution of the absolute values of the market returns. We consider returns larger than an α percentile, with α varying from 10% to 90%. Then, we compute the associated precision and\nFinally, in the Section 3.6 of the Supplementary information, the P&L obtained from this strategy is compared against other investment alternatives such as strategies based on the DFA signals alone. We show that our approach outperforms the other strategies even when we do consider transaction costs. In fact, by assuming transaction costs of 10 basis-points for each portfolio rebalance, we still get a positive P&L of about 5.5% per year.",
    "context": "Summary of the chunk’s relevance: The table presents annual P&L results from following either the proposed I LTM t indicator or a Buy & Hold strategy, demonstrating the indicator’s performance across different moving window sizes (MV) and thresholds (PRCTILE). The results consistently show the indicator outperforming the simple Buy & Hold strategy, particularly during market downturns.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      6
    ],
    "id": "83e7fa20d1a0fbbbba62ba8daee4cd78d2c826e9e29bc2a5a21f0ade62a67337"
  },
  {
    "text": "In a highly interconnected /uniFB01 nancial system, it is of paramount relevance to detect the emergence of an abrupt transition from a stable con /uniFB01 guration to a state of instability 20,52 -55 . It is against this background that our results are derived in the context of a /uniFB01 nancial market in which we investigate how the effects of linkages at the micro-level may bring changes to the macrosystem 39 . The level of connectivity in /uniFB02 uences the probability of the system to remain stable. However, the role played by connectivity depends also on how the structure of the network interacts with additional factors which are speci /uniFB01 c for /uniFB01 nancial markets, such as investors heterogeneity, incentives to misbehave and price changes.\nIn this work, we have introduced an indicator that aims at detecting the emergence of instabilities in /uniFB01 nancial markets. The absence of a uni /uniFB01 ed quantitative framework to properly formalize the laws of motion of /uniFB01 nancial markets motivates the use of instruments derived from the network theory to detect the emergence of discontinuities and their temporal evolution.\nChanges in the market conditions are inspected through the analysis of the underlying system at different points in time. Phases of market instability are then assessed by the changes in the structure of the interactions among stock returns.\nIn particular, we have identi /uniFB01 ed phases of accumulation of instability by detecting the emergence of a sub-graph of stocks characterized by both high cohesiveness among its members and long-range memory, which we relate to herding behaviors and positive feedbacks. We summarize the dynamic of this sub-graph through a synthetic indicator which we show to be able to detect temporary transitions of the underlying system. To test this approach, we have also proposed illustrative investment strategies to identify the emergence of up and down market phases according to the signals provided by the indicator. Our results show that this methodology can timely recognize phases of increasing instability that are likely to drive the underlying system into a new market con /uniFB01 guration.\n\nDetects the emergence of instabilities in financial markets; focuses on identifying a sub-graph of stocks exhibiting high cohesiveness and long-range memory to signal potential market transitions.",
    "original_text": "In a highly interconnected /uniFB01 nancial system, it is of paramount relevance to detect the emergence of an abrupt transition from a stable con /uniFB01 guration to a state of instability 20,52 -55 . It is against this background that our results are derived in the context of a /uniFB01 nancial market in which we investigate how the effects of linkages at the micro-level may bring changes to the macrosystem 39 . The level of connectivity in /uniFB02 uences the probability of the system to remain stable. However, the role played by connectivity depends also on how the structure of the network interacts with additional factors which are speci /uniFB01 c for /uniFB01 nancial markets, such as investors heterogeneity, incentives to misbehave and price changes.\nIn this work, we have introduced an indicator that aims at detecting the emergence of instabilities in /uniFB01 nancial markets. The absence of a uni /uniFB01 ed quantitative framework to properly formalize the laws of motion of /uniFB01 nancial markets motivates the use of instruments derived from the network theory to detect the emergence of discontinuities and their temporal evolution.\nChanges in the market conditions are inspected through the analysis of the underlying system at different points in time. Phases of market instability are then assessed by the changes in the structure of the interactions among stock returns.\nIn particular, we have identi /uniFB01 ed phases of accumulation of instability by detecting the emergence of a sub-graph of stocks characterized by both high cohesiveness among its members and long-range memory, which we relate to herding behaviors and positive feedbacks. We summarize the dynamic of this sub-graph through a synthetic indicator which we show to be able to detect temporary transitions of the underlying system. To test this approach, we have also proposed illustrative investment strategies to identify the emergence of up and down market phases according to the signals provided by the indicator. Our results show that this methodology can timely recognize phases of increasing instability that are likely to drive the underlying system into a new market con /uniFB01 guration.",
    "context": "Detects the emergence of instabilities in financial markets; focuses on identifying a sub-graph of stocks exhibiting high cohesiveness and long-range memory to signal potential market transitions.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      6,
      7
    ],
    "id": "3308eef978b0d93b2af3ed8ce2bdf24ad745e39d2fab39fdc03e4d61cb429c53"
  },
  {
    "text": "Detrended /uniFB02 uctuation analysis . DFA 28 -30 is employed in the /uniFB01 rst step of the analysis to /uniFB01 lter stocks presenting long-term memory. The returns of these stocks are then clusterized according to their correlation values in order to identify the module approaching the phase transition towards a new equilibrium.\nThe DFA method comprises the following steps. In the /uniFB01 rst step, the data series y ( k ), consisting in the stocks returns, is shifted by its mean < y > and integrated (i.e., cumulatively summed) as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nIn the second step, the transformed series is segmented in windows of various length Δ l . For each segmentation, and repeatedly for all values of Δ l , the summed data are /uniFB01 t with a polynomial x Δ l ( k ). By this, the mean squared residual is found as:\nwhere L is the total number of data points. In our analysis, we have applied a linear /uniFB01 t with L set to 200 days. It is worth to remark that F ( Δ l ) can be viewed as the average of the summed squares of the residual computed in the windows.\nFinally, a log -log graph of F ( Δ l ) against Δ l is drawn. This relationship is expected to be linear if power law scaling is present. In other words, a straight line on this log -log graph indicates statistical self-af /uniFB01 nity expressed as F ( Δ l ) ∝ ( Δ l ) α . The scaling exponent α is calculated as the slope of a straight line /uniFB01 t to the log -log graph of Δ l against F ( Δ l ) using least squares. This scaling parameter is a measure of the presence of self-similarity and, therefore, of long-term memory in the signal, as it tracks down the scaling of dispersion around a regressor for increasing window sizes. In particular, the value of α can describe the following signal behaviors: if 0 < α < 0.5, then the signal has long-term memory and it is anti-correlated; if 0.5 < α < 1, the signal has long-term memory and it is correlated; if α = 0.5, the signal is uncorrelated (has no memory); /uniFB01 nally, if 1 < α < 2, the signal is nonstationary.\nThe /uniFB02 uctuation function has a relationship with the AC of stationary process 56 . Indeed, the square of the /uniFB02 uctuation function F ( Δ l ) can be written as a function of the autocorrelation as:\n<!-- formula-not-decoded -->\nbeing ACo( b ) the autocorrelation function. Thus, in terms of AC, for a linear detrending, it is straightforward to compute W ( Δ l ) and Lb ( Δ l ), as:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThe /uniFB02 uctuation function of DFA is therefore fully determined by the AC(1) and by the variance V (1) of the process. For a long-range correlated process, these components are dominant on all time windows and hence a single scaling range with the correct exponent exists.\nOur study starts by assessing the signi /uniFB01 cance of the DFA coef /uniFB01 cients considering the results computed from the original data and from surrogate data, namely, data obtained by independent time permutation for each stock returns. In other words, for a given time series we obtain its randomized (shuf /uniFB02 ed) counterpart by randomly rearranging time stamps attributed to each element in the series.\nBy comparing original results to those obtained for randomized data, we are able to wash out stocks that presents DFA coef /uniFB01 cients in line with the one observed from the shuf /uniFB02 ed case. Basically, we identify the 5 -95th percentiles of the DFA coef /uniFB01 cients distribution as reference thresholds for assessing the statistical signi /uniFB01 cance of the DFA value. Stocks with extreme values of the DFA, that is, stocks with DFA coef /uniFB01 cients belonging to the tails of the distribution, will be then clusterized to obtain an indicator of price co-movement whose dynamics will be used to identify the occurrence of market instabilities and, accordingly, to distinguish between upward and downward market phases.\nThe leading temporal module . In what follows, we describe the methodology that allows us to identify a general signal indicating an imminent bifurcation. This signal is associated with the presence of an LTM, whose statistical properties re /uniFB02 ect a transition of the underlying system to another state. In particular, it can be shown that, when a system is undergoing a bifurcation, the following general temporal and spatial properties hold: a group of stocks displays an average within PCC that drastically increases in absolute value; the average between PCC of stocks in this group and other stocks in the rest of the system will greatly decrease in absolute value; the average AC of stocks belonging to this group increases in absolute value. If all the three above-mentioned conditions are simultaneously satis /uniFB01 ed, we call the group of stocks ful /uniFB01 lling these requirements the LTM of the system 51,57,58 .\nWe now sketch the theoretical background at the basis of our indicator of market instability. Assume that the following discrete-time dynamical system describes the law of motion of a /uniFB01 nancial market, for example, in terms of stock prices or returns:\n<!-- formula-not-decoded -->\nwhere Z t ð Þ ¼ z 1 t ð Þ ; :::; z n t ð Þ ð Þ is a n -dimensional state vector representing stocks returns, P ¼ p 1 ; :::; p s /C0 /C1 is an s -dimensional parameter vector representing slowly changing factors (e.g., news on earnings or pro /uniFB01 ts, anticipated takeovers or mergers, etc.) and ε ¼ ε 1 ; :::; ε n ð Þ is a n -dimensional stochastic component with ε i Gaussian white noise with zero means and covariances κ ij ¼ Cov ð ε i ; ε j Þ . In general, we assume f : R n ´ R s ! R n is a nonlinear vector-valued function. In order to apply theoretical results on bifurcations of a general discrete-time dynamical model, we consider only the deterministic skeleton of the system, that is, we set ε ( t ) = 0. Furthermore, let us assume that the following conditions for Eq. (7) hold: /C22 Z is a /uniFB01 xed point of (7), that is /C22 Z ¼ f /C22 Z ; P ð Þ ; there exists a value P c such that one or a complex conjugate pair of the eigenvalues of the Jacobian matrix of Eq. (7) evaluated at the /uniFB01 xed point /C22 Z is equal to 1 in modulus; when P ≠ P c the eigenvalues of the Jacobian matrix of (7) are generally not 1 in modulus.\nThese conditions, along with other transversality conditions, imply that the system undergoes a transition at /C22 Z or a codimension-one bifurcation 59 . The parameter P c , at which the transition for the equilibrium value /C22 Z occurs, is called a bifurcation value (or a critical transition value) where a sudden qualitative or topological change takes place. The bifurcation is generic from a mathematical viewpoint, that is, almost all bifurcations for a general system satisfy these conditions. Around the /uniFB01 xed point /C22 Z , it is possible to linearize the system described by Eq. (7) as:\n<!-- formula-not-decoded -->\nwhere J ¼ J P ð Þ denotes the Jacobian matrix of (7). By de /uniFB01 ning X ¼ Z /C0 /C22 Z , it is possible to shift the /uniFB01 xed point to the origin, and the system characterized by Eq. (8) can be re-written as:\n<!-- formula-not-decoded -->\nwhere J is a full-rank matrix that also depends on the vector P . Since the Jacobian matrix J is of full rank, then there exists a full-rank matrix S satisfying:\n<!-- formula-not-decoded -->\nBy de /uniFB01 ning Y = S -1 X , and reintroducing the stochastic component ε , the linearized version of the original system can be re-written as:\n<!-- formula-not-decoded -->\n\nDetrended /uniFB02 uctuation analysis is employed to filter stocks presenting long-term memory. The returns of these stocks are then clusterized according to their correlation values to identify the module approaching a phase transition towards a new equilibrium.",
    "original_text": "Detrended /uniFB02 uctuation analysis . DFA 28 -30 is employed in the /uniFB01 rst step of the analysis to /uniFB01 lter stocks presenting long-term memory. The returns of these stocks are then clusterized according to their correlation values in order to identify the module approaching the phase transition towards a new equilibrium.\nThe DFA method comprises the following steps. In the /uniFB01 rst step, the data series y ( k ), consisting in the stocks returns, is shifted by its mean < y > and integrated (i.e., cumulatively summed) as follows:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nIn the second step, the transformed series is segmented in windows of various length Δ l . For each segmentation, and repeatedly for all values of Δ l , the summed data are /uniFB01 t with a polynomial x Δ l ( k ). By this, the mean squared residual is found as:\nwhere L is the total number of data points. In our analysis, we have applied a linear /uniFB01 t with L set to 200 days. It is worth to remark that F ( Δ l ) can be viewed as the average of the summed squares of the residual computed in the windows.\nFinally, a log -log graph of F ( Δ l ) against Δ l is drawn. This relationship is expected to be linear if power law scaling is present. In other words, a straight line on this log -log graph indicates statistical self-af /uniFB01 nity expressed as F ( Δ l ) ∝ ( Δ l ) α . The scaling exponent α is calculated as the slope of a straight line /uniFB01 t to the log -log graph of Δ l against F ( Δ l ) using least squares. This scaling parameter is a measure of the presence of self-similarity and, therefore, of long-term memory in the signal, as it tracks down the scaling of dispersion around a regressor for increasing window sizes. In particular, the value of α can describe the following signal behaviors: if 0 < α < 0.5, then the signal has long-term memory and it is anti-correlated; if 0.5 < α < 1, the signal has long-term memory and it is correlated; if α = 0.5, the signal is uncorrelated (has no memory); /uniFB01 nally, if 1 < α < 2, the signal is nonstationary.\nThe /uniFB02 uctuation function has a relationship with the AC of stationary process 56 . Indeed, the square of the /uniFB02 uctuation function F ( Δ l ) can be written as a function of the autocorrelation as:\n<!-- formula-not-decoded -->\nbeing ACo( b ) the autocorrelation function. Thus, in terms of AC, for a linear detrending, it is straightforward to compute W ( Δ l ) and Lb ( Δ l ), as:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThe /uniFB02 uctuation function of DFA is therefore fully determined by the AC(1) and by the variance V (1) of the process. For a long-range correlated process, these components are dominant on all time windows and hence a single scaling range with the correct exponent exists.\nOur study starts by assessing the signi /uniFB01 cance of the DFA coef /uniFB01 cients considering the results computed from the original data and from surrogate data, namely, data obtained by independent time permutation for each stock returns. In other words, for a given time series we obtain its randomized (shuf /uniFB02 ed) counterpart by randomly rearranging time stamps attributed to each element in the series.\nBy comparing original results to those obtained for randomized data, we are able to wash out stocks that presents DFA coef /uniFB01 cients in line with the one observed from the shuf /uniFB02 ed case. Basically, we identify the 5 -95th percentiles of the DFA coef /uniFB01 cients distribution as reference thresholds for assessing the statistical signi /uniFB01 cance of the DFA value. Stocks with extreme values of the DFA, that is, stocks with DFA coef /uniFB01 cients belonging to the tails of the distribution, will be then clusterized to obtain an indicator of price co-movement whose dynamics will be used to identify the occurrence of market instabilities and, accordingly, to distinguish between upward and downward market phases.\nThe leading temporal module . In what follows, we describe the methodology that allows us to identify a general signal indicating an imminent bifurcation. This signal is associated with the presence of an LTM, whose statistical properties re /uniFB02 ect a transition of the underlying system to another state. In particular, it can be shown that, when a system is undergoing a bifurcation, the following general temporal and spatial properties hold: a group of stocks displays an average within PCC that drastically increases in absolute value; the average between PCC of stocks in this group and other stocks in the rest of the system will greatly decrease in absolute value; the average AC of stocks belonging to this group increases in absolute value. If all the three above-mentioned conditions are simultaneously satis /uniFB01 ed, we call the group of stocks ful /uniFB01 lling these requirements the LTM of the system 51,57,58 .\nWe now sketch the theoretical background at the basis of our indicator of market instability. Assume that the following discrete-time dynamical system describes the law of motion of a /uniFB01 nancial market, for example, in terms of stock prices or returns:\n<!-- formula-not-decoded -->\nwhere Z t ð Þ ¼ z 1 t ð Þ ; :::; z n t ð Þ ð Þ is a n -dimensional state vector representing stocks returns, P ¼ p 1 ; :::; p s /C0 /C1 is an s -dimensional parameter vector representing slowly changing factors (e.g., news on earnings or pro /uniFB01 ts, anticipated takeovers or mergers, etc.) and ε ¼ ε 1 ; :::; ε n ð Þ is a n -dimensional stochastic component with ε i Gaussian white noise with zero means and covariances κ ij ¼ Cov ð ε i ; ε j Þ . In general, we assume f : R n ´ R s ! R n is a nonlinear vector-valued function. In order to apply theoretical results on bifurcations of a general discrete-time dynamical model, we consider only the deterministic skeleton of the system, that is, we set ε ( t ) = 0. Furthermore, let us assume that the following conditions for Eq. (7) hold: /C22 Z is a /uniFB01 xed point of (7), that is /C22 Z ¼ f /C22 Z ; P ð Þ ; there exists a value P c such that one or a complex conjugate pair of the eigenvalues of the Jacobian matrix of Eq. (7) evaluated at the /uniFB01 xed point /C22 Z is equal to 1 in modulus; when P ≠ P c the eigenvalues of the Jacobian matrix of (7) are generally not 1 in modulus.\nThese conditions, along with other transversality conditions, imply that the system undergoes a transition at /C22 Z or a codimension-one bifurcation 59 . The parameter P c , at which the transition for the equilibrium value /C22 Z occurs, is called a bifurcation value (or a critical transition value) where a sudden qualitative or topological change takes place. The bifurcation is generic from a mathematical viewpoint, that is, almost all bifurcations for a general system satisfy these conditions. Around the /uniFB01 xed point /C22 Z , it is possible to linearize the system described by Eq. (7) as:\n<!-- formula-not-decoded -->\nwhere J ¼ J P ð Þ denotes the Jacobian matrix of (7). By de /uniFB01 ning X ¼ Z /C0 /C22 Z , it is possible to shift the /uniFB01 xed point to the origin, and the system characterized by Eq. (8) can be re-written as:\n<!-- formula-not-decoded -->\nwhere J is a full-rank matrix that also depends on the vector P . Since the Jacobian matrix J is of full rank, then there exists a full-rank matrix S satisfying:\n<!-- formula-not-decoded -->\nBy de /uniFB01 ning Y = S -1 X , and reintroducing the stochastic component ε , the linearized version of the original system can be re-written as:\n<!-- formula-not-decoded -->",
    "context": "Detrended /uniFB02 uctuation analysis is employed to filter stocks presenting long-term memory. The returns of these stocks are then clusterized according to their correlation values to identify the module approaching a phase transition towards a new equilibrium.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      7
    ],
    "id": "6b8509405bb3caee764579df0fdee5e391cacdfb7a832e0e34cb5d680ee37ed6"
  },
  {
    "text": "By /uniFB01 xing the value of parameter P before reaching P c , either J or Λ is a constant matrix of full rank and we may end up with three cases: real and distinct eigenvalues, real and coincident eigenvalues, and complex eigenvalues.\nIf the sum of the dimensions of the eigenspaces with real eigenvalues is n , then there exists a non-singular matrix S satisfying Λ ¼ S /C0 1 JS ¼ diag λ 1 ; :::; λ n ð Þ being λ i the i th eigenvalue of the system (11). Without loss of generality, we may regard the /uniFB01 rst element λ 1 j j as being the nearest to 1, that is, the dominant eigenvalue, whose change leads to the state shift. If matrix J does not have linearly independent eigenvectors, there exists a non-singular matrix S making Λ block diagonal. We can always move the block with the largest eigenvalue in modulus, which is also the nearest to 1, to the /uniFB01 rst entry of Λ . Finally, in the case of complex eigenvalues there is a non-singular matrix S making Λ block diagonal where each two-dimensional block matrix has a pair of complex conjugated eigenvalues whose moduli are <1. As before we move the block in which the eigenvalues have the largest modulus to the /uniFB01 rst entry of Λ . Therefore, irrespective of which case occurs, the /uniFB01 rst element of Λ is the dominant eigenvalue, that is, the one nearest to 1 in modulus, whose change\nactually leads to the state shift from the /uniFB01 xed point. Furthermore, all the eigenvalues (or their moduli) of matrix Λ are within [0, 1) and there is at least one dominant eigenvalue approaching 1 in modulus when P → P c .\nFor simplicity, we shall show the statistical properties of the original variables Z considering only the case of real and distinct eigenvalues, but the same conclusion applies for the other two cases in a similar manner 58 .\nSince Λ is a full diagonal matrix, we have the variance V ( ⋅ ), the covariance C ( ⋅ ), the auto-covariance AC( ⋅ ), and the Pearson correlation coef /uniFB01 cient PCC( ⋅ ) of the autoregressive process expressed in Eq. (11) read as:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThe dynamics of the original variable can be written as:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThus, the variance and covariance of the original variables are given by:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThe correlation is given by:\n<!-- formula-not-decoded -->\nwhile the auto-covariance reads as:\n<!-- formula-not-decoded -->\nEquations (19) and (20) relate the empirical signals of the original system (7) with the value assumed by the dominant eigenvalue of the latent system (11). It is worth to note that an increase of the variance, covariance, and autocorrelation of the original system could be due to both a proximity of a tipping point or a strong and unexpected exogenous shock in the stochastic component of the autoregressive process in (11).\nThe temporal and spatial statistical properties that signal an imminent bifurcation can thus be summarized as follows: if a variable zi is related to y 1 , that is, s i 1 ≠ 0, then the absolute value of the auto-covariance AC ( zi ( t ), zi ( t -1)) increases greatly as λ 1 → 1; otherwise, it is bounded; if variables zi and zj are related to y 1 , that is, s i 1 ≠ 0, s j 1 ≠ 0, then j PCC ð z i ð t Þ ; z j ð t ÞÞj ! 1 as λ 1 → 1; if variables zi and zj are not related to y 1 , that is, s i 1 = 0, s j 1 = 0, then j PCC ð z i ð t Þ ; z j ð t ÞÞj ! a with a 2 0 ; 1 ð Þ as λ 1 → 1; if only variable zi is related to y 1 but zj is not, that is, s i 1 ≠ 0, s j 1 = 0, then j PCC ð z i ð t Þ ; z j ð t ÞÞj ! 0 as λ 1 → 1.\nLTM identi /uniFB01 cation . Stocks zi in the system are represented as a dynamical temporal graph Gt = ( Nt , Et ) composed by Nt nodes, while edges Et denote the pairwise correlation (PCC( zi ( t ), zj ( t ))) between each pair of stocks ' returns ( zi ( t ), zj ( t )) computed over a given moving window. This approach relies on the identi /uniFB01 cation of two main sets of stocks: (i) the LTM denoted as N LTM t and (ii) the remaining stocks Nt n N LTM t not belonging to the leading module. To detect whether the system is approaching a new equilibrium, we expect that 58,59 : (i) the absolute value of the auto-covariance of the time series of the LTM members in N LTM t increases; (ii) the absolute value of the correlation between stocks in the LTM increases as well; (iii) conversely, the absolute value of the correlation between a stock in N LTM t and another stock outside the LTM decreases to zero.\nMore practically, to identify the LTM we apply a hierarchical clustering procedure that distinguishes different groups or modules of stocks. We characterize each identi /uniFB01 ed module H by summarizing the statistical features reported above through a synthetic indicator. Let us denote the mean of the absolute value of the auto-covariance of the nodes in N H t as < j AC H t j > , the mean of the absolute value of the correlation coef /uniFB01 cients between members of the H-th module as < j PCC H t j > ,\nand let < j PCC e H t j > be the analogous between stocks in N H t and the remaining stocks. The corresponding synthetic indicator for stocks within each module is de /uniFB01 ned accordingly as:\n<!-- formula-not-decoded -->\nThen, the module with the highest value of I H t is assumed as the LTM of the underlying system and the corresponding indicator, labeled I LTM t , is employed for monitoring the reinforcement of market instabilities. This indicator is expected to sharply increase when a new phase is about to be reached by the underlying system, representing therefore an effective marker for the identi /uniFB01 cation of a cumulative process leading to a new system con /uniFB01 guration 51,58,60 . Hence, we expect the LTM to emerge more clearly when the system is experiencing a transition, meaning that its members become more cohesive and distinct from the rest of the network. In Supplementary information we present a pseudo-code that formalizes the procedure (see Supplementary Fig. 6).\nReporting summary . Further information on research design is available in the Nature Research Reporting Summary linked to this article.\n\nDetails the mathematical framework for identifying a critical transition in a financial system, specifically outlining how the dominant eigenvalue of a latent system influences the statistical properties of stocks and how this signals an impending market shift.",
    "original_text": "By /uniFB01 xing the value of parameter P before reaching P c , either J or Λ is a constant matrix of full rank and we may end up with three cases: real and distinct eigenvalues, real and coincident eigenvalues, and complex eigenvalues.\nIf the sum of the dimensions of the eigenspaces with real eigenvalues is n , then there exists a non-singular matrix S satisfying Λ ¼ S /C0 1 JS ¼ diag λ 1 ; :::; λ n ð Þ being λ i the i th eigenvalue of the system (11). Without loss of generality, we may regard the /uniFB01 rst element λ 1 j j as being the nearest to 1, that is, the dominant eigenvalue, whose change leads to the state shift. If matrix J does not have linearly independent eigenvectors, there exists a non-singular matrix S making Λ block diagonal. We can always move the block with the largest eigenvalue in modulus, which is also the nearest to 1, to the /uniFB01 rst entry of Λ . Finally, in the case of complex eigenvalues there is a non-singular matrix S making Λ block diagonal where each two-dimensional block matrix has a pair of complex conjugated eigenvalues whose moduli are <1. As before we move the block in which the eigenvalues have the largest modulus to the /uniFB01 rst entry of Λ . Therefore, irrespective of which case occurs, the /uniFB01 rst element of Λ is the dominant eigenvalue, that is, the one nearest to 1 in modulus, whose change\nactually leads to the state shift from the /uniFB01 xed point. Furthermore, all the eigenvalues (or their moduli) of matrix Λ are within [0, 1) and there is at least one dominant eigenvalue approaching 1 in modulus when P → P c .\nFor simplicity, we shall show the statistical properties of the original variables Z considering only the case of real and distinct eigenvalues, but the same conclusion applies for the other two cases in a similar manner 58 .\nSince Λ is a full diagonal matrix, we have the variance V ( ⋅ ), the covariance C ( ⋅ ), the auto-covariance AC( ⋅ ), and the Pearson correlation coef /uniFB01 cient PCC( ⋅ ) of the autoregressive process expressed in Eq. (11) read as:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThe dynamics of the original variable can be written as:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThus, the variance and covariance of the original variables are given by:\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThe correlation is given by:\n<!-- formula-not-decoded -->\nwhile the auto-covariance reads as:\n<!-- formula-not-decoded -->\nEquations (19) and (20) relate the empirical signals of the original system (7) with the value assumed by the dominant eigenvalue of the latent system (11). It is worth to note that an increase of the variance, covariance, and autocorrelation of the original system could be due to both a proximity of a tipping point or a strong and unexpected exogenous shock in the stochastic component of the autoregressive process in (11).\nThe temporal and spatial statistical properties that signal an imminent bifurcation can thus be summarized as follows: if a variable zi is related to y 1 , that is, s i 1 ≠ 0, then the absolute value of the auto-covariance AC ( zi ( t ), zi ( t -1)) increases greatly as λ 1 → 1; otherwise, it is bounded; if variables zi and zj are related to y 1 , that is, s i 1 ≠ 0, s j 1 ≠ 0, then j PCC ð z i ð t Þ ; z j ð t ÞÞj ! 1 as λ 1 → 1; if variables zi and zj are not related to y 1 , that is, s i 1 = 0, s j 1 = 0, then j PCC ð z i ð t Þ ; z j ð t ÞÞj ! a with a 2 0 ; 1 ð Þ as λ 1 → 1; if only variable zi is related to y 1 but zj is not, that is, s i 1 ≠ 0, s j 1 = 0, then j PCC ð z i ð t Þ ; z j ð t ÞÞj ! 0 as λ 1 → 1.\nLTM identi /uniFB01 cation . Stocks zi in the system are represented as a dynamical temporal graph Gt = ( Nt , Et ) composed by Nt nodes, while edges Et denote the pairwise correlation (PCC( zi ( t ), zj ( t ))) between each pair of stocks ' returns ( zi ( t ), zj ( t )) computed over a given moving window. This approach relies on the identi /uniFB01 cation of two main sets of stocks: (i) the LTM denoted as N LTM t and (ii) the remaining stocks Nt n N LTM t not belonging to the leading module. To detect whether the system is approaching a new equilibrium, we expect that 58,59 : (i) the absolute value of the auto-covariance of the time series of the LTM members in N LTM t increases; (ii) the absolute value of the correlation between stocks in the LTM increases as well; (iii) conversely, the absolute value of the correlation between a stock in N LTM t and another stock outside the LTM decreases to zero.\nMore practically, to identify the LTM we apply a hierarchical clustering procedure that distinguishes different groups or modules of stocks. We characterize each identi /uniFB01 ed module H by summarizing the statistical features reported above through a synthetic indicator. Let us denote the mean of the absolute value of the auto-covariance of the nodes in N H t as < j AC H t j > , the mean of the absolute value of the correlation coef /uniFB01 cients between members of the H-th module as < j PCC H t j > ,\nand let < j PCC e H t j > be the analogous between stocks in N H t and the remaining stocks. The corresponding synthetic indicator for stocks within each module is de /uniFB01 ned accordingly as:\n<!-- formula-not-decoded -->\nThen, the module with the highest value of I H t is assumed as the LTM of the underlying system and the corresponding indicator, labeled I LTM t , is employed for monitoring the reinforcement of market instabilities. This indicator is expected to sharply increase when a new phase is about to be reached by the underlying system, representing therefore an effective marker for the identi /uniFB01 cation of a cumulative process leading to a new system con /uniFB01 guration 51,58,60 . Hence, we expect the LTM to emerge more clearly when the system is experiencing a transition, meaning that its members become more cohesive and distinct from the rest of the network. In Supplementary information we present a pseudo-code that formalizes the procedure (see Supplementary Fig. 6).\nReporting summary . Further information on research design is available in the Nature Research Reporting Summary linked to this article.",
    "context": "Details the mathematical framework for identifying a critical transition in a financial system, specifically outlining how the dominant eigenvalue of a latent system influences the statistical properties of stocks and how this signals an impending market shift.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      8,
      7
    ],
    "id": "0d0d622ddba3e3adffc3406dd7b3b1f842ca3827c16bf4c90b61612961610a03"
  },
  {
    "text": "The data that support the /uniFB01 ndings of this study are available on request from the corresponding author A.S. The data are not publicly available due to privacy restrictions.\n\nData availability statement provided.",
    "original_text": "The data that support the /uniFB01 ndings of this study are available on request from the corresponding author A.S. The data are not publicly available due to privacy restrictions.",
    "context": "Data availability statement provided.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      8
    ],
    "id": "72a02b5e5252b7f2c4c03913d0e3c279573dff53fe875b06760b25f41bd605fe"
  },
  {
    "text": "Codes are available upon request.\nReceived: 22 July 2019; Accepted: 5 March 2020;\n\nProvides a methodological framework for identifying an imminent market bifurcation, specifically focusing on the detection of a long-term memory (LTM) and its associated indicators.",
    "original_text": "Codes are available upon request.\nReceived: 22 July 2019; Accepted: 5 March 2020;",
    "context": "Provides a methodological framework for identifying an imminent market bifurcation, specifically focusing on the detection of a long-term memory (LTM) and its associated indicators.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      8
    ],
    "id": "6877e6e87cb0917de060b1d2dd3fe9b5ec9f19a365608d42fbc02270f59fda36"
  },
  {
    "text": "1. Mantegna, R. N. & Stanley, H. E. Introduction to Econophysics: Correlations and Complexity in Finance (Cambridge Univ. Press, 1999).\n2. Orsenigo, L., Pammolli, F. & Riccaboni, M. Technological change and network dynamics. Res. Policy 30 , 485 -508 (2001).\n3. Durlauf, S. N. Complexity and empirical economics. Econ. J. 115 , F225 -F243 (2005).\n4. Scheffer, M. et al. Early-warning signals for critical transitions. Nature 461 , 53 (2009).\n5. Diks, C., Hommes, C. & Wang, J. Critical slowing down as an early warning signal for /uniFB01 nancial crises? Emp. Econ. 57 , 1 -28 (2015).\n6. Zhao, L. et al. Herd behavior in a complex adaptive system. Proc. Natl Acad. Sci. USA 108 , 15058 -15063 (2011).\n7. Hüsler, A., Sornette, D. & Hommes, C. Super-exponential bubbles in lab experiments: evidence for anchoring over-optimistic expectations on price. J. Econ. Behav. Org. 92 , 304 -316 (2013).\n8. Trueman, B. Analyst forecasts and herding behavior. Rev. Financial Stud. 7 , 97 -124 (1994).\n9. Sharma, M. S. & Bikhchandani, S. Herd Behavior in Financial Markets: A Review 0 -48 (International Monetary Fund, 2000).\n10. Banerjee, A. V. A simple model of herd behavior. Q. J. Econ. 107 , 797 -817 (1992).\n11. Sornette, D. Predictability of catastrophic events: material rupture, earthquakes, turbulence, /uniFB01 nancial crashes, and human birth. Proc. Natl Acad. Sci. USA 99 , 2522 -2529 (2002).\n12. Sornette, D. Critical market crashes. Phys. Rep. 378 , 1 -98 (2003).\n13. Sornette, D. Why Stock Markets Crash: Critical Events in Complex Financial Systems (Princeton Univ. Press, 2017).\n14. Schweitzer, F. et al. Economic networks: the new challenges. Science 325 , 422 -425 (2009).\n15. Baillie, R. T. & Bollerslev, T. Cointegration, fractional cointegration, and exchange rate dynamics. J. Finance 49 , 737 -745 (1994).\n16. Brenner, R. J. & Kroner, K. F. Arbitrage, cointegration, and testing the unbiasedness hypothesis in /uniFB01 nancial markets. J. Financial Quant. Anal. 30 , 23 -42 (1995).\n17. Forbes, K. J. & Rigobon, R. No contagion, only interdependence: measuring stock market comovements. J. Finance 57 , 2223 -2261 (2002).\n18. Forbes, K. J. & Chinn, M. D. A decomposition of global linkages in /uniFB01 nancial markets over time. Rev. Econ. Stat. 86 , 705 -722 (2004).\n\nProvides a historical overview of research on econophysics, financial crises, and network dynamics, establishing the context for analyzing financial system behavior.",
    "original_text": "1. Mantegna, R. N. & Stanley, H. E. Introduction to Econophysics: Correlations and Complexity in Finance (Cambridge Univ. Press, 1999).\n2. Orsenigo, L., Pammolli, F. & Riccaboni, M. Technological change and network dynamics. Res. Policy 30 , 485 -508 (2001).\n3. Durlauf, S. N. Complexity and empirical economics. Econ. J. 115 , F225 -F243 (2005).\n4. Scheffer, M. et al. Early-warning signals for critical transitions. Nature 461 , 53 (2009).\n5. Diks, C., Hommes, C. & Wang, J. Critical slowing down as an early warning signal for /uniFB01 nancial crises? Emp. Econ. 57 , 1 -28 (2015).\n6. Zhao, L. et al. Herd behavior in a complex adaptive system. Proc. Natl Acad. Sci. USA 108 , 15058 -15063 (2011).\n7. Hüsler, A., Sornette, D. & Hommes, C. Super-exponential bubbles in lab experiments: evidence for anchoring over-optimistic expectations on price. J. Econ. Behav. Org. 92 , 304 -316 (2013).\n8. Trueman, B. Analyst forecasts and herding behavior. Rev. Financial Stud. 7 , 97 -124 (1994).\n9. Sharma, M. S. & Bikhchandani, S. Herd Behavior in Financial Markets: A Review 0 -48 (International Monetary Fund, 2000).\n10. Banerjee, A. V. A simple model of herd behavior. Q. J. Econ. 107 , 797 -817 (1992).\n11. Sornette, D. Predictability of catastrophic events: material rupture, earthquakes, turbulence, /uniFB01 nancial crashes, and human birth. Proc. Natl Acad. Sci. USA 99 , 2522 -2529 (2002).\n12. Sornette, D. Critical market crashes. Phys. Rep. 378 , 1 -98 (2003).\n13. Sornette, D. Why Stock Markets Crash: Critical Events in Complex Financial Systems (Princeton Univ. Press, 2017).\n14. Schweitzer, F. et al. Economic networks: the new challenges. Science 325 , 422 -425 (2009).\n15. Baillie, R. T. & Bollerslev, T. Cointegration, fractional cointegration, and exchange rate dynamics. J. Finance 49 , 737 -745 (1994).\n16. Brenner, R. J. & Kroner, K. F. Arbitrage, cointegration, and testing the unbiasedness hypothesis in /uniFB01 nancial markets. J. Financial Quant. Anal. 30 , 23 -42 (1995).\n17. Forbes, K. J. & Rigobon, R. No contagion, only interdependence: measuring stock market comovements. J. Finance 57 , 2223 -2261 (2002).\n18. Forbes, K. J. & Chinn, M. D. A decomposition of global linkages in /uniFB01 nancial markets over time. Rev. Econ. Stat. 86 , 705 -722 (2004).",
    "context": "Provides a historical overview of research on econophysics, financial crises, and network dynamics, establishing the context for analyzing financial system behavior.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      8
    ],
    "id": "6defb5fe85666615256a3bfbe8f450e46437136a5aa6c6d8b3faca4e79de684b"
  },
  {
    "text": "19. Barberis, N., Shleifer, A. & Wurgler, J. Comovement. J. Financial Econ. 75 , 283 -317 (2005).\n20. Simon, H. A. & Ando, A. Aggregation of variables in dynamic systems. Econometrica 29 , 111 -138 (1961).\n21. Ando, A. & Fisher, F. M. Near-decomposability, partition and aggregation, and the relevance of stability discussions. Int. Econ. Rev. 4 , 53 -67 (1963).\n22. Simon, H. A. The Architecture of Complexity (MIT Press, Cambridge, 1996).\n23. Courtois, P. J. Decomposability: Queueing and Computer System Applications (Academic Press, 2014).\n24. Dakos, V. et al. Methods for detecting early warnings of critical transitions in time series illustrated using simulated ecological data. PLoS ONE 7 , e41010 (2012).\n25. Lenton, T., Livina, V., Dakos, V., Van Nes, E. & Scheffer, M. Early warning of climate tipping points from critical slowing down: comparing methods to improve robustness. Philos. Trans. R. Soc. Ser. A 370 , 1185 -1204 (2012).\n26. Lux, T. Herd behaviour, bubbles and crashes. Econ. J. 105 , 881 -896 (1995).\n27. Hong, H. & Stein, J. C. Differences of opinion, short-sales constraints, and market crashes. Rev. Financial Stud. 16 , 487 -525 (2003).\n28. Peng, C.-K. et al. Mosaic organization of dna nucleotides. Phys. Rev. E 49 , 1685 (1994).\n29. Viswanathan, G. M. et al. Lévy /uniFB02 ight search patterns of wandering albatrosses. Nature 381 , 413 (1996).\n30. Hardstone, R. et al. Detrended /uniFB02 uctuation analysis: a scale-free view on neuronal oscillations. Front. Physiol. 3 , 450 (2012).\n31. Preis, T., Moat, H. S. & Stanley, H. E. Quantifying trading behavior in /uniFB01 nancial markets using google trends. Scienti /uniFB01 c Rep. 3 , 1684 (2013).\n32. Zhong, X. & Raghib, M. Revisiting the use of web search data for stock market movements. Scienti /uniFB01 c Rep. 9 , 1 -8 (2019).\n33. Hommes, C. Modeling the stylized facts in /uniFB01 nance through simple nonlinear adaptive systems. Proc. Natl Acad. Sci. USA 99 , 7221 -7228 (2002).\n34. Hommes, C. Heterogeneous agent models in economics and /uniFB01 nance. Handb. Comput. Econ. 2 , 1109 -1186 (2006).\n35. Gabaix, X., Gopikrishnan, P., Plerou, V. & Stanley, H. E. A theory of powerlaw distributions in /uniFB01 nancial market /uniFB02 uctuations. Nature 423 , 267 (2003).\n36. Scheffer, M. et al. Anticipating critical transitions. Science 338 , 344 -348 (2012).\n37. Battiston, S., Caldarelli, G., May, R. M., Roukny, T. & Stiglitz, J. E. The price of complexity in /uniFB01 nancial networks. Proc. Natl Acad. Sci. USA 113 , 10031 -10036 (2016).\n38. Battiston, S. et al. Complexity theory and /uniFB01 nancial regulation. Science 351 , 818 -819 (2016).\n39. Corsi, F., Marmi, S. & Lillo, F. When micro prudence increases macro risk: the destabilizing effects of /uniFB01 nancial innovation, leverage, and diversi /uniFB01 cation. Oper. Res. 64 , 1073 -1088 (2016).\n40. Flori, A., Pammolli, F., Buldyrev, S. V., Regis, L. & Stanley, H. E. Communities and regularities in the behavior of investment fund managers. Proc. Natl Acad. Sci. USA 116 , 201802976 (2019).\n41. Fortunato, S. Community detection in graphs. Phys. Rep. 486 , 75 -174 (2010).\n42. MacMahon, M. & Garlaschelli, D. Community detection for correlation matrices. Phys. Rev. X 5 , 021006 (2015).\n43. Newman, M. Networks (Oxford Univ. Press, 2018).\n44. Stanley, H. E. Phase Transitions and Critical Phenomena (Clarendon Press, Oxford, 1971).\n45. Chiang, T. C., Jeon, B. N. & Li, H. Dynamic correlation analysis of /uniFB01 nancial contagion: evidence from Asian markets. J. Int. Money Finance 26 , 1206 -1228 (2007).\n46. Kindleberger, C. P. & Aliber, R. Z. Manias, Panics and Crashes: A History of Financial Crises (Palgrave Macmillan, 2011).\n47. Mera, K. & Renaud, B. Asia ' s Financial Crisis and the Role of Real Estate (Routledge, 2016).\n48. Chowdhury, B., Dungey, M. H., Kangogo, M., Sayeed, M. A. & Volkov, V. The changing network of /uniFB01 nancial market linkages: the asian experience. Int. Rev. Financial Anal. 64 , 71 -92 (2019).\n49. Jorion, P. et al. Financial Risk Manager Handbook , Vol. 406 (Wiley, 2007).\n50. Fleten, S.-E., Maribu, K. M. & Wangensteen, I. Optimal investment strategies in decentralized renewable power generation under uncertainty. Energy 32 , 803 -815 (2007).\n51. Chen, L., Liu, R., Liu, Z.-P., Li, M. & Aihara, K. Detecting early-warning signals for sudden deterioration of complex diseases by dynamical network biomarkers. Scienti /uniFB01 c Rep. 2 , 342 (2012).\n52. Gai, P. & Kapadia, S. Contagion in /uniFB01 nancial networks. Proc. R. Soc. Ser. A 466 , 2401 -2423 (2010).\n53. Gai, P., Haldane, A. & Kapadia, S. Complexity, concentration and contagion. J. Monetary Econ. 58 , 453 -470 (2011).\n54. Amini, H., Cont, R. & Minca, A. Stress testing the resilience of /uniFB01 nancial networks. Int. J. Theor. Appl. Finance 15 , 1250006 (2012).\n55. Acemoglu, D., Ozdaglar, A. & Tahbaz-Salehi, A. The Network Origins of Large Economic Downturns . Technical Report (National Bureau of Economic Research, 2013).\n56. Höll, M. & Kantz, H. The relationship between the detrendend /uniFB02 uctuation analysis and the autocorrelation function of a signal. Eur. Phys. J. B 88 , 327 (2015).\n57. Chen, L., Wang, R., Li, C.& Aihara, K. Modeling Biomolecular Networks in Cells: Structures and Dynamics (Springer Science & Business Media, 2010).\n58. Spelta, A., Pecora, N., Flori, A. & Pammolli, F. Transition Drivers and Crisis Signaling in Stock Markets . MPRA Paper 88127 (Univ. Library of Munich, Germany, 2018).\n59. Spelta, A., Flori, A., Pecora, N. & Pammolli, F. Financial crises: uncovering self-organized patterns and predicting stock markets instability. J. Bus. Res. https://doi.org/10.1016/j.jbusres.2019.10.043 (2019).\n60. Preis, T., Schneider, J. J. & Stanley, H. E. Switching processes in /uniFB01 nancial markets. Proc. Natl Acad. Sci. USA 108 , 7674 -7678 (2011).\n\nProvides a framework for detecting early warnings of critical transitions, including methods for improving robustness in climate tipping points and financial crises.",
    "original_text": "19. Barberis, N., Shleifer, A. & Wurgler, J. Comovement. J. Financial Econ. 75 , 283 -317 (2005).\n20. Simon, H. A. & Ando, A. Aggregation of variables in dynamic systems. Econometrica 29 , 111 -138 (1961).\n21. Ando, A. & Fisher, F. M. Near-decomposability, partition and aggregation, and the relevance of stability discussions. Int. Econ. Rev. 4 , 53 -67 (1963).\n22. Simon, H. A. The Architecture of Complexity (MIT Press, Cambridge, 1996).\n23. Courtois, P. J. Decomposability: Queueing and Computer System Applications (Academic Press, 2014).\n24. Dakos, V. et al. Methods for detecting early warnings of critical transitions in time series illustrated using simulated ecological data. PLoS ONE 7 , e41010 (2012).\n25. Lenton, T., Livina, V., Dakos, V., Van Nes, E. & Scheffer, M. Early warning of climate tipping points from critical slowing down: comparing methods to improve robustness. Philos. Trans. R. Soc. Ser. A 370 , 1185 -1204 (2012).\n26. Lux, T. Herd behaviour, bubbles and crashes. Econ. J. 105 , 881 -896 (1995).\n27. Hong, H. & Stein, J. C. Differences of opinion, short-sales constraints, and market crashes. Rev. Financial Stud. 16 , 487 -525 (2003).\n28. Peng, C.-K. et al. Mosaic organization of dna nucleotides. Phys. Rev. E 49 , 1685 (1994).\n29. Viswanathan, G. M. et al. Lévy /uniFB02 ight search patterns of wandering albatrosses. Nature 381 , 413 (1996).\n30. Hardstone, R. et al. Detrended /uniFB02 uctuation analysis: a scale-free view on neuronal oscillations. Front. Physiol. 3 , 450 (2012).\n31. Preis, T., Moat, H. S. & Stanley, H. E. Quantifying trading behavior in /uniFB01 nancial markets using google trends. Scienti /uniFB01 c Rep. 3 , 1684 (2013).\n32. Zhong, X. & Raghib, M. Revisiting the use of web search data for stock market movements. Scienti /uniFB01 c Rep. 9 , 1 -8 (2019).\n33. Hommes, C. Modeling the stylized facts in /uniFB01 nance through simple nonlinear adaptive systems. Proc. Natl Acad. Sci. USA 99 , 7221 -7228 (2002).\n34. Hommes, C. Heterogeneous agent models in economics and /uniFB01 nance. Handb. Comput. Econ. 2 , 1109 -1186 (2006).\n35. Gabaix, X., Gopikrishnan, P., Plerou, V. & Stanley, H. E. A theory of powerlaw distributions in /uniFB01 nancial market /uniFB02 uctuations. Nature 423 , 267 (2003).\n36. Scheffer, M. et al. Anticipating critical transitions. Science 338 , 344 -348 (2012).\n37. Battiston, S., Caldarelli, G., May, R. M., Roukny, T. & Stiglitz, J. E. The price of complexity in /uniFB01 nancial networks. Proc. Natl Acad. Sci. USA 113 , 10031 -10036 (2016).\n38. Battiston, S. et al. Complexity theory and /uniFB01 nancial regulation. Science 351 , 818 -819 (2016).\n39. Corsi, F., Marmi, S. & Lillo, F. When micro prudence increases macro risk: the destabilizing effects of /uniFB01 nancial innovation, leverage, and diversi /uniFB01 cation. Oper. Res. 64 , 1073 -1088 (2016).\n40. Flori, A., Pammolli, F., Buldyrev, S. V., Regis, L. & Stanley, H. E. Communities and regularities in the behavior of investment fund managers. Proc. Natl Acad. Sci. USA 116 , 201802976 (2019).\n41. Fortunato, S. Community detection in graphs. Phys. Rep. 486 , 75 -174 (2010).\n42. MacMahon, M. & Garlaschelli, D. Community detection for correlation matrices. Phys. Rev. X 5 , 021006 (2015).\n43. Newman, M. Networks (Oxford Univ. Press, 2018).\n44. Stanley, H. E. Phase Transitions and Critical Phenomena (Clarendon Press, Oxford, 1971).\n45. Chiang, T. C., Jeon, B. N. & Li, H. Dynamic correlation analysis of /uniFB01 nancial contagion: evidence from Asian markets. J. Int. Money Finance 26 , 1206 -1228 (2007).\n46. Kindleberger, C. P. & Aliber, R. Z. Manias, Panics and Crashes: A History of Financial Crises (Palgrave Macmillan, 2011).\n47. Mera, K. & Renaud, B. Asia ' s Financial Crisis and the Role of Real Estate (Routledge, 2016).\n48. Chowdhury, B., Dungey, M. H., Kangogo, M., Sayeed, M. A. & Volkov, V. The changing network of /uniFB01 nancial market linkages: the asian experience. Int. Rev. Financial Anal. 64 , 71 -92 (2019).\n49. Jorion, P. et al. Financial Risk Manager Handbook , Vol. 406 (Wiley, 2007).\n50. Fleten, S.-E., Maribu, K. M. & Wangensteen, I. Optimal investment strategies in decentralized renewable power generation under uncertainty. Energy 32 , 803 -815 (2007).\n51. Chen, L., Liu, R., Liu, Z.-P., Li, M. & Aihara, K. Detecting early-warning signals for sudden deterioration of complex diseases by dynamical network biomarkers. Scienti /uniFB01 c Rep. 2 , 342 (2012).\n52. Gai, P. & Kapadia, S. Contagion in /uniFB01 nancial networks. Proc. R. Soc. Ser. A 466 , 2401 -2423 (2010).\n53. Gai, P., Haldane, A. & Kapadia, S. Complexity, concentration and contagion. J. Monetary Econ. 58 , 453 -470 (2011).\n54. Amini, H., Cont, R. & Minca, A. Stress testing the resilience of /uniFB01 nancial networks. Int. J. Theor. Appl. Finance 15 , 1250006 (2012).\n55. Acemoglu, D., Ozdaglar, A. & Tahbaz-Salehi, A. The Network Origins of Large Economic Downturns . Technical Report (National Bureau of Economic Research, 2013).\n56. Höll, M. & Kantz, H. The relationship between the detrendend /uniFB02 uctuation analysis and the autocorrelation function of a signal. Eur. Phys. J. B 88 , 327 (2015).\n57. Chen, L., Wang, R., Li, C.& Aihara, K. Modeling Biomolecular Networks in Cells: Structures and Dynamics (Springer Science & Business Media, 2010).\n58. Spelta, A., Pecora, N., Flori, A. & Pammolli, F. Transition Drivers and Crisis Signaling in Stock Markets . MPRA Paper 88127 (Univ. Library of Munich, Germany, 2018).\n59. Spelta, A., Flori, A., Pecora, N. & Pammolli, F. Financial crises: uncovering self-organized patterns and predicting stock markets instability. J. Bus. Res. https://doi.org/10.1016/j.jbusres.2019.10.043 (2019).\n60. Preis, T., Schneider, J. J. & Stanley, H. E. Switching processes in /uniFB01 nancial markets. Proc. Natl Acad. Sci. USA 108 , 7674 -7678 (2011).",
    "context": "Provides a framework for detecting early warnings of critical transitions, including methods for improving robustness in climate tipping points and financial crises.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      9
    ],
    "id": "9eab4cf5eac7c01652e67804e047094ba4bc212064dc7e65707b28deff22f210"
  },
  {
    "text": "We acknowledge funding from the National Research Project (PNR) CRISIS Lab. We thank anonymous Reviewers and the Editor for their detailed and constructive feedback on our paper.\n\nAcknowledges funding sources and expresses gratitude for reviewer feedback, indicating a research paper's development and peer-review process.",
    "original_text": "We acknowledge funding from the National Research Project (PNR) CRISIS Lab. We thank anonymous Reviewers and the Editor for their detailed and constructive feedback on our paper.",
    "context": "Acknowledges funding sources and expresses gratitude for reviewer feedback, indicating a research paper's development and peer-review process.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      9
    ],
    "id": "c89afa650501d6b6b88b4d04714319fa343cbda7c9a8bb78f89e77ee399b75ed"
  },
  {
    "text": "A.S., A.F., N.P., F.P., and S.B.: designed the research, performed the research, analyzed the data, and wrote the paper.\n\nDetails the authors’ contributions to the research design, execution, and writing of the paper.",
    "original_text": "A.S., A.F., N.P., F.P., and S.B.: designed the research, performed the research, analyzed the data, and wrote the paper.",
    "context": "Details the authors’ contributions to the research design, execution, and writing of the paper.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      9
    ],
    "id": "a3851cb4e25b2d2ea5e6b84684b6499e471a94cbd9d0eeaffe924e3990aca05c"
  },
  {
    "text": "The authors declare no competing interests.\n\nStates the authors have no conflicts of interest.",
    "original_text": "The authors declare no competing interests.",
    "context": "States the authors have no conflicts of interest.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      9
    ],
    "id": "9ba9761df944344679e71f50f98a25479ec835067d14e2f3d453b7d21c5f9a4f"
  },
  {
    "text": "Supplementary information is available for this paper at https://doi.org/10.1038/s41467020-15356-z.\nCorrespondence and requests for materials should be addressed to A.S.\nPeer review information Nature Communications thanks Š tefan Lyócsa and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Peer reviewer reports are available.\nReprints and permission information is available at http://www.nature.com/reprints\nPublisher ' s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional af /uniFB01 liations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article ' s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article ' s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/ licenses/by/4.0/.\n© The Author(s) 2020\n\nProvides essential information for accessing supplementary materials and contact details for the corresponding author.",
    "original_text": "Supplementary information is available for this paper at https://doi.org/10.1038/s41467020-15356-z.\nCorrespondence and requests for materials should be addressed to A.S.\nPeer review information Nature Communications thanks Š tefan Lyócsa and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Peer reviewer reports are available.\nReprints and permission information is available at http://www.nature.com/reprints\nPublisher ' s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional af /uniFB01 liations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article ' s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article ' s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/ licenses/by/4.0/.\n© The Author(s) 2020",
    "context": "Provides essential information for accessing supplementary materials and contact details for the corresponding author.",
    "document": "s41467-020-15356-z.pdf",
    "pages": [
      9
    ],
    "id": "53bc6c4287fe964c11243a6405f8f575a02f73bd143097f7c3a0a932b21d3830"
  },
  {
    "text": "Christoph Schweimer ͷ , Bernhard C. Geiger ͷ * , Meizhu Wang ͷ , Sergiy Gogolenko ͸ , Imran Mahmood ͹ , Alireza Jahani ͹ , Diana Suleimenova ͹ * & Derek Groen ͹,ͺ\nAutomated construction of location graphs is instrumental but challenging, particularly in logistics optimisation problems and agent-based movement simulations. Hence, we propose an algorithm for automated construction of location graphs, in which vertices correspond to geographic locations of interest and edges to direct travelling routes between them. Our approach involves two steps. In the first step, we use a routing service to compute distances between all pairs of L locations, resulting in a complete graph. In the second step, we prune this graph by removing edges corresponding to indirect routes, identified using the triangle inequality. The computational complexity of this second step is O ( L 3 ) , which enables the computation of location graphs for all towns and cities on the road network of an entire continent. To illustrate the utility of our algorithm in an application, we constructed location graphs for four regions of different size and road infrastructures and compared them to manually created ground truths. Our algorithm simultaneously achieved precision and recall values around Ͷ.9 for a wide range of the single hyperparameter, suggesting that it is a valid approach to create large location graphs for which a manual creation is infeasible.\nGeographic information systems (GIS) and web mapping have evolved over the past three decades as technological advances enable the developments in geospatial data and mapping   usage 1 . Consequently, web mapping services, such as Google Maps, Bing Maps and OpenStreetMap (OSM), have emerged with various functionalities to inspect, visualise, analyse and model geospatial information. These commonly known services are available and accessible by everyone through interactive and visual interfaces, as well as provide an interface to download geographic information, including lists of cities or other points-of-interest within a geographic region, and routes between pairs of locations (if the service also provides routing capabilities).\nHowever, these interfaces have functional limitations, which do not allow the automatic generation of highly customised geographic information. In this work, we consider a relevant subclass of such customised geographic information: location graphs, in which locations of interest are connected by edges if there exists a direct route between them. Such location graphs are a prerequisite for many real-life problems, for example, route optimisation, load optimisation in electrical and transportation networks, and many others. Location graphs are also needed for agent-based modelling applications such as transportation of   goods 2 , evacuation   models 3,4 , traffic simulations 5 , disease   transmission 6 , movement of   people 7 , and migration   simulation 8 .\nPreviously, these location graphs were created manually. For example, for the migration study of Suleimenova et al. 8 , the length of a link (in km) was estimated using the OSM route planner for cars. In cases where obvious shorter routes are visible, the mapping marker was dragged to force the routing machine to calculate these shorter routes. Yet, such manual creation of location graphs is a time-consuming and error-prone procedure, and can only be accomplished for small sets of locations.\nWe propose an automated approach for the construction of location graphs for given lists of locations. This approach relies on a two-step procedure. In the first step, we utilise interfaces provided by mapping services, such as the Open Source Routing Machine (OSRM) from OSM, to compute route distances between all pairs of locations, essentially corresponding to a fully connected location graph. In the second step, edges are pruned from this fully connected location graph that correspond to indirect routes, i.e., to routes between two locations that pass through a third location in the location graph. Since the first step yields only a matrix of distances but no further information about the routes, finding these indirect routes is nontrivial. We approach this problem by making use of the triangle inequality: if the route between two locations has a distance similar to the sum of\nͷ Know-Center GmbH, Graz, Austria. ͸ High Performance Computing Center Stuttgart, Stuttgart, Germany.  ͹ Department of Computer Science, Brunel University London, London, UK.  ͺ Centre for Computational Science, University College London, London, UK. * email: geiger@ieee.org; diana.suleimenova@brunel.ac.uk\nVol.:ȋͬͭͮͯͰͱͲͳʹ͵Ȍ\nVol:.(1234567890)\ndistances of routes between these locations and a common third location, then it is probable that the considered route is indirect. While the first step of our procedure relies on existing algorithms for finding shortest paths in graphs, the second step presents our first contribution in the area of edge pruning algorithms (see 'Related work' section).\nAs our second contribution, we add a real-valued parameter β to our pruning algorithm that extends the flexibility of our approach and allows to control the quality of pruning. Specifically, when deciding whether a route shall be pruned, we compare the route distance between two locations to the sum of distances between these locations and a third one, multiplied by β . Thus, if β > 1 , the resulting pruned graph may not be completely pruned, but may rather be redundant by retaining edges corresponding to sub-optimal routes (i.e., with longer distances). If instead 0 < β < 1 , then the resulting graph is lossy in the sense that not all shortest paths are retained. Thus, the parameter β allows trading between the quality (in terms of redundancy and path quality) and complexity (in terms of edge set size) of the simplified graph. As a consequence, our approach complements and extends the work of Zhou et al. 9,10 , who approached graph simplification by pruning a given number of edges such that path quality is maximised while we control the quality of the lossy pruning by the relaxation parameter 0 < β ≤ 1 .\nOur third contribution is to validate the applicability of our approach and investigate its limitations by applying it to four different scenarios. We constructed location graphs for two small regions in Europe and for two large regions in Africa, respectively, and compared the results to manually created ground truths. In three of the four regions, we achieved an F1-score (see 'Results' section for a definition) exceeding 0.9 for the same value of the single parameter of our method. We furthermore showed that our approach scales well to larger location sets, thus enabling the creation of location graphs with tens of thousands of locations. The implementation of our method is available at https://  github.  com/  djgro  en/  Extra  ctMap.\n\nThis chunk proposes an algorithm for automatically constructing location graphs, which are crucial for logistics and agent-based modeling. The algorithm involves two steps: first, calculating distances between all location pairs using a routing service; and second, pruning indirect routes based on the triangle inequality. The method achieves high precision and recall (around 0.9) when evaluated against manually created ground truths, demonstrating its validity for large location sets.",
    "original_text": "Christoph Schweimer ͷ , Bernhard C. Geiger ͷ * , Meizhu Wang ͷ , Sergiy Gogolenko ͸ , Imran Mahmood ͹ , Alireza Jahani ͹ , Diana Suleimenova ͹ * & Derek Groen ͹,ͺ\nAutomated construction of location graphs is instrumental but challenging, particularly in logistics optimisation problems and agent-based movement simulations. Hence, we propose an algorithm for automated construction of location graphs, in which vertices correspond to geographic locations of interest and edges to direct travelling routes between them. Our approach involves two steps. In the first step, we use a routing service to compute distances between all pairs of L locations, resulting in a complete graph. In the second step, we prune this graph by removing edges corresponding to indirect routes, identified using the triangle inequality. The computational complexity of this second step is O ( L 3 ) , which enables the computation of location graphs for all towns and cities on the road network of an entire continent. To illustrate the utility of our algorithm in an application, we constructed location graphs for four regions of different size and road infrastructures and compared them to manually created ground truths. Our algorithm simultaneously achieved precision and recall values around Ͷ.9 for a wide range of the single hyperparameter, suggesting that it is a valid approach to create large location graphs for which a manual creation is infeasible.\nGeographic information systems (GIS) and web mapping have evolved over the past three decades as technological advances enable the developments in geospatial data and mapping   usage 1 . Consequently, web mapping services, such as Google Maps, Bing Maps and OpenStreetMap (OSM), have emerged with various functionalities to inspect, visualise, analyse and model geospatial information. These commonly known services are available and accessible by everyone through interactive and visual interfaces, as well as provide an interface to download geographic information, including lists of cities or other points-of-interest within a geographic region, and routes between pairs of locations (if the service also provides routing capabilities).\nHowever, these interfaces have functional limitations, which do not allow the automatic generation of highly customised geographic information. In this work, we consider a relevant subclass of such customised geographic information: location graphs, in which locations of interest are connected by edges if there exists a direct route between them. Such location graphs are a prerequisite for many real-life problems, for example, route optimisation, load optimisation in electrical and transportation networks, and many others. Location graphs are also needed for agent-based modelling applications such as transportation of   goods 2 , evacuation   models 3,4 , traffic simulations 5 , disease   transmission 6 , movement of   people 7 , and migration   simulation 8 .\nPreviously, these location graphs were created manually. For example, for the migration study of Suleimenova et al. 8 , the length of a link (in km) was estimated using the OSM route planner for cars. In cases where obvious shorter routes are visible, the mapping marker was dragged to force the routing machine to calculate these shorter routes. Yet, such manual creation of location graphs is a time-consuming and error-prone procedure, and can only be accomplished for small sets of locations.\nWe propose an automated approach for the construction of location graphs for given lists of locations. This approach relies on a two-step procedure. In the first step, we utilise interfaces provided by mapping services, such as the Open Source Routing Machine (OSRM) from OSM, to compute route distances between all pairs of locations, essentially corresponding to a fully connected location graph. In the second step, edges are pruned from this fully connected location graph that correspond to indirect routes, i.e., to routes between two locations that pass through a third location in the location graph. Since the first step yields only a matrix of distances but no further information about the routes, finding these indirect routes is nontrivial. We approach this problem by making use of the triangle inequality: if the route between two locations has a distance similar to the sum of\nͷ Know-Center GmbH, Graz, Austria. ͸ High Performance Computing Center Stuttgart, Stuttgart, Germany.  ͹ Department of Computer Science, Brunel University London, London, UK.  ͺ Centre for Computational Science, University College London, London, UK. * email: geiger@ieee.org; diana.suleimenova@brunel.ac.uk\nVol.:ȋͬͭͮͯͰͱͲͳʹ͵Ȍ\nVol:.(1234567890)\ndistances of routes between these locations and a common third location, then it is probable that the considered route is indirect. While the first step of our procedure relies on existing algorithms for finding shortest paths in graphs, the second step presents our first contribution in the area of edge pruning algorithms (see 'Related work' section).\nAs our second contribution, we add a real-valued parameter β to our pruning algorithm that extends the flexibility of our approach and allows to control the quality of pruning. Specifically, when deciding whether a route shall be pruned, we compare the route distance between two locations to the sum of distances between these locations and a third one, multiplied by β . Thus, if β > 1 , the resulting pruned graph may not be completely pruned, but may rather be redundant by retaining edges corresponding to sub-optimal routes (i.e., with longer distances). If instead 0 < β < 1 , then the resulting graph is lossy in the sense that not all shortest paths are retained. Thus, the parameter β allows trading between the quality (in terms of redundancy and path quality) and complexity (in terms of edge set size) of the simplified graph. As a consequence, our approach complements and extends the work of Zhou et al. 9,10 , who approached graph simplification by pruning a given number of edges such that path quality is maximised while we control the quality of the lossy pruning by the relaxation parameter 0 < β ≤ 1 .\nOur third contribution is to validate the applicability of our approach and investigate its limitations by applying it to four different scenarios. We constructed location graphs for two small regions in Europe and for two large regions in Africa, respectively, and compared the results to manually created ground truths. In three of the four regions, we achieved an F1-score (see 'Results' section for a definition) exceeding 0.9 for the same value of the single parameter of our method. We furthermore showed that our approach scales well to larger location sets, thus enabling the creation of location graphs with tens of thousands of locations. The implementation of our method is available at https://  github.  com/  djgro  en/  Extra  ctMap.",
    "context": "This chunk proposes an algorithm for automatically constructing location graphs, which are crucial for logistics and agent-based modeling. The algorithm involves two steps: first, calculating distances between all location pairs using a routing service; and second, pruning indirect routes based on the triangle inequality. The method achieves high precision and recall (around 0.9) when evaluated against manually created ground truths, demonstrating its validity for large location sets.",
    "document": "s41598-021-90943-8.pdf",
    "pages": [
      1,
      2
    ],
    "id": "c23b16b5603d0427291d7a52525d06e0d6b2d450254a73c423af1aa410b4ca52"
  },
  {
    "text": "The shortest path algorithms for route planning can be categorised into static, dynamic, time-dependent, stochastic, parametric, alternative and weighted region shortest-path   algorithms 11,12 . These algorithms establish the algorithmic basis for state-of-the-art route planning engines such as Google Maps, Bing Maps, or OSRM. The static category includes single-source and all-pairs shortest-path algorithms that differ in terms of a given edge to other edges or all-pairs to other edges in the graph. One of the most known shortest-path algorithms was proposed by   Dijkstra 13 . It finds a shortest path between two vertices in a graph. Dijkstra's algorithm has numerous variations that are commonly applied to speed-up computing and tackle diverse problems of general and complex   graphs 11,14 . Dynamic algorithms consider insertion and deletion of edges, as well as a computation of the distances between single-source or all-pairs edges in the graph. Other categories refer to changes over time, uncertainty in edges, specific parameter values, avoiding given edges and weighted subdivision edges. In this work, we focus our interest on the category of batched shortest path algorithms which are commonly used for computing distance matrices in route planning   engines 12 .\nState-of-the-art route planning engines implement an API for finding travel distances and journey duration of fastest routes between all pairs of supplied origins using a given mode of travel. Examples of these include Distance Matrix Service of Google Maps, Distance Matrix API of Bing Maps, and Table Service of OSRM. Online routing services impose different constraints on the size and quantity of such API queries. In particular, Bing API allows up to 2500 origins-destinations pairs, while Google API establishes pricing per origin-destination pair in the Distance Matrix queries. Moreover, online services usually have a limited uncustomizable set of travel modes, which prevents tailoring models for speed of traveller movement on different terrains and road types. Being a free open-source off-line tool, OSRM relaxes these   limitations 15 .\nOSRM implements multilevel Dijkstra's (MLD) and contraction hierarchies (CH) algorithms for   routing 15 . Both methods consist of preprocessing and query phases. The preprocessing phase attempts to annotate and simplify the complicated route network in order to drastically reduce duration of further shortest-path and batched shortest-path queries. MLD belongs to the family of separator-based shortest-path   techniques 11,12 . Conceptually, it differs from the celebrated customizable route planning (CRP)   algorithm 16,17 only by the hierarchical partitioning approach used in the preprocessing phase: canonical CRP applies patented graph partitioning with natural cuts (PUNCH) approach, while MLD opts for inertial flow   approach 18 . Contraction hierarchies is a classic hierarchical shortest-path   algorithm 11,12 , widely discussed in the   literature 19,20 .\nNetwork simplification by edge pruning emerged in various contexts and has been addressed under different names by a number of   authors 9,10,21-25 . Specifically, the authors propose and study a generic path-oriented framework for graph   simplification 9,10,25 . This framework targets to simplify a graph by reducing the number of edges while preserving the maximum path quality metric for any pair of vertices in the graph. It covers a broad class of optimisation problems for probabilistic graphs, flow graphs, and distance graphs. Distance graph pruning, as it is investigated in this work, can be viewed as a special case of the path-oriented graph simplification where the inverse of the path length serves as a path quality metric. Toivonen et al. 25 introduce four generic strategies for lossless path-oriented graph simplification, where the term lossless in the context of distance graphs implies that all fastest routes between pairs of locations are preserved in the pruned graph. Later this approach was extended to a lossy graph pruning with a given number of edges to   remove 9,10 .\nOur pruning approach based on the triangle inequality closely relates to the Static-Triangle strategy from Toivonen et al. 25 which has a time complexity of O ( L · R ) , where L and R are the number of locations and routes in the original graph, respectively. For general (potentially sparse) graphs, this strategy is sub-optimal in the sense that the obtained graph may contain redundant routes, and the authors thus also propose an alternative, optimal strategy (called Iterative-Global) with a higher time complexity of O ( R ( R + L ) log L ) . However, for a complete location graph in which route distances satisfy the triangle inequality and ignoring the effect of ties, the Static-Triangle strategy and our own approach can be shown to be optimal in the sense of eliminating all\nredundant routes. In this case, since R = L 2 , the time complexity of our approach is O ( L 3 ) , which compares favourably with the time complexity of O ( L 4 log L ) of the optimal Iterative-Global   strategy 25 . Since the first step of our two-step approach results with a complete location graph where the route distances satisfy the triangle inequality, we can reap the benefits of this reduced time complexity without loss of optimality.\n\nProvides a two-step algorithm for automated location graph construction, using routing service distances and pruning based on the triangle inequality to eliminate redundant routes.",
    "original_text": "The shortest path algorithms for route planning can be categorised into static, dynamic, time-dependent, stochastic, parametric, alternative and weighted region shortest-path   algorithms 11,12 . These algorithms establish the algorithmic basis for state-of-the-art route planning engines such as Google Maps, Bing Maps, or OSRM. The static category includes single-source and all-pairs shortest-path algorithms that differ in terms of a given edge to other edges or all-pairs to other edges in the graph. One of the most known shortest-path algorithms was proposed by   Dijkstra 13 . It finds a shortest path between two vertices in a graph. Dijkstra's algorithm has numerous variations that are commonly applied to speed-up computing and tackle diverse problems of general and complex   graphs 11,14 . Dynamic algorithms consider insertion and deletion of edges, as well as a computation of the distances between single-source or all-pairs edges in the graph. Other categories refer to changes over time, uncertainty in edges, specific parameter values, avoiding given edges and weighted subdivision edges. In this work, we focus our interest on the category of batched shortest path algorithms which are commonly used for computing distance matrices in route planning   engines 12 .\nState-of-the-art route planning engines implement an API for finding travel distances and journey duration of fastest routes between all pairs of supplied origins using a given mode of travel. Examples of these include Distance Matrix Service of Google Maps, Distance Matrix API of Bing Maps, and Table Service of OSRM. Online routing services impose different constraints on the size and quantity of such API queries. In particular, Bing API allows up to 2500 origins-destinations pairs, while Google API establishes pricing per origin-destination pair in the Distance Matrix queries. Moreover, online services usually have a limited uncustomizable set of travel modes, which prevents tailoring models for speed of traveller movement on different terrains and road types. Being a free open-source off-line tool, OSRM relaxes these   limitations 15 .\nOSRM implements multilevel Dijkstra's (MLD) and contraction hierarchies (CH) algorithms for   routing 15 . Both methods consist of preprocessing and query phases. The preprocessing phase attempts to annotate and simplify the complicated route network in order to drastically reduce duration of further shortest-path and batched shortest-path queries. MLD belongs to the family of separator-based shortest-path   techniques 11,12 . Conceptually, it differs from the celebrated customizable route planning (CRP)   algorithm 16,17 only by the hierarchical partitioning approach used in the preprocessing phase: canonical CRP applies patented graph partitioning with natural cuts (PUNCH) approach, while MLD opts for inertial flow   approach 18 . Contraction hierarchies is a classic hierarchical shortest-path   algorithm 11,12 , widely discussed in the   literature 19,20 .\nNetwork simplification by edge pruning emerged in various contexts and has been addressed under different names by a number of   authors 9,10,21-25 . Specifically, the authors propose and study a generic path-oriented framework for graph   simplification 9,10,25 . This framework targets to simplify a graph by reducing the number of edges while preserving the maximum path quality metric for any pair of vertices in the graph. It covers a broad class of optimisation problems for probabilistic graphs, flow graphs, and distance graphs. Distance graph pruning, as it is investigated in this work, can be viewed as a special case of the path-oriented graph simplification where the inverse of the path length serves as a path quality metric. Toivonen et al. 25 introduce four generic strategies for lossless path-oriented graph simplification, where the term lossless in the context of distance graphs implies that all fastest routes between pairs of locations are preserved in the pruned graph. Later this approach was extended to a lossy graph pruning with a given number of edges to   remove 9,10 .\nOur pruning approach based on the triangle inequality closely relates to the Static-Triangle strategy from Toivonen et al. 25 which has a time complexity of O ( L · R ) , where L and R are the number of locations and routes in the original graph, respectively. For general (potentially sparse) graphs, this strategy is sub-optimal in the sense that the obtained graph may contain redundant routes, and the authors thus also propose an alternative, optimal strategy (called Iterative-Global) with a higher time complexity of O ( R ( R + L ) log L ) . However, for a complete location graph in which route distances satisfy the triangle inequality and ignoring the effect of ties, the Static-Triangle strategy and our own approach can be shown to be optimal in the sense of eliminating all\nredundant routes. In this case, since R = L 2 , the time complexity of our approach is O ( L 3 ) , which compares favourably with the time complexity of O ( L 4 log L ) of the optimal Iterative-Global   strategy 25 . Since the first step of our two-step approach results with a complete location graph where the route distances satisfy the triangle inequality, we can reap the benefits of this reduced time complexity without loss of optimality.",
    "context": "Provides a two-step algorithm for automated location graph construction, using routing service distances and pruning based on the triangle inequality to eliminate redundant routes.",
    "document": "s41598-021-90943-8.pdf",
    "pages": [
      2,
      3
    ],
    "id": "ddc5ea82c3fd01fe163f571a960d8b54bcfa4b39c01122a693cdb441637e01e6"
  },
  {
    "text": "We are given a set of L locations L = { l 1, . . . , lL } in a geographical region. We are interested in a weighted graph G = ( L , E , D ) with vertices L , edges E corresponding to direct routes between locations, and edge weights D corresponding to route distances that keeps only fastest (or shortest) paths between all pairs of vertices from L .\nThe problem of finding an optimal location graph can be formalised as follows. We assume a weighted, potentially  directed  route  graph G = ( L G , EG ) with LG vertices  is  given.  Each  edge e := ( u , v ) ∈ EG corresponds  to  a  route  connecting  two  locations u and v from L G and  has  a  positive-valued  weight dG ( e ) ∈ R + that corresponds to the route distance between u and v .  A path P is  a sequence of edges, e.g. P = (( u 1, u 2 ) , ( u 2, u 3 ) , . . . , ( uk -1 , uk )) =: [ u 1 -u 2 -··· -uk ] .  We denote by u 1 G /squiggleright uk the set of all feasible paths between u 1 and uk in G . The length of the shortest path between u and v is thus defined as\n<!-- formula-not-decoded -->\nFor the given subset of locations L ⊆ L G , our goal is to find a weighted graph G = ( L , E ) with a minimum number of edges such that Q ( u , v ; G ) = Q ( u , v ; G ) for all { u , v } ⊆ L . For the sake of brevity, we limit further discussion to the undirected graphs. Nevertheless, our approach straightforwardly extends to the directed graphs.\nTo create the graph G , we propose a two-step procedure. In the first step, we use a routing service to find route distances between all pairs of locations. Assuming that the distances are symmetric, we terminate with an undirected fully connected graph G ∗ = ( L , [ L ] 2 , D ∗ ) , where [ L ] 2 is the set of two-element subsets of L and where D ∗ = [ d ∗ i , j ] is the matrix of distances between locations with d ∗ i , j = d G ∗ ( { l i , l j } ) . Many of the distances computed by the route planner will correspond to indirect routes, as a route between two locations in L may pass through another location in L . Therefore, in a second step, we use the distance matrix D ∗ to identify edges in G ∗ that correspond to redundant routes, and remove them to obtain G . In this section, we will give an overview of this two-step procedure.\nStep ͷ: Creating a fully connected graph via route planning. For route planning, we rely on map data from OSM, together with the C++ routing machine from the OSRM Project (http://  proje  ct-  osrm.  org), i.e., we work with locally downloaded map data and a C++ wrapper for OSRM, allowing requests for large sets of locations L . However, any other routing service can be used, including online services for smaller sets of locations. In our experiments, we obtained pairwise distances between up to L = 18000 locations. The result is a distance matrix D ∗ = [ d ∗ i , j ] , with d ∗ i , j being the distance between locations l i and l j . If there is no route between l i and l j , then the respective distance is set to ∞ . Throughout this work and in pruning algorithm implementation, we assume that the matrix D ∗ is symmetric and has an all-zero main diagonal, i.e., L ( L -1 )/ 2 degrees of freedom.\nStep ͸: Algorithm for pruning redundant routes. Of the L ( L -1 )/ 2 route distances obtained in the previous step, a significant portion will represent indirect routes. For example, suppose that locations l 1 , l 2 , and l 3 lie on the same road in a geographical region, with location l 2 lying between the other two. The road network has an edge from l 1 to l 2 and an edge from l 2 to l 3 , but no direct edge from l 1 to l 3 . Thus, for the construction of the weight matrix D = [ di , j ] in our desired graph G ,  we need to set d 1,3 = d 3,1 = ∞ and ensure that the edge { l 1, l 3 } /negationslash ∈ E .\nIn order to detect indirect routes, we make use of the following reasoning. If l 2 lies on the same road and between l 1 and l 3 , then one may expect that d ∗ 1,2 + d ∗ 2,3 ≈ d ∗ 1,3 . In fact, in most cases we will have d ∗ 1,2 + d ∗ 2,3 > d ∗ 1,3 , because l 2 may not lie directly on the route between l 1 and l 3 . At the same time, if l 2 lies on the same road and between l 1 and l 3 , then d ∗ 1,3 will be the longest of the three routes, i.e., d ∗ 1,3 ≥ max { d ∗ 1,2 , d ∗ 2,3 } . Thus, if in a triangle of locations l i , l j , and lk with distances d ∗ i , j , d ∗ i , k , and d ∗ j , k , the largest distance is larger than the sum of the two smaller distances, then it is very likely that the largest distance corresponds to an indirect route, which subsequently is removed from G ∗ to arrive at G .\nVol.:(0123456789)\nVol:.(1234567890)\nThe pseudocode in Algorithm 1 summarises these ideas. Note that, by the restriction that i < j < k in line 2, it only operates on the upper triangle of D ∗ , since we assume that the matrix D ∗ is symmetric. Since the algorithm iterates over all L ( L -1 )( L -2 )/ 6 possible triples of locations, the computational complexity is O ( L 3 ) .\nIt is important to highlight that Algorithm 1 executes route pruning on a copy of the fully connected graph (see line 1) while checking the pruning condition on the input graph G ∗ (see line 5). Otherwise, the triplets order may impact the results of pruning and lead to incorrect conclusions. In particular, Fig. 1 illustrates an example when the natural lexicographic order of triangle traversal leads to incorrect pruning (Fig. 1b), whereas a slightly modified order produces the right answer (Fig. 1c).\nAs can be seen in line 5 of Algorithm 1, we added a parameter β in order to relax the condition posed by the triangle inequality. A value β < 1 allows removing the longest side of a triangular route even if it is slightly shorter than the sum of the two remaining routes. This makes sense if three locations lie along a road, but getting to these locations requires a short detour (e.g. getting off the highway and to the city centre before getting back on the highway). The larger β , the more conservative is our pruning algorithm. Rather than such a multiplicative relaxation, allowing the largest distance to exceed the sum of the other two distances by some percentage, an additive relaxation is possible as well, or a combination thereof (e.g. by replacing the condition in line 5 by s -d ∗ a , b > min { d ∗ a , b /β , d ∗ a , b + δ } , where δ is a tunable parameter corresponding to an absolute distance).\nThe idea of triangular pruning extends naturally to sparse or directed input graphs G ∗ = ( L , E ∗ , D ∗ ) . If the graph is directed, then E ∗ is a subset of L 2 and D ∗ need not be symmetric anymore. Such a situation can occur in cases in which distances between locations depend on the direction between them, e.g. caused by one-way streets. If the graph is sparse, then E ∗ is a proper subset of L 2 (in the directed case) or [ L ] 2 (in the undirected case). This can be caused by prior information on the road network, for example, or by adjustments made in Step 1 of our approach.\n\nProvides a method for automated construction of location graphs, utilizing routing services to compute distances between locations and pruning indirect routes based on the triangle inequality. The approach achieves precision and recall values around 0.9 for a wide range of parameters, demonstrating its validity for creating large location graphs.",
    "original_text": "We are given a set of L locations L = { l 1, . . . , lL } in a geographical region. We are interested in a weighted graph G = ( L , E , D ) with vertices L , edges E corresponding to direct routes between locations, and edge weights D corresponding to route distances that keeps only fastest (or shortest) paths between all pairs of vertices from L .\nThe problem of finding an optimal location graph can be formalised as follows. We assume a weighted, potentially  directed  route  graph G = ( L G , EG ) with LG vertices  is  given.  Each  edge e := ( u , v ) ∈ EG corresponds  to  a  route  connecting  two  locations u and v from L G and  has  a  positive-valued  weight dG ( e ) ∈ R + that corresponds to the route distance between u and v .  A path P is  a sequence of edges, e.g. P = (( u 1, u 2 ) , ( u 2, u 3 ) , . . . , ( uk -1 , uk )) =: [ u 1 -u 2 -··· -uk ] .  We denote by u 1 G /squiggleright uk the set of all feasible paths between u 1 and uk in G . The length of the shortest path between u and v is thus defined as\n<!-- formula-not-decoded -->\nFor the given subset of locations L ⊆ L G , our goal is to find a weighted graph G = ( L , E ) with a minimum number of edges such that Q ( u , v ; G ) = Q ( u , v ; G ) for all { u , v } ⊆ L . For the sake of brevity, we limit further discussion to the undirected graphs. Nevertheless, our approach straightforwardly extends to the directed graphs.\nTo create the graph G , we propose a two-step procedure. In the first step, we use a routing service to find route distances between all pairs of locations. Assuming that the distances are symmetric, we terminate with an undirected fully connected graph G ∗ = ( L , [ L ] 2 , D ∗ ) , where [ L ] 2 is the set of two-element subsets of L and where D ∗ = [ d ∗ i , j ] is the matrix of distances between locations with d ∗ i , j = d G ∗ ( { l i , l j } ) . Many of the distances computed by the route planner will correspond to indirect routes, as a route between two locations in L may pass through another location in L . Therefore, in a second step, we use the distance matrix D ∗ to identify edges in G ∗ that correspond to redundant routes, and remove them to obtain G . In this section, we will give an overview of this two-step procedure.\nStep ͷ: Creating a fully connected graph via route planning. For route planning, we rely on map data from OSM, together with the C++ routing machine from the OSRM Project (http://  proje  ct-  osrm.  org), i.e., we work with locally downloaded map data and a C++ wrapper for OSRM, allowing requests for large sets of locations L . However, any other routing service can be used, including online services for smaller sets of locations. In our experiments, we obtained pairwise distances between up to L = 18000 locations. The result is a distance matrix D ∗ = [ d ∗ i , j ] , with d ∗ i , j being the distance between locations l i and l j . If there is no route between l i and l j , then the respective distance is set to ∞ . Throughout this work and in pruning algorithm implementation, we assume that the matrix D ∗ is symmetric and has an all-zero main diagonal, i.e., L ( L -1 )/ 2 degrees of freedom.\nStep ͸: Algorithm for pruning redundant routes. Of the L ( L -1 )/ 2 route distances obtained in the previous step, a significant portion will represent indirect routes. For example, suppose that locations l 1 , l 2 , and l 3 lie on the same road in a geographical region, with location l 2 lying between the other two. The road network has an edge from l 1 to l 2 and an edge from l 2 to l 3 , but no direct edge from l 1 to l 3 . Thus, for the construction of the weight matrix D = [ di , j ] in our desired graph G ,  we need to set d 1,3 = d 3,1 = ∞ and ensure that the edge { l 1, l 3 } /negationslash ∈ E .\nIn order to detect indirect routes, we make use of the following reasoning. If l 2 lies on the same road and between l 1 and l 3 , then one may expect that d ∗ 1,2 + d ∗ 2,3 ≈ d ∗ 1,3 . In fact, in most cases we will have d ∗ 1,2 + d ∗ 2,3 > d ∗ 1,3 , because l 2 may not lie directly on the route between l 1 and l 3 . At the same time, if l 2 lies on the same road and between l 1 and l 3 , then d ∗ 1,3 will be the longest of the three routes, i.e., d ∗ 1,3 ≥ max { d ∗ 1,2 , d ∗ 2,3 } . Thus, if in a triangle of locations l i , l j , and lk with distances d ∗ i , j , d ∗ i , k , and d ∗ j , k , the largest distance is larger than the sum of the two smaller distances, then it is very likely that the largest distance corresponds to an indirect route, which subsequently is removed from G ∗ to arrive at G .\nVol.:(0123456789)\nVol:.(1234567890)\nThe pseudocode in Algorithm 1 summarises these ideas. Note that, by the restriction that i < j < k in line 2, it only operates on the upper triangle of D ∗ , since we assume that the matrix D ∗ is symmetric. Since the algorithm iterates over all L ( L -1 )( L -2 )/ 6 possible triples of locations, the computational complexity is O ( L 3 ) .\nIt is important to highlight that Algorithm 1 executes route pruning on a copy of the fully connected graph (see line 1) while checking the pruning condition on the input graph G ∗ (see line 5). Otherwise, the triplets order may impact the results of pruning and lead to incorrect conclusions. In particular, Fig. 1 illustrates an example when the natural lexicographic order of triangle traversal leads to incorrect pruning (Fig. 1b), whereas a slightly modified order produces the right answer (Fig. 1c).\nAs can be seen in line 5 of Algorithm 1, we added a parameter β in order to relax the condition posed by the triangle inequality. A value β < 1 allows removing the longest side of a triangular route even if it is slightly shorter than the sum of the two remaining routes. This makes sense if three locations lie along a road, but getting to these locations requires a short detour (e.g. getting off the highway and to the city centre before getting back on the highway). The larger β , the more conservative is our pruning algorithm. Rather than such a multiplicative relaxation, allowing the largest distance to exceed the sum of the other two distances by some percentage, an additive relaxation is possible as well, or a combination thereof (e.g. by replacing the condition in line 5 by s -d ∗ a , b > min { d ∗ a , b /β , d ∗ a , b + δ } , where δ is a tunable parameter corresponding to an absolute distance).\nThe idea of triangular pruning extends naturally to sparse or directed input graphs G ∗ = ( L , E ∗ , D ∗ ) . If the graph is directed, then E ∗ is a subset of L 2 and D ∗ need not be symmetric anymore. Such a situation can occur in cases in which distances between locations depend on the direction between them, e.g. caused by one-way streets. If the graph is sparse, then E ∗ is a proper subset of L 2 (in the directed case) or [ L ] 2 (in the undirected case). This can be caused by prior information on the road network, for example, or by adjustments made in Step 1 of our approach.",
    "context": "Provides a method for automated construction of location graphs, utilizing routing services to compute distances between locations and pruning indirect routes based on the triangle inequality. The approach achieves precision and recall values around 0.9 for a wide range of parameters, demonstrating its validity for creating large location graphs.",
    "document": "s41598-021-90943-8.pdf",
    "pages": [
      3,
      4
    ],
    "id": "c9d918d3c86d65489b6f6fa83f1f50344682d4756d2bb2677f20a1eddfb61ee8"
  },
  {
    "text": "We close this section by showing that Algorithm 1 terminates with a completely pruned graph also in settings different from the one considered here. For general graphs G ∗ , an edge { l 1, lk } is redundant if and only if there is a path P = [ l 1 -l 2 -··· -lk ] that is shorter than d ∗ 1, k . This consideration is the motivation behind the 'Global' strategies of Toivonen et al. 25 Now suppose that the graph G ∗ is complete and satisfies the triangle inequality. In other words, if P = [ l 1 -l 2 -··· -lk ] is a path in this graph, then for every vertex l j , j ∈ { 2, 3, . . . , k -1 } , we have that the length of P in G ∗ is at least d ∗ 1, j + d ∗ j , k (such as in the graph that we obtain in step 1). Then, it is apparent that the edge { l 1, lk } is redundant if and only if there is a location l j such that d ∗ 1, j + d ∗ j , k < d ∗ 1, k . This shows that for these types of graphs the 'Triangle' strategies of Toivonen et al. 25 and our Algorithm 1 are optimal.\n\nValidates the applicability of the proposed location graph construction algorithm for large geographical regions.",
    "original_text": "We close this section by showing that Algorithm 1 terminates with a completely pruned graph also in settings different from the one considered here. For general graphs G ∗ , an edge { l 1, lk } is redundant if and only if there is a path P = [ l 1 -l 2 -··· -lk ] that is shorter than d ∗ 1, k . This consideration is the motivation behind the 'Global' strategies of Toivonen et al. 25 Now suppose that the graph G ∗ is complete and satisfies the triangle inequality. In other words, if P = [ l 1 -l 2 -··· -lk ] is a path in this graph, then for every vertex l j , j ∈ { 2, 3, . . . , k -1 } , we have that the length of P in G ∗ is at least d ∗ 1, j + d ∗ j , k (such as in the graph that we obtain in step 1). Then, it is apparent that the edge { l 1, lk } is redundant if and only if there is a location l j such that d ∗ 1, j + d ∗ j , k < d ∗ 1, k . This shows that for these types of graphs the 'Triangle' strategies of Toivonen et al. 25 and our Algorithm 1 are optimal.",
    "context": "Validates the applicability of the proposed location graph construction algorithm for large geographical regions.",
    "document": "s41598-021-90943-8.pdf",
    "pages": [
      4
    ],
    "id": "d6daa53795f36c9b6b0adfad0123b491fdd79321ef7ff8b40bde8dac5c0149af"
  },
  {
    "text": "To validate our route pruning approach, understand its limitations and its dependence on the parameter β , we tested it in four geographical regions, namely the federal state of Styria in Austria, a region at the GermanAustrian border, the Central African Republic and South Sudan countries.\nFor Step 1 of our approach we relied on OSM map data downloaded from https://  downl  oad.  geofa  brik.  de and applied an offline version of OSRM to compute route distances (shortest driving time) between several location types, e.g. established cities, small towns and (temporary) refugee camps, in the four considered geographical regions. We subsequently applied Algorithm 1 for Step 2 to obtain the pruned location graph G .\nThe accuracy of Algorithm 1 w.r.t. a manually created ground truth of direct driving routes is measured in terms of Precision, Recall and F1-score. To calculate these three performance indicators, the number of True Positives (TP), False Positives (FP) and False Negatives (FN) is needed. In our study, a TP is a route that is part of the ground truth and that is also detected by the pruning algorithm, a FP is a route that is not part of the ground truth, but is labelled as a direct route by the pruning algorithm, and a FN is a route that is part of the ground truth, but pruned from the fully connected graph by the algorithm. From these, Precision, Recall, and F1-score are calculated as follows:\nPrecision\n=\nTP\nTP\nTP\n+\nFP\nRecall\n=\nTP\n+\nFN\n<!-- formula-not-decoded -->\nIn addition to computing quantitative performance measures, we visualised our results in Figs. 2,  3 and 4 (see also Supplementary Figure S1). These figures were generated using the OSMnx Python package, which is based on OSM to create, analyse, and visualise street   networks 26 .\nFigure 1. Impact of triplets order on pruning results.\nFigure 2. Connections of 15 locations in the federal state of Styria, Austria with β = 0.95 .\nCreation of the ground truth. We created the ground truth of direct driving connections for each of the four regions with OSM by inspecting if the fastest route (shortest time) between each location pair is direct. A connection between two locations is labelled direct if there is no other location on or nearby the fastest route. In most cases it was clear if a direct driving route between two locations exists, but there are also ambiguous situations (e.g., route from l 1 to l 2 is direct, but indirect from l 2 to l 1 ) and potential sources of error (e.g., small locations or refugee camps, especially in large regions, might not be marked explicitly in OSM), such that the creation of the ground truth was not straightforward (see Supplementary Note 1 for details). Even if the ground truth is created to the best of our knowledge, some uncertainty remains. Thus, the reported performance measures have to be interpreted accordingly.\nFederal state of Styria, Austria. For the region in the federal state of Styria in Austria, we extracted towns and cities within a rectangle with the geographic coordinates N 47.0 -N 47.5 and E 14.6 -E 16.0 from OSM. The OSM Overpass   API 27  returned 15 locations within this area, 14 towns and one bigger city, Graz. Therefore, the fully connected graph of this region contains 105 driving routes connecting the 15 locations. We obtained 29 direct driving routes between the 15 locations as the ground truth.\nVol.:(0123456789)\nVol:.(1234567890)\nFigure 3. Connections of 23 locations in the border region between Germany and Austria with β = 0.95 .\nFigure 4. Connections of 62 locations in the Central African Republic with β = 0.95 .\nTable 1. Results for the triangle inequality pruned graph in Styria, Austria with 15 locations.\n\n0.900, Ground truth routes = 29. 0.900, Routes after pruning = 26. 0.900, TP = 26. 0.900, FP = 0. 0.900, FN = 3. 0.900, Precision = 1.00. 0.900, Recall = 0.90. 0.900, F1-score = 0.95. 0.925, Ground truth routes = 29. 0.925, Routes after pruning = 27. 0.925, TP = 27. 0.925, FP = 0. 0.925, FN = 2. 0.925, Precision = 1.00. 0.925, Recall = 0.93. 0.925, F1-score = 0.96. 0.950, Ground truth routes = 29. 0.950, Routes after pruning = 30. 0.950, TP = 28. 0.950, FP = 2. 0.950, FN = 1. 0.950, Precision = 0.93. 0.950, Recall = 0.97. 0.950, F1-score = 0.95. 0.975, Ground truth routes = 29. 0.975, Routes after pruning = 39. 0.975, TP = 29. 0.975, FP = 10. 0.975, FN = 0. 0.975, Precision = 0.74. 0.975, Recall = 1.00. 0.975, F1-score = 0.85\nThe fully connected graph was pruned with Algorithm 1, 'Route Pruning for General Undirected Graphs' for several values of the pruning parameter β . T able 1 contains the results for the pruning parameters 0.9, 0.925, 0.95, and 0.975. For β ∈ [ 0.9, 0.95 ] , Precision, Recall and F1-Score are all above 0.9. Figure 2 visualises the results for β = 0.95 with the established ground truth and pruned connections as well as the suggested fastest driving routes.\nFor β = 0.95 , the pruning algorithm returns 30 direct driving routes between the 15 locations. 28 of the 29 ground truth routes are detected, it prunes one route that is part of the ground truth and declares two routes as direct connections that are not part of the ground truth. The route [Frohnleiten - Knittelfeld] in the central part of the region is 69 km long and is pruned from the fully connected graph, but is part of the ground truth. The\nTable 2. Results for the triangle inequality pruned graph in the border region between Germany and Austria with 23 locations.\n\n0.90, Ground truth routes = 57. 0.90, Routes after pruning = 53. 0.90, TP = 39. 0.90, FP = 14. 0.90, FN = 18. 0.90, Precision = 0.73. 0.90, Recall = 0.68. 0.90, F1-score = 0.71. 0.91, Ground truth routes = 57. 0.91, Routes after pruning = 53. 0.91, TP = 39. 0.91, FP = 14. 0.91, FN = 18. 0.91, Precision = 0.73. 0.91, Recall = 0.68. 0.91, F1-score = 0.71. 0.92, Ground truth routes = 57. 0.92, Routes after pruning = 57. 0.92, TP = 42. 0.92, FP = 15. 0.92, FN = 15. 0.92, Precision = 0.74. 0.92, Recall = 0.74. 0.92, F1-score = 0.74. 0.93, Ground truth routes = 57. 0.93, Routes after pruning = 61. 0.93, TP = 44. 0.93, FP = 17. 0.93, FN = 13. 0.93, Precision = 0.72. 0.93, Recall = 0.77. 0.93, F1-score = 0.75. 0.94, Ground truth routes = 57. 0.94, Routes after pruning = 62. 0.94, TP = 44. 0.94, FP = 18. 0.94, FN = 13. 0.94, Precision = 0.71. 0.94, Recall = 0.77. 0.94, F1-score = 0.74. 0.95, Ground truth routes = 57. 0.95, Routes after pruning = 71. 0.95, TP = 47. 0.95, FP = 24. 0.95, FN = 10. 0.95, Precision = 0.66. 0.95, Recall = 0.82. 0.95, F1-score = 0.73\n\nValidates route pruning approach; dependent on parameter β; tests in four regions.",
    "original_text": "To validate our route pruning approach, understand its limitations and its dependence on the parameter β , we tested it in four geographical regions, namely the federal state of Styria in Austria, a region at the GermanAustrian border, the Central African Republic and South Sudan countries.\nFor Step 1 of our approach we relied on OSM map data downloaded from https://  downl  oad.  geofa  brik.  de and applied an offline version of OSRM to compute route distances (shortest driving time) between several location types, e.g. established cities, small towns and (temporary) refugee camps, in the four considered geographical regions. We subsequently applied Algorithm 1 for Step 2 to obtain the pruned location graph G .\nThe accuracy of Algorithm 1 w.r.t. a manually created ground truth of direct driving routes is measured in terms of Precision, Recall and F1-score. To calculate these three performance indicators, the number of True Positives (TP), False Positives (FP) and False Negatives (FN) is needed. In our study, a TP is a route that is part of the ground truth and that is also detected by the pruning algorithm, a FP is a route that is not part of the ground truth, but is labelled as a direct route by the pruning algorithm, and a FN is a route that is part of the ground truth, but pruned from the fully connected graph by the algorithm. From these, Precision, Recall, and F1-score are calculated as follows:\nPrecision\n=\nTP\nTP\nTP\n+\nFP\nRecall\n=\nTP\n+\nFN\n<!-- formula-not-decoded -->\nIn addition to computing quantitative performance measures, we visualised our results in Figs. 2,  3 and 4 (see also Supplementary Figure S1). These figures were generated using the OSMnx Python package, which is based on OSM to create, analyse, and visualise street   networks 26 .\nFigure 1. Impact of triplets order on pruning results.\nFigure 2. Connections of 15 locations in the federal state of Styria, Austria with β = 0.95 .\nCreation of the ground truth. We created the ground truth of direct driving connections for each of the four regions with OSM by inspecting if the fastest route (shortest time) between each location pair is direct. A connection between two locations is labelled direct if there is no other location on or nearby the fastest route. In most cases it was clear if a direct driving route between two locations exists, but there are also ambiguous situations (e.g., route from l 1 to l 2 is direct, but indirect from l 2 to l 1 ) and potential sources of error (e.g., small locations or refugee camps, especially in large regions, might not be marked explicitly in OSM), such that the creation of the ground truth was not straightforward (see Supplementary Note 1 for details). Even if the ground truth is created to the best of our knowledge, some uncertainty remains. Thus, the reported performance measures have to be interpreted accordingly.\nFederal state of Styria, Austria. For the region in the federal state of Styria in Austria, we extracted towns and cities within a rectangle with the geographic coordinates N 47.0 -N 47.5 and E 14.6 -E 16.0 from OSM. The OSM Overpass   API 27  returned 15 locations within this area, 14 towns and one bigger city, Graz. Therefore, the fully connected graph of this region contains 105 driving routes connecting the 15 locations. We obtained 29 direct driving routes between the 15 locations as the ground truth.\nVol.:(0123456789)\nVol:.(1234567890)\nFigure 3. Connections of 23 locations in the border region between Germany and Austria with β = 0.95 .\nFigure 4. Connections of 62 locations in the Central African Republic with β = 0.95 .\nTable 1. Results for the triangle inequality pruned graph in Styria, Austria with 15 locations.\n\n0.900, Ground truth routes = 29. 0.900, Routes after pruning = 26. 0.900, TP = 26. 0.900, FP = 0. 0.900, FN = 3. 0.900, Precision = 1.00. 0.900, Recall = 0.90. 0.900, F1-score = 0.95. 0.925, Ground truth routes = 29. 0.925, Routes after pruning = 27. 0.925, TP = 27. 0.925, FP = 0. 0.925, FN = 2. 0.925, Precision = 1.00. 0.925, Recall = 0.93. 0.925, F1-score = 0.96. 0.950, Ground truth routes = 29. 0.950, Routes after pruning = 30. 0.950, TP = 28. 0.950, FP = 2. 0.950, FN = 1. 0.950, Precision = 0.93. 0.950, Recall = 0.97. 0.950, F1-score = 0.95. 0.975, Ground truth routes = 29. 0.975, Routes after pruning = 39. 0.975, TP = 29. 0.975, FP = 10. 0.975, FN = 0. 0.975, Precision = 0.74. 0.975, Recall = 1.00. 0.975, F1-score = 0.85\nThe fully connected graph was pruned with Algorithm 1, 'Route Pruning for General Undirected Graphs' for several values of the pruning parameter β . T able 1 contains the results for the pruning parameters 0.9, 0.925, 0.95, and 0.975. For β ∈ [ 0.9, 0.95 ] , Precision, Recall and F1-Score are all above 0.9. Figure 2 visualises the results for β = 0.95 with the established ground truth and pruned connections as well as the suggested fastest driving routes.\nFor β = 0.95 , the pruning algorithm returns 30 direct driving routes between the 15 locations. 28 of the 29 ground truth routes are detected, it prunes one route that is part of the ground truth and declares two routes as direct connections that are not part of the ground truth. The route [Frohnleiten - Knittelfeld] in the central part of the region is 69 km long and is pruned from the fully connected graph, but is part of the ground truth. The\nTable 2. Results for the triangle inequality pruned graph in the border region between Germany and Austria with 23 locations.\n\n0.90, Ground truth routes = 57. 0.90, Routes after pruning = 53. 0.90, TP = 39. 0.90, FP = 14. 0.90, FN = 18. 0.90, Precision = 0.73. 0.90, Recall = 0.68. 0.90, F1-score = 0.71. 0.91, Ground truth routes = 57. 0.91, Routes after pruning = 53. 0.91, TP = 39. 0.91, FP = 14. 0.91, FN = 18. 0.91, Precision = 0.73. 0.91, Recall = 0.68. 0.91, F1-score = 0.71. 0.92, Ground truth routes = 57. 0.92, Routes after pruning = 57. 0.92, TP = 42. 0.92, FP = 15. 0.92, FN = 15. 0.92, Precision = 0.74. 0.92, Recall = 0.74. 0.92, F1-score = 0.74. 0.93, Ground truth routes = 57. 0.93, Routes after pruning = 61. 0.93, TP = 44. 0.93, FP = 17. 0.93, FN = 13. 0.93, Precision = 0.72. 0.93, Recall = 0.77. 0.93, F1-score = 0.75. 0.94, Ground truth routes = 57. 0.94, Routes after pruning = 62. 0.94, TP = 44. 0.94, FP = 18. 0.94, FN = 13. 0.94, Precision = 0.71. 0.94, Recall = 0.77. 0.94, F1-score = 0.74. 0.95, Ground truth routes = 57. 0.95, Routes after pruning = 71. 0.95, TP = 47. 0.95, FP = 24. 0.95, FN = 10. 0.95, Precision = 0.66. 0.95, Recall = 0.82. 0.95, F1-score = 0.73",
    "context": "Validates route pruning approach; dependent on parameter β; tests in four regions.",
    "document": "s41598-021-90943-8.pdf",
    "pages": [
      4,
      5,
      6,
      7
    ],
    "id": "0a1c7ee2005dedae1016a00485591382bf415ff871e624695b8f1f28b33cba7b"
  },
  {
    "text": "Table 3. Results for the triangle inequality pruned graph for the Central African Republic with 62 locations.\n\n0.80, Ground truth routes = 146. 0.80, Routes after pruning = 104. 0.80, TP = 104. 0.80, FP = 0. 0.80, FN = 42. 0.80, Precision = 1.00. 0.80, Recall = 0.71. 0.80, F1-score = 0.83. 0.85, Ground truth routes = 146. 0.85, Routes after pruning = 116. 0.85, TP = 115. 0.85, FP = 1. 0.85, FN = 31. 0.85, Precision = 0.99. 0.85, Recall = 0.79. 0.85, F1-score = 0.88. 0.90, Ground truth routes = 146. 0.90, Routes after pruning = 126. 0.90, TP = 124. 0.90, FP = 2. 0.90, FN = 22. 0.90, Precision = 0.98. 0.90, Recall = 0.85. 0.90, F1-score = 0.91. 0.95, Ground truth routes = 146. 0.95, Routes after pruning = 149. 0.95, TP = 138. 0.95, FP = 11. 0.95, FN = 8. 0.95, Precision = 0.93. 0.95, Recall = 0.95. 0.95, F1-score = 0.94. 0.99, Ground truth routes = 146. 0.99, Routes after pruning = 193. 0.99, TP = 146. 0.99, FP = 47. 0.99, FN = 0. 0.99, Precision = 0.76. 0.99, Recall = 1.00. 0.99, F1-score = 0.86\nalgorithm detects the route [Frohnleiten - Leoben - Knittelfeld], which totals 72 km, and the route [Frohnleiten - Bruck an der Mur - Knittelfeld] which totals 71 km. As 72 · 0.95 = 68.4 < 69 , the route [Frohnleiten - Knittelfeld] is pruned from the fully connected graph.\nFor this β , the algorithm also keeps two routes of the fully connected graph that are not part of the established ground truth. The first one is the route [Frohnleiten - Graz] in the southern part which passes by the location Gratwein-Straßengel on a highway, but not directly through the location. For this route, one could argue that it is direct because it does not go through the location, but we decided to not include it in the ground truth as the highway passes Gratwein-Straßengel very close by. The second route that the algorithm labels as direct, but that is not part of the ground truth, is the connection [Bruck an der Mur - Trofaiach] in the north. It is 26 km long and goes directly through Leoben, but not through the marked OSM position of Leoben. The distances of the respective single routes [Bruck an der Mur - Leoben] and [Leoben - Trofaiach] are 16 km and 12 km and add up to a total distance of 28 km. As 28 · 0.95 = 26.6 > 26 , the algorithm declares the route [Bruck an der Mur Trofaiach] as a direct one for β = 0.95 .\nBorder region between Germany and Austria. For the region around the German-Austrian border near Salzburg, we extracted towns and cities within a rectangular region that has the geographic coordinates N 47.6 -N 47.9 and E 12.0 -E 13.1 with the OSM Overpass API. This region has 23 locations, 22 towns and one bigger city, Salzburg. 12 locations are in Germany and 11 are in Austria. We computed the driving distance between each pair of locations with OSRM, which resulted in 253 driving routes, and established 57 direct routes connecting the 23 locations as the ground truth. The results for the pruning parameters between 0.90 and 0.95 are listed in Table 2 and the region is visualised for β = 0.95 in Fig. 3. The best F1-score is obtained with β = 0.93 ,  while the best balance between Precision and Recall is obtained with β = 0.92 .  In terms of the F1-score, the results for smaller and larger values of β are still similar.\nThe area around the location Rosenheim in the western part of the region causes problems. The locations are connected via the fastest driving route (shortest time) and therefore they are often connected via the highway ' Autobahn A8' . Using this road is the fastest connection between two locations in terms of time, but it is not the shortest route in terms of distance. For instance, the fastest route between the two locations Kolbermoor (west of Rosenheim) and Prien am Chiemsee (east of Rosenheim) is 33 km long and it takes 30 min via the Autobahn A8 according to OSM. An alternative route that takes more time uses the shortest distance between the two locations and passes directly through Rosenheim. The first intermediary route [Kolbermoor - Rosenheim] is a 6.1 km long country road that takes 11 min to drive. The second intermediary route [Rosenheim - Prien am Chiemsee] is a 21 km long country road that takes 22 min. Adding the two intermediary distances and driving times equals 27.1 km and 33 min, respectively, compared to the fastest driving route with 33 km and 30 min. The route [Kolbermoor - Prien am Chiemsee] will therefore always be removed from the fully connected graph by the route pruning algorithm, independent of the pruning parameter β < 1 , even though a direct, faster route exists.\nCentral African Republic and neighbouring locations. As a third region, we chose a conflict scenario in the Central African Republic (CAR) which includes cities, towns and several refugee camps in CAR and in neighbouring countries. The 62 locations of this region are within the geographic coordinates N 2 -N 10.5 and E 13 -E 27 ,  and the fully connected graph consists of 62 nodes and 1891 edges. For the ground truth, we detected 146 direct routes connecting the 62 locations. Table 3 summarises the results for the pruning param-\nVol.:(0123456789)\nVol:.(1234567890)\nTable 4. Results for the triangle inequality pruned graph for the South Sudan case study with 93 locations.\n\n0.80, Ground truth routes = 178. 0.80, Routes after pruning = 134. 0.80, TP = 134. 0.80, FP = 0. 0.80, FN = 44. 0.80, Precision = 1.00. 0.80, Recall = 0.75. 0.80, F1-score = 0.86. 0.85, Ground truth routes = 178. 0.85, Routes after pruning = 149. 0.85, TP = 146. 0.85, FP = 3. 0.85, FN = 32. 0.85, Precision = 0.98. 0.85, Recall = 0.82. 0.85, F1-score = 0.89. 0.90, Ground truth routes = 178. 0.90, Routes after pruning = 162. 0.90, TP = 156. 0.90, FP = 6. 0.90, FN = 22. 0.90, Precision = 0.96. 0.90, Recall = 0.88. 0.90, F1-score = 0.92. 0.95, Ground truth routes = 178. 0.95, Routes after pruning = 197. 0.95, TP = 169. 0.95, FP = 28. 0.95, FN = 9. 0.95, Precision = 0.86. 0.95, Recall = 0.95. 0.95, F1-score = 0.90. 0.99, Ground truth routes = 178. 0.99, Routes after pruning = 326. 0.99, TP = 175. 0.99, FP = 151. 0.99, FN = 3. 0.99, Precision = 0.53. 0.99, Recall = 0.98. 0.99, F1-score = 0.69\neters 0.80, 0.85, 0.90, 0.95, and 0.99. The pruning parameter β = 0.95 (see Fig. 4) returned the best result for this region with Precision, Recall and F1-score all above 0.9. After applying the route pruning algorithm on the fully connected graph, 149 routes are labelled as direct connections. 138 routes that are part of the ground truth are detected by the algorithm, 8 routes that are in the ground truth are not labelled as direct routes and 11 routes that are not part of the ground truth are labelled as direct routes by the algorithm.\n\nThe chunk describes the results of applying a route pruning algorithm to a graph of locations, specifically focusing on the Central African Republic case study. The algorithm, based on the triangle inequality, reduces the number of direct routes while maintaining a high precision (1.00) and recall (0.75) with an F1-score of 0.86.  The algorithm identifies and removes indirect routes, leading to 149 direct routes after pruning, compared to 178 in the ground truth.",
    "original_text": "Table 3. Results for the triangle inequality pruned graph for the Central African Republic with 62 locations.\n\n0.80, Ground truth routes = 146. 0.80, Routes after pruning = 104. 0.80, TP = 104. 0.80, FP = 0. 0.80, FN = 42. 0.80, Precision = 1.00. 0.80, Recall = 0.71. 0.80, F1-score = 0.83. 0.85, Ground truth routes = 146. 0.85, Routes after pruning = 116. 0.85, TP = 115. 0.85, FP = 1. 0.85, FN = 31. 0.85, Precision = 0.99. 0.85, Recall = 0.79. 0.85, F1-score = 0.88. 0.90, Ground truth routes = 146. 0.90, Routes after pruning = 126. 0.90, TP = 124. 0.90, FP = 2. 0.90, FN = 22. 0.90, Precision = 0.98. 0.90, Recall = 0.85. 0.90, F1-score = 0.91. 0.95, Ground truth routes = 146. 0.95, Routes after pruning = 149. 0.95, TP = 138. 0.95, FP = 11. 0.95, FN = 8. 0.95, Precision = 0.93. 0.95, Recall = 0.95. 0.95, F1-score = 0.94. 0.99, Ground truth routes = 146. 0.99, Routes after pruning = 193. 0.99, TP = 146. 0.99, FP = 47. 0.99, FN = 0. 0.99, Precision = 0.76. 0.99, Recall = 1.00. 0.99, F1-score = 0.86\nalgorithm detects the route [Frohnleiten - Leoben - Knittelfeld], which totals 72 km, and the route [Frohnleiten - Bruck an der Mur - Knittelfeld] which totals 71 km. As 72 · 0.95 = 68.4 < 69 , the route [Frohnleiten - Knittelfeld] is pruned from the fully connected graph.\nFor this β , the algorithm also keeps two routes of the fully connected graph that are not part of the established ground truth. The first one is the route [Frohnleiten - Graz] in the southern part which passes by the location Gratwein-Straßengel on a highway, but not directly through the location. For this route, one could argue that it is direct because it does not go through the location, but we decided to not include it in the ground truth as the highway passes Gratwein-Straßengel very close by. The second route that the algorithm labels as direct, but that is not part of the ground truth, is the connection [Bruck an der Mur - Trofaiach] in the north. It is 26 km long and goes directly through Leoben, but not through the marked OSM position of Leoben. The distances of the respective single routes [Bruck an der Mur - Leoben] and [Leoben - Trofaiach] are 16 km and 12 km and add up to a total distance of 28 km. As 28 · 0.95 = 26.6 > 26 , the algorithm declares the route [Bruck an der Mur Trofaiach] as a direct one for β = 0.95 .\nBorder region between Germany and Austria. For the region around the German-Austrian border near Salzburg, we extracted towns and cities within a rectangular region that has the geographic coordinates N 47.6 -N 47.9 and E 12.0 -E 13.1 with the OSM Overpass API. This region has 23 locations, 22 towns and one bigger city, Salzburg. 12 locations are in Germany and 11 are in Austria. We computed the driving distance between each pair of locations with OSRM, which resulted in 253 driving routes, and established 57 direct routes connecting the 23 locations as the ground truth. The results for the pruning parameters between 0.90 and 0.95 are listed in Table 2 and the region is visualised for β = 0.95 in Fig. 3. The best F1-score is obtained with β = 0.93 ,  while the best balance between Precision and Recall is obtained with β = 0.92 .  In terms of the F1-score, the results for smaller and larger values of β are still similar.\nThe area around the location Rosenheim in the western part of the region causes problems. The locations are connected via the fastest driving route (shortest time) and therefore they are often connected via the highway ' Autobahn A8' . Using this road is the fastest connection between two locations in terms of time, but it is not the shortest route in terms of distance. For instance, the fastest route between the two locations Kolbermoor (west of Rosenheim) and Prien am Chiemsee (east of Rosenheim) is 33 km long and it takes 30 min via the Autobahn A8 according to OSM. An alternative route that takes more time uses the shortest distance between the two locations and passes directly through Rosenheim. The first intermediary route [Kolbermoor - Rosenheim] is a 6.1 km long country road that takes 11 min to drive. The second intermediary route [Rosenheim - Prien am Chiemsee] is a 21 km long country road that takes 22 min. Adding the two intermediary distances and driving times equals 27.1 km and 33 min, respectively, compared to the fastest driving route with 33 km and 30 min. The route [Kolbermoor - Prien am Chiemsee] will therefore always be removed from the fully connected graph by the route pruning algorithm, independent of the pruning parameter β < 1 , even though a direct, faster route exists.\nCentral African Republic and neighbouring locations. As a third region, we chose a conflict scenario in the Central African Republic (CAR) which includes cities, towns and several refugee camps in CAR and in neighbouring countries. The 62 locations of this region are within the geographic coordinates N 2 -N 10.5 and E 13 -E 27 ,  and the fully connected graph consists of 62 nodes and 1891 edges. For the ground truth, we detected 146 direct routes connecting the 62 locations. Table 3 summarises the results for the pruning param-\nVol.:(0123456789)\nVol:.(1234567890)\nTable 4. Results for the triangle inequality pruned graph for the South Sudan case study with 93 locations.\n\n0.80, Ground truth routes = 178. 0.80, Routes after pruning = 134. 0.80, TP = 134. 0.80, FP = 0. 0.80, FN = 44. 0.80, Precision = 1.00. 0.80, Recall = 0.75. 0.80, F1-score = 0.86. 0.85, Ground truth routes = 178. 0.85, Routes after pruning = 149. 0.85, TP = 146. 0.85, FP = 3. 0.85, FN = 32. 0.85, Precision = 0.98. 0.85, Recall = 0.82. 0.85, F1-score = 0.89. 0.90, Ground truth routes = 178. 0.90, Routes after pruning = 162. 0.90, TP = 156. 0.90, FP = 6. 0.90, FN = 22. 0.90, Precision = 0.96. 0.90, Recall = 0.88. 0.90, F1-score = 0.92. 0.95, Ground truth routes = 178. 0.95, Routes after pruning = 197. 0.95, TP = 169. 0.95, FP = 28. 0.95, FN = 9. 0.95, Precision = 0.86. 0.95, Recall = 0.95. 0.95, F1-score = 0.90. 0.99, Ground truth routes = 178. 0.99, Routes after pruning = 326. 0.99, TP = 175. 0.99, FP = 151. 0.99, FN = 3. 0.99, Precision = 0.53. 0.99, Recall = 0.98. 0.99, F1-score = 0.69\neters 0.80, 0.85, 0.90, 0.95, and 0.99. The pruning parameter β = 0.95 (see Fig. 4) returned the best result for this region with Precision, Recall and F1-score all above 0.9. After applying the route pruning algorithm on the fully connected graph, 149 routes are labelled as direct connections. 138 routes that are part of the ground truth are detected by the algorithm, 8 routes that are in the ground truth are not labelled as direct routes and 11 routes that are not part of the ground truth are labelled as direct routes by the algorithm.",
    "context": "The chunk describes the results of applying a route pruning algorithm to a graph of locations, specifically focusing on the Central African Republic case study. The algorithm, based on the triangle inequality, reduces the number of direct routes while maintaining a high precision (1.00) and recall (0.75) with an F1-score of 0.86.  The algorithm identifies and removes indirect routes, leading to 149 direct routes after pruning, compared to 178 in the ground truth.",
    "document": "s41598-021-90943-8.pdf",
    "pages": [
      8,
      7
    ],
    "id": "caa7fb6d4814c2714368b9654bc2b3a8e78a7f2b27b0f7cec486d1a664e6127d"
  },
  {
    "text": "In 3 of the 8 direct routes that were not detected by the algorithm, the location Mbile in the southwestern part of the region is involved, which is only 11 km away from the location Lolo. For instance, the route [Baboua - Mbile] is direct with a distance of 299 km. Adding up the distances of the routes [Baboua - Lolo] with 295 km and [Lolo - Mbile] with 11 km results in a total distance of 306 km. As 306 · 0.95 = 290.7 < 299 , the route between Baboua and Mbile is pruned by the algorithm. For two other undetected direct routes, the distance is over 600 km. In the remaining three cases, direct connections between the two locations exist, but there are indirect routes that are only slightly longer.\nFor 5 of the 11 FPs, the routes go through the location Mbres, which is in the eastern part. The geographic coordinates of this location are off such that the five routes go through the location itself, but not the marked position in OSM. In the other 6 cases, the actual driving route is very close to other locations, such that they were not labelled as direct driving routes for the ground truth.\nSouth Sudan, Africa and locations in neighbouring countries. The fourth examined region is a conflict scenario in South Sudan, Africa, including several locations in neighbouring countries. The geographic coordinates of this region are approximately N 1 -N 16 and E 25 -E 35 and the fully connected graph has a total of 93 locations which are connected by 4278 edges. The ground truth of direct driving connections was created in two steps. In the first step, we obtained 142 direct routes connecting the 93 locations. There were several potential sources of error in the creation of the ground truth, especially for a region with many locations and several small refugee camps that are not marked explicitly in OSM. Thus, after considering the results of our automated location graph construction approach, this initial version of a ground truth was revisited. In this second pass, we discovered 178 direct routes between the locations and updated the ground truth by adding 46 direct routes and removing 10 routes that were found to be indirect.\nIn Table 4, we summarise the results for the pruning parameters 0.8, 0.85, 0.9, 0.95 and 0.99 with the updated ground truth. The pruning parameter β = 0.95 returned an F1-score over 0.9 with precision 0.86 and recall 0.95. After applying the route pruning algorithm on the fully connected graph, 197 routes were labelled as direct connections, of which 169 routes are also in the ground truth. 9 routes in the ground truth were missed by the pruning algorithm (see Supplementary Figure S1).\nFor 9 of the 28 FPs, the route between the locations goes directly through another location in OSM. In most of these cases, the route does not go through the marked position of the intermediate location, but through the location itself such that these routes were labelled as indirect. The offset of the position marker adds enough distance to get a different result when applying Algorithm 1. For 17 connections, there is a third location nearby the route that is suggested by OSM such that they were not labelled as direct for the ground truth. The distance between locations is sometimes relatively big with more than 300 km. In such a case, if there was a location near the road (which, for these large distances can still be several kilometres), we declared this route as indirect. We might have been too conservative in the creation of the ground truth by labelling these routes as indirect. Thus, some of these 17 routes are worth discussing and could potentially also be part of the ground truth. For the remaining two connections, it was not perfectly clear if the routes are direct or indirect, as both involve a region where three refugee camps are within a small area (eastern part of the region). In both cases it was decided to label the routes as indirect, since they have a third location nearby the road that is taken, but one can also argue that they are actually direct.\nBesides the 28 FPs, there are also 9 FNs. This could on the one side be due to some wrong entries in the ground truth (routes added that should not be in the ground truth) or due to the large distance between most of the locations pairs. For 7 instances, the distance between the locations is more than 700 km. In these cases another location could be relatively far off the route, but the pruning algorithm will eliminate it. One of these 7 routes is the connection [Rubkona - South_Darfur], which is 1434 km long in our records. It is therefore sufficient to find a third intermediary location that increases the total distance to less than 1509 km to not label it as a direct route with β = 0.95 . Here, the location East_Darfur is causing the issue. The distance [Rubkona - East_Darfur] is 471 km and [East_Darfur - South_Darfur] is 954 km. Adding up those two gives a total distance of 1425 km, which is smaller than 1509 km such that the connection is removed. The remaining 2 routes were pruned because there is another location nearby the route.\nTable 5. Performance of the triangular pruning for the undirected routing table.\n\nSouth Sudan (all settlements), Number of locations = 1783. South Sudan (all settlements), Routing = 3.15 s. South Sudan (all settlements), Pruning.Serial = 1.74 s. South Sudan (all settlements), Pruning.128 cores = 43.38 ms. Africa (cities/towns), Number of locations = 9807. Africa (cities/towns), Routing = 130.01 s. Africa (cities/towns), Pruning.Serial = 585.89 s. Africa (cities/towns), Pruning.128 cores = 62.22 s. Australia &Oceania (cities/towns), Number of locations = 1693. Australia &Oceania (cities/towns), Routing = 119.38 s. Australia &Oceania (cities/towns), Pruning.Serial = -. Australia &Oceania (cities/towns), Pruning.128 cores = 49.78 ms. Europe (cities/towns), Number of locations = 18,091. Europe (cities/towns), Routing = 5.01 h. Europe (cities/towns), Pruning.Serial = -. Europe (cities/towns), Pruning.128 cores = 316.36 s. North America (cities/towns), Number of locations = 9959. North America (cities/towns), Routing = 3.06 h. North America (cities/towns), Pruning.Serial = -. North America (cities/towns), Pruning.128 cores = 72.78 s. South America (cities/towns), Number of locations = 8591. South America (cities/towns), Routing = 29.38m. South America (cities/towns), Pruning.Serial = -. South America (cities/towns), Pruning.128 cores = 71.72 s. Central America (cities/towns), Number of locations = 1948. Central America (cities/towns), Routing = 66.303 s. Central America (cities/towns), Pruning.Serial = -. Central America (cities/towns), Pruning.128 cores = 41.12 ms\nRuntime. To evaluate the performance of our approach, we benchmarked the multi-threaded C++ implementation of Algorithm 1 with naïve round-robin parallelization on a single Hewlett Packard Enterprise's (HPE) Apollo node. The HPE Apollo system is equipped with two 64-core AMD EPYC 7742 CPUs and 256GB DRAM. The codes are available at https://  github.  com/  djgro  en/  Extra  ctMap. In this benchmark, they were compiled with GCC 9.3 and linked against the latest version of OSRM C++ library, available from the master branch in the official GitHub repository of OSRM back-end. The distance matrix is calculated with the contraction hierarchies (CH) algorithm. The benchmark is performed on input OSM maps, downloaded from https://  downl  oad.  geofa brik.  de. Locations correspond to the settlements from the OSM maps tagged with place equal to city , town , or village .\nIn Table 5, we summarise results of the benchmark on the level of countries and continents. Despite cubic complexity, Algorithm 1 performs well on the real world applications. We also demonstrate that our implementation of Algorithm 1 in Table 5 allows to construct location graphs for ∼ 10k locations on the route networks of the entire continents in reasonable time. In all benchmarks, the multi-core implementation of the pruning step takes order of magnitude less time than the construction of the distance matrix where we used highly optimized multi-threaded OSRM library.\nNote that, similar to Floyd-Warshall all-pairs shortest path   algorithm 28 , Algorithm 1 enables applying cacheoblivious 29 and communication-avoiding 30 speed-up techniques to give better cache locality and reduce communication complexity of the basic algorithm. Moreover, since in contrast to Floyd-Warshall, Algorithm 1 is embarrassingly parallel in terms of triangle traversal, it has higher potential for improving cache locality and reducing communication costs.\n\nProvides a detailed breakdown of the challenges in creating a ground truth for the location graph, highlighting issues like ambiguous routes, refugee camps with limited OSM data, and the impact of indirect routes on pruning accuracy.",
    "original_text": "In 3 of the 8 direct routes that were not detected by the algorithm, the location Mbile in the southwestern part of the region is involved, which is only 11 km away from the location Lolo. For instance, the route [Baboua - Mbile] is direct with a distance of 299 km. Adding up the distances of the routes [Baboua - Lolo] with 295 km and [Lolo - Mbile] with 11 km results in a total distance of 306 km. As 306 · 0.95 = 290.7 < 299 , the route between Baboua and Mbile is pruned by the algorithm. For two other undetected direct routes, the distance is over 600 km. In the remaining three cases, direct connections between the two locations exist, but there are indirect routes that are only slightly longer.\nFor 5 of the 11 FPs, the routes go through the location Mbres, which is in the eastern part. The geographic coordinates of this location are off such that the five routes go through the location itself, but not the marked position in OSM. In the other 6 cases, the actual driving route is very close to other locations, such that they were not labelled as direct driving routes for the ground truth.\nSouth Sudan, Africa and locations in neighbouring countries. The fourth examined region is a conflict scenario in South Sudan, Africa, including several locations in neighbouring countries. The geographic coordinates of this region are approximately N 1 -N 16 and E 25 -E 35 and the fully connected graph has a total of 93 locations which are connected by 4278 edges. The ground truth of direct driving connections was created in two steps. In the first step, we obtained 142 direct routes connecting the 93 locations. There were several potential sources of error in the creation of the ground truth, especially for a region with many locations and several small refugee camps that are not marked explicitly in OSM. Thus, after considering the results of our automated location graph construction approach, this initial version of a ground truth was revisited. In this second pass, we discovered 178 direct routes between the locations and updated the ground truth by adding 46 direct routes and removing 10 routes that were found to be indirect.\nIn Table 4, we summarise the results for the pruning parameters 0.8, 0.85, 0.9, 0.95 and 0.99 with the updated ground truth. The pruning parameter β = 0.95 returned an F1-score over 0.9 with precision 0.86 and recall 0.95. After applying the route pruning algorithm on the fully connected graph, 197 routes were labelled as direct connections, of which 169 routes are also in the ground truth. 9 routes in the ground truth were missed by the pruning algorithm (see Supplementary Figure S1).\nFor 9 of the 28 FPs, the route between the locations goes directly through another location in OSM. In most of these cases, the route does not go through the marked position of the intermediate location, but through the location itself such that these routes were labelled as indirect. The offset of the position marker adds enough distance to get a different result when applying Algorithm 1. For 17 connections, there is a third location nearby the route that is suggested by OSM such that they were not labelled as direct for the ground truth. The distance between locations is sometimes relatively big with more than 300 km. In such a case, if there was a location near the road (which, for these large distances can still be several kilometres), we declared this route as indirect. We might have been too conservative in the creation of the ground truth by labelling these routes as indirect. Thus, some of these 17 routes are worth discussing and could potentially also be part of the ground truth. For the remaining two connections, it was not perfectly clear if the routes are direct or indirect, as both involve a region where three refugee camps are within a small area (eastern part of the region). In both cases it was decided to label the routes as indirect, since they have a third location nearby the road that is taken, but one can also argue that they are actually direct.\nBesides the 28 FPs, there are also 9 FNs. This could on the one side be due to some wrong entries in the ground truth (routes added that should not be in the ground truth) or due to the large distance between most of the locations pairs. For 7 instances, the distance between the locations is more than 700 km. In these cases another location could be relatively far off the route, but the pruning algorithm will eliminate it. One of these 7 routes is the connection [Rubkona - South_Darfur], which is 1434 km long in our records. It is therefore sufficient to find a third intermediary location that increases the total distance to less than 1509 km to not label it as a direct route with β = 0.95 . Here, the location East_Darfur is causing the issue. The distance [Rubkona - East_Darfur] is 471 km and [East_Darfur - South_Darfur] is 954 km. Adding up those two gives a total distance of 1425 km, which is smaller than 1509 km such that the connection is removed. The remaining 2 routes were pruned because there is another location nearby the route.\nTable 5. Performance of the triangular pruning for the undirected routing table.\n\nSouth Sudan (all settlements), Number of locations = 1783. South Sudan (all settlements), Routing = 3.15 s. South Sudan (all settlements), Pruning.Serial = 1.74 s. South Sudan (all settlements), Pruning.128 cores = 43.38 ms. Africa (cities/towns), Number of locations = 9807. Africa (cities/towns), Routing = 130.01 s. Africa (cities/towns), Pruning.Serial = 585.89 s. Africa (cities/towns), Pruning.128 cores = 62.22 s. Australia &Oceania (cities/towns), Number of locations = 1693. Australia &Oceania (cities/towns), Routing = 119.38 s. Australia &Oceania (cities/towns), Pruning.Serial = -. Australia &Oceania (cities/towns), Pruning.128 cores = 49.78 ms. Europe (cities/towns), Number of locations = 18,091. Europe (cities/towns), Routing = 5.01 h. Europe (cities/towns), Pruning.Serial = -. Europe (cities/towns), Pruning.128 cores = 316.36 s. North America (cities/towns), Number of locations = 9959. North America (cities/towns), Routing = 3.06 h. North America (cities/towns), Pruning.Serial = -. North America (cities/towns), Pruning.128 cores = 72.78 s. South America (cities/towns), Number of locations = 8591. South America (cities/towns), Routing = 29.38m. South America (cities/towns), Pruning.Serial = -. South America (cities/towns), Pruning.128 cores = 71.72 s. Central America (cities/towns), Number of locations = 1948. Central America (cities/towns), Routing = 66.303 s. Central America (cities/towns), Pruning.Serial = -. Central America (cities/towns), Pruning.128 cores = 41.12 ms\nRuntime. To evaluate the performance of our approach, we benchmarked the multi-threaded C++ implementation of Algorithm 1 with naïve round-robin parallelization on a single Hewlett Packard Enterprise's (HPE) Apollo node. The HPE Apollo system is equipped with two 64-core AMD EPYC 7742 CPUs and 256GB DRAM. The codes are available at https://  github.  com/  djgro  en/  Extra  ctMap. In this benchmark, they were compiled with GCC 9.3 and linked against the latest version of OSRM C++ library, available from the master branch in the official GitHub repository of OSRM back-end. The distance matrix is calculated with the contraction hierarchies (CH) algorithm. The benchmark is performed on input OSM maps, downloaded from https://  downl  oad.  geofa brik.  de. Locations correspond to the settlements from the OSM maps tagged with place equal to city , town , or village .\nIn Table 5, we summarise results of the benchmark on the level of countries and continents. Despite cubic complexity, Algorithm 1 performs well on the real world applications. We also demonstrate that our implementation of Algorithm 1 in Table 5 allows to construct location graphs for ∼ 10k locations on the route networks of the entire continents in reasonable time. In all benchmarks, the multi-core implementation of the pruning step takes order of magnitude less time than the construction of the distance matrix where we used highly optimized multi-threaded OSRM library.\nNote that, similar to Floyd-Warshall all-pairs shortest path   algorithm 28 , Algorithm 1 enables applying cacheoblivious 29 and communication-avoiding 30 speed-up techniques to give better cache locality and reduce communication complexity of the basic algorithm. Moreover, since in contrast to Floyd-Warshall, Algorithm 1 is embarrassingly parallel in terms of triangle traversal, it has higher potential for improving cache locality and reducing communication costs.",
    "context": "Provides a detailed breakdown of the challenges in creating a ground truth for the location graph, highlighting issues like ambiguous routes, refugee camps with limited OSM data, and the impact of indirect routes on pruning accuracy.",
    "document": "s41598-021-90943-8.pdf",
    "pages": [
      8,
      9
    ],
    "id": "4a013cc134fe6eb5839552168fc18dc8b654cee1bac0cdfea6c16cd6aabd808a"
  },
  {
    "text": "In this work, we produce optimal location graphs by proposing a computationally efficient two-step approach: in the first step, pairwise distances between locations of interest are computed with state-of-the-art batched shortest path algorithms, such as MLD or CH in a time complexity of O (( | EG | + LG log LG ) L ) . In the second step, these pairwise paths are then pruned with Algorithm 1 in a time complexity of O ( L 3 ) .\nIntroducing the parameter β to Algorithm 1 further adds flexibility to our approach, making it applicable to both lossy edge pruning ( 0 < β < 1 ) in the spirit of Zhou et al. 9,10 or the creation of location graphs with additional indirect routes ( β > 1 ). As our results show, the location graphs constructed using our two-step approach agree well with manually created location graphs. In three of the four case studies we achieved F1-scores exceeding 0.9, and the runtime of the pruning algorithm is still acceptable even for thousands of locations, for which a manual creation of the location graph would be infeasible.\nWe have made the general observation that small values of β lead to strong pruning, i.e., large Precision and, if direct routes are removed, small Recall. In contrast, large values of β imply conservative pruning, resulting in large Recall and, if too many indirect routes are kept, small Precision (this will continue to hold naturally if β exceeds 1). While we have observed that the highest F1-scores are achieved for β ∈ [ 0.9, 0.95 ] in all four scenarios, the optimal value depends not only on the geographical region (and the degree to which a road network is established), but also on the type of locations (major cities vs. small villages). This dependence on the general road infrastructure is also reflected in the runtime experiments (in Table 5), which show vastly different routing times for Africa, South America, and North America despite similar numbers of locations.\nWe have observed that, even with careful tuning of β , the resulting location graph may still differ from a manually created ground truth. Especially for routes with a long distance between a location pair, the multiplicative factor β may result in pruned direct routes if a third location is close to this direct route. We have seen such examples in the CAR and the South Sudan case studies. We believe that similar considerations will hold for routes with short distances if the multiplicative factor is replaced by an additive factor, as suggested at the end of the Methods section. Therefore, the selection of these hyperparameters always has to be guided by the application setup (structure of the road network and distribution of locations), application requirements (sparse and lossy or dense and redundant location graphs), and by results from cross-validation.\nHowever, we believe that such inaccuracies do not appear as roadblocks in many of the applications for which location graphs are required. Considering the example of forced migration simulation with agent-based models from Suleimenova et al. 8 , the existence of indirect routes in G is less problematic than missing routes, ensuring that the location graph is connected. Moreover, considering the multi-graph nature of the actual road\nVol.:(0123456789)\nnetwork and the fact that the algorithm may prune direct routes when locations are close to each other or close to a direct connection, we argue that these errors are acceptable as long as the path distance between a set of locations in G is within a reasonable range to the actual road distance between these locations, cf. Eq. (1). Since some of the mentioned limitations are also shared by other graph pruning   algorithms 9,10,25 , we are convinced that the improved computational complexity, the added flexibility due to the hyperparameter β , and the remarkable performance of our approach as confirmed in our experimental study present a valid contribution.\nReceived: 21 January 2021; Accepted: 17 May 2021\n\nProposed two-step approach for creating optimal location graphs: first, pairwise distances are computed using state-of-the-art shortest path algorithms; second, Algorithm 1 is applied for pruning, with the parameter β offering flexibility for both lossy edge pruning and creating location graphs with indirect routes.",
    "original_text": "In this work, we produce optimal location graphs by proposing a computationally efficient two-step approach: in the first step, pairwise distances between locations of interest are computed with state-of-the-art batched shortest path algorithms, such as MLD or CH in a time complexity of O (( | EG | + LG log LG ) L ) . In the second step, these pairwise paths are then pruned with Algorithm 1 in a time complexity of O ( L 3 ) .\nIntroducing the parameter β to Algorithm 1 further adds flexibility to our approach, making it applicable to both lossy edge pruning ( 0 < β < 1 ) in the spirit of Zhou et al. 9,10 or the creation of location graphs with additional indirect routes ( β > 1 ). As our results show, the location graphs constructed using our two-step approach agree well with manually created location graphs. In three of the four case studies we achieved F1-scores exceeding 0.9, and the runtime of the pruning algorithm is still acceptable even for thousands of locations, for which a manual creation of the location graph would be infeasible.\nWe have made the general observation that small values of β lead to strong pruning, i.e., large Precision and, if direct routes are removed, small Recall. In contrast, large values of β imply conservative pruning, resulting in large Recall and, if too many indirect routes are kept, small Precision (this will continue to hold naturally if β exceeds 1). While we have observed that the highest F1-scores are achieved for β ∈ [ 0.9, 0.95 ] in all four scenarios, the optimal value depends not only on the geographical region (and the degree to which a road network is established), but also on the type of locations (major cities vs. small villages). This dependence on the general road infrastructure is also reflected in the runtime experiments (in Table 5), which show vastly different routing times for Africa, South America, and North America despite similar numbers of locations.\nWe have observed that, even with careful tuning of β , the resulting location graph may still differ from a manually created ground truth. Especially for routes with a long distance between a location pair, the multiplicative factor β may result in pruned direct routes if a third location is close to this direct route. We have seen such examples in the CAR and the South Sudan case studies. We believe that similar considerations will hold for routes with short distances if the multiplicative factor is replaced by an additive factor, as suggested at the end of the Methods section. Therefore, the selection of these hyperparameters always has to be guided by the application setup (structure of the road network and distribution of locations), application requirements (sparse and lossy or dense and redundant location graphs), and by results from cross-validation.\nHowever, we believe that such inaccuracies do not appear as roadblocks in many of the applications for which location graphs are required. Considering the example of forced migration simulation with agent-based models from Suleimenova et al. 8 , the existence of indirect routes in G is less problematic than missing routes, ensuring that the location graph is connected. Moreover, considering the multi-graph nature of the actual road\nVol.:(0123456789)\nnetwork and the fact that the algorithm may prune direct routes when locations are close to each other or close to a direct connection, we argue that these errors are acceptable as long as the path distance between a set of locations in G is within a reasonable range to the actual road distance between these locations, cf. Eq. (1). Since some of the mentioned limitations are also shared by other graph pruning   algorithms 9,10,25 , we are convinced that the improved computational complexity, the added flexibility due to the hyperparameter β , and the remarkable performance of our approach as confirmed in our experimental study present a valid contribution.\nReceived: 21 January 2021; Accepted: 17 May 2021",
    "context": "Proposed two-step approach for creating optimal location graphs: first, pairwise distances are computed using state-of-the-art shortest path algorithms; second, Algorithm 1 is applied for pruning, with the parameter β offering flexibility for both lossy edge pruning and creating location graphs with indirect routes.",
    "document": "s41598-021-90943-8.pdf",
    "pages": [
      9,
      10
    ],
    "id": "1bfbc896c41045ae6031808618a734889d7a5d76de93e26e592b4e2f7d3a30d0"
  },
  {
    "text": "1.  Veenendaal, B. Eras of web mapping developments: Past, present and future. International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences , Vol. XLI-B4, 247-252 (2016).\n2.  Démare, T., Bertelle, C., Dutot, A. & Lévêque, L. Modeling logistic systems with an agent-based model and dynamic graphs. J. Transp. Geogr. 62 , 51-65 (2017).\n3.  Carver, S. & Quincey, D. A conceptual design of spatio-temporal agent-based model for volcanic evacuation. Systems 5 , 53 (2017).\n4.  Zhu, Y., Xie, K., Ozbay, K. & Yang, H. Hurricane evacuation modeling using behavior models and scenario-driven agent-based simulations. Procedia Comput. Sci. 130 , 836-843 (2018).\n5.  Zhao, B., Kumar, K., Casey, G. & Soga, K. Agent-based model (ABM) for city-scale traffic simulation: A case study on San Francisco. In International Conference on Smart Infrastructure and Construction (ICSIC) Driving data-informed decision-making , 203-212 (ICE Publishing, 2019).\n6.  Mahmood, I. et al. FACS: A geospatial agent-based simulator for analysing COVID-19 spread and public health measures on local regions. J. Simul. 1-19 (2020).\n7.  Kerridge, J., Hine, J. & Wigan, M. Agent-based modelling of pedestrian movements: The questions that need to be asked and answered. Environ. Plan. B Plan. Des. 28 , 327-341 (2001).\n8.  Suleimenova, D., Bell, D. & Groen, D. A generalized simulation development approach for predicting refugee destinations. Sci. Rep. 7 , 13377 (2017).\n9.  Zhou, F., Malher, S. & Toivonen, H. Network simplification with minimal loss of connectivity. In 2010 IEEE International Conference on Data Mining , 659-668 (2010).\n10.  Zhou, F., Mahler, S. & Toivonen, H. Simplification of networks by edge pruning. In Bisociative Knowledge Discovery: An Introduction to Concept, Algorithms, Tools, and Applications (ed. Berthold M. R.) 179-198 (Springer, 2012).\n11.  Madkour, A., Aref, W. G., Rehman, F., Rahman, M. A. & Basalamah, S. A survey of shortest-path algorithms. Available at https:// arxiv.  org/  abs/  1705.  02044 (2017).\n12.  Bast, H. et al. Route planning in transportation networks. In Algorithm Engineering: Selected Results and Surveys (eds Kliemann, L. & Sanders, P.) 19-80 (Springer International Publishing, 2016).\n13.  Dijkstra, E. W . A note on two problems in connection with graphs. Numerische mathematik 1 , 269-271 (1959).\n14.  Holzer, M., Schulz, F., Wagner, D. & Willhalm, T. Combining speed-up techniques for shortest-path computations. J. Exp. Algorithm. 10 , 2-5 (2005).\n15.  Luxen, D. & Vetter, C. Real-time routing with OpenStreetMap data. In Proceedings of the 19th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems , 513-516 (Association for Computing Machinery, 2011).\n16.  Delling, D. & Werneck, R. F. Patent US 2013/0231862 A1: Customizable route planning. Available at https://  paten  ts.  google.  com/ patent/  US201  30231  862A1/ (2013).\n17.  Delling, D., Goldberg, A. V ., Pajor, T. & Werneck, R. F. Customizable route planning in road networks. Transp. Sci. 51 , 566-591 (2017).\n18.  Schild, A. & Sommer, C. On balanced separators in road networks. In Experimental Algorithms (ed. Bampis, E.) 286-297 (Springer International Publishing, 2015).\n19.  Geisberger, R., Sanders, P., Schultes, D. & Delling, D. Contraction hierarchies: Faster and simpler hierarchical routing in road networks. In Experimental Algorithms (ed. McGeoch, C. C.) 319-333 (Springer, 2008).\n20.  Geisberger, R., Sanders, P ., Schultes, D. & Vetter, C. Exact routing in large road networks using contraction hierarchies. Transp. Sci. 46 , 388-404 (2012).\n21.  Ruan, N., Jin, R. & Huang, Y. Distance preserving graph simplification. In 2011 IEEE 11th International Conference on Data Mining , 1200-1205 (2011).\n22.  Mengiste, S. A., Aertsen, A. & Kumar, A. Effect of edge pruning on structural controllability and observability of complex networks. Sci. Rep. 5 , 18145 (2015).\n23.  Sumith, N., Annappa, B. & Bhattacharya, S. Social network pruning for building optimal social network: A user perspective. Knowl.-Based Syst. 117 , 101-110 (2017).\n24.  Reza, T., Ripeanu, M., Tripoul, N., Sanders, G. & Pearce, R. PruneJuice: Pruning trillion-edge graphs to a precise pattern-matching solution. In Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis , Vol. 21, 1-17 (IEEE Press, 2018).\n25.  Toivonen, H., Mahler, S. & Zhou, F. A framework for path-oriented network simplification. In Advances in Intelligent Data Analysis IX (eds Cohen, P. R. et al. ) 220-231 (Springer, 2010).\n26.  Boeing, G. OSMnx: New methods for acquiring, constructing, analyzing, and visualizing complex street networks. Comput. Environ. Urban Syst. 65 , 126-139 (2017).\n27.  OpenStreetMap. Available at https://  www.  opens  treet  map.  org.\n28.  Floyd, R. W. Algorithm 97: Shortest path. Commun. ACM 5 , 345 (1962).\n29.  Park, J. S., Penner, M. & Prasanna, V . K. Optimizing graph algorithms for improved cache performance. IEEE Trans. Parallel Distrib. Syst. 15 , 769-782 (2004).\n30.  Solomonik, E., Buluc, A. & Demmel, J. Minimizing communication in all-pairs shortest paths. In Proceedings of the 2013 IEEE 27th International Symposium on Parallel and Distributed Processing , 548-559 (IEEE Computer Society, 2013).\n\nProvides a literature review of various modeling approaches for agent-based simulations, including hurricane evacuation, traffic simulation, and social network analysis.",
    "original_text": "1.  Veenendaal, B. Eras of web mapping developments: Past, present and future. International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences , Vol. XLI-B4, 247-252 (2016).\n2.  Démare, T., Bertelle, C., Dutot, A. & Lévêque, L. Modeling logistic systems with an agent-based model and dynamic graphs. J. Transp. Geogr. 62 , 51-65 (2017).\n3.  Carver, S. & Quincey, D. A conceptual design of spatio-temporal agent-based model for volcanic evacuation. Systems 5 , 53 (2017).\n4.  Zhu, Y., Xie, K., Ozbay, K. & Yang, H. Hurricane evacuation modeling using behavior models and scenario-driven agent-based simulations. Procedia Comput. Sci. 130 , 836-843 (2018).\n5.  Zhao, B., Kumar, K., Casey, G. & Soga, K. Agent-based model (ABM) for city-scale traffic simulation: A case study on San Francisco. In International Conference on Smart Infrastructure and Construction (ICSIC) Driving data-informed decision-making , 203-212 (ICE Publishing, 2019).\n6.  Mahmood, I. et al. FACS: A geospatial agent-based simulator for analysing COVID-19 spread and public health measures on local regions. J. Simul. 1-19 (2020).\n7.  Kerridge, J., Hine, J. & Wigan, M. Agent-based modelling of pedestrian movements: The questions that need to be asked and answered. Environ. Plan. B Plan. Des. 28 , 327-341 (2001).\n8.  Suleimenova, D., Bell, D. & Groen, D. A generalized simulation development approach for predicting refugee destinations. Sci. Rep. 7 , 13377 (2017).\n9.  Zhou, F., Malher, S. & Toivonen, H. Network simplification with minimal loss of connectivity. In 2010 IEEE International Conference on Data Mining , 659-668 (2010).\n10.  Zhou, F., Mahler, S. & Toivonen, H. Simplification of networks by edge pruning. In Bisociative Knowledge Discovery: An Introduction to Concept, Algorithms, Tools, and Applications (ed. Berthold M. R.) 179-198 (Springer, 2012).\n11.  Madkour, A., Aref, W. G., Rehman, F., Rahman, M. A. & Basalamah, S. A survey of shortest-path algorithms. Available at https:// arxiv.  org/  abs/  1705.  02044 (2017).\n12.  Bast, H. et al. Route planning in transportation networks. In Algorithm Engineering: Selected Results and Surveys (eds Kliemann, L. & Sanders, P.) 19-80 (Springer International Publishing, 2016).\n13.  Dijkstra, E. W . A note on two problems in connection with graphs. Numerische mathematik 1 , 269-271 (1959).\n14.  Holzer, M., Schulz, F., Wagner, D. & Willhalm, T. Combining speed-up techniques for shortest-path computations. J. Exp. Algorithm. 10 , 2-5 (2005).\n15.  Luxen, D. & Vetter, C. Real-time routing with OpenStreetMap data. In Proceedings of the 19th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems , 513-516 (Association for Computing Machinery, 2011).\n16.  Delling, D. & Werneck, R. F. Patent US 2013/0231862 A1: Customizable route planning. Available at https://  paten  ts.  google.  com/ patent/  US201  30231  862A1/ (2013).\n17.  Delling, D., Goldberg, A. V ., Pajor, T. & Werneck, R. F. Customizable route planning in road networks. Transp. Sci. 51 , 566-591 (2017).\n18.  Schild, A. & Sommer, C. On balanced separators in road networks. In Experimental Algorithms (ed. Bampis, E.) 286-297 (Springer International Publishing, 2015).\n19.  Geisberger, R., Sanders, P., Schultes, D. & Delling, D. Contraction hierarchies: Faster and simpler hierarchical routing in road networks. In Experimental Algorithms (ed. McGeoch, C. C.) 319-333 (Springer, 2008).\n20.  Geisberger, R., Sanders, P ., Schultes, D. & Vetter, C. Exact routing in large road networks using contraction hierarchies. Transp. Sci. 46 , 388-404 (2012).\n21.  Ruan, N., Jin, R. & Huang, Y. Distance preserving graph simplification. In 2011 IEEE 11th International Conference on Data Mining , 1200-1205 (2011).\n22.  Mengiste, S. A., Aertsen, A. & Kumar, A. Effect of edge pruning on structural controllability and observability of complex networks. Sci. Rep. 5 , 18145 (2015).\n23.  Sumith, N., Annappa, B. & Bhattacharya, S. Social network pruning for building optimal social network: A user perspective. Knowl.-Based Syst. 117 , 101-110 (2017).\n24.  Reza, T., Ripeanu, M., Tripoul, N., Sanders, G. & Pearce, R. PruneJuice: Pruning trillion-edge graphs to a precise pattern-matching solution. In Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis , Vol. 21, 1-17 (IEEE Press, 2018).\n25.  Toivonen, H., Mahler, S. & Zhou, F. A framework for path-oriented network simplification. In Advances in Intelligent Data Analysis IX (eds Cohen, P. R. et al. ) 220-231 (Springer, 2010).\n26.  Boeing, G. OSMnx: New methods for acquiring, constructing, analyzing, and visualizing complex street networks. Comput. Environ. Urban Syst. 65 , 126-139 (2017).\n27.  OpenStreetMap. Available at https://  www.  opens  treet  map.  org.\n28.  Floyd, R. W. Algorithm 97: Shortest path. Commun. ACM 5 , 345 (1962).\n29.  Park, J. S., Penner, M. & Prasanna, V . K. Optimizing graph algorithms for improved cache performance. IEEE Trans. Parallel Distrib. Syst. 15 , 769-782 (2004).\n30.  Solomonik, E., Buluc, A. & Demmel, J. Minimizing communication in all-pairs shortest paths. In Proceedings of the 2013 IEEE 27th International Symposium on Parallel and Distributed Processing , 548-559 (IEEE Computer Society, 2013).",
    "context": "Provides a literature review of various modeling approaches for agent-based simulations, including hurricane evacuation, traffic simulation, and social network analysis.",
    "document": "s41598-021-90943-8.pdf",
    "pages": [
      10
    ],
    "id": "1d360cb576c8d6eb69a39fa653f7d2200749a55d68496f33fc7d990bdf29056c"
  },
  {
    "text": "The presented work was developed in the project HPC and Big Data Technologies for Global Systems (HiDALGO), under grant agreement No.824115. The Know-Center is funded within the Austrian COMET Program-Competence Centers for Excellent Technologies-under the auspices of the Austrian Federal Ministry for Climate Action, Environment, Energy, Mobility, Innovation and Technology, the Austrian Federal Ministry\nScientific Reports\n|        (2021) 11:11547  |\nVol:.(1234567890)\nfor Digital and Economic Affairs and by the State of Styria. COMET is managed by the Austrian Research Promotion Agency FFG.\nMap data copyrighted OpenStreetMap contributors and available from https://  www.  opens  treet  map.  org.\n\nDetails the funding and data sources for the research project.",
    "original_text": "The presented work was developed in the project HPC and Big Data Technologies for Global Systems (HiDALGO), under grant agreement No.824115. The Know-Center is funded within the Austrian COMET Program-Competence Centers for Excellent Technologies-under the auspices of the Austrian Federal Ministry for Climate Action, Environment, Energy, Mobility, Innovation and Technology, the Austrian Federal Ministry\nScientific Reports\n|        (2021) 11:11547  |\nVol:.(1234567890)\nfor Digital and Economic Affairs and by the State of Styria. COMET is managed by the Austrian Research Promotion Agency FFG.\nMap data copyrighted OpenStreetMap contributors and available from https://  www.  opens  treet  map.  org.",
    "context": "Details the funding and data sources for the research project.",
    "document": "s41598-021-90943-8.pdf",
    "pages": [
      10,
      11
    ],
    "id": "7c6cbfc4f0168a3a650dbf80134428f2ab640815eb988489a9c773986d02760c"
  },
  {
    "text": "C.S. and B.C.G. conceived the algorithm, conducted experiments, analysed the results and wrote the manuscript. M.W. and S.G. prepared the source code. S.G. also conducted performance measurements, formalised the problem and wrote the manuscript. D.S. coordinated the study and wrote the manuscript. I.M. and A.J. contributed to discussions and reviewed the manuscript. D.G. assessed the manuscript and participated in its thorough revision. All authors reviewed the manuscript.\n\nDetails the collaborative process of algorithm development, including experimental design, code preparation, performance analysis, and manuscript writing.",
    "original_text": "C.S. and B.C.G. conceived the algorithm, conducted experiments, analysed the results and wrote the manuscript. M.W. and S.G. prepared the source code. S.G. also conducted performance measurements, formalised the problem and wrote the manuscript. D.S. coordinated the study and wrote the manuscript. I.M. and A.J. contributed to discussions and reviewed the manuscript. D.G. assessed the manuscript and participated in its thorough revision. All authors reviewed the manuscript.",
    "context": "Details the collaborative process of algorithm development, including experimental design, code preparation, performance analysis, and manuscript writing.",
    "document": "s41598-021-90943-8.pdf",
    "pages": [
      11
    ],
    "id": "df1793d1e56f5ee9f6ed45f50a68374e41602f3ed6aa3de4032e463d45b7ae90"
  },
  {
    "text": "The authors declare no competing interests.\n\nStates that the authors have no competing interests.",
    "original_text": "The authors declare no competing interests.",
    "context": "States that the authors have no competing interests.",
    "document": "s41598-021-90943-8.pdf",
    "pages": [
      11
    ],
    "id": "9ba9761df944344679e71f50f98a25479ec835067d14e2f3d453b7d21c5f9a4f"
  },
  {
    "text": "Supplementary Information The online version contains supplementary material available at https://  doi.  org/ 10.  1038/  s41598-  021-  90943-8.\nCorrespondence and requests for materials should be addressed to B.C.G. or D.S.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nOpen Access This  article  is  licensed  under  a  Creative  Commons  Attribution  4.0  International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://  creat  iveco  mmons.  org/  licen  ses/  by/4.  0/.\n© The Author(s) 2021\nVol.:(0123456789)\n\nDetails the licensing, contact information, and reprint/permissions details for the article.",
    "original_text": "Supplementary Information The online version contains supplementary material available at https://  doi.  org/ 10.  1038/  s41598-  021-  90943-8.\nCorrespondence and requests for materials should be addressed to B.C.G. or D.S.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nOpen Access This  article  is  licensed  under  a  Creative  Commons  Attribution  4.0  International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://  creat  iveco  mmons.  org/  licen  ses/  by/4.  0/.\n© The Author(s) 2021\nVol.:(0123456789)",
    "context": "Details the licensing, contact information, and reprint/permissions details for the article.",
    "document": "s41598-021-90943-8.pdf",
    "pages": [
      11
    ],
    "id": "4c23b793f9d940b02b96ed6425d27ac6e4573c3548eb0a9f96908d5c1c5d9bcc"
  },
  {
    "text": "Yuwang Wang 1 ,  Yang Liu 1 , Jinli Suo 1 , Guohai Situ 2 , Chang Qiao 1 & Qionghai Dai 1\nComputational ghost imaging (CGI) achieves single-pixel imaging by using a Spatial Light Modulator (SLM) to generate structured illuminations for spatially resolved information encoding. The imaging speed of CGI is limited by the modulation frequency of available SLMs, and sets back its practical applications. This paper proposes to bypass this limitation by trading off SLM's redundant spatial resolution for multiplication of the modulation frequency. Specifically, a pair of galvanic mirrors sweeping across the high resolution SLM multiply the modulation frequency within the spatial resolution gap between SLM and the final reconstruction. A proof-of-principle setup with two middle end galvanic mirrors achieves ghost imaging as fast as 42 Hz at 80 × 80-pixel resolution, 5 times faster than state-of-the-arts, and holds potential for one magnitude further multiplication by hardware upgrading. Our approach brings a significant improvement in the imaging speed of ghost imaging and pushes ghost imaging towards practical applications.\nOriginated in quantum optics 1,2 , recently ghost imaging draws wide attentions due to that the single-pixel imaging scheme can acts as a viable alternative when array sensors are unavailable, too expensive or of poor performance. After evolution to classical light then to computational imaging scheme 3 , ghost imaging has been studied widely in the past decade. Due to the intrinsic trading-time-for-spatial-resolvability mechanism of single-pixel imaging, the imaging speed and quality of ghost imaging is closely related to the number of illumination patterns for encoding the scene information, given a specific spatial resolution. To raise the imaging speed and quality, researchers have made great efforts to suppress sensor noise 4,5 or optimize the reconstruction algorithm 6-9 . Differently, other researchers utilize the information from spatial structure of the target scene to project scene adaptive patterns 10 .\nAll of the work on computational ghost imaging use a SLM working at tens or hundreds hertz to generate illumination patterns, and the low coding efficiency highly limits the imaging speed and quality and sets back putting ghost imaging into practical use. Recently, researchers attempt to capture dynamic scenes under the single pixel scheme by making full use of the modulation speed of currently available fastest SLM. Matthew et al. 11 achieve 10 Hz 32 × 32-pixel imaging or 2.5 Hz 64 × 64 pixel imaging, with a digital mirror device (DMD) allowing binary patterns to be preloaded and displayed at a maximum rate of 20.7 kHz, which is the toplimit for the modulation speed of current SLM. Suo et al. 12 propose a self synchronized scheme for easy use of such high speed SLM. Later, together with a reconstruction algorithm utilizing spatio-temporal redundancies of nature scenes, Li et al. 13 achieve 16 Hz 64 × 64-pixel computational ghost imaging. In spite of all these great efforts, the illumination patterning is still not fast enough and largely limits the imaging speed of computational ghost imaging.\nWe notice that the spatial resolution of computational ghost imaging (typically less than 100 × 100 pixels) is much lower than that of SLM (usually higher than 800 × 600 pixels). Unitizing this gap, we propose a sweeping based approach to multiply the illumination modulation speed, by trading off relatively redundant spatial resolution of SLM for a much faster modulation. During the duration of each DMD pattern, we modulate the illumination by driving it sweeping the DMD subregions to produce a series of low resolution patterns.\nThe scheme is sketched in Fig. 1, two galvanic mirrors rotating around vertical axes are used to build a horizontal scanning device. These two mirrors are kept parallel to each other during rotating, so the beam leaving GM2 keeps parallel to the beam entering GM1 and hits the DMD at the same incident angle, which enables the DMD reflecting back the beam exactly in the opposite direction. In this way, the beam entering and leaving GM1 are along the same line but with opposite directions, and the patterns at different positions of DMD are aligned and in the same propagation direction (see Supplementary Video). In our setup, the illumination pattern is jointly determined by the pattern shown on the DMD and the position of scanning beam on the DMD. The position of\n1 Department of Automation, Tsinghua University, Beijing, 100084, China.  2 Shanghai Institute of Optics and Fine Mechanics, Shanghai, 201800, China. Correspondence and requests for materials should be addressed to J.S. (email: jlsuo@tsinghua.edu.cn)\nr eceived: 01 November 2016 a ccepted: 22 February 2017 P ublished: 30 March 2017\nFigure 1. (Better view in color version) Experimental setup for sweeping based high speed ghost imaging\nsystem. The laser beam is first collimated and expanded by lens #1 (L1) and lens #2 (L2), then enter the sweeping system which consists of galvanic mirror #1 (GM1) and galvanic mirror #2 (GM2). As the two galvanic mirrors rotate around the vertical axes, the beam sweeping the DMD along the horizontal direction. The beam is coded and reflected back by the digital mirror device (DMD), through GM2 and GM1, then spitted by the beam splitter (BS) and projected onto the scene after being magnified to proper size by lens #3 (L3), and the beam is shaped by a square aperture (SA). The beam entering and leaving the DMD are labeled in light green and light orange colors respectively. Light encoding the scene information is collected by lens (L4) and then recorded by a photodiode (PD). The PD output and GM2's rotating position are digitized by an analogto-digital conversion card on the computer for further processing. GM1 and GM2 are set to rotate at the same frequency, amplitude and phase, which ensures the beam between GM2 and DMD being parallel with the beam entering GM1.\nthe scanning beam on the DMD is determined by the angle of the galvanic mirror, which can be read out from the output voltage from galvanic mirror servo board. We calibrate the mapping between the scanning position and the output voltage of galvanic mirrors by displaying specific patterns and measuring the photodiode output. The detailed description of this calibration method is in Sec. Methods. As for the light source, we use a 532 nm green laser (DJ532-40 from Thorlabs), and the resolution of DMD (Texas Instrument DLP Discovery 4100, 7XGA) is 1024 × 768 pixels with maximum of 20 k hertz projection of binary patterns. GM1 and GM2 are both single axis scanning devices (GVS011 from Thorlabs).\nIn our setup, the coded patterns are generated by scanning the DMD, and the final modulation frequency of our system is\n<!-- formula-not-decoded -->\nwhere Fg is the scanning frequency of the two galvanic mirrors GM1 and GM2, and ∆ x is the scanning range of the beam on the DMD (The detailed description of this variable is in Sec. Methods), b is the binning number of DMD mirrors in one direction, and δ is the size of each micro-mirror of the DMD. In our setup, Fg is set to be 200 Hz, and the scanning distance ∆ x is 6.6 mm. The smallest b we are using is 2, because imaging at b = 1 requires a much more precise mechanical and optical mounting. The size of the DMD mirror is 13.6 µ m. Therefore, the binary pattern modulation speed of our system is 97 kHz, which is about 5 times faster than that of the fastest DMD. Please note that our pattern modulation speed is highly limited by the galvanic mirror's working frequency Fg , for some high-end galvanic mirrors with the working frequency achieving 1 kHz e.g. CTI 6200 H, the speed of the same pixel resolution will be as high as 485 kHz. The pixel resolution is mainly determined by the size of the scanning beam, given the specifications of DMD. In our setup, the diameter of the beam is 4mm, and the largest pixel resolution we can achieve is about 200 × 200.\n\nIntroduces a novel approach to accelerate ghost imaging by trading spatial resolution for modulation frequency, utilizing sweeping galvanic mirrors to multiply the SLM's modulation speed.",
    "original_text": "Yuwang Wang 1 ,  Yang Liu 1 , Jinli Suo 1 , Guohai Situ 2 , Chang Qiao 1 & Qionghai Dai 1\nComputational ghost imaging (CGI) achieves single-pixel imaging by using a Spatial Light Modulator (SLM) to generate structured illuminations for spatially resolved information encoding. The imaging speed of CGI is limited by the modulation frequency of available SLMs, and sets back its practical applications. This paper proposes to bypass this limitation by trading off SLM's redundant spatial resolution for multiplication of the modulation frequency. Specifically, a pair of galvanic mirrors sweeping across the high resolution SLM multiply the modulation frequency within the spatial resolution gap between SLM and the final reconstruction. A proof-of-principle setup with two middle end galvanic mirrors achieves ghost imaging as fast as 42 Hz at 80 × 80-pixel resolution, 5 times faster than state-of-the-arts, and holds potential for one magnitude further multiplication by hardware upgrading. Our approach brings a significant improvement in the imaging speed of ghost imaging and pushes ghost imaging towards practical applications.\nOriginated in quantum optics 1,2 , recently ghost imaging draws wide attentions due to that the single-pixel imaging scheme can acts as a viable alternative when array sensors are unavailable, too expensive or of poor performance. After evolution to classical light then to computational imaging scheme 3 , ghost imaging has been studied widely in the past decade. Due to the intrinsic trading-time-for-spatial-resolvability mechanism of single-pixel imaging, the imaging speed and quality of ghost imaging is closely related to the number of illumination patterns for encoding the scene information, given a specific spatial resolution. To raise the imaging speed and quality, researchers have made great efforts to suppress sensor noise 4,5 or optimize the reconstruction algorithm 6-9 . Differently, other researchers utilize the information from spatial structure of the target scene to project scene adaptive patterns 10 .\nAll of the work on computational ghost imaging use a SLM working at tens or hundreds hertz to generate illumination patterns, and the low coding efficiency highly limits the imaging speed and quality and sets back putting ghost imaging into practical use. Recently, researchers attempt to capture dynamic scenes under the single pixel scheme by making full use of the modulation speed of currently available fastest SLM. Matthew et al. 11 achieve 10 Hz 32 × 32-pixel imaging or 2.5 Hz 64 × 64 pixel imaging, with a digital mirror device (DMD) allowing binary patterns to be preloaded and displayed at a maximum rate of 20.7 kHz, which is the toplimit for the modulation speed of current SLM. Suo et al. 12 propose a self synchronized scheme for easy use of such high speed SLM. Later, together with a reconstruction algorithm utilizing spatio-temporal redundancies of nature scenes, Li et al. 13 achieve 16 Hz 64 × 64-pixel computational ghost imaging. In spite of all these great efforts, the illumination patterning is still not fast enough and largely limits the imaging speed of computational ghost imaging.\nWe notice that the spatial resolution of computational ghost imaging (typically less than 100 × 100 pixels) is much lower than that of SLM (usually higher than 800 × 600 pixels). Unitizing this gap, we propose a sweeping based approach to multiply the illumination modulation speed, by trading off relatively redundant spatial resolution of SLM for a much faster modulation. During the duration of each DMD pattern, we modulate the illumination by driving it sweeping the DMD subregions to produce a series of low resolution patterns.\nThe scheme is sketched in Fig. 1, two galvanic mirrors rotating around vertical axes are used to build a horizontal scanning device. These two mirrors are kept parallel to each other during rotating, so the beam leaving GM2 keeps parallel to the beam entering GM1 and hits the DMD at the same incident angle, which enables the DMD reflecting back the beam exactly in the opposite direction. In this way, the beam entering and leaving GM1 are along the same line but with opposite directions, and the patterns at different positions of DMD are aligned and in the same propagation direction (see Supplementary Video). In our setup, the illumination pattern is jointly determined by the pattern shown on the DMD and the position of scanning beam on the DMD. The position of\n1 Department of Automation, Tsinghua University, Beijing, 100084, China.  2 Shanghai Institute of Optics and Fine Mechanics, Shanghai, 201800, China. Correspondence and requests for materials should be addressed to J.S. (email: jlsuo@tsinghua.edu.cn)\nr eceived: 01 November 2016 a ccepted: 22 February 2017 P ublished: 30 March 2017\nFigure 1. (Better view in color version) Experimental setup for sweeping based high speed ghost imaging\nsystem. The laser beam is first collimated and expanded by lens #1 (L1) and lens #2 (L2), then enter the sweeping system which consists of galvanic mirror #1 (GM1) and galvanic mirror #2 (GM2). As the two galvanic mirrors rotate around the vertical axes, the beam sweeping the DMD along the horizontal direction. The beam is coded and reflected back by the digital mirror device (DMD), through GM2 and GM1, then spitted by the beam splitter (BS) and projected onto the scene after being magnified to proper size by lens #3 (L3), and the beam is shaped by a square aperture (SA). The beam entering and leaving the DMD are labeled in light green and light orange colors respectively. Light encoding the scene information is collected by lens (L4) and then recorded by a photodiode (PD). The PD output and GM2's rotating position are digitized by an analogto-digital conversion card on the computer for further processing. GM1 and GM2 are set to rotate at the same frequency, amplitude and phase, which ensures the beam between GM2 and DMD being parallel with the beam entering GM1.\nthe scanning beam on the DMD is determined by the angle of the galvanic mirror, which can be read out from the output voltage from galvanic mirror servo board. We calibrate the mapping between the scanning position and the output voltage of galvanic mirrors by displaying specific patterns and measuring the photodiode output. The detailed description of this calibration method is in Sec. Methods. As for the light source, we use a 532 nm green laser (DJ532-40 from Thorlabs), and the resolution of DMD (Texas Instrument DLP Discovery 4100, 7XGA) is 1024 × 768 pixels with maximum of 20 k hertz projection of binary patterns. GM1 and GM2 are both single axis scanning devices (GVS011 from Thorlabs).\nIn our setup, the coded patterns are generated by scanning the DMD, and the final modulation frequency of our system is\n<!-- formula-not-decoded -->\nwhere Fg is the scanning frequency of the two galvanic mirrors GM1 and GM2, and ∆ x is the scanning range of the beam on the DMD (The detailed description of this variable is in Sec. Methods), b is the binning number of DMD mirrors in one direction, and δ is the size of each micro-mirror of the DMD. In our setup, Fg is set to be 200 Hz, and the scanning distance ∆ x is 6.6 mm. The smallest b we are using is 2, because imaging at b = 1 requires a much more precise mechanical and optical mounting. The size of the DMD mirror is 13.6 µ m. Therefore, the binary pattern modulation speed of our system is 97 kHz, which is about 5 times faster than that of the fastest DMD. Please note that our pattern modulation speed is highly limited by the galvanic mirror's working frequency Fg , for some high-end galvanic mirrors with the working frequency achieving 1 kHz e.g. CTI 6200 H, the speed of the same pixel resolution will be as high as 485 kHz. The pixel resolution is mainly determined by the size of the scanning beam, given the specifications of DMD. In our setup, the diameter of the beam is 4mm, and the largest pixel resolution we can achieve is about 200 × 200.",
    "context": "Introduces a novel approach to accelerate ghost imaging by trading spatial resolution for modulation frequency, utilizing sweeping galvanic mirrors to multiply the SLM's modulation speed.",
    "document": "srep45325.pdf",
    "pages": [
      1,
      2
    ],
    "id": "955d58e1ed48bae0818eee811eabcd6d4fff3554362df0c201b980984ef69a21"
  },
  {
    "text": "From Eq. 1, the final illumination patterning speed is independent of the frequency of DMD, but the setting of the DMD's working frequency is non-trivial. In order to avoid repetitive coding patterns during sweeping of the galvanic mirror pair, the pattern elapse of the DMD should be shorter than the half period of the galvanic mirror. In other words, we can set the DMD at any frame rate between 2 Fg and its maximum frequency. However, because\nFigure 2. Simulation results on the parameter setting of DMD's frame rate, specifically the ratio k between the frame rates of galvanic mirror and DMD. ( a ) visualizes the performance under varying k values at different noise levels with the largest RMSE being 0.44 and smallest RMSE being 0.15. ( b and c ) displays some exemplar reconstructed binary and gray-scale images respectively simulated with the same noise level σ = 0.1%.\nthe successive scanned sub patterns from a high resolution random pattern is not entirely independent from each other, and this will mathematically degenerate the reconstruction performance. Here we denote the number of scanned consecutive patterns during each DMD period as k , and conduct a simulation experiment to test its effects on the final reconstruction and guide the setting of DMD frame rate.\nSpecifically, we firstly slide a 80 × 80-pixel sub-window over a high resolution random binary pattern to generate k small patterns. Then, we use these patterns to code the information of a binary image composed of characters 'SINGLE PIXEL' , also with 80 × 80 pixels. The correlated measurements are then generated by their inner products superimposed with white gaussian noise. In this experiment, we vary the parameter k from 1 to 100 at interval of 2, and the standard deviation of the noise σ increases from 0 to 0.5% at interval of 0.02%. For reconstruction, we utilize compressive sensing (CS) based method 6 with 35% sample rate, i.e. 2240 patterns. The optimization function is defined as\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nHere X denotes the transmission function of the sample, Ψ is the operator transforming the image to a space with sparse representation, and Pi and yi are the i th patten and corresponding correlated measurement of PD, respectively. In terms of quantitative evaluation, we adopt RMSE to measure the reconstruction quality. Figure 2(a) visualizes the reconstruction performance at different settings, and some reconstructed images are displayed in Fig. 2(b) with the noise level σ = 0.1%. We also simulate the imaging result for a gray-scale scene and show the result on Fig. 2(c) following the same simulation setting with the binary simulation shown in Fig. 2(b). Here k = 1 means that all the patterns are independent. It is obvious that the imaging quality turns worse as k increases, but even with large noise the result at large k still restores a decent image, both visually and quantitatively. In implementation, one should firstly measure the system noise and then choose proper DMD frequency accordingly. In this paper, we set the DMD update frequency as 20 Fg , which is smaller than the DMD's maximum frame rate.\nUnder the above settings, we firstly evaluate the basic imaging performance of the proposed imaging scheme on a static sample, USAF 1951 resolution test chart. The results are shown in Fig. 3, which displays the reconstruction result without scanning (i.e., with both galvanic mirrors hold still) and that with galvanic mirror scanning over the DMD. In implementation, we bin 2 × 2 pixels on the DMD as a super pixel. The pixel resolution of the reconstruction is 80 × 80. For the scanning acquisition, we set DMD working at 4 k Hz and galvanic mirrors working at 200 Hz, and the modulation speed of the illumination is 97 kHz. For the non-scanning acquisition, we set the DMD working at 20 k Hz and the galvanic mirrors working at non-scanning mode. For both experiments, the number of patterns for imaging is 2240. Thus, the acquisition time of scanning version is about 1/5 of that of non-scanning version.From the comparison of the imaging results one can see that our reconstruction is of decent quality, with slightly higher background noise. This is mainly caused by imperfect mechanical modulation and minorly decreased independence among the random patterns. In spite of the performance degradation,\nFigure 3. Imaging performance of our setup. The top row of ( a and b ) show the reconstruction results of the resolution chart without galvanic mirror scanning using 2240 and 448 patterns respectively. The top row of ( c ) shows the reconstruction result of the resolution chart with galvanic mirror scanning using 2240 patterns. The bottom row of ( a , b and c ) compare the profile of two local bars of corresponding results plotted in solid, dashdot and dashed line respectively.\nexperimentally the reconstruction is still of much higher quality than direct reconstruction at five times reduced sampling rate (shown in Fig. 3(b and c)), and this verifies the effectiveness of the proposed sweeping strategy.\nTo demonstrate the performance of our system working at high frame rate, we conduct single pixel imaging on a dynamic scene. Specifically, we put another DMD on the sample plane, and display video sequences at 42 Hz. Since the DMD projects gray-scale images by temporally multiplexing the binary mirrors which is inconsistent with nature dynamic scenes, we choose to let the DMD work at binary mode. The DMD frames are different from real moving scenes, and the difference is similar to the motion blur of frame-based video recording. For usual scenes not moving quite fast, the video shows no noticeable blur and our demonstration of dynamic imaging is reasonable. The object frames is synchronized by the spike signal in the measurement of the PD, which occurs when DMD refreshes a new frame. For reconstruction, we retrieve each frame using the compressive sensing based algorithm incorporating both spatial and temporal prior constraint, with the optimization function defined as\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere Xt is the sample at time t , Ψ and Φ are transformations applied to the image for sparse representation, and their minimization imposes smoothness constraint on intra and inter frames, respectively. The parameter λ is a weighting coefficient balancing two constraints. Empirically, a fast moving scenes favors a small λ , and vice versa. One can choose the parameter adaptively as in ref. 13. In this paper, without of loss of generality, we set λ = 0.9 empirically according to the dynamics of daily scenes. We show the reconstructed results of two realtime dynamic scenes in Fig. 4. The scenes are projected by another DMD working at 42 frames per second. From the results one can see that our setup can achieve real time single pixel imaging at decent imaging quality. The tiny difference in noise level is due to that the reconstruction algorithm is better at handling the frames with smaller white regions and a simpler structure. This imaging speed is 5 times higher than that of existing counterparts (i.e., DMD's maximum frequency) and can be accelerated further (please see sec. Discussions), while the previous systems are limited to the patterning frequency of SLM.\nIn our approach, due to the generating mechanism of illumination patterning, two adjacent patterns are shifted counterparts in the scanning direction (except for the pairs across two DMD periods). For simplicity, we\nFigure 4. Performance of our setup on two dynamic scenes.\nassume the scanning direction is horizontal. Denoting two consecutive patterns as Pi and Pi -1 , and the pixel indices along horizontal and vertical directions as u and v , the patterns satisfy Pi -1 ( u , v ) = Pi ( u -1, v ). Let X denote the transmission function of the sample, then the correlated measurement for Pi recorded by the photodiode would be yi = ∑ u , v Pi ( u , v ) X ( u , v ). We do subtraction between yi and yi -1 and get\n<!-- formula-not-decoded -->\nAbove derivations tell that, we can reconstruct the edges of X from Pi , Pi -1 and corresponding correlated measurements yi and yi -1 . Utilizing this property, we reconstruct the horizontal edge of two scenes, as displayed in Fig. 5. Here we set the pixel resolution as 80 × 80 pixels, project 2240 patterns and use the same compressive sensing based algorithm as in the simulation experiment. From comparison between (b) (d) and (c) (e), we can see that our approach can easily detect the edges in the target scene. This application holds potential for some computer vision tasks, such as motion detection, tracking, etc. Recall that our detection reduces the number of requisite patterns by 50% compared to the method proposed by Liu et al. 14 .\n\nGuides the setting of DMD frame rate by considering the trade-off between DMD frequency and pattern independence during sweeping.",
    "original_text": "From Eq. 1, the final illumination patterning speed is independent of the frequency of DMD, but the setting of the DMD's working frequency is non-trivial. In order to avoid repetitive coding patterns during sweeping of the galvanic mirror pair, the pattern elapse of the DMD should be shorter than the half period of the galvanic mirror. In other words, we can set the DMD at any frame rate between 2 Fg and its maximum frequency. However, because\nFigure 2. Simulation results on the parameter setting of DMD's frame rate, specifically the ratio k between the frame rates of galvanic mirror and DMD. ( a ) visualizes the performance under varying k values at different noise levels with the largest RMSE being 0.44 and smallest RMSE being 0.15. ( b and c ) displays some exemplar reconstructed binary and gray-scale images respectively simulated with the same noise level σ = 0.1%.\nthe successive scanned sub patterns from a high resolution random pattern is not entirely independent from each other, and this will mathematically degenerate the reconstruction performance. Here we denote the number of scanned consecutive patterns during each DMD period as k , and conduct a simulation experiment to test its effects on the final reconstruction and guide the setting of DMD frame rate.\nSpecifically, we firstly slide a 80 × 80-pixel sub-window over a high resolution random binary pattern to generate k small patterns. Then, we use these patterns to code the information of a binary image composed of characters 'SINGLE PIXEL' , also with 80 × 80 pixels. The correlated measurements are then generated by their inner products superimposed with white gaussian noise. In this experiment, we vary the parameter k from 1 to 100 at interval of 2, and the standard deviation of the noise σ increases from 0 to 0.5% at interval of 0.02%. For reconstruction, we utilize compressive sensing (CS) based method 6 with 35% sample rate, i.e. 2240 patterns. The optimization function is defined as\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nHere X denotes the transmission function of the sample, Ψ is the operator transforming the image to a space with sparse representation, and Pi and yi are the i th patten and corresponding correlated measurement of PD, respectively. In terms of quantitative evaluation, we adopt RMSE to measure the reconstruction quality. Figure 2(a) visualizes the reconstruction performance at different settings, and some reconstructed images are displayed in Fig. 2(b) with the noise level σ = 0.1%. We also simulate the imaging result for a gray-scale scene and show the result on Fig. 2(c) following the same simulation setting with the binary simulation shown in Fig. 2(b). Here k = 1 means that all the patterns are independent. It is obvious that the imaging quality turns worse as k increases, but even with large noise the result at large k still restores a decent image, both visually and quantitatively. In implementation, one should firstly measure the system noise and then choose proper DMD frequency accordingly. In this paper, we set the DMD update frequency as 20 Fg , which is smaller than the DMD's maximum frame rate.\nUnder the above settings, we firstly evaluate the basic imaging performance of the proposed imaging scheme on a static sample, USAF 1951 resolution test chart. The results are shown in Fig. 3, which displays the reconstruction result without scanning (i.e., with both galvanic mirrors hold still) and that with galvanic mirror scanning over the DMD. In implementation, we bin 2 × 2 pixels on the DMD as a super pixel. The pixel resolution of the reconstruction is 80 × 80. For the scanning acquisition, we set DMD working at 4 k Hz and galvanic mirrors working at 200 Hz, and the modulation speed of the illumination is 97 kHz. For the non-scanning acquisition, we set the DMD working at 20 k Hz and the galvanic mirrors working at non-scanning mode. For both experiments, the number of patterns for imaging is 2240. Thus, the acquisition time of scanning version is about 1/5 of that of non-scanning version.From the comparison of the imaging results one can see that our reconstruction is of decent quality, with slightly higher background noise. This is mainly caused by imperfect mechanical modulation and minorly decreased independence among the random patterns. In spite of the performance degradation,\nFigure 3. Imaging performance of our setup. The top row of ( a and b ) show the reconstruction results of the resolution chart without galvanic mirror scanning using 2240 and 448 patterns respectively. The top row of ( c ) shows the reconstruction result of the resolution chart with galvanic mirror scanning using 2240 patterns. The bottom row of ( a , b and c ) compare the profile of two local bars of corresponding results plotted in solid, dashdot and dashed line respectively.\nexperimentally the reconstruction is still of much higher quality than direct reconstruction at five times reduced sampling rate (shown in Fig. 3(b and c)), and this verifies the effectiveness of the proposed sweeping strategy.\nTo demonstrate the performance of our system working at high frame rate, we conduct single pixel imaging on a dynamic scene. Specifically, we put another DMD on the sample plane, and display video sequences at 42 Hz. Since the DMD projects gray-scale images by temporally multiplexing the binary mirrors which is inconsistent with nature dynamic scenes, we choose to let the DMD work at binary mode. The DMD frames are different from real moving scenes, and the difference is similar to the motion blur of frame-based video recording. For usual scenes not moving quite fast, the video shows no noticeable blur and our demonstration of dynamic imaging is reasonable. The object frames is synchronized by the spike signal in the measurement of the PD, which occurs when DMD refreshes a new frame. For reconstruction, we retrieve each frame using the compressive sensing based algorithm incorporating both spatial and temporal prior constraint, with the optimization function defined as\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere Xt is the sample at time t , Ψ and Φ are transformations applied to the image for sparse representation, and their minimization imposes smoothness constraint on intra and inter frames, respectively. The parameter λ is a weighting coefficient balancing two constraints. Empirically, a fast moving scenes favors a small λ , and vice versa. One can choose the parameter adaptively as in ref. 13. In this paper, without of loss of generality, we set λ = 0.9 empirically according to the dynamics of daily scenes. We show the reconstructed results of two realtime dynamic scenes in Fig. 4. The scenes are projected by another DMD working at 42 frames per second. From the results one can see that our setup can achieve real time single pixel imaging at decent imaging quality. The tiny difference in noise level is due to that the reconstruction algorithm is better at handling the frames with smaller white regions and a simpler structure. This imaging speed is 5 times higher than that of existing counterparts (i.e., DMD's maximum frequency) and can be accelerated further (please see sec. Discussions), while the previous systems are limited to the patterning frequency of SLM.\nIn our approach, due to the generating mechanism of illumination patterning, two adjacent patterns are shifted counterparts in the scanning direction (except for the pairs across two DMD periods). For simplicity, we\nFigure 4. Performance of our setup on two dynamic scenes.\nassume the scanning direction is horizontal. Denoting two consecutive patterns as Pi and Pi -1 , and the pixel indices along horizontal and vertical directions as u and v , the patterns satisfy Pi -1 ( u , v ) = Pi ( u -1, v ). Let X denote the transmission function of the sample, then the correlated measurement for Pi recorded by the photodiode would be yi = ∑ u , v Pi ( u , v ) X ( u , v ). We do subtraction between yi and yi -1 and get\n<!-- formula-not-decoded -->\nAbove derivations tell that, we can reconstruct the edges of X from Pi , Pi -1 and corresponding correlated measurements yi and yi -1 . Utilizing this property, we reconstruct the horizontal edge of two scenes, as displayed in Fig. 5. Here we set the pixel resolution as 80 × 80 pixels, project 2240 patterns and use the same compressive sensing based algorithm as in the simulation experiment. From comparison between (b) (d) and (c) (e), we can see that our approach can easily detect the edges in the target scene. This application holds potential for some computer vision tasks, such as motion detection, tracking, etc. Recall that our detection reduces the number of requisite patterns by 50% compared to the method proposed by Liu et al. 14 .",
    "context": "Guides the setting of DMD frame rate by considering the trade-off between DMD frequency and pattern independence during sweeping.",
    "document": "srep45325.pdf",
    "pages": [
      2,
      3,
      4,
      5
    ],
    "id": "bcc42fe45c2589bab4eab21c07aa50d930e642822965cd269cc97049791d7f36"
  },
  {
    "text": "In this paper, we propose to address the speed issue of current computational ghost imaging systems, via bypassing the restrictions from the maximum patterning frequency of SLMs. Specifically, we take advantage of the redundancy of SLM's spatial resolution and conduct illumination patterning by introducing a pair of galvanic mirrors sweeping periodically over a high resolution SLM. In our proof-of-principle setup, we achieve 5 times faster speed than existing fastest system, and it can be raised further by using higher end elements.\nIn our system, the speed is largely determined by the operating frequency of galvanic mirror and the size of the DMD micro-mirrors. There are several ways for achieving a much higher efficiency. Firstly, in current setup, we can use a higher end galvanic mirror for further acceleration. For example, using CTI 6200H with 1 kHz working frequency will increase the speed by 5 times, and achieve 210 frames per second at 80 × 80-pixel resolution. Secondly, we can also use an acoustic optical deflectors (AOD) for sweeping, which can produce 20 kHz scanning frequency, and thus 20 times faster illumination patterning, i.e., 9.5 MHz. Note that even with a high end galvanic mirror or AOD, their scanning frequency is sill smaller than half of the DMD's maximum refreshing rate (i.e., 10 KHz), so the acceleration is feasible in implementation. Thirdly, replacing the DMD with one with smaller mirror size (i.e., δ ) will bring further acceleration. For example, use the Texas Instrument DLP Discovery 4100,.9XGA (the size of whose micro mirrors is δ = 10.8 µ m ) instead of the adopted.7XGA will speed up the imaging by another 20%. In applications, one can choose task specific settings and optical elements.\nThe pixel resolution is jointly determined by two factors-the size of the SLM entry and that of the scanning beam. Using a DMD with smaller micro mirrors will increase the pixel resolution of the final reconstruction. While under the same scanning rate, we need to trade the final frame rate for higher resolution. Given the DMD specifications (mainly its micro mirror size), the pixel resolution depends on the size of the scanning beam hitting the DMD. The beam size is limited by the size of galvanic mirror. For now, the off-the-shelf galvanic mirrors usually support 3~7 mm beam in diameter, which will provide about 200 × 200-pixel resolution, and this is higher than that of state-of-the-art ghost imaging systems. Usually a larger galvanic mirror supports a lower working\n(a\n(b)\nFigure 5. Edge detection result on a scene. ( a ) Imaging result of the sample. ( b ) vs. ( c ) Edge detection results by reconstruction from measurements calculated following Eq. 6 and taking direct horizontal derivative to the image in ( a ).\noperating frequency (i.e., slower modulation), so one need to trade off between the pixel resolution and imaging speed in real applications.\nThere are also some limitations in our approach. The system needs careful mechanical mount for calibration, but this issue can be addressed effectively by designing a customized programmable mount for the DMD, galvanic mirrors and light source. Another limitation is that currently the scheme works for structuring light using random patterns, and is inapplicable for other patterns with specific structures, e.g. Hadamard and sinusoidal patterns.\n\nAddresses the speed issue of current computational ghost imaging systems by leveraging SLM redundancy and introducing sweeping galvanic mirrors, achieving 5x faster speed than existing systems with potential for further acceleration through higher-end components and smaller DMD mirrors.",
    "original_text": "In this paper, we propose to address the speed issue of current computational ghost imaging systems, via bypassing the restrictions from the maximum patterning frequency of SLMs. Specifically, we take advantage of the redundancy of SLM's spatial resolution and conduct illumination patterning by introducing a pair of galvanic mirrors sweeping periodically over a high resolution SLM. In our proof-of-principle setup, we achieve 5 times faster speed than existing fastest system, and it can be raised further by using higher end elements.\nIn our system, the speed is largely determined by the operating frequency of galvanic mirror and the size of the DMD micro-mirrors. There are several ways for achieving a much higher efficiency. Firstly, in current setup, we can use a higher end galvanic mirror for further acceleration. For example, using CTI 6200H with 1 kHz working frequency will increase the speed by 5 times, and achieve 210 frames per second at 80 × 80-pixel resolution. Secondly, we can also use an acoustic optical deflectors (AOD) for sweeping, which can produce 20 kHz scanning frequency, and thus 20 times faster illumination patterning, i.e., 9.5 MHz. Note that even with a high end galvanic mirror or AOD, their scanning frequency is sill smaller than half of the DMD's maximum refreshing rate (i.e., 10 KHz), so the acceleration is feasible in implementation. Thirdly, replacing the DMD with one with smaller mirror size (i.e., δ ) will bring further acceleration. For example, use the Texas Instrument DLP Discovery 4100,.9XGA (the size of whose micro mirrors is δ = 10.8 µ m ) instead of the adopted.7XGA will speed up the imaging by another 20%. In applications, one can choose task specific settings and optical elements.\nThe pixel resolution is jointly determined by two factors-the size of the SLM entry and that of the scanning beam. Using a DMD with smaller micro mirrors will increase the pixel resolution of the final reconstruction. While under the same scanning rate, we need to trade the final frame rate for higher resolution. Given the DMD specifications (mainly its micro mirror size), the pixel resolution depends on the size of the scanning beam hitting the DMD. The beam size is limited by the size of galvanic mirror. For now, the off-the-shelf galvanic mirrors usually support 3~7 mm beam in diameter, which will provide about 200 × 200-pixel resolution, and this is higher than that of state-of-the-art ghost imaging systems. Usually a larger galvanic mirror supports a lower working\n(a\n(b)\nFigure 5. Edge detection result on a scene. ( a ) Imaging result of the sample. ( b ) vs. ( c ) Edge detection results by reconstruction from measurements calculated following Eq. 6 and taking direct horizontal derivative to the image in ( a ).\noperating frequency (i.e., slower modulation), so one need to trade off between the pixel resolution and imaging speed in real applications.\nThere are also some limitations in our approach. The system needs careful mechanical mount for calibration, but this issue can be addressed effectively by designing a customized programmable mount for the DMD, galvanic mirrors and light source. Another limitation is that currently the scheme works for structuring light using random patterns, and is inapplicable for other patterns with specific structures, e.g. Hadamard and sinusoidal patterns.",
    "context": "Addresses the speed issue of current computational ghost imaging systems by leveraging SLM redundancy and introducing sweeping galvanic mirrors, achieving 5x faster speed than existing systems with potential for further acceleration through higher-end components and smaller DMD mirrors.",
    "document": "srep45325.pdf",
    "pages": [
      5,
      6
    ],
    "id": "5cbca6cfa3e0bd1051cd1ace35dff61a4800d056ffa7f7b6cd73e5edf8e53208"
  },
  {
    "text": "Geometry of light path. In our computational ghost imaging system, during each DMD's pattern duration, the sequential low resolution coded illumination patterns depend on the high resolution pattern on the DMD and the positions (i.e., subregions) where the beam hits the DMD. The hitting position is determined by the rotation angles of the two galvanic mirrors. Here we analyze the geometric relation between the rotating angles of the galvanic mirror pair and the hitting position.\nAs illustrated in Fig. 6(a), galvanic mirror #1 (GM1) and galvanic mirror #2 (GM2) rotate around their axes represented as red dots, and the distance between two axes is d . We let GM2 always rotated to the same pose with GM1, and thus keep the outgoing beam from GM2 in parallel with the incoming beam to GM1. The distance d between the two galvanic mirrors is 12 cm and the distance l between DMD and the galvanic mirror 2 is about 8 cm. We use lens #3 (focal length = 15  cm) shown in Fig. 1 to image the DMD onto the object plane. At initial time t = 0, the beam is shown by solid black lines, and two galvanic mirrors are shown by solid blue bar. The input voltage of GM1 and GM2 are both initialized to be 0 V , and the position and pose of GM1 and GM2 are adjusted to ensure the beam entering GM1 and GM2 exactly at the position of the rotating axis. Let θ denote the rotating angle of GM1, and x denote the distance from the beam's hitting positions at GM1 to that at the DMD, then we can get x = d sin(2 θ ). At time t = ∆ t , the beam is shown by dashed black line, GM1 and GM2 are shown in dotted blue bar. The rotating angle of GM1 and GM2 now is θ + ∆ θ , the distance x changes to x +∆ x correspondingly, with ∆ x = -d sin(2 ∆ θ ). In our setup, ∆ x determines the coded pattern of the beam given the image shown on the DMD, and ∆ θ = kU , where U is the GM's input voltage controlling its rotating angle, and k is the scaling coefficient between GM's input voltage and the output rotation angle. Let p 0 be the hitting position (in pixel) at DMD at initial state, and the pixel size of DMD be δ 0 , we can obtain the relation between the beam's pixel position at DMD p and the GM's input voltage U as\nFigure 6. (Better view in color version) ( a ) Geometry of the scanning system with two galvanic mirrors from top view. Galvanic mirror #1 (GM1) and galvanic mirror #2 (GM2) are rotating around the axes denoted by two red points. GM1 and GM2 at rotation angle θ and θ + ∆ θ are shown in solid and dashed line respectively. The corresponding beams of θ and θ + ∆ θ are also shown in solid and dashed line respectively. Here d = 12  cm and l = 8  cm. ( b ) The setup for calibration. We shape the beam with a square aperture (SA), and use a converging lens (CL) to collect all the photons to the photodiode (PD). ( c ) A square frame is shown on the DMD which can be covered by the scanning beam at a selected rotating angle. ( d ) The output signal during calibration, red solid line is the output voltage of GM1, and blue dashed curve is the output PD. ( e ) The calibrated mapping between the scanning beam's hitting position and GM1 ′ s output voltage. Each maker × implies a calibrated mapping pairs between the square border position and related GM voltage output, and the line is linear regression result of those markers.\n<!-- formula-not-decoded -->\nSince the scanning range of GM is small (less than 4 degree), Eq. 7 can be approximated to be\n<!-- formula-not-decoded -->\nCalibration. In the calibration step, we build the mapping between the position where the scanning beam is hitting and the related GM's input voltage U . Please note that, since our GM have rotation angle output signal in voltage which is more accurate than input voltage signal, we use this output voltage U instead of input voltage. Thus, given an arbitrary U , we can retrieve the region on the DMD covered by the scanning beam. The setup for calibration is shown in Fig. 6(b), all the light through the square aperture is collected into the PD directly. Thus at a fixed rotation angle of GM, only the light reflected by a square region on the DMD will reach PD finally. The calibration includes the following two steps: (1) locate the positions of the ends of beam scanning to determine the scanning range; (2) extract mapping relationship between the GM voltage outputs and corresponding position of the square regions within the scanning range.\nIn the first step, we give a DC voltage to the GM pair, to drive the non-scanning beam hitting two ends of the working region of DMD respectively. Then, at each end position, one-pixel-width horizontal and vertical lines are projected on the DMD to detect the square hitting region of the scanning beam. Since the measurements at the PD will have a step change at the boundary of the hitting region, we can locate the borderlines of the square by thresholding the PD outputs.\nIn the second step, as illustration in Subsec. Geometry of light path, with a small scanning range of GM, the mapping relationship between the GM voltage outputs and corresponding position of the square regions can be approximated to be linear relationship. We generate a series of square frame patterns of the hitting region size within the range and display them on the DMD sequentially, as Fig. 6(c) shows. During the elapse of each square frame pattern, we drive GM1 and GM2 with sine wave voltages and the measurements of PD changes with the rotation angle of GM1 and GM2. Figure 6(d) plots the PD signals of a frame pattern (in blue dashed curve) and the GM's output voltage (in solid red line). The peak position implies that the scanning beam hits exactly on the square frame shown on the DMD and reflected beam is through SA. As the dashed lines denotes, the corresponding GM voltage U 0 is recorded as the mapping of the square region. We plot the centroid of each square region and the corresponding GM voltage in Fig. 6(e), from which we can see the linear correlation defined in Eq. 7 is satisfied when the rotation angle is small.\n\nAnalysis of the geometric relationship between galvanic mirror rotation and beam hitting position.",
    "original_text": "Geometry of light path. In our computational ghost imaging system, during each DMD's pattern duration, the sequential low resolution coded illumination patterns depend on the high resolution pattern on the DMD and the positions (i.e., subregions) where the beam hits the DMD. The hitting position is determined by the rotation angles of the two galvanic mirrors. Here we analyze the geometric relation between the rotating angles of the galvanic mirror pair and the hitting position.\nAs illustrated in Fig. 6(a), galvanic mirror #1 (GM1) and galvanic mirror #2 (GM2) rotate around their axes represented as red dots, and the distance between two axes is d . We let GM2 always rotated to the same pose with GM1, and thus keep the outgoing beam from GM2 in parallel with the incoming beam to GM1. The distance d between the two galvanic mirrors is 12 cm and the distance l between DMD and the galvanic mirror 2 is about 8 cm. We use lens #3 (focal length = 15  cm) shown in Fig. 1 to image the DMD onto the object plane. At initial time t = 0, the beam is shown by solid black lines, and two galvanic mirrors are shown by solid blue bar. The input voltage of GM1 and GM2 are both initialized to be 0 V , and the position and pose of GM1 and GM2 are adjusted to ensure the beam entering GM1 and GM2 exactly at the position of the rotating axis. Let θ denote the rotating angle of GM1, and x denote the distance from the beam's hitting positions at GM1 to that at the DMD, then we can get x = d sin(2 θ ). At time t = ∆ t , the beam is shown by dashed black line, GM1 and GM2 are shown in dotted blue bar. The rotating angle of GM1 and GM2 now is θ + ∆ θ , the distance x changes to x +∆ x correspondingly, with ∆ x = -d sin(2 ∆ θ ). In our setup, ∆ x determines the coded pattern of the beam given the image shown on the DMD, and ∆ θ = kU , where U is the GM's input voltage controlling its rotating angle, and k is the scaling coefficient between GM's input voltage and the output rotation angle. Let p 0 be the hitting position (in pixel) at DMD at initial state, and the pixel size of DMD be δ 0 , we can obtain the relation between the beam's pixel position at DMD p and the GM's input voltage U as\nFigure 6. (Better view in color version) ( a ) Geometry of the scanning system with two galvanic mirrors from top view. Galvanic mirror #1 (GM1) and galvanic mirror #2 (GM2) are rotating around the axes denoted by two red points. GM1 and GM2 at rotation angle θ and θ + ∆ θ are shown in solid and dashed line respectively. The corresponding beams of θ and θ + ∆ θ are also shown in solid and dashed line respectively. Here d = 12  cm and l = 8  cm. ( b ) The setup for calibration. We shape the beam with a square aperture (SA), and use a converging lens (CL) to collect all the photons to the photodiode (PD). ( c ) A square frame is shown on the DMD which can be covered by the scanning beam at a selected rotating angle. ( d ) The output signal during calibration, red solid line is the output voltage of GM1, and blue dashed curve is the output PD. ( e ) The calibrated mapping between the scanning beam's hitting position and GM1 ′ s output voltage. Each maker × implies a calibrated mapping pairs between the square border position and related GM voltage output, and the line is linear regression result of those markers.\n<!-- formula-not-decoded -->\nSince the scanning range of GM is small (less than 4 degree), Eq. 7 can be approximated to be\n<!-- formula-not-decoded -->\nCalibration. In the calibration step, we build the mapping between the position where the scanning beam is hitting and the related GM's input voltage U . Please note that, since our GM have rotation angle output signal in voltage which is more accurate than input voltage signal, we use this output voltage U instead of input voltage. Thus, given an arbitrary U , we can retrieve the region on the DMD covered by the scanning beam. The setup for calibration is shown in Fig. 6(b), all the light through the square aperture is collected into the PD directly. Thus at a fixed rotation angle of GM, only the light reflected by a square region on the DMD will reach PD finally. The calibration includes the following two steps: (1) locate the positions of the ends of beam scanning to determine the scanning range; (2) extract mapping relationship between the GM voltage outputs and corresponding position of the square regions within the scanning range.\nIn the first step, we give a DC voltage to the GM pair, to drive the non-scanning beam hitting two ends of the working region of DMD respectively. Then, at each end position, one-pixel-width horizontal and vertical lines are projected on the DMD to detect the square hitting region of the scanning beam. Since the measurements at the PD will have a step change at the boundary of the hitting region, we can locate the borderlines of the square by thresholding the PD outputs.\nIn the second step, as illustration in Subsec. Geometry of light path, with a small scanning range of GM, the mapping relationship between the GM voltage outputs and corresponding position of the square regions can be approximated to be linear relationship. We generate a series of square frame patterns of the hitting region size within the range and display them on the DMD sequentially, as Fig. 6(c) shows. During the elapse of each square frame pattern, we drive GM1 and GM2 with sine wave voltages and the measurements of PD changes with the rotation angle of GM1 and GM2. Figure 6(d) plots the PD signals of a frame pattern (in blue dashed curve) and the GM's output voltage (in solid red line). The peak position implies that the scanning beam hits exactly on the square frame shown on the DMD and reflected beam is through SA. As the dashed lines denotes, the corresponding GM voltage U 0 is recorded as the mapping of the square region. We plot the centroid of each square region and the corresponding GM voltage in Fig. 6(e), from which we can see the linear correlation defined in Eq. 7 is satisfied when the rotation angle is small.",
    "context": "Analysis of the geometric relationship between galvanic mirror rotation and beam hitting position.",
    "document": "srep45325.pdf",
    "pages": [
      6,
      7
    ],
    "id": "dbbd98107e864b77f7995f170b12134ca9b426b25f77f1ad194b81e7732ee67e"
  },
  {
    "text": "1.  Pittman, T., Shih, Y ., Strekalov, D. & Sergienko, A. Optical imaging by means of two-photon quantum entanglement. Phys. Rev. A 52, R3429 (1995).\n2.  Strekalov, D., Sergienko, A., Klyshko, D. & Shih, Y. Observation of two-photon 'ghost' interference and diffraction. Phys. Rev. Lett. 74, 3600 (1995).\n3.  Erkmen, B. I. & Shapiro, J. H. Ghost imaging: from quantum to classical to computational. Advances in Optics and Photonics 2, 405-450, URL http://aop.osa.org/abstract.cfm?URI = aop-2-4-405(2010).\n4.  Ferri, F., Magatti, D., Lugiato, L. & Gatti, A. Differential ghost imaging. Phys. Rev. Lett. 104, 253603 (2010).\n5.  Sun, B., Welsh, S. S., Edgar, M. P ., Shapiro, J. H. & Padgett, M. J. Normalized ghost imaging. Opt. Express 20, 16892-16901 (2012).\n6.  Katz, O., Bromberg, Y. & Silberberg, Y. Compressive ghost imaging. Appl. Phys. Lett. 95, 131110 (2009).\n7.  Zhang, Z., Ma, X. & Zhong, J. Single-pixel imaging by means of fourier spectrum acquisition. Nat. Commun 6 (2015).\n8.  Duarte, M. F. et al. Single-pixel imaging via compressive sampling. IEEE Signal Proc. Mag. 25, 83 (2008).\n9.  Hu, X., Suo, J., Yue, T., Bian, L. & Dai, Q. Patch-primitive driven compressive ghost imaging. Opt. Express 23, 11092-11104 (2015).\n10.  Soldevila, F., Salvador-Balaguer, E., Clemente, P., Tajahuerce, E. & Lancis, J. High-resolution adaptive imaging with a single photodiode. Scientific Reports 5, 14300 (2015).\n11.  Edgar, M. P . et al. Simultaneous real-time visible and infrared video with single-pixel detectors. Sci Rep 5 (2015).\n12.  Suo, J. et al. A self-synchronized high speed computational ghost imaging system: A leap towards dynamic capturing. Opt. Laser Technol. 74, 65-71 (2015).\n13.  Li, Z., Suo, J., Hu, X. & Dai, Q. Content-adaptive ghost imaging of dynamic scenes. Opt. Express 24, 7328-36 (2016).\n14.  Liu, X.-F., Yao, X.-R., Lan, R.-M., Wang, C. & Zhai, G.-J. Edge detection based on gradient ghost imaging. Opt. Express 23, 33802-33811, http://www.opticsexpress.org/abstract.cfm?URI = oe-23-26-33802(2015).\n\nProvides a list of relevant scientific publications related to ghost imaging techniques, including foundational work on quantum ghost imaging, early demonstrations of classical ghost imaging, and subsequent advancements in compressive ghost imaging and adaptive ghost imaging.",
    "original_text": "1.  Pittman, T., Shih, Y ., Strekalov, D. & Sergienko, A. Optical imaging by means of two-photon quantum entanglement. Phys. Rev. A 52, R3429 (1995).\n2.  Strekalov, D., Sergienko, A., Klyshko, D. & Shih, Y. Observation of two-photon 'ghost' interference and diffraction. Phys. Rev. Lett. 74, 3600 (1995).\n3.  Erkmen, B. I. & Shapiro, J. H. Ghost imaging: from quantum to classical to computational. Advances in Optics and Photonics 2, 405-450, URL http://aop.osa.org/abstract.cfm?URI = aop-2-4-405(2010).\n4.  Ferri, F., Magatti, D., Lugiato, L. & Gatti, A. Differential ghost imaging. Phys. Rev. Lett. 104, 253603 (2010).\n5.  Sun, B., Welsh, S. S., Edgar, M. P ., Shapiro, J. H. & Padgett, M. J. Normalized ghost imaging. Opt. Express 20, 16892-16901 (2012).\n6.  Katz, O., Bromberg, Y. & Silberberg, Y. Compressive ghost imaging. Appl. Phys. Lett. 95, 131110 (2009).\n7.  Zhang, Z., Ma, X. & Zhong, J. Single-pixel imaging by means of fourier spectrum acquisition. Nat. Commun 6 (2015).\n8.  Duarte, M. F. et al. Single-pixel imaging via compressive sampling. IEEE Signal Proc. Mag. 25, 83 (2008).\n9.  Hu, X., Suo, J., Yue, T., Bian, L. & Dai, Q. Patch-primitive driven compressive ghost imaging. Opt. Express 23, 11092-11104 (2015).\n10.  Soldevila, F., Salvador-Balaguer, E., Clemente, P., Tajahuerce, E. & Lancis, J. High-resolution adaptive imaging with a single photodiode. Scientific Reports 5, 14300 (2015).\n11.  Edgar, M. P . et al. Simultaneous real-time visible and infrared video with single-pixel detectors. Sci Rep 5 (2015).\n12.  Suo, J. et al. A self-synchronized high speed computational ghost imaging system: A leap towards dynamic capturing. Opt. Laser Technol. 74, 65-71 (2015).\n13.  Li, Z., Suo, J., Hu, X. & Dai, Q. Content-adaptive ghost imaging of dynamic scenes. Opt. Express 24, 7328-36 (2016).\n14.  Liu, X.-F., Yao, X.-R., Lan, R.-M., Wang, C. & Zhai, G.-J. Edge detection based on gradient ghost imaging. Opt. Express 23, 33802-33811, http://www.opticsexpress.org/abstract.cfm?URI = oe-23-26-33802(2015).",
    "context": "Provides a list of relevant scientific publications related to ghost imaging techniques, including foundational work on quantum ghost imaging, early demonstrations of classical ghost imaging, and subsequent advancements in compressive ghost imaging and adaptive ghost imaging.",
    "document": "srep45325.pdf",
    "pages": [
      8
    ],
    "id": "7fa77225446cf7c5ef0b6fe5de484fbeb36f05fe138870e76724425db981eb19"
  },
  {
    "text": "The work is supported by the National Natural Science Foundation of China (Grant No. 61327902) and National Natural Science Foundation of China (61377005), and the Chinese Academy of Sciences (QYZDB-SSW-JSC002).\n\nFunding sources for the research are detailed, including specific grants from the National Natural Science Foundation of China and the Chinese Academy of Sciences.",
    "original_text": "The work is supported by the National Natural Science Foundation of China (Grant No. 61327902) and National Natural Science Foundation of China (61377005), and the Chinese Academy of Sciences (QYZDB-SSW-JSC002).",
    "context": "Funding sources for the research are detailed, including specific grants from the National Natural Science Foundation of China and the Chinese Academy of Sciences.",
    "document": "srep45325.pdf",
    "pages": [
      8
    ],
    "id": "7c5d84800170d9ad938208fc3652aad3761335322541fa0ddd5061bf6809ac44"
  },
  {
    "text": "Y. Wang and J. Suo proposed the scheme and designed the experiments. Y. Wang and Y. Liu built the system and conducted the experiments, and C. Qiao conducted data acquisition and processing. Y. Wang, J. Suo wrote the manuscript. G. Situ and Q. Dai helped revise the manuscript and got involved in the discussions.\n\nDetails the roles of key researchers in designing, building, and writing the manuscript.",
    "original_text": "Y. Wang and J. Suo proposed the scheme and designed the experiments. Y. Wang and Y. Liu built the system and conducted the experiments, and C. Qiao conducted data acquisition and processing. Y. Wang, J. Suo wrote the manuscript. G. Situ and Q. Dai helped revise the manuscript and got involved in the discussions.",
    "context": "Details the roles of key researchers in designing, building, and writing the manuscript.",
    "document": "srep45325.pdf",
    "pages": [
      8
    ],
    "id": "e066d39e14614d6b337501787c2b75fdf726ec99f0cde72c7c2f1930b1df16b3"
  },
  {
    "text": "Supplementary information accompanies this paper at http://www.nature.com/srep\nCompeting Interests: The authors declare no competing financial interests.\nHow to cite this article : Wang, Y. et al. High Speed Computational Ghost Imaging via Spatial Sweeping. Sci. Rep. 7 , 45325; doi: 10.1038/srep45325 (2017).\nPublisher's note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nThis work is licensed under a Creative Commons Attribution 4.0 International License. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in the credit line; if the material is not included under the Creative Commons license, users will need to obtain permission from the license holder to reproduce the material. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/\n© The Author(s) 2017\n\nRepresents the standard copyright and licensing information for the published article.",
    "original_text": "Supplementary information accompanies this paper at http://www.nature.com/srep\nCompeting Interests: The authors declare no competing financial interests.\nHow to cite this article : Wang, Y. et al. High Speed Computational Ghost Imaging via Spatial Sweeping. Sci. Rep. 7 , 45325; doi: 10.1038/srep45325 (2017).\nPublisher's note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nThis work is licensed under a Creative Commons Attribution 4.0 International License. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in the credit line; if the material is not included under the Creative Commons license, users will need to obtain permission from the license holder to reproduce the material. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/\n© The Author(s) 2017",
    "context": "Represents the standard copyright and licensing information for the published article.",
    "document": "srep45325.pdf",
    "pages": [
      8
    ],
    "id": "6354ca5835b51b9836327961c792a7fd4e82af4b92bb33ba966b8fb63bde010f"
  },
  {
    "text": "Daniel S. Zachary *\nResources Centre for Environmental Technologies, Public Research Centre Henri Tudor, 29, avenue J.F. Kennedy, Grand Duchy of Luxembourg.\nThis paper develops a framework to determine the sustainability of a general activity. We define an activity as an action or process that uses one or more resources and that responds either wholly or partially to a demand. A definition for sustainability is developed and is contingent on whether or not an activity can be sustained according to the available resources, the duration of an activity, the cost of its execution, or whether substitution is possible. A sustainability condition is met when the duration, cost and the chain of dependent activities satisfies the demand. Two conditions for sustainability are developed: a strong condition when the demand is met with no substitution and a weak condition when the demand is met via substitution. In the latter case, we show that the set of all sustainable activities is a subset of a N-level union of sustainable activities and forms a topological cover.\nI nthe 21st Century, humans are faced with the extraordinary challenge of developing methodologies to manage Earth's limited resources in a sustainable way 1-4 . A widely accepted definition of sustainability has its origins in the Brundtland Commission statement of 1987, Our Common Future 5 : ''Development that meets the needs of the present without compromising the ability of future generations to meet their own needs.'' This normative description summarizes the document and proposes a set of fundamental qualities for sustainability, including the notion of being global (encompassing everyone) and providing for intra- and intergenerational equity and justice. This document proposes the reduction of inequities for all as well as providing a way to protect the common good for the poorest and wealthiest nations, not only for the present, but also for future generations. Indeed the very notion of sustainability, 'needs to be sustained' in light of the self-evident prospect that we have a limited supply of available resources.\nAs a consequence, scientists continue to seek a better understanding of the fundamental character of the interactions between nature and society and to explore whether these interactions are along sustainable trajectories. Policy makers have also tried to implement these concepts. Kofi Annan's report to the United Nations, We the peoples, The role of the United Nations in the 21st Century 6 , echoes this notion. Activated by an international effort, Annan claimed, ''globalization must be built on the great enabling force of the market … and requiring a broader effort to create a shared future, based upon our common humanity in all its diversity.'' In the years following the Brundtland Report, a set of three pillars (we use interchangeably with the term dimensions) were conceived, namely, the environmental, economic, and social pillars. Then in 1992, the United Nations held the ''The Earth Summit'' conference in Rio de Janeiro. It was here that the agenda for the 21st century was established (Agenda 21) and the 'triplet' was solidified into the well known Venn diagram that has since become an icon for visualizing sustainability.\nIn the attempts to connect the potentially conflicting issues from each dimension, many subsequent interpretations and a host of definitions based on different academic disciplinary perspectives, ideological preferences, [and] even political expediency were developed. Some well known and constructive definitions include those for sustainable production 1 , sustainable biophysical systems 2 , eco-systems goods and services 4 , and others. Using some of these definitions, holistic approaches have been sought in response to the diverse challenges 7 and have had some success in building frameworks to address sustainability problems.\nThere is a clear imbalance in terms of analytical work on the three dimensions. Environmental and the closelyrelated ecological sciences, most naturally accessible to modelling, have developed the first among the three 8-11 , followed by quantitative economic approaches towards sustainability 12 , and then much later by work on social sustainability 13 . Social sustainability is also commonly understood as a 'weak pillar', in part because of the difficulty in formulating issues into an analytic framework. Some progress has none-the-less been made 14-16 . Despite the differences of the three, there is a general agreement that holistic approaches to sustainable development must encompass 1) the interactions across all three dimensions, 2) multiscale approaches 17 , and 3) complexity 18-20 . Apart from these concerns, the modeller must balance detail and relevance since a model will only be as good as the available input information and the associated uncertainties. Forecasting sustainability trajectories\nSUBJECT AREAS: MATHEMATICS AND COMPUTING ENVIRONMENTAL ECONOMICS\nReceived 24 December 2013\nAccepted 14 May 2014\nPublished 12 June 2014\nCorrespondence and requests for materials should be addressed to D.S.Z. (d.s.zachary@ jhu.edu; dan.s. zachary@gmail.com)\n* Current address: Whiting School of Engineering, The Johns Hopkins University, 6810 Deerpath Road, #100 Elkridge, MD 21076.\n1. Equilibrium-Neoclassical, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = Anthropocentric: welfare should be non-decreasing; SD should be based on technology and substitution; optimizing environmental externalities; the maintaining of aggregate stock of natural and economic capital; policy needed when individual objectives conflict.. 2. Neo-Austrian-Temporal, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = Teleological sequence of conscious and goal-oriented adaptation; preventing irreversible patterns; maintaining organization level (negentropy) in economic system; optimizing dynamic processes of extraction, production, consumption, recycling and waste treatment.. 3. Ecological-Evolutionary, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = Maintaining resilience of natural systems, allowing for fluctuation and cycles (regular destruction); learning from uncertainty in natural processes; no domination of food chains by humans; fostering balanced nutrient flows in ecosystems.. 4. Evolutionary-Technological, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = Maintaining co-evolutionary adaptive capacity in terms ofknowledgeand technology to react to uncertainties; fostering economic diversity of actors, sectors and technologies.. 5. Physico-Economic, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = Restrictions on materials and energy flows in/out of the economy; industrial metabolism based on materials product chain policy: integrated waste treatment, abatement, recycling and product development.. 6. Biophysical-Energy, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = A steady state with minimum materials and energy throughput; maintaining physical and biological stocks and biodiversity; transition to energy systems with minimum pollution effects.. 7. Systems-Ecological, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = Controlling direct and indirect human effects on ecosystems; balance between material inputs and outputs to human systems; minimum stress factors on ecosystems, both local and global.. 8. Ecological Engineering, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = Integration of human benefits and environmental quality and functions by manipulation of ecosystems, utilizing resilience, self-organization, self-regulation and functions of natural systems for human purposes.. 9. Human Ecology, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = Remain within the carrying capacity (logistic growth); limited scale of economy and population, consumption oriented toward basic needs; occupy a modest place within the ecosystem food web and biosphere; always consider multiplier effects of human actions in space and time.. 10. Socio-Biological, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = Maintain cultural and social system of interactions with ecosystems; respect for nature integrated in culture; survival of group important.. 11. Historical-Institutional, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = Equal attention to interests of nature, sectors and future generations; integrating institutional arrangements for economic andenvironmental policy; creating institutional long-run support for natures interests; holistic instead of partial solutions, based on a hierarchy of values.. 12. Ethical-Utopian, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = New individual value systems and respect for nature and future generations, basic needs fulfilment, long-run policy based on changing values and encouraging citizen (altruistic) as opposed to individual (egoistic) behaviour.\nis an even more difficult task when so many unknowns, both present and future, must be taken into account. A seemingly good solution for the present might turn out to be disastrous later on 3 . Currently, no clear methodology links the three dimensions, and no forecast method provides a vivid picture as to how we should use our limited resources.\nthe first volume of Ecological Economics, the ''Transdisciplinary Journal of the ISEE'', was published 26 . Today, EE is considered a well established scientific community. A good review of EE by Kastenhofer et al. (2011) 25 provides an overview of this period and includes some of the more recent works 29 .\n\nDevelops a framework to determine the sustainability of a general activity, defining sustainability as the ability of an activity to be sustained according to available resources, duration, cost, or substitution possibilities. Proposes two conditions for sustainability: a strong condition requiring no substitution and a weak condition allowing for substitution.",
    "original_text": "Daniel S. Zachary *\nResources Centre for Environmental Technologies, Public Research Centre Henri Tudor, 29, avenue J.F. Kennedy, Grand Duchy of Luxembourg.\nThis paper develops a framework to determine the sustainability of a general activity. We define an activity as an action or process that uses one or more resources and that responds either wholly or partially to a demand. A definition for sustainability is developed and is contingent on whether or not an activity can be sustained according to the available resources, the duration of an activity, the cost of its execution, or whether substitution is possible. A sustainability condition is met when the duration, cost and the chain of dependent activities satisfies the demand. Two conditions for sustainability are developed: a strong condition when the demand is met with no substitution and a weak condition when the demand is met via substitution. In the latter case, we show that the set of all sustainable activities is a subset of a N-level union of sustainable activities and forms a topological cover.\nI nthe 21st Century, humans are faced with the extraordinary challenge of developing methodologies to manage Earth's limited resources in a sustainable way 1-4 . A widely accepted definition of sustainability has its origins in the Brundtland Commission statement of 1987, Our Common Future 5 : ''Development that meets the needs of the present without compromising the ability of future generations to meet their own needs.'' This normative description summarizes the document and proposes a set of fundamental qualities for sustainability, including the notion of being global (encompassing everyone) and providing for intra- and intergenerational equity and justice. This document proposes the reduction of inequities for all as well as providing a way to protect the common good for the poorest and wealthiest nations, not only for the present, but also for future generations. Indeed the very notion of sustainability, 'needs to be sustained' in light of the self-evident prospect that we have a limited supply of available resources.\nAs a consequence, scientists continue to seek a better understanding of the fundamental character of the interactions between nature and society and to explore whether these interactions are along sustainable trajectories. Policy makers have also tried to implement these concepts. Kofi Annan's report to the United Nations, We the peoples, The role of the United Nations in the 21st Century 6 , echoes this notion. Activated by an international effort, Annan claimed, ''globalization must be built on the great enabling force of the market … and requiring a broader effort to create a shared future, based upon our common humanity in all its diversity.'' In the years following the Brundtland Report, a set of three pillars (we use interchangeably with the term dimensions) were conceived, namely, the environmental, economic, and social pillars. Then in 1992, the United Nations held the ''The Earth Summit'' conference in Rio de Janeiro. It was here that the agenda for the 21st century was established (Agenda 21) and the 'triplet' was solidified into the well known Venn diagram that has since become an icon for visualizing sustainability.\nIn the attempts to connect the potentially conflicting issues from each dimension, many subsequent interpretations and a host of definitions based on different academic disciplinary perspectives, ideological preferences, [and] even political expediency were developed. Some well known and constructive definitions include those for sustainable production 1 , sustainable biophysical systems 2 , eco-systems goods and services 4 , and others. Using some of these definitions, holistic approaches have been sought in response to the diverse challenges 7 and have had some success in building frameworks to address sustainability problems.\nThere is a clear imbalance in terms of analytical work on the three dimensions. Environmental and the closelyrelated ecological sciences, most naturally accessible to modelling, have developed the first among the three 8-11 , followed by quantitative economic approaches towards sustainability 12 , and then much later by work on social sustainability 13 . Social sustainability is also commonly understood as a 'weak pillar', in part because of the difficulty in formulating issues into an analytic framework. Some progress has none-the-less been made 14-16 . Despite the differences of the three, there is a general agreement that holistic approaches to sustainable development must encompass 1) the interactions across all three dimensions, 2) multiscale approaches 17 , and 3) complexity 18-20 . Apart from these concerns, the modeller must balance detail and relevance since a model will only be as good as the available input information and the associated uncertainties. Forecasting sustainability trajectories\nSUBJECT AREAS: MATHEMATICS AND COMPUTING ENVIRONMENTAL ECONOMICS\nReceived 24 December 2013\nAccepted 14 May 2014\nPublished 12 June 2014\nCorrespondence and requests for materials should be addressed to D.S.Z. (d.s.zachary@ jhu.edu; dan.s. zachary@gmail.com)\n* Current address: Whiting School of Engineering, The Johns Hopkins University, 6810 Deerpath Road, #100 Elkridge, MD 21076.\n1. Equilibrium-Neoclassical, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = Anthropocentric: welfare should be non-decreasing; SD should be based on technology and substitution; optimizing environmental externalities; the maintaining of aggregate stock of natural and economic capital; policy needed when individual objectives conflict.. 2. Neo-Austrian-Temporal, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = Teleological sequence of conscious and goal-oriented adaptation; preventing irreversible patterns; maintaining organization level (negentropy) in economic system; optimizing dynamic processes of extraction, production, consumption, recycling and waste treatment.. 3. Ecological-Evolutionary, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = Maintaining resilience of natural systems, allowing for fluctuation and cycles (regular destruction); learning from uncertainty in natural processes; no domination of food chains by humans; fostering balanced nutrient flows in ecosystems.. 4. Evolutionary-Technological, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = Maintaining co-evolutionary adaptive capacity in terms ofknowledgeand technology to react to uncertainties; fostering economic diversity of actors, sectors and technologies.. 5. Physico-Economic, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = Restrictions on materials and energy flows in/out of the economy; industrial metabolism based on materials product chain policy: integrated waste treatment, abatement, recycling and product development.. 6. Biophysical-Energy, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = A steady state with minimum materials and energy throughput; maintaining physical and biological stocks and biodiversity; transition to energy systems with minimum pollution effects.. 7. Systems-Ecological, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = Controlling direct and indirect human effects on ecosystems; balance between material inputs and outputs to human systems; minimum stress factors on ecosystems, both local and global.. 8. Ecological Engineering, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = Integration of human benefits and environmental quality and functions by manipulation of ecosystems, utilizing resilience, self-organization, self-regulation and functions of natural systems for human purposes.. 9. Human Ecology, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = Remain within the carrying capacity (logistic growth); limited scale of economy and population, consumption oriented toward basic needs; occupy a modest place within the ecosystem food web and biosphere; always consider multiplier effects of human actions in space and time.. 10. Socio-Biological, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = Maintain cultural and social system of interactions with ecosystems; respect for nature integrated in culture; survival of group important.. 11. Historical-Institutional, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = Equal attention to interests of nature, sectors and future generations; integrating institutional arrangements for economic andenvironmental policy; creating institutional long-run support for natures interests; holistic instead of partial solutions, based on a hierarchy of values.. 12. Ethical-Utopian, Table 1 | Adapted from van den Bergh 40 'Theoretical Perspectives on Sustainable Development'.Characterization = New individual value systems and respect for nature and future generations, basic needs fulfilment, long-run policy based on changing values and encouraging citizen (altruistic) as opposed to individual (egoistic) behaviour.\nis an even more difficult task when so many unknowns, both present and future, must be taken into account. A seemingly good solution for the present might turn out to be disastrous later on 3 . Currently, no clear methodology links the three dimensions, and no forecast method provides a vivid picture as to how we should use our limited resources.\nthe first volume of Ecological Economics, the ''Transdisciplinary Journal of the ISEE'', was published 26 . Today, EE is considered a well established scientific community. A good review of EE by Kastenhofer et al. (2011) 25 provides an overview of this period and includes some of the more recent works 29 .",
    "context": "Develops a framework to determine the sustainability of a general activity, defining sustainability as the ability of an activity to be sustained according to available resources, duration, cost, or substitution possibilities. Proposes two conditions for sustainability: a strong condition requiring no substitution and a weak condition allowing for substitution.",
    "document": "srep05215.pdf",
    "pages": [
      1,
      2
    ],
    "id": "169d62a15e20613e11d5da43124b0ba602b113ed2d36c46360058d46c4cc9a0c"
  },
  {
    "text": "In this paper, we develop a methodology linking the three dimensions and providing a means to distinguish between sustainable and non-sustainable activities. We provide a simple parametrization that connects renewable and non-renewable resources associated with the activities. In the next section, we recount the major historical steps in bridging sustainability dimensions.\n\nDevelops a framework for determining sustainability, defining sustainability based on resource availability, duration, cost, and substitution possibilities. Proposes two conditions: a strong condition requiring no substitution and a weak condition allowing for substitution. Highlights the interconnectedness of the three dimensions and the need for a holistic approach.",
    "original_text": "In this paper, we develop a methodology linking the three dimensions and providing a means to distinguish between sustainable and non-sustainable activities. We provide a simple parametrization that connects renewable and non-renewable resources associated with the activities. In the next section, we recount the major historical steps in bridging sustainability dimensions.",
    "context": "Develops a framework for determining sustainability, defining sustainability based on resource availability, duration, cost, and substitution possibilities. Proposes two conditions: a strong condition requiring no substitution and a weak condition allowing for substitution. Highlights the interconnectedness of the three dimensions and the need for a holistic approach.",
    "document": "srep05215.pdf",
    "pages": [
      2
    ],
    "id": "b03bf7d3ffe1b7cede3b53b3206b0450ce16e7067e4147e54a38c511e0a52585"
  },
  {
    "text": "Early interdisciplinary work came in the 1940s, when Karl William Kapp, considered one of the founders of ecological economics (EE), proposed the connections between societal and economic activities 21 , ''Social cost[s]… are all direct and indirect losses sustained by third persons or the general public as a result of unrestrained economic activities.'' In these early years, the societal dimension was treated as a subsystem of the ecosystem. In the same time period, Karl Polanyi produced his work on The Great Transformation (1944) 22 and developed the concept of societal and cultural economics. In the 1960s, a number of essays describing the interactions between the economic and ecosystems appeared, including Kenneth E. Boulding's work ''The economics of the coming spaceship Earth'' 23 and Herman E. Daly's contribution entitled ''On Economics as a life cycle'' 24 . In the 1980s, a next major step came when regular scientific activities commenced in the field of EE. In 1984, the symposium entitled ''Integrating Ecology and Economics'' was hosted in Sweden by Ann-Mari Jansson 8 and brought together, for the first time, a large number of ecosystem ecologists and mainstream environmental economists. In 1988, the International Society for Ecological Economics (ISEE) was founded, and, in February 1989,\nAlso, during this period, the interdisciplinary fields of sustainability science (SS) 28,30-32 emerged as a set of disciplines addressing the central issues of sustainability. A comprehensive definition is proposed by Kieffer et al. (2003) 33 : ''[Sustainable science is] the cultivation, integration, and application of knowledge about Earth systems gained especially from the holistic and historical sciences ... coordinated with knowledge about human interrelationships gained from the social sciences and humanities, in order to evaluate, mitigate, and minimize the consequences''.\nDespite the definitions available, no consensus on a theoretical framework has yet been defined in either interdisciplinary movements 25,27 . A current priority theme is the development of strategies dealing with the dynamical interfaces between dimensions 25,34 and the uncertainties of measurable parameters.\nAmodern technique in dealing with the problem of interfaces has been the use of systems approaches (SA) where tools have traditionally been used to treat two or more domains of research. In SA, different metrics for each discipline are treated separately 3 . The method uses dynamic processes 35 (time dependent), is cross dimensional (economic, social, and environmental) and is able to handle complex problems 36 (interconnected, interwoven, with feedback) as well as problems with multi-variables. As with most approaches dealing with complex issues, SA employs numerical methods that can handle problems of this nature, but these approaches are not always easily interpretable.\nAnother quantitative method includes the use of sustainability assessment maps (SAMs). These maps use indicators such as costs, profits, numbers of jobs, environmental impact and types and quantities of natural capital. A score is set up to weigh each decision in\nterms of its sustainability 3 . The approach is appealing for policy makers because it is easy to communicate and understand, although it should not be considered a panacea 37 . Some problems include the aforementioned 'imbalance between ecological modelling and the other two branches' 8-11 , the weak integration of data into a single, dynamical framework since SAMs are static 37 , the unresolved issue of what to do when conflicting goals and interactions between indicators have not been sufficiently considered, and finally, the mismatch of scales (local to global). The last issues is especially important in the social dimension 11,37 .\nSeveral other approaches have been pursued to overcome these challenges, including the multivariate analysis 38 and comprehensive indicator approaches 39 , both having a certain degree of success in specific applications 37 . The issues of complexity, non-locality, and multi-scales, still proved difficult to address with these methods, both in terms of calculation and interpretation. It is not surprising that no single approach for modelling sustainability has yet emerged. Furthermore, no consensus exists for a comprehensive list of sustainability topics. A good compilation of theoretical approaches has nonetheless been developed (adapted from van den Bergh, 1996) 40 and is given in Table 1.\nThe previously mentioned approaches suggest the interconnectivity of sustainability. An underlying concept connecting the dimensions is cost . This is not necessarily a monetary value, except in the obvious case of commodities in the economic dimension, but a parameter used to weight and compare decisions. Monetary values are more difficult to assign to environmental costs, although methodology has been developed to do so 41,42 . Impact has also been evaluated in other contexts, for example, direct human activity in the environment via population growth 43 . Also, not surprisingly, it is difficult to assign a monetary cost for societal concerns, but a relative weighting can still be done, especially for issues at a local level where a consensus on the social cost is easier to obtain.\nFor example, when dealing only with energy resources and longterm planning, the model should provide solutions that approach those of classic technoeconomic models (e.g., MARKAL, the MARket ALlocation model) 44 . When details are available (e.g., costs of fuel, maintenance, and investments), the model should be readily adaptable to search for and test different hypotheses. Indeed, the model should provide a means to make decisions or at least be flexible enough so that a decision process can be developed around the core model.\nThe model should also be adaptable to handle dynamic systems such as the dynamic resource stocks of limited resources (e.g., fossil fuels) or linked (indirect) resources typically modelled in ecological economic literature (e.g., tuna: the harvesting of this top predator fish and its dependency on herrings).\nEnvironmental costs, though less tangible, can also be determined via an impact or damage. A number of air pollutants, including CO2, nitrous oxides, hydrocarbons, and particulate matter are emitted from the vehicle and damage or impact the health of the surrounding human and plant environment. Although difficult to calculate on an individual basis, the damage to humans can be assessed in terms of the health costs or decreased longevity as a result of the diminished environment 42 .\nThe social costs depend on the social milieu. For example, the social acceptability of taking the bike instead of the car, may or may not be important. In some localities, residents consider it to be more socially responsible to use a bike for a short trip, especially if the other choice was an oversized car; yet in other societies, for cultural reasons, the consideration is just the opposite. This reality suggests that social costs may still be evaluated, but more as a means of weighing two or more decisions based on their local acceptance.\nIn this paper, we break down the concept of sustainable development into its building block activities; these can then be tested for the sustainability condition as we will show in the next few sections.\nFigure 1 | Schematic of dependencies (N 5 5) for the base activity x 1 1 using capacity c 1 1 .\nFurthermore, we propose that sustainable development occurs only when the ensemble of activities that make up the development are all sustainable.\n\nEarly interdisciplinary work came in the 1940s, when Karl William Kapp, considered one of the founders of ecological economics (EE), proposed the connections between societal and economic activities.",
    "original_text": "Early interdisciplinary work came in the 1940s, when Karl William Kapp, considered one of the founders of ecological economics (EE), proposed the connections between societal and economic activities 21 , ''Social cost[s]… are all direct and indirect losses sustained by third persons or the general public as a result of unrestrained economic activities.'' In these early years, the societal dimension was treated as a subsystem of the ecosystem. In the same time period, Karl Polanyi produced his work on The Great Transformation (1944) 22 and developed the concept of societal and cultural economics. In the 1960s, a number of essays describing the interactions between the economic and ecosystems appeared, including Kenneth E. Boulding's work ''The economics of the coming spaceship Earth'' 23 and Herman E. Daly's contribution entitled ''On Economics as a life cycle'' 24 . In the 1980s, a next major step came when regular scientific activities commenced in the field of EE. In 1984, the symposium entitled ''Integrating Ecology and Economics'' was hosted in Sweden by Ann-Mari Jansson 8 and brought together, for the first time, a large number of ecosystem ecologists and mainstream environmental economists. In 1988, the International Society for Ecological Economics (ISEE) was founded, and, in February 1989,\nAlso, during this period, the interdisciplinary fields of sustainability science (SS) 28,30-32 emerged as a set of disciplines addressing the central issues of sustainability. A comprehensive definition is proposed by Kieffer et al. (2003) 33 : ''[Sustainable science is] the cultivation, integration, and application of knowledge about Earth systems gained especially from the holistic and historical sciences ... coordinated with knowledge about human interrelationships gained from the social sciences and humanities, in order to evaluate, mitigate, and minimize the consequences''.\nDespite the definitions available, no consensus on a theoretical framework has yet been defined in either interdisciplinary movements 25,27 . A current priority theme is the development of strategies dealing with the dynamical interfaces between dimensions 25,34 and the uncertainties of measurable parameters.\nAmodern technique in dealing with the problem of interfaces has been the use of systems approaches (SA) where tools have traditionally been used to treat two or more domains of research. In SA, different metrics for each discipline are treated separately 3 . The method uses dynamic processes 35 (time dependent), is cross dimensional (economic, social, and environmental) and is able to handle complex problems 36 (interconnected, interwoven, with feedback) as well as problems with multi-variables. As with most approaches dealing with complex issues, SA employs numerical methods that can handle problems of this nature, but these approaches are not always easily interpretable.\nAnother quantitative method includes the use of sustainability assessment maps (SAMs). These maps use indicators such as costs, profits, numbers of jobs, environmental impact and types and quantities of natural capital. A score is set up to weigh each decision in\nterms of its sustainability 3 . The approach is appealing for policy makers because it is easy to communicate and understand, although it should not be considered a panacea 37 . Some problems include the aforementioned 'imbalance between ecological modelling and the other two branches' 8-11 , the weak integration of data into a single, dynamical framework since SAMs are static 37 , the unresolved issue of what to do when conflicting goals and interactions between indicators have not been sufficiently considered, and finally, the mismatch of scales (local to global). The last issues is especially important in the social dimension 11,37 .\nSeveral other approaches have been pursued to overcome these challenges, including the multivariate analysis 38 and comprehensive indicator approaches 39 , both having a certain degree of success in specific applications 37 . The issues of complexity, non-locality, and multi-scales, still proved difficult to address with these methods, both in terms of calculation and interpretation. It is not surprising that no single approach for modelling sustainability has yet emerged. Furthermore, no consensus exists for a comprehensive list of sustainability topics. A good compilation of theoretical approaches has nonetheless been developed (adapted from van den Bergh, 1996) 40 and is given in Table 1.\nThe previously mentioned approaches suggest the interconnectivity of sustainability. An underlying concept connecting the dimensions is cost . This is not necessarily a monetary value, except in the obvious case of commodities in the economic dimension, but a parameter used to weight and compare decisions. Monetary values are more difficult to assign to environmental costs, although methodology has been developed to do so 41,42 . Impact has also been evaluated in other contexts, for example, direct human activity in the environment via population growth 43 . Also, not surprisingly, it is difficult to assign a monetary cost for societal concerns, but a relative weighting can still be done, especially for issues at a local level where a consensus on the social cost is easier to obtain.\nFor example, when dealing only with energy resources and longterm planning, the model should provide solutions that approach those of classic technoeconomic models (e.g., MARKAL, the MARket ALlocation model) 44 . When details are available (e.g., costs of fuel, maintenance, and investments), the model should be readily adaptable to search for and test different hypotheses. Indeed, the model should provide a means to make decisions or at least be flexible enough so that a decision process can be developed around the core model.\nThe model should also be adaptable to handle dynamic systems such as the dynamic resource stocks of limited resources (e.g., fossil fuels) or linked (indirect) resources typically modelled in ecological economic literature (e.g., tuna: the harvesting of this top predator fish and its dependency on herrings).\nEnvironmental costs, though less tangible, can also be determined via an impact or damage. A number of air pollutants, including CO2, nitrous oxides, hydrocarbons, and particulate matter are emitted from the vehicle and damage or impact the health of the surrounding human and plant environment. Although difficult to calculate on an individual basis, the damage to humans can be assessed in terms of the health costs or decreased longevity as a result of the diminished environment 42 .\nThe social costs depend on the social milieu. For example, the social acceptability of taking the bike instead of the car, may or may not be important. In some localities, residents consider it to be more socially responsible to use a bike for a short trip, especially if the other choice was an oversized car; yet in other societies, for cultural reasons, the consideration is just the opposite. This reality suggests that social costs may still be evaluated, but more as a means of weighing two or more decisions based on their local acceptance.\nIn this paper, we break down the concept of sustainable development into its building block activities; these can then be tested for the sustainability condition as we will show in the next few sections.\nFigure 1 | Schematic of dependencies (N 5 5) for the base activity x 1 1 using capacity c 1 1 .\nFurthermore, we propose that sustainable development occurs only when the ensemble of activities that make up the development are all sustainable.",
    "context": "Early interdisciplinary work came in the 1940s, when Karl William Kapp, considered one of the founders of ecological economics (EE), proposed the connections between societal and economic activities.",
    "document": "srep05215.pdf",
    "pages": [
      2,
      3
    ],
    "id": "fbe53cf5453dd96b052574fe08010992227bcd8418db44eccaf7cfc36cde30f9"
  },
  {
    "text": "Someactivities will have immediate and direct consequences on their surroundings; others will have delayed impact. It is therefore important to model the activity with its associated impact and cost and capture both the immediate and the future consequences. The temporal behavior of activities and their costs must therefore be taken into account and modeled in a way that also reflects the limited knowledge that is available. We are now prepared to provide a general approach.\n\nModels the interplay between activities, demand, and time, emphasizing the need to account for both immediate and long-term impacts and acknowledging the uncertainty in available knowledge.",
    "original_text": "Someactivities will have immediate and direct consequences on their surroundings; others will have delayed impact. It is therefore important to model the activity with its associated impact and cost and capture both the immediate and the future consequences. The temporal behavior of activities and their costs must therefore be taken into account and modeled in a way that also reflects the limited knowledge that is available. We are now prepared to provide a general approach.",
    "context": "Models the interplay between activities, demand, and time, emphasizing the need to account for both immediate and long-term impacts and acknowledging the uncertainty in available knowledge.",
    "document": "srep05215.pdf",
    "pages": [
      3
    ],
    "id": "9d11bad72e32c62fd71a4329c5fd04907b48fe3a1f5d974f72e6c7eed34e3362"
  },
  {
    "text": "Wedefine a set of n time-dependent activities , i , with label ai ( t ), i 5 1, …, N , each with capacity c i ( t ), c i ( t ) 5 h xi ( t )/ h t , D ~ X i ~ 1 , ... N ci t ð Þ .\nWe use the notion of capacity as that required or needed to meet a certain demand, contrary to the available capacity that may exceed the requirements of the demand. Consequently the resource xi is diminished according to this rate in response to meeting a single demand D .\nAnactivity is equivalent to a set of actions or processes with a base , 5 1, and , 5 2, …, N representing substitution or replacement activities. To lighten the notation, we hereafter drop the time in the following definitions.\nDefinition 1 . A general activity ,\n<!-- formula-not-decoded -->\nis , 2 1 removed from the base activity and has capacity c ' i , j , k , ... , using resources x ' i , j , k , ... . The substitution level , has an equivalent number of running indices i, j, k, ….\nBase activities a 1 i , i 5 1, … ni and substitutes , 5 2, 3, … are defined for each i . The first substitute activities are a 2 i , j i j i : 5 1, …, n i , j , etc. A level-dependent n replaces N . To lighten the notation, we drop the i subscript on j hereafter.\nReturning to our short trip example, supposing that the bike is chosen as the mode of transportation, then this single activity meets the demand, c 1 [Joules] 5 D [Joules], fulfilled by the consumption of an apple. An activity may require several resources. For example, a multiple list of objects, representing the four basic food groups,\n<!-- formula-not-decoded -->\nmight be desirable to fulfill the needed demand. Substitution may occur here, if 'tuna' x 1 1 is not available but a another fish (e.g., salmon or grouper) is and would be designated a 2 1 . This example is time independent. An example of a base activity and three levels of dependencies are sketched in Figure 1. Here we show one capacity at the base level c 1 1 and its substitutes up through level five. Both renewable and non-renewable resources are shown along with a proposed actual example.\nDefinition 2 . The activity a ' i , j , ... occurs over a duration t ' i , j , ... and has a maximum allowable duration t ' , max i , j , ... ,\n<!-- formula-not-decoded -->\nThis condition ensures that each activity occurs in a feasible time period defined either when the activity is available or when resources are available.\nTwo assumptions have been made: First, we have considered a nonrenewable resource. A time dependent and renewable stock (e.g., a fish population), can be treated in this same framework, but with additional considerations on x ' i , j , ... , see Appendix (A dynamic reservoir). Second, we have assumed that only one activity is associated with a resource. In reality, several activities can be linked to a single resource and, in this case, we need to partition the resource for each activity. A multiple and simultaneous set of demands can be subsequently met by individual resources xi . Petroleum meeting the demand for transportation xpetrol R trans and petroleum used as raw material for the plastic industry, xpetrol R plastic , is an example. The resource xpetrol R trans is implicitly used only for transportation while xpetrol R plastic is implicitly only used for plastics, xtotpetrol 5 xpetrol R trans 1 xpetrol R plastic . The uncertainty in the demand for transportation and plastics implies that some of the petroleum reservoir initially allocated for transportation could be used for the plastic industry and visa-versa. The time dependency on the activity, the cost, the demand, and any other parameters is implicit and generally not known. For these reasons, only a limited amount of modeling can be done for long-term environmental impact and cost.\nThe cost Ci for activity ai for a one-level dependent activity (temporarily dropping the activity subscript) is composed of three parts: economic CF , social CS , and environmental CE . For extended planning, the technoeconomic costs include the standard investment, operational (maintenance), energy and replacement (salvage) values. The environmental cost CE is proportional to the impact I , the pollutant p , and ultimately the required capacity c , using a set of conversion constants, a , b , and c ,\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nIn this simple example where neither feedback nor other non-linear behavior occur, the environmental cost is directly proportional to the pollutant. Even in this example, the pollutant may accumulate over time and the net pollution p , resulting from the activity, can be diminished by dispersive action and/or by sequestration. Given a constant activity over a period of time t , the accumulated pollution pACC and cost are,\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere g 5 a 3 b 3 c . Behind this simple relationship lies environmental impacts and therefore costs having non-linear responses to\nFigure 2 | A set of single-level activities fulfilling a flat demand D.\nthe accumulated pollution levels resulting from complex physical processes and dynamic capacities.\nFinally, social costs are even more challenging to model, but in principle, CS ( t ), if known or agreed upon, can also be included in a dynamic way.\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nDefinition 4 . If either the duration, Eq. 2, or the costs, Eqs. 9 - 11, are not satisfied, then substitution may occur if it is available,\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThough it is possible that c ' i , j , ... § c ' z 1 i , j , ... , there is no guarantee that any additional capacity will be useful for other activities. For example, replacing one type of bread with another bread is acceptable, yet a salad replaced with twice as much bread would not meet the requirement (demand) for providing the four food groups.\niff, and\nFigure 3 | Multi-levels of activities and the dynamic connection between demand, activity, dependency, and time.\n\nDefines a framework for modeling sustainable development by linking activities to demand, capacity, and substitution.",
    "original_text": "Wedefine a set of n time-dependent activities , i , with label ai ( t ), i 5 1, …, N , each with capacity c i ( t ), c i ( t ) 5 h xi ( t )/ h t , D ~ X i ~ 1 , ... N ci t ð Þ .\nWe use the notion of capacity as that required or needed to meet a certain demand, contrary to the available capacity that may exceed the requirements of the demand. Consequently the resource xi is diminished according to this rate in response to meeting a single demand D .\nAnactivity is equivalent to a set of actions or processes with a base , 5 1, and , 5 2, …, N representing substitution or replacement activities. To lighten the notation, we hereafter drop the time in the following definitions.\nDefinition 1 . A general activity ,\n<!-- formula-not-decoded -->\nis , 2 1 removed from the base activity and has capacity c ' i , j , k , ... , using resources x ' i , j , k , ... . The substitution level , has an equivalent number of running indices i, j, k, ….\nBase activities a 1 i , i 5 1, … ni and substitutes , 5 2, 3, … are defined for each i . The first substitute activities are a 2 i , j i j i : 5 1, …, n i , j , etc. A level-dependent n replaces N . To lighten the notation, we drop the i subscript on j hereafter.\nReturning to our short trip example, supposing that the bike is chosen as the mode of transportation, then this single activity meets the demand, c 1 [Joules] 5 D [Joules], fulfilled by the consumption of an apple. An activity may require several resources. For example, a multiple list of objects, representing the four basic food groups,\n<!-- formula-not-decoded -->\nmight be desirable to fulfill the needed demand. Substitution may occur here, if 'tuna' x 1 1 is not available but a another fish (e.g., salmon or grouper) is and would be designated a 2 1 . This example is time independent. An example of a base activity and three levels of dependencies are sketched in Figure 1. Here we show one capacity at the base level c 1 1 and its substitutes up through level five. Both renewable and non-renewable resources are shown along with a proposed actual example.\nDefinition 2 . The activity a ' i , j , ... occurs over a duration t ' i , j , ... and has a maximum allowable duration t ' , max i , j , ... ,\n<!-- formula-not-decoded -->\nThis condition ensures that each activity occurs in a feasible time period defined either when the activity is available or when resources are available.\nTwo assumptions have been made: First, we have considered a nonrenewable resource. A time dependent and renewable stock (e.g., a fish population), can be treated in this same framework, but with additional considerations on x ' i , j , ... , see Appendix (A dynamic reservoir). Second, we have assumed that only one activity is associated with a resource. In reality, several activities can be linked to a single resource and, in this case, we need to partition the resource for each activity. A multiple and simultaneous set of demands can be subsequently met by individual resources xi . Petroleum meeting the demand for transportation xpetrol R trans and petroleum used as raw material for the plastic industry, xpetrol R plastic , is an example. The resource xpetrol R trans is implicitly used only for transportation while xpetrol R plastic is implicitly only used for plastics, xtotpetrol 5 xpetrol R trans 1 xpetrol R plastic . The uncertainty in the demand for transportation and plastics implies that some of the petroleum reservoir initially allocated for transportation could be used for the plastic industry and visa-versa. The time dependency on the activity, the cost, the demand, and any other parameters is implicit and generally not known. For these reasons, only a limited amount of modeling can be done for long-term environmental impact and cost.\nThe cost Ci for activity ai for a one-level dependent activity (temporarily dropping the activity subscript) is composed of three parts: economic CF , social CS , and environmental CE . For extended planning, the technoeconomic costs include the standard investment, operational (maintenance), energy and replacement (salvage) values. The environmental cost CE is proportional to the impact I , the pollutant p , and ultimately the required capacity c , using a set of conversion constants, a , b , and c ,\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nIn this simple example where neither feedback nor other non-linear behavior occur, the environmental cost is directly proportional to the pollutant. Even in this example, the pollutant may accumulate over time and the net pollution p , resulting from the activity, can be diminished by dispersive action and/or by sequestration. Given a constant activity over a period of time t , the accumulated pollution pACC and cost are,\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere g 5 a 3 b 3 c . Behind this simple relationship lies environmental impacts and therefore costs having non-linear responses to\nFigure 2 | A set of single-level activities fulfilling a flat demand D.\nthe accumulated pollution levels resulting from complex physical processes and dynamic capacities.\nFinally, social costs are even more challenging to model, but in principle, CS ( t ), if known or agreed upon, can also be included in a dynamic way.\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nDefinition 4 . If either the duration, Eq. 2, or the costs, Eqs. 9 - 11, are not satisfied, then substitution may occur if it is available,\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nThough it is possible that c ' i , j , ... § c ' z 1 i , j , ... , there is no guarantee that any additional capacity will be useful for other activities. For example, replacing one type of bread with another bread is acceptable, yet a salad replaced with twice as much bread would not meet the requirement (demand) for providing the four food groups.\niff, and\nFigure 3 | Multi-levels of activities and the dynamic connection between demand, activity, dependency, and time.",
    "context": "Defines a framework for modeling sustainable development by linking activities to demand, capacity, and substitution.",
    "document": "srep05215.pdf",
    "pages": [
      3,
      4,
      5
    ],
    "id": "5901258cf8699779bcbe99240f274a9e6539bdacd0eb99c70f2ebc8965cd80be"
  },
  {
    "text": "Consider a set of n independent activities with only one level (no substitution possible), a 1 i , i 5 1, … n , with capacities c 1 i , durations t 1 i , total cost C 1 i , and each partially fulfilling a demand D in period t 5 [ t a , t b ], then (using light notation),\nX\n<!-- formula-not-decoded -->\nThe time dependent activities of capacities c 1 i are sketched in Figure 2. In this example, the capacities are time dependent but with equivalent duration t .\n\nProvides a framework for assessing the sustainability of activities, considering capacities, durations, and costs.  Introduces a model for evaluating activities within a dynamic system, highlighting the importance of considering both renewable and non-renewable resources and potential substitution.",
    "original_text": "Consider a set of n independent activities with only one level (no substitution possible), a 1 i , i 5 1, … n , with capacities c 1 i , durations t 1 i , total cost C 1 i , and each partially fulfilling a demand D in period t 5 [ t a , t b ], then (using light notation),\nX\n<!-- formula-not-decoded -->\nThe time dependent activities of capacities c 1 i are sketched in Figure 2. In this example, the capacities are time dependent but with equivalent duration t .",
    "context": "Provides a framework for assessing the sustainability of activities, considering capacities, durations, and costs.  Introduces a model for evaluating activities within a dynamic system, highlighting the importance of considering both renewable and non-renewable resources and potential substitution.",
    "document": "srep05215.pdf",
    "pages": [
      5
    ],
    "id": "02941fbaa3b1681579806bea3c3fcdec722c241ddc3771661989fab87fc396e0"
  },
  {
    "text": "Consider a set of activities a ' i , j , ... with N levels, each using resources, x ' i , j , ... with capacities c ' i , j , ... and with base activities a 1 i each partially fulfilling a demand D in a period t . Wenow distinguish between the set of all sustainable a ' i , j , ... [ S ' , unsustainable ~ a ' i , j , ... [ ~ S ' , and total activities /C22 S ' ~ ~ S ' | S ' . A demand D is fulfilled when there exists a combination of activities, via substitution, X\nX\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n!\n<!-- formula-not-decoded -->\n. . .\nN times ð Þ\n. . .\n0\n1\n!\n<!-- formula-not-decoded -->\nEquation 22 is truncated on the third term, and therefore, the unsustainable term shows up in this example and is assumed to be replaced by either sustainable or unsustainable capacities of higher levels, if they exit. Then the set capacities, represented by the activities a ' i , j , ... n o x ~ a ' i , j , ... , for all i , j , …, and all , 5 1, …, N , is a collection of (sustainable) sets, and if,\n/C8\n/C9\n<!-- formula-not-decoded -->\nis an indexed family of sets S , , then C is a topological cover of x if [\n<!-- formula-not-decoded -->\nan N -level union of S , . A similar argument can be made for the unsustainable set ~ x . Figure 3 sketches a dynamic, multi-layer picture of activities and their dependencies. Here, details of a second level are also shown. Combining Definitions 1, 2, and 4 and Eqs. 22, we propose a condition for sustainability.\nProposition 1 . Demand D, satisfied by sustainable activities a ' i , j , ... , with capacities c ' i , j , ... , in period t ,\n0\n1\n!\n<!-- formula-not-decoded -->\nis sustainable if the following conditions are met ,\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nand\nX\n<!-- formula-not-decoded -->\nor if the collection of sustainable sets, n o\n<!-- formula-not-decoded -->\n[\n<!-- formula-not-decoded -->\n/C8\nwhere\n/C9\n<!-- formula-not-decoded -->\nwhere C is a cover of x , and x is a set of activities with capacities c, such that, X\n<!-- formula-not-decoded -->\nwhere V /C3 5 V r | V nr , represents all i, j, … that fulfill the demand; V r and V nr are the sets of renewables and non-renewables available to V * . Subsequently, x is a subset of an N-level union of S , . If Conditions 1 - 3 hold then D is fulfilled by sustainable activities, a, either directly (Condition 3 - strong sustainability) or by substitution (Condition 3 - weak sustainability).\nThese definitions are not to be confused with Strong and Weak Sustainability based on the use of natural capital 1 . A list of parameters used in the definitions is given in Table 2. Equations 22 27 do not represent an optimization problem, but by equating the activities to the demand, the formulation is structured similar to that of a dual solution typically found in cost minimization (linear program) problems with constraints.\nTable 2 | List of parameters and indices used in the model\n\ni,j,… a , b ,…, Definition = Indices for direct activities in level one ( i ), level two ( j ), etc. Indices for first level indirect activities, second level indirect activities, etc.. ,, Definition = Level (of substitution).. a, Definition = Activity.. c, Definition = Capacity of activity.. x, Definition = Resource stock of activity.. t, Definition = Duration of activity.. C F ,C E ,C S ,, Definition = Cost of activity: economic, environmental, social.. D, Definition = Demand.. C, Definition = Cover of sustainable activities.. S , , ~ S ' , /C22 S ', Definition = Set of all sustainable, unsustainable, and total activities of level , .. x , ~ x , /C22 x, Definition = Set of all sustainable, unsustainable, and total activities.\n\nDefines a framework for modeling activities and their impact, including capacity, substitution, and temporal behavior. Proposes a condition for sustainability based on a set of activities fulfilling a demand, considering both sustainable and unsustainable options.",
    "original_text": "Consider a set of activities a ' i , j , ... with N levels, each using resources, x ' i , j , ... with capacities c ' i , j , ... and with base activities a 1 i each partially fulfilling a demand D in a period t . Wenow distinguish between the set of all sustainable a ' i , j , ... [ S ' , unsustainable ~ a ' i , j , ... [ ~ S ' , and total activities /C22 S ' ~ ~ S ' | S ' . A demand D is fulfilled when there exists a combination of activities, via substitution, X\nX\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n!\n<!-- formula-not-decoded -->\n. . .\nN times ð Þ\n. . .\n0\n1\n!\n<!-- formula-not-decoded -->\nEquation 22 is truncated on the third term, and therefore, the unsustainable term shows up in this example and is assumed to be replaced by either sustainable or unsustainable capacities of higher levels, if they exit. Then the set capacities, represented by the activities a ' i , j , ... n o x ~ a ' i , j , ... , for all i , j , …, and all , 5 1, …, N , is a collection of (sustainable) sets, and if,\n/C8\n/C9\n<!-- formula-not-decoded -->\nis an indexed family of sets S , , then C is a topological cover of x if [\n<!-- formula-not-decoded -->\nan N -level union of S , . A similar argument can be made for the unsustainable set ~ x . Figure 3 sketches a dynamic, multi-layer picture of activities and their dependencies. Here, details of a second level are also shown. Combining Definitions 1, 2, and 4 and Eqs. 22, we propose a condition for sustainability.\nProposition 1 . Demand D, satisfied by sustainable activities a ' i , j , ... , with capacities c ' i , j , ... , in period t ,\n0\n1\n!\n<!-- formula-not-decoded -->\nis sustainable if the following conditions are met ,\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nand\nX\n<!-- formula-not-decoded -->\nor if the collection of sustainable sets, n o\n<!-- formula-not-decoded -->\n[\n<!-- formula-not-decoded -->\n/C8\nwhere\n/C9\n<!-- formula-not-decoded -->\nwhere C is a cover of x , and x is a set of activities with capacities c, such that, X\n<!-- formula-not-decoded -->\nwhere V /C3 5 V r | V nr , represents all i, j, … that fulfill the demand; V r and V nr are the sets of renewables and non-renewables available to V * . Subsequently, x is a subset of an N-level union of S , . If Conditions 1 - 3 hold then D is fulfilled by sustainable activities, a, either directly (Condition 3 - strong sustainability) or by substitution (Condition 3 - weak sustainability).\nThese definitions are not to be confused with Strong and Weak Sustainability based on the use of natural capital 1 . A list of parameters used in the definitions is given in Table 2. Equations 22 27 do not represent an optimization problem, but by equating the activities to the demand, the formulation is structured similar to that of a dual solution typically found in cost minimization (linear program) problems with constraints.\nTable 2 | List of parameters and indices used in the model\n\ni,j,… a , b ,…, Definition = Indices for direct activities in level one ( i ), level two ( j ), etc. Indices for first level indirect activities, second level indirect activities, etc.. ,, Definition = Level (of substitution).. a, Definition = Activity.. c, Definition = Capacity of activity.. x, Definition = Resource stock of activity.. t, Definition = Duration of activity.. C F ,C E ,C S ,, Definition = Cost of activity: economic, environmental, social.. D, Definition = Demand.. C, Definition = Cover of sustainable activities.. S , , ~ S ' , /C22 S ', Definition = Set of all sustainable, unsustainable, and total activities of level , .. x , ~ x , /C22 x, Definition = Set of all sustainable, unsustainable, and total activities.",
    "context": "Defines a framework for modeling activities and their impact, including capacity, substitution, and temporal behavior. Proposes a condition for sustainability based on a set of activities fulfilling a demand, considering both sustainable and unsustainable options.",
    "document": "srep05215.pdf",
    "pages": [
      5,
      6
    ],
    "id": "4603ef5df13f3a09d6cf9a4c25320b7b5fd98175fa7b16a3e86f5d99feb266d3"
  },
  {
    "text": "Examples are useful to elaborate the notion of activities and their classification. We start with the simple example, the need (demand) to take a short trip and propose that only two modes of transportation are available: a bike and a car. First, we consider the resources required by each activity. Does the biker have enough stored biochemical energy available or is there enough gas in the car tank? If not, what are the costs of a snack for the biker to 'energize up' or the costs to refuel the car? If a longer trip was planned, we would have to include the purchase (replacement) and operational costs (maintenance) for both car and bike and include the amount recovered (salvage value) for each. For longer planning scenarios, covering decades of time, we must also consider the projected change of technology (e.g., a petroleum or diesel car replaced by a fuel cell car).\nIf the bike is chosen and the rider has enough energy to make the trip, then the trip is sustainable since t , t max and we assume Cbike , C max . According to this scheme, the journey completed by a car is also sustainable if the car completes the journey before the tank is empty ( t , t max ) and Ccar , C max .\nOther indirect consequences come out of this example. Theoretically, the 'clean' bike trip can negatively impact the environment. Prior to the trip, the bicyclist might be inclined to eat an apple for energy and therefore indirectly supports an apple farmer. A truck driver who is transporting the apple to the market is also supported by this activity, albeit marginally. The truck consequently contributes to the emission load. Therefore, secondary environmental costs that are not usually foreseen when riding a bike might be taken into account. To be thorough, we should not stop here; we should include the manufacturing costs needed to build the truck, bike, or car. We should also include the impact from the exhaust of all the commuters who journeyed to and from the factory to build the bike, car, or truck. The list would go on and on and the problem\nTable 3 | Eight possibilities for sustainable (S) or unsustainable (US) activities\n\nUS US US US US US, Level(s).Level(s) = single single. US US US US US US, Duration.t , t max = no. US US US US US US, Cost.C , C max = no. , Level(s).Level(s) = . , Duration.t , t max = no. , Cost.C , C max = yes. , Level(s).Level(s) = single. , Duration.t , t max = yes. , Cost.C , C max = no. S/US, Level(s).Level(s) = single. S/US, Duration.t , t max = yes. S/US, Cost.C , C max = yes. , Level(s).Level(s) = multiple. , Duration.t , t max = no. , Cost.C , C max = no. , Level(s).Level(s) = multiple. , Duration.t , t max = no. , Cost.C , C max = yes. , Level(s).Level(s) = multiple. , Duration.t , t max = yes. , Cost.C , C max = no. S/US, Level(s).Level(s) = multiple. S/US, Duration.t , t max = yes. S/US, Cost.C , C max = yes\nquickly becomes an intractable one to assess. Although arguably small, the ramifications of activities should be considered. To a degree, these are reminiscent of the Life Cycle Assessment considerations of the 'cradle-to-grave' analysis and Boundary Critique issues 45 associated with typical rigid boundaries of problems. The model should be expandable to account for these issues by adding 'higher-order terms'. The question of how these terms are weighted is probably less important than being consistent. (See Appendix item Generalizing the activity to include higher-order dependencies.)\nOne method to determine the higher-order terms in this case would be using the fraction of the weight of the one apple eaten compared to the weight of the entire lot on the truck. This information might be very difficult to ascertain and statistical models would be needed. The number of linked activities or levels N or the number of higher-order considerations will each have to be determined in light of the knowledge and uncertainty of the problem. When the higher-order term is sufficiently uncertain, the calculation should then stop.\nNow consider two sets of capacities, renewables V r and nonrenewables V nr , as given in Figure 1,\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n/C8\n/C9\n/C8\n/C9\nc 1 1 , c 2 11 , c 4 1121 , c 4 1122 [ V nr , c 3 111 , c 5 11221 , c 3 112 [ V r , and as an example, an 'actual' situation, c 3 111 , c 4 1121 , c 5 11221 , c 2 12 /C8 /C9 [ V /C3 , V /C3 ( V nr | V nr . Using the capacities c r and c nr and current levels c r (0), c nr (0),\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere f nr ( t ) and f r ( t ) are representative growth or decay functions for renewables and non-renewables and from Eq. 19,\n<!-- formula-not-decoded -->\nThis example represents, among many others, a mixed resources problem with uniform evolution functions for renewable and nonrenewable resources. Though the demand in this example is constant, it could be time dependent and therefore represents a national transportation activity. Scenarios developing replacement technologies (fuel cells) using renewable energies (solar, wind, etc.), for the most part, have not yet been developed on large scales, and therefore, the uncertainty of replaceable technologies limits the knowledge for substitution. Not surprisingly, this important demand is considered unsustainable and, subsequently, is a major area of concern and requires ongoing development and research.\nApart from the previously mentioned energy, transportation and ecological applications, other prime domains include agriculture, business, and architecture, and these can also be addressed with the model. A number of parameters and concepts can directly address the general definitions. Sustainable agriculture, for example, is characterized by a number of requirements and can be met by the framework developed here. These include:\n1. Satisfying human food and fiber needs (activities meeting a demand, Eqs. 18, 19).\n2. Enhancing environmental quality and the natural resource based upon the agricultural economy (reducing I and therefore CE , Eqs. 4-6).\n3. Making the most efficient use of non-renewable resources and on-farm resources (replacing ~ c 0 s with higher level c 's or with other c 's having lower costs).\n4. Integrating, where appropriate, natural biological cycles and controls (replacing ~ c 0 s with higher level c 's or with other c 's having lower costs, Eqs. 19-22).\n5. Sustaining the economic viability of farm operations (reducing CF , Eq. 9), and enhancing the quality of life for farmers and society as a whole (reducing CS , Eq. 10) 48 .\nSustainable business includes activities that are environmentally friendly processes, products, and manufacturing activities (reducing CE , Eqs. 4-6, 11) while maintaining a profit (reducing CF , Eq. 9) 49 .\nFinally, sustainable architecture is typified by the promotion of smart growth and shorter commuting distances (reducing CE Eqs.46, 11), reducing the environmental footprint (or activities with acceptable CF , Eq. 10 and CE , Eqs. 4-6, 11), and avoidance of globally uniform design (reducing CS , Eq. 10).\n\nProvides supporting evidence for the argument on economic inequality.",
    "original_text": "Examples are useful to elaborate the notion of activities and their classification. We start with the simple example, the need (demand) to take a short trip and propose that only two modes of transportation are available: a bike and a car. First, we consider the resources required by each activity. Does the biker have enough stored biochemical energy available or is there enough gas in the car tank? If not, what are the costs of a snack for the biker to 'energize up' or the costs to refuel the car? If a longer trip was planned, we would have to include the purchase (replacement) and operational costs (maintenance) for both car and bike and include the amount recovered (salvage value) for each. For longer planning scenarios, covering decades of time, we must also consider the projected change of technology (e.g., a petroleum or diesel car replaced by a fuel cell car).\nIf the bike is chosen and the rider has enough energy to make the trip, then the trip is sustainable since t , t max and we assume Cbike , C max . According to this scheme, the journey completed by a car is also sustainable if the car completes the journey before the tank is empty ( t , t max ) and Ccar , C max .\nOther indirect consequences come out of this example. Theoretically, the 'clean' bike trip can negatively impact the environment. Prior to the trip, the bicyclist might be inclined to eat an apple for energy and therefore indirectly supports an apple farmer. A truck driver who is transporting the apple to the market is also supported by this activity, albeit marginally. The truck consequently contributes to the emission load. Therefore, secondary environmental costs that are not usually foreseen when riding a bike might be taken into account. To be thorough, we should not stop here; we should include the manufacturing costs needed to build the truck, bike, or car. We should also include the impact from the exhaust of all the commuters who journeyed to and from the factory to build the bike, car, or truck. The list would go on and on and the problem\nTable 3 | Eight possibilities for sustainable (S) or unsustainable (US) activities\n\nUS US US US US US, Level(s).Level(s) = single single. US US US US US US, Duration.t , t max = no. US US US US US US, Cost.C , C max = no. , Level(s).Level(s) = . , Duration.t , t max = no. , Cost.C , C max = yes. , Level(s).Level(s) = single. , Duration.t , t max = yes. , Cost.C , C max = no. S/US, Level(s).Level(s) = single. S/US, Duration.t , t max = yes. S/US, Cost.C , C max = yes. , Level(s).Level(s) = multiple. , Duration.t , t max = no. , Cost.C , C max = no. , Level(s).Level(s) = multiple. , Duration.t , t max = no. , Cost.C , C max = yes. , Level(s).Level(s) = multiple. , Duration.t , t max = yes. , Cost.C , C max = no. S/US, Level(s).Level(s) = multiple. S/US, Duration.t , t max = yes. S/US, Cost.C , C max = yes\nquickly becomes an intractable one to assess. Although arguably small, the ramifications of activities should be considered. To a degree, these are reminiscent of the Life Cycle Assessment considerations of the 'cradle-to-grave' analysis and Boundary Critique issues 45 associated with typical rigid boundaries of problems. The model should be expandable to account for these issues by adding 'higher-order terms'. The question of how these terms are weighted is probably less important than being consistent. (See Appendix item Generalizing the activity to include higher-order dependencies.)\nOne method to determine the higher-order terms in this case would be using the fraction of the weight of the one apple eaten compared to the weight of the entire lot on the truck. This information might be very difficult to ascertain and statistical models would be needed. The number of linked activities or levels N or the number of higher-order considerations will each have to be determined in light of the knowledge and uncertainty of the problem. When the higher-order term is sufficiently uncertain, the calculation should then stop.\nNow consider two sets of capacities, renewables V r and nonrenewables V nr , as given in Figure 1,\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n/C8\n/C9\n/C8\n/C9\nc 1 1 , c 2 11 , c 4 1121 , c 4 1122 [ V nr , c 3 111 , c 5 11221 , c 3 112 [ V r , and as an example, an 'actual' situation, c 3 111 , c 4 1121 , c 5 11221 , c 2 12 /C8 /C9 [ V /C3 , V /C3 ( V nr | V nr . Using the capacities c r and c nr and current levels c r (0), c nr (0),\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nwhere f nr ( t ) and f r ( t ) are representative growth or decay functions for renewables and non-renewables and from Eq. 19,\n<!-- formula-not-decoded -->\nThis example represents, among many others, a mixed resources problem with uniform evolution functions for renewable and nonrenewable resources. Though the demand in this example is constant, it could be time dependent and therefore represents a national transportation activity. Scenarios developing replacement technologies (fuel cells) using renewable energies (solar, wind, etc.), for the most part, have not yet been developed on large scales, and therefore, the uncertainty of replaceable technologies limits the knowledge for substitution. Not surprisingly, this important demand is considered unsustainable and, subsequently, is a major area of concern and requires ongoing development and research.\nApart from the previously mentioned energy, transportation and ecological applications, other prime domains include agriculture, business, and architecture, and these can also be addressed with the model. A number of parameters and concepts can directly address the general definitions. Sustainable agriculture, for example, is characterized by a number of requirements and can be met by the framework developed here. These include:\n1. Satisfying human food and fiber needs (activities meeting a demand, Eqs. 18, 19).\n2. Enhancing environmental quality and the natural resource based upon the agricultural economy (reducing I and therefore CE , Eqs. 4-6).\n3. Making the most efficient use of non-renewable resources and on-farm resources (replacing ~ c 0 s with higher level c 's or with other c 's having lower costs).\n4. Integrating, where appropriate, natural biological cycles and controls (replacing ~ c 0 s with higher level c 's or with other c 's having lower costs, Eqs. 19-22).\n5. Sustaining the economic viability of farm operations (reducing CF , Eq. 9), and enhancing the quality of life for farmers and society as a whole (reducing CS , Eq. 10) 48 .\nSustainable business includes activities that are environmentally friendly processes, products, and manufacturing activities (reducing CE , Eqs. 4-6, 11) while maintaining a profit (reducing CF , Eq. 9) 49 .\nFinally, sustainable architecture is typified by the promotion of smart growth and shorter commuting distances (reducing CE Eqs.46, 11), reducing the environmental footprint (or activities with acceptable CF , Eq. 10 and CE , Eqs. 4-6, 11), and avoidance of globally uniform design (reducing CS , Eq. 10).",
    "context": "Provides supporting evidence for the argument on economic inequality.",
    "document": "srep05215.pdf",
    "pages": [
      6,
      7
    ],
    "id": "e9e9c05fe24d6f932428f286181713becd41ab2bbdf2d06516109205e89e8e3d"
  },
  {
    "text": "An activity is characterized by cost, duration, and whether it is a single or multi-level activity. A natural set of 2 3 possibilities representing these combinations are given in Table 3. We note that the single or multi-level classification is determined by the activity using a resource (energy or stock) that is being depleted over the duration of the activity. This is distinguished from the wear or small depredation (e.g., tire wear during a bike ride) of the material in period t .\nAset of activities where higher-order terms can be important, such as the harvesting of fish (e.g., the Atlantic bluefin tuna), may be tested for sustainability according to the scheme. The harvesting of tuna will also affect the lower parts of the food chain (e.g., herring, mackerel, sardine), and in turn, these will affect even smaller fish. An unsustainable situation will arise if any of the links, all critical to the higher link, becomes unsustainable or if substitution was not possible at any of the endangered links. Unmanaged activity could lead to unfavourable consequences (e.g., overshoot and collapse of the tuna, mackerel, or sardine population).\n\nCharacterizes activities based on cost, duration, and level of complexity, highlighting the distinction between resource depletion and minor wear.  Suggests that assessing higher-order impacts, such as fisheries, is crucial for sustainability evaluation.",
    "original_text": "An activity is characterized by cost, duration, and whether it is a single or multi-level activity. A natural set of 2 3 possibilities representing these combinations are given in Table 3. We note that the single or multi-level classification is determined by the activity using a resource (energy or stock) that is being depleted over the duration of the activity. This is distinguished from the wear or small depredation (e.g., tire wear during a bike ride) of the material in period t .\nAset of activities where higher-order terms can be important, such as the harvesting of fish (e.g., the Atlantic bluefin tuna), may be tested for sustainability according to the scheme. The harvesting of tuna will also affect the lower parts of the food chain (e.g., herring, mackerel, sardine), and in turn, these will affect even smaller fish. An unsustainable situation will arise if any of the links, all critical to the higher link, becomes unsustainable or if substitution was not possible at any of the endangered links. Unmanaged activity could lead to unfavourable consequences (e.g., overshoot and collapse of the tuna, mackerel, or sardine population).",
    "context": "Characterizes activities based on cost, duration, and level of complexity, highlighting the distinction between resource depletion and minor wear.  Suggests that assessing higher-order impacts, such as fisheries, is crucial for sustainability evaluation.",
    "document": "srep05215.pdf",
    "pages": [
      7
    ],
    "id": "0c6ce9940064f36379d33190947da453989ee6ce4cd914317b4202dbc7ebb3c9"
  },
  {
    "text": "Although the three-dimensional (pillar) view of sustainability is widely accepted, there remains a number of fundamental criticisms in terms of its modelling, especially in terms of the social pillar and its interfaces. Literature demonstrates a degree of criticism in terms of the interfaces, especially between the environmental and social dimensions 15 . Theys 46 provides a way to proceed, noting that:\nLocal territories are the level at which the questions of socially sustainable development become concrete and where the interactions between the different dimensions are most explicit, and where participation and dialogue are the most feasible.\nTable 4 | Response of the model to the theoretical perspectives (See text and Table 2 for symbols)\n\n1. Equilibrium-Neoclassical 2. Neo-Austrian-Temporal 3. Ecological-Evolutionary 4. Evolutionary-Technological 5. Physico-Economic 6. Biophysical-Energy 7. Systems-Ecological 8. Ecological Engineering 9. Human Ecology 10. Socio-Biological 11. Historical-Institutional 12. Ethical-Utopian, Model = S , C F , C E , t F , t E S , C F , C E , t F , t E A , S , C E , t E A , S C F , C E , t F , t E A , C E , t E A , S , C E , t E A , S , C E , C S , t E , t S A , S , C E , C S , t E , t S C E , C S , t E , t S A , S , C , t A , S , C E , C S , t E , t S\nNosingle approach has yet emerged to bridge the gap between the local social issues, involving geographers, planners and landowners who deal with issues that immediately impact the local community, and global issues, involving economists, large enterprises, consumer organisations, international NGOs and diplomats who deal more with topics revolving around the global commons such as eco-taxes and emissions trading 46,15 . In the absence of well-defined analytic constructs such as the higher-order terms and social costs, social scientists will have difficulty connecting the local and global in an analytic way. We also note similar activities may have similar costs, and in this case a simplification could be made, e.g. CE 5 CE , i for i representing two or more activities. Theys recommends a multi-tier system to address these separate social issues, similar to the indirect (higher-order) terms considered here.\nBeyond this concern, connecting the social to the other dimensions is still problematic. An important study in the late 1990s was conducted by the Dutch Ministry of the Environment with the aim of gaining knowledge on the environmental and social interactions. A set of indicators was used to build an operational linkage between the social and environmental dimensions. As suspected, the study found that in some cases, social policies (e.g., reinforcement of consumption) were determined to adversely affect the environment (e.g., loss in eco-efficiency) 15,47 . The study concluded that in the absence of economic indicators, the model was too rudimentary to represent the complex interactions and causal relations between social and environmental indicators 15 . It was concluded that a holistic framework 47 encompassing the three dimensions was still needed but that treatment of the social-environmental interaction was most easily handled, as previously mentioned, at the local level 46 . Translated to the approach presented here, the local or base activity , 5 1 is where social issues become more concrete. Perhaps consensus at the local level is one way to proceed. On the other hand, consensus of social costs on the global level, although more difficult, presents more noble ambitions in terms of tackling the issues of sustainability.\nAs noted earlier, a good model should be 'falsifiable' to the criteria it was meant to test; clearly some examples in the economic and environmental dimension are more readily testable than in the social dimension. Indeed, a proper balance must be achieved when providing numbers for costs, durations and substitutable activities, when knowledge permits. Only when these figures with their respective levels of uncertainty are tabulated can a full validation be determined.\nAlthough numbers are not given here, Table 4 does provide a starting point in response to the not-so-quantifiable theoretical issues of sustainability. The terms CF , CE , CS , C , t F , t E , t S , t , the characterization of the activity A , and the possibility of substitution S indicates where the model could respond.\nThe model can be used to provide a response function for activities that use a mix of renewable and non-renewable resources. Any attempt to model these functions must consider the uncertainties in the dynamics of the demand and resources available, both for the present and the future.\nFinally, we conclude that there is no panacea for solving all of the issues. Though the economic values can be determined from markets and environmental costs from measurements and models, the social costs will continue to be difficult to pin down. We observe from Table 1 the terms 'balance', 'restrictions', 'maintaining', 'controlling', 'integrating', 'preventing' are difficult to quantify. Some social issues, e.g., Socio-Biological, do not lend themselves easily to a quantitative model while others, e.g., Ecological Engineering, involving ecological resilience, do. Evidently, the role of uncertainty analysis will continue to play an important role in advancing the work and in connecting the dimensions in a coherent way.\n\nHighlights the challenges in modeling the social pillar of sustainability, particularly in connecting local social issues with global concerns, and suggests that consensus at the local level may be a useful approach.",
    "original_text": "Although the three-dimensional (pillar) view of sustainability is widely accepted, there remains a number of fundamental criticisms in terms of its modelling, especially in terms of the social pillar and its interfaces. Literature demonstrates a degree of criticism in terms of the interfaces, especially between the environmental and social dimensions 15 . Theys 46 provides a way to proceed, noting that:\nLocal territories are the level at which the questions of socially sustainable development become concrete and where the interactions between the different dimensions are most explicit, and where participation and dialogue are the most feasible.\nTable 4 | Response of the model to the theoretical perspectives (See text and Table 2 for symbols)\n\n1. Equilibrium-Neoclassical 2. Neo-Austrian-Temporal 3. Ecological-Evolutionary 4. Evolutionary-Technological 5. Physico-Economic 6. Biophysical-Energy 7. Systems-Ecological 8. Ecological Engineering 9. Human Ecology 10. Socio-Biological 11. Historical-Institutional 12. Ethical-Utopian, Model = S , C F , C E , t F , t E S , C F , C E , t F , t E A , S , C E , t E A , S C F , C E , t F , t E A , C E , t E A , S , C E , t E A , S , C E , C S , t E , t S A , S , C E , C S , t E , t S C E , C S , t E , t S A , S , C , t A , S , C E , C S , t E , t S\nNosingle approach has yet emerged to bridge the gap between the local social issues, involving geographers, planners and landowners who deal with issues that immediately impact the local community, and global issues, involving economists, large enterprises, consumer organisations, international NGOs and diplomats who deal more with topics revolving around the global commons such as eco-taxes and emissions trading 46,15 . In the absence of well-defined analytic constructs such as the higher-order terms and social costs, social scientists will have difficulty connecting the local and global in an analytic way. We also note similar activities may have similar costs, and in this case a simplification could be made, e.g. CE 5 CE , i for i representing two or more activities. Theys recommends a multi-tier system to address these separate social issues, similar to the indirect (higher-order) terms considered here.\nBeyond this concern, connecting the social to the other dimensions is still problematic. An important study in the late 1990s was conducted by the Dutch Ministry of the Environment with the aim of gaining knowledge on the environmental and social interactions. A set of indicators was used to build an operational linkage between the social and environmental dimensions. As suspected, the study found that in some cases, social policies (e.g., reinforcement of consumption) were determined to adversely affect the environment (e.g., loss in eco-efficiency) 15,47 . The study concluded that in the absence of economic indicators, the model was too rudimentary to represent the complex interactions and causal relations between social and environmental indicators 15 . It was concluded that a holistic framework 47 encompassing the three dimensions was still needed but that treatment of the social-environmental interaction was most easily handled, as previously mentioned, at the local level 46 . Translated to the approach presented here, the local or base activity , 5 1 is where social issues become more concrete. Perhaps consensus at the local level is one way to proceed. On the other hand, consensus of social costs on the global level, although more difficult, presents more noble ambitions in terms of tackling the issues of sustainability.\nAs noted earlier, a good model should be 'falsifiable' to the criteria it was meant to test; clearly some examples in the economic and environmental dimension are more readily testable than in the social dimension. Indeed, a proper balance must be achieved when providing numbers for costs, durations and substitutable activities, when knowledge permits. Only when these figures with their respective levels of uncertainty are tabulated can a full validation be determined.\nAlthough numbers are not given here, Table 4 does provide a starting point in response to the not-so-quantifiable theoretical issues of sustainability. The terms CF , CE , CS , C , t F , t E , t S , t , the characterization of the activity A , and the possibility of substitution S indicates where the model could respond.\nThe model can be used to provide a response function for activities that use a mix of renewable and non-renewable resources. Any attempt to model these functions must consider the uncertainties in the dynamics of the demand and resources available, both for the present and the future.\nFinally, we conclude that there is no panacea for solving all of the issues. Though the economic values can be determined from markets and environmental costs from measurements and models, the social costs will continue to be difficult to pin down. We observe from Table 1 the terms 'balance', 'restrictions', 'maintaining', 'controlling', 'integrating', 'preventing' are difficult to quantify. Some social issues, e.g., Socio-Biological, do not lend themselves easily to a quantitative model while others, e.g., Ecological Engineering, involving ecological resilience, do. Evidently, the role of uncertainty analysis will continue to play an important role in advancing the work and in connecting the dimensions in a coherent way.",
    "context": "Highlights the challenges in modeling the social pillar of sustainability, particularly in connecting local social issues with global concerns, and suggests that consensus at the local level may be a useful approach.",
    "document": "srep05215.pdf",
    "pages": [
      7
    ],
    "id": "894b4f5ff477a717381d5c8c400308cf20b2de38fe16ae53e56b9c83d576f8b6"
  },
  {
    "text": "This paper adapts an analytic framework and approach in determining the sustainability of a general activity. Costs are used to\nconnect the three dimensions and their theoretical maximum values, namely economic, social, and environmental. These maximumsare used to test if the activity is sustainable. We also show that when cost and duration constraints are satisfied, an activity is classified as sustainable according to a strong condition (no substitution required) or weak condition (substitution required). We have shown that in the weak case, the set of all sustainable activities is a subset of an N -level union of sustainable activities, the set of which is a topological cover of sustainable activities. The number of levels is limited by the knowledge and uncertainty of the substitutable activities. A simple classification of activities is given, though quantification will be required to advance the work. What is surprising is that the basic framework is simple, closely following the three dimensions (pillars) of sustainability. While these three have been discussed for some time, the literature to connect them has been sparse. Perhaps this methodology will assist in their connection and allow for more quantitative studies in the field of sustainability science.\n\nProvides a framework for assessing sustainability by linking economic, social, and environmental dimensions, using cost and duration constraints to classify activities as sustainable (with or without substitution). Highlights the need for further quantification and suggests the methodology could aid in connecting these sustainability pillars.",
    "original_text": "This paper adapts an analytic framework and approach in determining the sustainability of a general activity. Costs are used to\nconnect the three dimensions and their theoretical maximum values, namely economic, social, and environmental. These maximumsare used to test if the activity is sustainable. We also show that when cost and duration constraints are satisfied, an activity is classified as sustainable according to a strong condition (no substitution required) or weak condition (substitution required). We have shown that in the weak case, the set of all sustainable activities is a subset of an N -level union of sustainable activities, the set of which is a topological cover of sustainable activities. The number of levels is limited by the knowledge and uncertainty of the substitutable activities. A simple classification of activities is given, though quantification will be required to advance the work. What is surprising is that the basic framework is simple, closely following the three dimensions (pillars) of sustainability. While these three have been discussed for some time, the literature to connect them has been sparse. Perhaps this methodology will assist in their connection and allow for more quantitative studies in the field of sustainability science.",
    "context": "Provides a framework for assessing sustainability by linking economic, social, and environmental dimensions, using cost and duration constraints to classify activities as sustainable (with or without substitution). Highlights the need for further quantification and suggests the methodology could aid in connecting these sustainability pillars.",
    "document": "srep05215.pdf",
    "pages": [
      8,
      7
    ],
    "id": "0153de0d600087f79abeec98e12703472d4525487ba23916a0b9bddc6e29cff8"
  },
  {
    "text": "1. Solow, R. M. [Resources for the Future] An Almost Practical Step Toward Sustainability. Res. Pol. 19 , 162-172 (1993).\n2. Fuwa, K. [Defining and Measurement of Sustainability: The Biophysical Foundations in Defining and Measuring Sustainability. The Biogeophysical Foundation [Mohan Munasinghe, Walter Shearer, (Eds.)] (Washington D.C., 1995).\n3. Clayton, A. M. H. & Radcliffe, N. J. Sustainability: A systems approach (Earthscan Publications Limited, London, 1997).\n4. Tester, J. W. et al . Sustainable Energy (M.I.T. Press, Cambridge Massachusetts, 2005).\n5. World Commission on Environment and Development (WCED). Our Common Future (Oxford University Press, New York, 1987).\n6. Annan, K. A. We, the Peoples: The Role of the United Nations in the 21st Century, (United Nations Department of Public Information, NY., 2000).\n7. Dray, A., Perez, P., Le Page, C., D'Aquino, P. & White, I. Who wants to terminate the game? The role of vested interests and metaplayers in the ATOLLGAME experience. Simul. Gaming 38 , 494-511 (2007).\n8. Jansson, A. M. Integration of Economy and Ecology: An Outlook for the Eighties [Jansson, A. M. (ed)] (University of Stockholm Press, Stockholm, 1984).\n9. Polunin, N. Our Global Environment and the World Campaign for the Biosphere. Environ. Conserv. 9 , 115-121 (1982).\n10. Clift, R. Metrics for supply chain sustainability. Clean Tech. and Environ. Pol. 5 , 240-247 (2003).\n11. Seuring, S. & Muller, M. From a literature review to a conceptual framework for sustainable supply chain management. J. Cleaner Prod. 16 , 1699-1710 (2008).\n12. Baumga ¨rtner, S. & Quaas M. What is sustainability economics? Ecol. Econ. 69 , 445-450 (2010).\n13. Kopfmu ¨ller, J. et al . Nachhaltige Entwicklung Integrativ Betrachtet (Sigma-Verlag, Berlin, 2001).\n14. Becker, D. R., Harris, C. C., McLaughlin, W. J. & Nielsen, E. A. A participatory approach to social impact assessment: the interactive community forum. Environ. Impact Assess. Rev., 23 , May 2003, 367-382 (2003).\n15. Lehtonen, M. The environmental-social interface of sustainable development: capabilities, social capital, institutions. Ecol. Econ. 49 , 199-214 (2004).\n16. Mahmoudi, H., Renn, O., Vanclay, F., Hoffmann, V. & Karami, E. A framework for combining social impact assessment and risk assessment. Envir. Impact Assess. Rev., 43 , November, 1-8 (2013).\n17. Kates, R. W. et al . Sustainability Science. Sci. 292 , 641-642 (2001).\n18. Star, S. L. & Griesemer, J. R. Institutional ecology, 'translations' and boundary objects: amateurs and professionals in Berkeley's museum of vertebrate zoology, 1907-39. Soc. Stud. of Sci. 19 , 387-420 (1989).\n19. Wood, D. & Stocker, L. Coastal adaptation to climate change: Towards reflexive governance. Int J Sci in Soc. 1 , 137-145 (2009).\n20. Stocker, L., Burke, G., Kennedy, D. & Wood, D. Sustainability and climate adaptation: Using Google Earth to engage stakeholders. Ecol. Econ. 80 , 15-24 (2012).\n21. Kapp, K. W. Toward a Science of Man in Society (The Hague: Martinus Nijhoff, 1961).\n22. Polanyi, K. The Great Transformation: The Political and Economic Origins of Our Time (Beacon Press, Boston, 1944).\n23. Boulding, K. E. [The economics of the coming spaceship Earth] Environmental Quality in a Growing Economy. Resources for the Future [Jarett, H (ed )][3-14] (Johns Hopkins University Press, Baltimore, 3-14 1966).\n24. Daly, H. E. On economics as a life science. J. Polit. Econ. 76 , 392-406 (1968).\n25. Kastenhofer, K., Bechtold, U. & Wilfing, H. Sustaining sustainability science: The role of established inter-disciplines. Ecol. Econ. 70 , 835-843 (2011).\n26. Costanza, R. & Daly, H. E. (eds.) Ecological Economics. Spec. Iss. Ecol. Mod. , 38 , 190 (1987).\n27. Faber, M. How to be an ecological economist. Ecol. Econ. 66 , 1-7 (2008).\n28. Funtowicz, S. O. & Ravetz, J. R. Science for the post-normal age. Futures 25 , 739-755 (1999).\n29. Gowdy, J. & Mesner, S. The evolution of Georgescu-Roegen's bioeconomics. Rev. Soc. Econ. 56 , 136-156 (1998).\n30. Clark, W. C. & Dickson, N. M. Sustainability science: the emerging research program. Proc. Natl. Acad. Sci., USA 100,8059-8061 (2003).\n31. Kates, R. W. What kind of a science is sustainability science? Proc. Natl. Acad. Sci., USA 108 , 19449-19450 (2011).\n32. Bettencourt, L. M. A. & Kaur, J. Evolution and structure of sustainability science. Proc. Natl. Acad. Sci., USA 108 , 19540-19545 (2011).\n33. Kieffer, S. W., Barton, P., Palmer, A. R., Reitan, P. H. & Zen, E. Megascale events: Natural disasters and human behavior. Geol. Soc. America [Abstracts with programs], 432 (2003).\n34. Berkes, F., Colding, J. & Folke, C. Navigating social-ecological systems. Building resilience for complexity and change (Cambridge University Press, Cambridge, 2003).\n35. Avelino, F. & Rotmans, J. A dynamic conceptualization of power for sustainability research. J. of Cleaner Prod. 19 , 796-804 (2011).\n36. Tainter, J. A. Social complexity and sustainability. Ecol. Compl. 3 , June 2006, 91-103 (2006).\n37. Binder, C. R., Schmid, A. & Steinberger, J. K. Sustainability solution space of the Swiss milk value added chain. J. Ecol. Econ. 83 , 210-220 (2012).\n38. Ronchi, E., Federico, A. & Musmeci, F. A system oriented integrated indicator for sustainable development in Italy. Ecol. Indic. 2 , 197-210 (2002).\n39. Paracchini, M. L., Pacini, C., Jones, L. M. & Pe ´rez-Soba, M. An aggregation framework to link indicators associated with multifunctional land use to the stakeholder evaluation of policy options. Ecol. Indic. 11 ; DOI;10.1016/ j.ecolind.2009.04.006 (2011).\n40. Bergh, van den J. [Sustainable Development and Management]. Ecological economics and sustainable development: Theory, methods, and applications [59] (Edward Elgar Publishing, Cheltenham, U.K., 1996).\n41. Anand, S. & Hanson, K. Disability-adjusted life years: a critical review. J. Health Econ. 16 , 685-702 (1997).\n42. Zachary, D. S., Drouet, L., Leopold, U. & Reis, L. A. Trade-offs between energy cost and health impact in a regional coupled energy-air quality model. Envir. Res. Let. 6 ; DOI:10.1088/1748-9326/6/2/024021 (2011).\n43. Ehrlich, P. R. & Holdren, J. P. Impact of population growth. Sci. 171 , 1212-1217 (1971).\n44. Fishbone, L. G. et al . Users Guide for MARKAL: A Multi-period, Linear Programming Model for Energy Systems Analysis (BNL Upton, NY, and KFA, Julich, Germany, BNL 51701 1983).\n\nProvides a list of academic sources related to sustainability science and modeling, including theoretical frameworks, indicators, and case studies.",
    "original_text": "1. Solow, R. M. [Resources for the Future] An Almost Practical Step Toward Sustainability. Res. Pol. 19 , 162-172 (1993).\n2. Fuwa, K. [Defining and Measurement of Sustainability: The Biophysical Foundations in Defining and Measuring Sustainability. The Biogeophysical Foundation [Mohan Munasinghe, Walter Shearer, (Eds.)] (Washington D.C., 1995).\n3. Clayton, A. M. H. & Radcliffe, N. J. Sustainability: A systems approach (Earthscan Publications Limited, London, 1997).\n4. Tester, J. W. et al . Sustainable Energy (M.I.T. Press, Cambridge Massachusetts, 2005).\n5. World Commission on Environment and Development (WCED). Our Common Future (Oxford University Press, New York, 1987).\n6. Annan, K. A. We, the Peoples: The Role of the United Nations in the 21st Century, (United Nations Department of Public Information, NY., 2000).\n7. Dray, A., Perez, P., Le Page, C., D'Aquino, P. & White, I. Who wants to terminate the game? The role of vested interests and metaplayers in the ATOLLGAME experience. Simul. Gaming 38 , 494-511 (2007).\n8. Jansson, A. M. Integration of Economy and Ecology: An Outlook for the Eighties [Jansson, A. M. (ed)] (University of Stockholm Press, Stockholm, 1984).\n9. Polunin, N. Our Global Environment and the World Campaign for the Biosphere. Environ. Conserv. 9 , 115-121 (1982).\n10. Clift, R. Metrics for supply chain sustainability. Clean Tech. and Environ. Pol. 5 , 240-247 (2003).\n11. Seuring, S. & Muller, M. From a literature review to a conceptual framework for sustainable supply chain management. J. Cleaner Prod. 16 , 1699-1710 (2008).\n12. Baumga ¨rtner, S. & Quaas M. What is sustainability economics? Ecol. Econ. 69 , 445-450 (2010).\n13. Kopfmu ¨ller, J. et al . Nachhaltige Entwicklung Integrativ Betrachtet (Sigma-Verlag, Berlin, 2001).\n14. Becker, D. R., Harris, C. C., McLaughlin, W. J. & Nielsen, E. A. A participatory approach to social impact assessment: the interactive community forum. Environ. Impact Assess. Rev., 23 , May 2003, 367-382 (2003).\n15. Lehtonen, M. The environmental-social interface of sustainable development: capabilities, social capital, institutions. Ecol. Econ. 49 , 199-214 (2004).\n16. Mahmoudi, H., Renn, O., Vanclay, F., Hoffmann, V. & Karami, E. A framework for combining social impact assessment and risk assessment. Envir. Impact Assess. Rev., 43 , November, 1-8 (2013).\n17. Kates, R. W. et al . Sustainability Science. Sci. 292 , 641-642 (2001).\n18. Star, S. L. & Griesemer, J. R. Institutional ecology, 'translations' and boundary objects: amateurs and professionals in Berkeley's museum of vertebrate zoology, 1907-39. Soc. Stud. of Sci. 19 , 387-420 (1989).\n19. Wood, D. & Stocker, L. Coastal adaptation to climate change: Towards reflexive governance. Int J Sci in Soc. 1 , 137-145 (2009).\n20. Stocker, L., Burke, G., Kennedy, D. & Wood, D. Sustainability and climate adaptation: Using Google Earth to engage stakeholders. Ecol. Econ. 80 , 15-24 (2012).\n21. Kapp, K. W. Toward a Science of Man in Society (The Hague: Martinus Nijhoff, 1961).\n22. Polanyi, K. The Great Transformation: The Political and Economic Origins of Our Time (Beacon Press, Boston, 1944).\n23. Boulding, K. E. [The economics of the coming spaceship Earth] Environmental Quality in a Growing Economy. Resources for the Future [Jarett, H (ed )][3-14] (Johns Hopkins University Press, Baltimore, 3-14 1966).\n24. Daly, H. E. On economics as a life science. J. Polit. Econ. 76 , 392-406 (1968).\n25. Kastenhofer, K., Bechtold, U. & Wilfing, H. Sustaining sustainability science: The role of established inter-disciplines. Ecol. Econ. 70 , 835-843 (2011).\n26. Costanza, R. & Daly, H. E. (eds.) Ecological Economics. Spec. Iss. Ecol. Mod. , 38 , 190 (1987).\n27. Faber, M. How to be an ecological economist. Ecol. Econ. 66 , 1-7 (2008).\n28. Funtowicz, S. O. & Ravetz, J. R. Science for the post-normal age. Futures 25 , 739-755 (1999).\n29. Gowdy, J. & Mesner, S. The evolution of Georgescu-Roegen's bioeconomics. Rev. Soc. Econ. 56 , 136-156 (1998).\n30. Clark, W. C. & Dickson, N. M. Sustainability science: the emerging research program. Proc. Natl. Acad. Sci., USA 100,8059-8061 (2003).\n31. Kates, R. W. What kind of a science is sustainability science? Proc. Natl. Acad. Sci., USA 108 , 19449-19450 (2011).\n32. Bettencourt, L. M. A. & Kaur, J. Evolution and structure of sustainability science. Proc. Natl. Acad. Sci., USA 108 , 19540-19545 (2011).\n33. Kieffer, S. W., Barton, P., Palmer, A. R., Reitan, P. H. & Zen, E. Megascale events: Natural disasters and human behavior. Geol. Soc. America [Abstracts with programs], 432 (2003).\n34. Berkes, F., Colding, J. & Folke, C. Navigating social-ecological systems. Building resilience for complexity and change (Cambridge University Press, Cambridge, 2003).\n35. Avelino, F. & Rotmans, J. A dynamic conceptualization of power for sustainability research. J. of Cleaner Prod. 19 , 796-804 (2011).\n36. Tainter, J. A. Social complexity and sustainability. Ecol. Compl. 3 , June 2006, 91-103 (2006).\n37. Binder, C. R., Schmid, A. & Steinberger, J. K. Sustainability solution space of the Swiss milk value added chain. J. Ecol. Econ. 83 , 210-220 (2012).\n38. Ronchi, E., Federico, A. & Musmeci, F. A system oriented integrated indicator for sustainable development in Italy. Ecol. Indic. 2 , 197-210 (2002).\n39. Paracchini, M. L., Pacini, C., Jones, L. M. & Pe ´rez-Soba, M. An aggregation framework to link indicators associated with multifunctional land use to the stakeholder evaluation of policy options. Ecol. Indic. 11 ; DOI;10.1016/ j.ecolind.2009.04.006 (2011).\n40. Bergh, van den J. [Sustainable Development and Management]. Ecological economics and sustainable development: Theory, methods, and applications [59] (Edward Elgar Publishing, Cheltenham, U.K., 1996).\n41. Anand, S. & Hanson, K. Disability-adjusted life years: a critical review. J. Health Econ. 16 , 685-702 (1997).\n42. Zachary, D. S., Drouet, L., Leopold, U. & Reis, L. A. Trade-offs between energy cost and health impact in a regional coupled energy-air quality model. Envir. Res. Let. 6 ; DOI:10.1088/1748-9326/6/2/024021 (2011).\n43. Ehrlich, P. R. & Holdren, J. P. Impact of population growth. Sci. 171 , 1212-1217 (1971).\n44. Fishbone, L. G. et al . Users Guide for MARKAL: A Multi-period, Linear Programming Model for Energy Systems Analysis (BNL Upton, NY, and KFA, Julich, Germany, BNL 51701 1983).",
    "context": "Provides a list of academic sources related to sustainability science and modeling, including theoretical frameworks, indicators, and case studies.",
    "document": "srep05215.pdf",
    "pages": [
      8
    ],
    "id": "9f0ba0a20650675039c6d9a372fc9b49d0510573c96f2c7fa3a7192fe702588a"
  },
  {
    "text": "45. Churchman, C. W. Operations research as a profession. Man. Sci. 17 , 37-53 (1970).\n46. Theys J. L'approche territoriale du ''de ´veloppement durable'', condition d'une prise en compte de sa dimension sociale. De ´veloppement Durable and Territoires, September 23 (2002).\n47. Hukkinen, J. From groundless universalism to grounded generalism: improving ecological economic indicators of human-environmental interaction. Ecol. Econ. 44 , 11-27 (2003).\n48. Gold, M. What is sustainable agriculture? (United States Department of Agriculture, Alternative Farming Systems Information Center, 2009).\n49. Anderson, D. R. The critical importance of sustainability risk management. Risk Manag. 53 , 66-74 (2006).\n\nThis section explores the professionalization of operations research and expands on the concept of sustainability risk management, alongside examining definitions of sustainable agriculture and the importance of integrating sustainability considerations into risk assessment.",
    "original_text": "45. Churchman, C. W. Operations research as a profession. Man. Sci. 17 , 37-53 (1970).\n46. Theys J. L'approche territoriale du ''de ´veloppement durable'', condition d'une prise en compte de sa dimension sociale. De ´veloppement Durable and Territoires, September 23 (2002).\n47. Hukkinen, J. From groundless universalism to grounded generalism: improving ecological economic indicators of human-environmental interaction. Ecol. Econ. 44 , 11-27 (2003).\n48. Gold, M. What is sustainable agriculture? (United States Department of Agriculture, Alternative Farming Systems Information Center, 2009).\n49. Anderson, D. R. The critical importance of sustainability risk management. Risk Manag. 53 , 66-74 (2006).",
    "context": "This section explores the professionalization of operations research and expands on the concept of sustainability risk management, alongside examining definitions of sustainable agriculture and the importance of integrating sustainability considerations into risk assessment.",
    "document": "srep05215.pdf",
    "pages": [
      8
    ],
    "id": "fae8821f53ff2bd74320e5d3c5735e6a84fd96eda29747f4caa72ff62b07a813"
  },
  {
    "text": "The author wishes to acknowledge support from the Ministie `re de la Culture, de l'Enseignement Superieur et de la Recherche (MCESR). The author is also very grateful to William Zachary for his helpful reviewing of the manuscript.\n\nAcknowledges funding support and expresses gratitude for manuscript review.",
    "original_text": "The author wishes to acknowledge support from the Ministie `re de la Culture, de l'Enseignement Superieur et de la Recherche (MCESR). The author is also very grateful to William Zachary for his helpful reviewing of the manuscript.",
    "context": "Acknowledges funding support and expresses gratitude for manuscript review.",
    "document": "srep05215.pdf",
    "pages": [
      8
    ],
    "id": "966f25fc1e064ec85e755e1b96d03311a6866567cc48f0af20334bcb058bca3a"
  },
  {
    "text": "Supplementary information accompanies this paper at http://www.nature.com/ scientificreports\nCompeting financial interests: The authors declare no competing financial interests.\nHow to cite this article: Zachary, D.S. On the sustainability of an activity. Sci. Rep. 4 , 5215; DOI:10.1038/srep05215 (2014).\nThis work is licensed under a Creative Commons Attribution 4.0 International License. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in the credit line; if the material is not included under the Creative Commons license, users will need to obtain permission from the license holder in order to reproduce the material. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/\n\nProvides citation information and details the article's licensing terms.",
    "original_text": "Supplementary information accompanies this paper at http://www.nature.com/ scientificreports\nCompeting financial interests: The authors declare no competing financial interests.\nHow to cite this article: Zachary, D.S. On the sustainability of an activity. Sci. Rep. 4 , 5215; DOI:10.1038/srep05215 (2014).\nThis work is licensed under a Creative Commons Attribution 4.0 International License. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in the credit line; if the material is not included under the Creative Commons license, users will need to obtain permission from the license holder in order to reproduce the material. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/",
    "context": "Provides citation information and details the article's licensing terms.",
    "document": "srep05215.pdf",
    "pages": [
      8
    ],
    "id": "252407baa49253a859aee8b4d0e30a8e8eef9b3dde319db3027b8d1623b1f319"
  },
  {
    "text": "SUBJECT AREAS: APPLIED PHYSICS SCIENTIFIC DATA\nSTATISTICAL PHYSICS, THERMODYNAMICS AND NONLINEAR DYNAMICS\nReceived\n12 November 2013 Accepted 10 March 2014 Published 27 March 2014\nCorrespondence and requests for materials should be addressed to M.C. (matthieu. cristelli@roma1.infn.it)\n\nIntroduces the central thesis about technical trading and its potential for detectable signals in price time series.",
    "original_text": "SUBJECT AREAS: APPLIED PHYSICS SCIENTIFIC DATA\nSTATISTICAL PHYSICS, THERMODYNAMICS AND NONLINEAR DYNAMICS\nReceived\n12 November 2013 Accepted 10 March 2014 Published 27 March 2014\nCorrespondence and requests for materials should be addressed to M.C. (matthieu. cristelli@roma1.infn.it)",
    "context": "Introduces the central thesis about technical trading and its potential for detectable signals in price time series.",
    "document": "srep04487.pdf",
    "pages": [
      1
    ],
    "id": "deb5c98c3fe7c73683486aac8b6a2b88adaf5f50e5c6d006c181ae822c838785"
  },
  {
    "text": "Federico Garzarelli 1 , Matthieu Cristelli 2 , Gabriele Pompa 3 , Andrea Zaccaria 2 & Luciano Pietronero 1,2,4\n1 ''Sapienza'', Universita ` di Roma, Dip. Fisica, P. le A. Moro 2, 00185, Roma, Italy, 2 Institute of Complex Systems, CNR, Roma, 3 IMT, Institute for Advanced Studies, Piazza S. Ponziano, 6, 55100 Lucca, Italy, 4 London Institute for Mathematical Sciences, South Street 22, Mayfair London, UK.\nTechnical trading represents a class of investment strategies for Financial Markets based on the analysis of trends and recurrent patterns in price time series. According standard economical theories these strategies should not be used because they cannot be profitable. On the contrary, it is well-known that technical traders exist and operate on different time scales. In this paper we investigate if technical trading produces detectable signals in price time series and if some kind of memory effects are introduced in the price dynamics. In particular, we focus on a specific figure called supports and resistances. We first develop a criterion to detect the potential values of supports and resistances. Then we show that memory effects in the price dynamics are associated to these selected values. In fact we show that prices more likely re-bounce than cross these values. Such an effect is a quantitative evidence of the so-called self-fulfilling prophecy, that is the self-reinforcement of agents' belief and sentiment about future stock prices' behavior.\nP hysical and mathematical methods derived from Complex Systems Theory and Statistical Physics have been shown to be effective tools 1-4 to provide a quantitative description and an explanation of many social 5-7 and economical phenomena 8-11 .\nIn the last two decades Financial Markets have appeared as natural candidates for this inter-disciplinary application of methods deriving from Physics because a systematic approach to the issues set by this field can be undertaken. In fact since twenty years there exists a huge amount of high frequency data from stock exchanges which permit to perform experimental analyses as in Natural Sciences. Therefore Financial Markets appear as a good playground where models and theories can be tested. In addition the methods of Physics have proved to be very effective in this field and have often given rise to concrete financial applications.\nThe major contributions of Physics to the comprehension of Financial Markets are focused on one hand on the analysis of financial time series' properties and on the other hand on agent-based modeling 12,13 . The former contribution provides fundamental insights in the non trivial nature of the stochastic process performed by stock prices 14-18 and in the role of the dynamic interplay between agents in the explanation of the behavior of the order impact on prices 15,19-24 . The latter approach instead has tried to overcome the traditional economical models based on concepts like price equilibrium and homogeneity of agents in order to investigate the role of heterogeneity of agents and strategies with respect to the price dynamics 13,25-33 .\nIn this paper we focus our attention on a puzzling issue which can be put halfway between these two approaches: technical trading. According to the standard economical theory of Financial Markets the strategies based on the analysis of trends and recurrent patterns (known indeed as technical trading or chartist strategies) should not be used if all agents were rational because prices should follow their fundamental values 34,35 and no exploitation opportunities should be present 36 . Consequently these speculative strategies cannot be profitable in such a scenario.\nIt is instead well-known that chartists (i.e. technical traders) exist and operate on different time scales ranging from seconds to months. Technical analysis has been widely used and studied over the years. A comprehensive and recent review of the most important results about the existence of technical signals and their profitability is the one of Park and Irwin 37 . They stress the existence of a number of problems in performing such studies and the consequent presence of mixed conclusions. Here we quickly summarizes some of these studies.\nSince the pioneering work of Smidt 38 several studies have proved the widespread use of technical analysis among traders (for a recent survey, see 39 ). Despite this evidence, the academic world has remained quite skeptical because of the acceptance of the efficient market hypothesis 36 and the negative results found in early studies. More recently, a renewed interest for technical analysis led to various papers. Here we will mainly consider the ones regarding the specific techniques we are going to study in this paper, supports and resistances, which are the\ntendency to bounce on specific price values. Some of the most striking and seminal results have been obtained by Brock et al. 40 . They showed a positive correspondence between the presence of supports and resistance and the following returns. However, they did not consider transaction costs, so the profitability issue was missing. Sullivan et al. 41 studied a huge sets of trading rules, finding that, typically, the best rule inside a sample does not generate a profit in the following years. This has been interpreted as an evidence for an evolution of markets towards a more efficient state. Finally, we cite Osler 42 who took into account the connection between supports and resistances and price clustering, that is, the tendency agents have to place orders at round prices.\nAs far as we know, the econophysics community did not address the supports and resistance issue in a direct way. However, some studies can be reinterpreted in this perspective. For example, Preis et al. 43 extensively studied trend switches at different time scales, finding evidences for an universal, scale free behavior of transaction volume and intertrade times and in Ref. 44 evidence is provided that financial high frequency patterns tend to repeat themselves. See also 45 for an extensive review. In a series of papers (see for example 46 ), Sornette and coauthors have proposed a log-periodic power law as an empirical fit in order to predict financial crashes. This approach has been widely discussed; however we do not believe this is the place where this debate should be addressed, also because we will take into account much shorter time scales.\nIn this paper we investigate if this specific chartist strategy produces a measurable effect on the statistical properties of price time series that is if there exist special values on which prices tend to bounce. As we are going to see, the first task that we must address consists in the formalization of this strategy in a suitable mathematical framework.\nOnce a quantitative criterion to select potential supports and resistances is developed, we investigate if these selected values introduce memory effects in price evolution.\nWe observe that: i) the probability of re-bouncing on these selected values is higher than what expected and ii) the more the number of bounces on these values increases, the higher is the probability of bouncing again on them. In terms of agents' sentiment we can say that the more agents observe bounces the more they expect that the price will again bounce on that value and their beliefs introduce a positive feedback which in turn reinforces the support or the resistance.\nThe classical and the technical approaches . The classical approach in the study of the market dynamics is to build a stochastic model for the price dynamics with the so called martingale property E ( xt 1 1 j xt , xt 2 1, …, x 0 ) 5 xt ; t 47-50 . The use of a martingale for the description of the price dynamics naturally arises from the efficient market hypothesis and from the empirical evidence of the absence of simple autocorrelation between price increments (i.e. returns). The consequence of this kind of model for the price is that it is impossible to extract any information on the future price movements from an analysis of the past variations.\nOn the contrary, technical analysis is the study of the market behavior underpinned on the inspection of the price graphs, with the aim to speculate on the future value of prices. According to the technical approach, the analysis of the past prices can lead to the forecast of the future value of prices. This approach is based upon three basic assumptions 51 :\n- . the market discounts everything : the price reflects all the possible causes of the price movements (investors' psychology, political contingencies and so on) so the price graph is the only tool to be considered in order to make a prevision.\n- . price moves in trends : the price moves as a part of a trend, which can have three directions: up, down, sideways. According to the technical approach, a trend is more likely to continue than to stop. The ultimate goal of the technical analysis is to spot a trend in its early stage and to exploit it by investing in its direction.\n- . history repeats itself : Thousands of price graphs of the past have been analyzed and some figures (or patterns) of the price graphs have been linked to an upward or downward trend 51 . The technical analysis argues that a price trend reflects the market psychology. The hypothesis of the technical analysis is that if these patterns anticipated a specific trend in the past they would do the same in the future. As the psychology of the investors do not change over time, an investor would always react in the same way when he undergoes the same conditions.\n\nIntroduces the central thesis about technical trading and its potential for detectable signals and memory effects in price dynamics.",
    "original_text": "Federico Garzarelli 1 , Matthieu Cristelli 2 , Gabriele Pompa 3 , Andrea Zaccaria 2 & Luciano Pietronero 1,2,4\n1 ''Sapienza'', Universita ` di Roma, Dip. Fisica, P. le A. Moro 2, 00185, Roma, Italy, 2 Institute of Complex Systems, CNR, Roma, 3 IMT, Institute for Advanced Studies, Piazza S. Ponziano, 6, 55100 Lucca, Italy, 4 London Institute for Mathematical Sciences, South Street 22, Mayfair London, UK.\nTechnical trading represents a class of investment strategies for Financial Markets based on the analysis of trends and recurrent patterns in price time series. According standard economical theories these strategies should not be used because they cannot be profitable. On the contrary, it is well-known that technical traders exist and operate on different time scales. In this paper we investigate if technical trading produces detectable signals in price time series and if some kind of memory effects are introduced in the price dynamics. In particular, we focus on a specific figure called supports and resistances. We first develop a criterion to detect the potential values of supports and resistances. Then we show that memory effects in the price dynamics are associated to these selected values. In fact we show that prices more likely re-bounce than cross these values. Such an effect is a quantitative evidence of the so-called self-fulfilling prophecy, that is the self-reinforcement of agents' belief and sentiment about future stock prices' behavior.\nP hysical and mathematical methods derived from Complex Systems Theory and Statistical Physics have been shown to be effective tools 1-4 to provide a quantitative description and an explanation of many social 5-7 and economical phenomena 8-11 .\nIn the last two decades Financial Markets have appeared as natural candidates for this inter-disciplinary application of methods deriving from Physics because a systematic approach to the issues set by this field can be undertaken. In fact since twenty years there exists a huge amount of high frequency data from stock exchanges which permit to perform experimental analyses as in Natural Sciences. Therefore Financial Markets appear as a good playground where models and theories can be tested. In addition the methods of Physics have proved to be very effective in this field and have often given rise to concrete financial applications.\nThe major contributions of Physics to the comprehension of Financial Markets are focused on one hand on the analysis of financial time series' properties and on the other hand on agent-based modeling 12,13 . The former contribution provides fundamental insights in the non trivial nature of the stochastic process performed by stock prices 14-18 and in the role of the dynamic interplay between agents in the explanation of the behavior of the order impact on prices 15,19-24 . The latter approach instead has tried to overcome the traditional economical models based on concepts like price equilibrium and homogeneity of agents in order to investigate the role of heterogeneity of agents and strategies with respect to the price dynamics 13,25-33 .\nIn this paper we focus our attention on a puzzling issue which can be put halfway between these two approaches: technical trading. According to the standard economical theory of Financial Markets the strategies based on the analysis of trends and recurrent patterns (known indeed as technical trading or chartist strategies) should not be used if all agents were rational because prices should follow their fundamental values 34,35 and no exploitation opportunities should be present 36 . Consequently these speculative strategies cannot be profitable in such a scenario.\nIt is instead well-known that chartists (i.e. technical traders) exist and operate on different time scales ranging from seconds to months. Technical analysis has been widely used and studied over the years. A comprehensive and recent review of the most important results about the existence of technical signals and their profitability is the one of Park and Irwin 37 . They stress the existence of a number of problems in performing such studies and the consequent presence of mixed conclusions. Here we quickly summarizes some of these studies.\nSince the pioneering work of Smidt 38 several studies have proved the widespread use of technical analysis among traders (for a recent survey, see 39 ). Despite this evidence, the academic world has remained quite skeptical because of the acceptance of the efficient market hypothesis 36 and the negative results found in early studies. More recently, a renewed interest for technical analysis led to various papers. Here we will mainly consider the ones regarding the specific techniques we are going to study in this paper, supports and resistances, which are the\ntendency to bounce on specific price values. Some of the most striking and seminal results have been obtained by Brock et al. 40 . They showed a positive correspondence between the presence of supports and resistance and the following returns. However, they did not consider transaction costs, so the profitability issue was missing. Sullivan et al. 41 studied a huge sets of trading rules, finding that, typically, the best rule inside a sample does not generate a profit in the following years. This has been interpreted as an evidence for an evolution of markets towards a more efficient state. Finally, we cite Osler 42 who took into account the connection between supports and resistances and price clustering, that is, the tendency agents have to place orders at round prices.\nAs far as we know, the econophysics community did not address the supports and resistance issue in a direct way. However, some studies can be reinterpreted in this perspective. For example, Preis et al. 43 extensively studied trend switches at different time scales, finding evidences for an universal, scale free behavior of transaction volume and intertrade times and in Ref. 44 evidence is provided that financial high frequency patterns tend to repeat themselves. See also 45 for an extensive review. In a series of papers (see for example 46 ), Sornette and coauthors have proposed a log-periodic power law as an empirical fit in order to predict financial crashes. This approach has been widely discussed; however we do not believe this is the place where this debate should be addressed, also because we will take into account much shorter time scales.\nIn this paper we investigate if this specific chartist strategy produces a measurable effect on the statistical properties of price time series that is if there exist special values on which prices tend to bounce. As we are going to see, the first task that we must address consists in the formalization of this strategy in a suitable mathematical framework.\nOnce a quantitative criterion to select potential supports and resistances is developed, we investigate if these selected values introduce memory effects in price evolution.\nWe observe that: i) the probability of re-bouncing on these selected values is higher than what expected and ii) the more the number of bounces on these values increases, the higher is the probability of bouncing again on them. In terms of agents' sentiment we can say that the more agents observe bounces the more they expect that the price will again bounce on that value and their beliefs introduce a positive feedback which in turn reinforces the support or the resistance.\nThe classical and the technical approaches . The classical approach in the study of the market dynamics is to build a stochastic model for the price dynamics with the so called martingale property E ( xt 1 1 j xt , xt 2 1, …, x 0 ) 5 xt ; t 47-50 . The use of a martingale for the description of the price dynamics naturally arises from the efficient market hypothesis and from the empirical evidence of the absence of simple autocorrelation between price increments (i.e. returns). The consequence of this kind of model for the price is that it is impossible to extract any information on the future price movements from an analysis of the past variations.\nOn the contrary, technical analysis is the study of the market behavior underpinned on the inspection of the price graphs, with the aim to speculate on the future value of prices. According to the technical approach, the analysis of the past prices can lead to the forecast of the future value of prices. This approach is based upon three basic assumptions 51 :\n- . the market discounts everything : the price reflects all the possible causes of the price movements (investors' psychology, political contingencies and so on) so the price graph is the only tool to be considered in order to make a prevision.\n- . price moves in trends : the price moves as a part of a trend, which can have three directions: up, down, sideways. According to the technical approach, a trend is more likely to continue than to stop. The ultimate goal of the technical analysis is to spot a trend in its early stage and to exploit it by investing in its direction.\n- . history repeats itself : Thousands of price graphs of the past have been analyzed and some figures (or patterns) of the price graphs have been linked to an upward or downward trend 51 . The technical analysis argues that a price trend reflects the market psychology. The hypothesis of the technical analysis is that if these patterns anticipated a specific trend in the past they would do the same in the future. As the psychology of the investors do not change over time, an investor would always react in the same way when he undergoes the same conditions.",
    "context": "Introduces the central thesis about technical trading and its potential for detectable signals and memory effects in price dynamics.",
    "document": "srep04487.pdf",
    "pages": [
      1,
      2
    ],
    "id": "e83f8c86f46fdb1eb654fdac67ce9e38dc8fe6a9b1a2f5df8de8a7be635d9b19"
  },
  {
    "text": "One reason for the technical analysis to work could be the existence of a feedback effect called self-fulfilling prophecy . Financial markets have a unique feature: the study of the market affects the market itself because the results of studies on it will be probably used in the decision processes by the investors (other disciplines, such as physics, do not have to face this issue). The spread of the technical analysis entails that a large number of investors have become familiar with the use of the so called figures . A figure is a specific pattern of the price associated to a future bullish or bearish trend. Therefore, it is believed that a large amount of money have been moved in reply to bullish or bearish figures causing price changes. In a market, if a large number of investors has the same expectations on the future value of the price and they react in the same way to this expectation they will operate in such a way to fulfill their own expectations. As a consequence, the theories that predicted those expectation will gain investors' trust triggering a positive feedback loop. In this paper we tried to measure quantitatively the trust on one of the figures of technical analysis.\nSupports and resistances . Let us now describe a particular figure: supports and resistances. The definition of support and resistance of the technical analysis is rather qualitative: a support is a price level, local minimum of the price, where the price will bounce on other times afterward while a resistance is a price level, local maximum of the price, where the price will bounce on other times afterward. We expect that when a substantial number of investors detect a support or a resistance the probability that the price bounces on the support or resistance level is larger than the probability the price crosses the support or resistance level. Whether the investors regard a local minimum or maximum as a support or a resistance or not can be related to: i) the number of previous bounces on a given price level, ii) the time scale. The investors could a priori look at heterogeneous time scales. This introduces two parameters which we allow to vary during the analysis in order to understand if and how they affect our results.\n\nDescribes the concept of self-fulfilling prophecy in financial markets and introduces the definition of supports and resistances as qualitative price levels, anticipating a higher probability of bounces on these levels due to investor sentiment and expectations.",
    "original_text": "One reason for the technical analysis to work could be the existence of a feedback effect called self-fulfilling prophecy . Financial markets have a unique feature: the study of the market affects the market itself because the results of studies on it will be probably used in the decision processes by the investors (other disciplines, such as physics, do not have to face this issue). The spread of the technical analysis entails that a large number of investors have become familiar with the use of the so called figures . A figure is a specific pattern of the price associated to a future bullish or bearish trend. Therefore, it is believed that a large amount of money have been moved in reply to bullish or bearish figures causing price changes. In a market, if a large number of investors has the same expectations on the future value of the price and they react in the same way to this expectation they will operate in such a way to fulfill their own expectations. As a consequence, the theories that predicted those expectation will gain investors' trust triggering a positive feedback loop. In this paper we tried to measure quantitatively the trust on one of the figures of technical analysis.\nSupports and resistances . Let us now describe a particular figure: supports and resistances. The definition of support and resistance of the technical analysis is rather qualitative: a support is a price level, local minimum of the price, where the price will bounce on other times afterward while a resistance is a price level, local maximum of the price, where the price will bounce on other times afterward. We expect that when a substantial number of investors detect a support or a resistance the probability that the price bounces on the support or resistance level is larger than the probability the price crosses the support or resistance level. Whether the investors regard a local minimum or maximum as a support or a resistance or not can be related to: i) the number of previous bounces on a given price level, ii) the time scale. The investors could a priori look at heterogeneous time scales. This introduces two parameters which we allow to vary during the analysis in order to understand if and how they affect our results.",
    "context": "Describes the concept of self-fulfilling prophecy in financial markets and introduces the definition of supports and resistances as qualitative price levels, anticipating a higher probability of bounces on these levels due to investor sentiment and expectations.",
    "document": "srep04487.pdf",
    "pages": [
      2
    ],
    "id": "8c2dda92bd15dc29196d8dd6b4a8dab81582ccc88def7b64aa66a191b875973a"
  },
  {
    "text": "Empirical evidence of memory effects . The analyses presented in this paper are carried out on the high frequency ( tick-by-tick i.e. we have a record of the price for every operation), time series of the price of 9 stocks of the London Stock Exchange in 2002 (251 trading days). The analyzed stocks are: AstraZeNeca (AZN), British Petroleum (BP), GlaxoSmithKline (GSK), Heritage Financial Group (HBOS), Royal Bank of Scotland Group (RBS), Rio Tinto (RIO), Royal Dutch Shell (SHEL), Unilever (ULVR), Vodafone Group (VOD).\nThe price of these stocks is measured in ticks . A tick is the minimum change of the price. The time is measured in seconds. We choose to adopt the physical time because we believe that investors perceive this one. We checked that the results are different as we analyze the data with the time in ticks or in seconds. A measure of the time in ticks would make difficult to compare and aggregate the results for different stocks. In fact, while the physical time of trading does not change from stock to stock the number of operations per day can be very different.\nWe measure the conditional probability of a bounce p ( b j bprev ) given bprev previous bounces. This is the probability that the price bounces on a local maximum or minimum given bprev previous bounces. Practically, we record if the price, when is within the stripe of a support or resistance, bounces or crosses it for every day of\nFigure 1 | Graphs of the conditional probability of bounce on a resistance/support given the occurrence of bprev previous bounces. Time scales: T 5 45 (panel (a) and (c)) and 60 seconds (panel (b) and (d)). The data are obtained aggregating the result of the 9 stocks considered. The data of the stocks are shown as red circles while the data of the time series of the shuffled returns of the price are shown as black circles. The graphs in the left refer to the resistances while the ones on the right refer to the supports.\n/C0\n/C1\n/C0\n/C1\ntrading and for every stock. We assume that all the supports or resistances detected in different days of the considered year are statistically equal. As a result we obtain the bounce frequency f b bprev /C12 /C12 /C0 /C1 ~ nbprev N for the total year (where N is the total number of events for a specific number of previous bounces). Now we can estimate p ( b j bprev ) with the method of the Bayesian inference 52,53 : we infer p ( b j bprev ) from the number of bounces nbprev and from the total number of trials N assuming that nbprev is a realization of a Bernoulli process because when the price is contained into the stripe of a previous local minimum or maximum it can only bounce on it or cross it (see Supporting Information for further details on the modeling of bounce events).\nUsing this framework we can evaluate the expected value and the variance of p ( b j bprev ) using the Bayes theorem (see Supporting Information for mathematical details of the derivation)\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nIn fig. 1 and fig. 2 the conditional probabilities are shown for different time scales. The data of the stocks have been compared to the time series of the shuffled returns of the price. In this way we can compare the stock data with a time series with the same statistical properties but without any memory effect. As shown in the graphs, the probabilities of bounce of the shuffled time series are nearly 1 5 2 while the probabilities of bounce of the stock data are well above 1 5 2. For the shuffled series the probability is in almost all cases larger than 1 5 2, this small bias towards a value larger than 1 5 2 is due to the finiteness of the stripe. A similar bias would be observed also for a series generated by a Random-Walk. However, we observe that this is intrinsic asymmetry is at least one order of magnitude smaller than the effect measured in the non-shuffled case. In addition to this, it is noticeable that the probability of bounce rises up as bprev increases. Conversely, the probability of bounce of the\nFigure 2 | Graphs of the conditional probability of bounce on a resistance/support given the occurrence of bprev previous bounces. Time scales: T 5 90 (panel (a) and (c)) and 180 seconds (panel (b) and (d)). The data are obtained aggregating the result of the 9 stocks considered. The data of the stocks are shown as red circles while the data of the time series of the shuffled returns of the price are shown as black circles. The graphs in the left refer to the resistances while the ones on the right refer to the supports.\nshuffled time series is nearly constant. The increase of p ( b j bprev ) of the stocks with bprev can be interpreted as the growth of the investors' trust on the support or the resistance as the number of bounces grows. The more the number of previous bounces on a certain price level the stronger the trust that the support or the resistance cannot be broken soon. As we outlined above, a feedback effect holds and an increase of the investors' trust on a support or a resistance entails a decrease of the probability of crossing that level of price.\n( p ( b j bprev ) 5 c ) is performed both on the stock data and on the data of the shuffled time series and we compute\nP\n/C12\n/C0\n/C1\n/C2\n/C3\n<!-- formula-not-decoded -->\nWehaveperformed a x 2 test to verify if the hypothesis of growth of p ( b j bprev ) is statistically meaningful. The independence test\nThen we compute the p-value associated to a x 2 distribution with 3 degrees of freedom. We choose a significance level /C22 a ~ 0 : 05. If p /C0 value v /C22 a the independence hypothesis is rejected while if p /C0 value § /C22 a it is accepted. The results are shown in table 1. The results show that there is a clear increase of the investors' memory on\nTable 1 | The table shows the p-values for the stock data and for the time series of the shuffled returns for different time scale and for the supports and resistances. Up to the 60-90 seconds timescale we find that the increase of p ( b | bprev ) with respect to bprev is statistically significant\n\nStocks resistances, 1 sec. = , 0.001. Stocks resistances, 15 sec. = , 0.001. Stocks resistances, 30 sec. = , 0.001. Stocks resistances, 45 sec. = , 0.001. Stocks resistances, 60 sec. = 0.002. Stocks resistances, 90 sec. = 0.13. Stocks resistances, 180 sec. = 0.38. supports, 1 sec. = , 0.001. supports, 15 sec. = , 0.001. supports, 30 sec. = , 0.001. supports, 45 sec. = , 0.001. supports, 60 sec. = 0.001. supports, 90 sec. = 0.17. supports, 180 sec. = 0.99\nFigure 3 | Graphs of the slope of the best fit line of p(b |b prev ) at different time scales ranging from 1 to 180 s in the case of resistances (panel (a)) and supports (panel (b)). The slopes of the original data are compared with the slopes of the shuffled data.\nthe supports/resistances as the number of previous bounces increases for the time scales of 45, 60 and 90 seconds. Conversely, this memory do not increase from the statistical significancy point of view at the time scale of 180 seconds.\nAs a further proof of the statistical significance of the memory effect observed, we perform a Kolmogorov-Smirnov test (see Ref. 54) in order to assess whether the bounce frequencies estimated from the reshuffled series are compatible with the posterior distribution found for bounce frequency. We indeed find that reshuffled events are statistically different from empirical values (details on the implementation of the Kolmogorov-Smirnov test are discussed in Supporting Information).\nWe consider the slope of the weighted linear fit of p ( b j bprev ) (shown in figures 1 and 2) at different time scales in order to study how the memory effect changes with the time scale considered. The slopes are shown in fig. 3, resistances in the left graph, supports in the right graph. The best fit lines of p ( b j bprev ) are always upward sloping and decrease as the time scale increases. There are differences between resistances and supports as far as higher timescales are concerned. In particular, the slopes relative to the resistances decay slower that the slopes relative to the supports. While the former are statistically different from 0 on all time scale investigated the latter tends to 0 for scales above 150 s.\nIn summary this analysis shows that the memory effect decreases as the time scale increases. We find that it disappears at the time scale larger than 180 s-180 s is the maximum scale we investigate - for resistances and 150 s for supports.\n\nProvides supporting evidence for the argument on economic inequality.",
    "original_text": "Empirical evidence of memory effects . The analyses presented in this paper are carried out on the high frequency ( tick-by-tick i.e. we have a record of the price for every operation), time series of the price of 9 stocks of the London Stock Exchange in 2002 (251 trading days). The analyzed stocks are: AstraZeNeca (AZN), British Petroleum (BP), GlaxoSmithKline (GSK), Heritage Financial Group (HBOS), Royal Bank of Scotland Group (RBS), Rio Tinto (RIO), Royal Dutch Shell (SHEL), Unilever (ULVR), Vodafone Group (VOD).\nThe price of these stocks is measured in ticks . A tick is the minimum change of the price. The time is measured in seconds. We choose to adopt the physical time because we believe that investors perceive this one. We checked that the results are different as we analyze the data with the time in ticks or in seconds. A measure of the time in ticks would make difficult to compare and aggregate the results for different stocks. In fact, while the physical time of trading does not change from stock to stock the number of operations per day can be very different.\nWe measure the conditional probability of a bounce p ( b j bprev ) given bprev previous bounces. This is the probability that the price bounces on a local maximum or minimum given bprev previous bounces. Practically, we record if the price, when is within the stripe of a support or resistance, bounces or crosses it for every day of\nFigure 1 | Graphs of the conditional probability of bounce on a resistance/support given the occurrence of bprev previous bounces. Time scales: T 5 45 (panel (a) and (c)) and 60 seconds (panel (b) and (d)). The data are obtained aggregating the result of the 9 stocks considered. The data of the stocks are shown as red circles while the data of the time series of the shuffled returns of the price are shown as black circles. The graphs in the left refer to the resistances while the ones on the right refer to the supports.\n/C0\n/C1\n/C0\n/C1\ntrading and for every stock. We assume that all the supports or resistances detected in different days of the considered year are statistically equal. As a result we obtain the bounce frequency f b bprev /C12 /C12 /C0 /C1 ~ nbprev N for the total year (where N is the total number of events for a specific number of previous bounces). Now we can estimate p ( b j bprev ) with the method of the Bayesian inference 52,53 : we infer p ( b j bprev ) from the number of bounces nbprev and from the total number of trials N assuming that nbprev is a realization of a Bernoulli process because when the price is contained into the stripe of a previous local minimum or maximum it can only bounce on it or cross it (see Supporting Information for further details on the modeling of bounce events).\nUsing this framework we can evaluate the expected value and the variance of p ( b j bprev ) using the Bayes theorem (see Supporting Information for mathematical details of the derivation)\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nIn fig. 1 and fig. 2 the conditional probabilities are shown for different time scales. The data of the stocks have been compared to the time series of the shuffled returns of the price. In this way we can compare the stock data with a time series with the same statistical properties but without any memory effect. As shown in the graphs, the probabilities of bounce of the shuffled time series are nearly 1 5 2 while the probabilities of bounce of the stock data are well above 1 5 2. For the shuffled series the probability is in almost all cases larger than 1 5 2, this small bias towards a value larger than 1 5 2 is due to the finiteness of the stripe. A similar bias would be observed also for a series generated by a Random-Walk. However, we observe that this is intrinsic asymmetry is at least one order of magnitude smaller than the effect measured in the non-shuffled case. In addition to this, it is noticeable that the probability of bounce rises up as bprev increases. Conversely, the probability of bounce of the\nFigure 2 | Graphs of the conditional probability of bounce on a resistance/support given the occurrence of bprev previous bounces. Time scales: T 5 90 (panel (a) and (c)) and 180 seconds (panel (b) and (d)). The data are obtained aggregating the result of the 9 stocks considered. The data of the stocks are shown as red circles while the data of the time series of the shuffled returns of the price are shown as black circles. The graphs in the left refer to the resistances while the ones on the right refer to the supports.\nshuffled time series is nearly constant. The increase of p ( b j bprev ) of the stocks with bprev can be interpreted as the growth of the investors' trust on the support or the resistance as the number of bounces grows. The more the number of previous bounces on a certain price level the stronger the trust that the support or the resistance cannot be broken soon. As we outlined above, a feedback effect holds and an increase of the investors' trust on a support or a resistance entails a decrease of the probability of crossing that level of price.\n( p ( b j bprev ) 5 c ) is performed both on the stock data and on the data of the shuffled time series and we compute\nP\n/C12\n/C0\n/C1\n/C2\n/C3\n<!-- formula-not-decoded -->\nWehaveperformed a x 2 test to verify if the hypothesis of growth of p ( b j bprev ) is statistically meaningful. The independence test\nThen we compute the p-value associated to a x 2 distribution with 3 degrees of freedom. We choose a significance level /C22 a ~ 0 : 05. If p /C0 value v /C22 a the independence hypothesis is rejected while if p /C0 value § /C22 a it is accepted. The results are shown in table 1. The results show that there is a clear increase of the investors' memory on\nTable 1 | The table shows the p-values for the stock data and for the time series of the shuffled returns for different time scale and for the supports and resistances. Up to the 60-90 seconds timescale we find that the increase of p ( b | bprev ) with respect to bprev is statistically significant\n\nStocks resistances, 1 sec. = , 0.001. Stocks resistances, 15 sec. = , 0.001. Stocks resistances, 30 sec. = , 0.001. Stocks resistances, 45 sec. = , 0.001. Stocks resistances, 60 sec. = 0.002. Stocks resistances, 90 sec. = 0.13. Stocks resistances, 180 sec. = 0.38. supports, 1 sec. = , 0.001. supports, 15 sec. = , 0.001. supports, 30 sec. = , 0.001. supports, 45 sec. = , 0.001. supports, 60 sec. = 0.001. supports, 90 sec. = 0.17. supports, 180 sec. = 0.99\nFigure 3 | Graphs of the slope of the best fit line of p(b |b prev ) at different time scales ranging from 1 to 180 s in the case of resistances (panel (a)) and supports (panel (b)). The slopes of the original data are compared with the slopes of the shuffled data.\nthe supports/resistances as the number of previous bounces increases for the time scales of 45, 60 and 90 seconds. Conversely, this memory do not increase from the statistical significancy point of view at the time scale of 180 seconds.\nAs a further proof of the statistical significance of the memory effect observed, we perform a Kolmogorov-Smirnov test (see Ref. 54) in order to assess whether the bounce frequencies estimated from the reshuffled series are compatible with the posterior distribution found for bounce frequency. We indeed find that reshuffled events are statistically different from empirical values (details on the implementation of the Kolmogorov-Smirnov test are discussed in Supporting Information).\nWe consider the slope of the weighted linear fit of p ( b j bprev ) (shown in figures 1 and 2) at different time scales in order to study how the memory effect changes with the time scale considered. The slopes are shown in fig. 3, resistances in the left graph, supports in the right graph. The best fit lines of p ( b j bprev ) are always upward sloping and decrease as the time scale increases. There are differences between resistances and supports as far as higher timescales are concerned. In particular, the slopes relative to the resistances decay slower that the slopes relative to the supports. While the former are statistically different from 0 on all time scale investigated the latter tends to 0 for scales above 150 s.\nIn summary this analysis shows that the memory effect decreases as the time scale increases. We find that it disappears at the time scale larger than 180 s-180 s is the maximum scale we investigate - for resistances and 150 s for supports.",
    "context": "Provides supporting evidence for the argument on economic inequality.",
    "document": "srep04487.pdf",
    "pages": [
      2,
      3,
      4,
      5
    ],
    "id": "0a0121958a11d76c53e56a224c3c984118b595ce9c8d255e961ead02ced19d98"
  },
  {
    "text": "Distribution of local minima/maxima . Westudythe distribution of supports and resistances in order to assess whether the price is more likely to bounce on some particular levels rather than on others. It is possible in principle that round values of the price (e.g. 100 £ ) are favored levels for psychological reasons. We produced a histogram of the local maxima/minima for every stock and time scale. The histogram of the local minima relative to VOD at the time scale of 60 seconds is shown in fig. 4 as an example. We find no evidence of\nb)\nFigure 4 | Histogram of the resistance price levels (panel (a)) and supports (panel (b)) of VOD for the 251 trading days of 2002 for the time scale 60 s. Both resistance and support levels are compared with the histogram of price levels for this time scale. We do not observe significant excess around round numbers or anomalies with respect to the histogram of the price levels. We find similar results, i.e. absence of anomalies in the histogram for supports and resistances, for all stocks and all time scales investigated.\nFigure 5 | Graph of the conditional probability of bounce on a resistance/support given the occurrence of bprev previous bounces for a fractional random walk (we used the average daily Hurst exponent equal to 0.44). The red circles refers to supports, the black ones to resistances. The persistency deriving from a Hurst exponent smaller than 0.5 is not able to explain the probability of bounce observed in Figs. 1 and 2.\nhighly preferred prices in any of the histograms produced. As a further proof, we also compare in that figure the histogram of support and resistance levels with the price level histogram and we do not observe any anomaly.\nthe price from the support or the resistance between two consecutive bounces.\nLong memory of the price and antipersistency . The analysis of the conditional probability p ( b j bprev ) proves the existence of a long memory in the price time series.\nHowever, it is a well-known results that stock prices exhibit deviations from a purely diffusion especially at short time scales. We indeed find that the mean of the Hurst exponent Æ H æ is always less than the value of 0.5 and therefore there is an anticorrelation effect of the price increments for the 9 stocks analyzed. The Hurst exponent is estimated via the detrended fluctuation method 55,56 . It is useful to recall what the Hurst exponent provides about the autocorrelation of the time series:\n- . if H , 0.5 one has negative correlation and antipersistent behavior\n- . if H 5 0.5 one has no correlation\n- . if H . 0.5 one has positive correlation and persistent behavior\nTherefore the anticorrelation of the price increments might lead to an increase of the bounces and therefore it could mimic a memory of the price on a support or resistance. We perform an analysis of the bounces on a antipersistent fractional random walk to verify if the memory effect depends on the antipersistent nature of the price in the time scale of the day. We choose a fractional random walk with the Hurst exponent H 5 Æ Hstock æ 5 0.44 given by the average over the H exponents of the different stocks. The result is shown in fig. 5. The conditional probabilities p ( b j bprev ) are very close to 0.5 and it is clear that p ( b j bprev ) is almost constant as expected. These two results prove that the memory effect of the price does not depend on its antipersistent features, or at least the antipersistency is not able to explain the pattern observed in figs. 1 and 2.\nFeatures of the bounces . In this section we want to describe two statistical features of the bounces as a sort of Stylized Facts of these two figures of technical trading techniques: the time t occurring between two consecutive bounces and the maximum distance d of\nThe time of recurrence t is defined as the time between an exit of the price from the stripe centered on the support or resistance and the return of the price in the same stripe, as shown in fig. 6 panel a. Westudy the distribution of t for different time scales (45, 60, 90 and 180 seconds). We point out that, being t measured in terms of the considered time scale, we can compare the four histograms. We find that a power law fit describes well the histograms of t and, as an example, in fig. 6 panel b we report the histogram for the time scale 60 seconds.\nWe instead call d the maximum distance reached by the price before the next bounce. We show in fig. 6 panel a, how the maximum distance d is defined. We study the distribution of d for different time scales (45, 60, 90 and 180 seconds). In this case a power law fit does not describe accurately the histogram of d and instead the behavior appears to be compatible at all scales with an exponentially truncated power law as shown in Fig. 6 panel c.\n\nThe analysis reveals a long-term memory effect in the price time series, specifically, the probability of bouncing on supports and resistances increases with the number of previous bounces. This memory effect is not explained by an antipersistent fractional random walk and appears to be statistically significant across different time scales.",
    "original_text": "Distribution of local minima/maxima . Westudythe distribution of supports and resistances in order to assess whether the price is more likely to bounce on some particular levels rather than on others. It is possible in principle that round values of the price (e.g. 100 £ ) are favored levels for psychological reasons. We produced a histogram of the local maxima/minima for every stock and time scale. The histogram of the local minima relative to VOD at the time scale of 60 seconds is shown in fig. 4 as an example. We find no evidence of\nb)\nFigure 4 | Histogram of the resistance price levels (panel (a)) and supports (panel (b)) of VOD for the 251 trading days of 2002 for the time scale 60 s. Both resistance and support levels are compared with the histogram of price levels for this time scale. We do not observe significant excess around round numbers or anomalies with respect to the histogram of the price levels. We find similar results, i.e. absence of anomalies in the histogram for supports and resistances, for all stocks and all time scales investigated.\nFigure 5 | Graph of the conditional probability of bounce on a resistance/support given the occurrence of bprev previous bounces for a fractional random walk (we used the average daily Hurst exponent equal to 0.44). The red circles refers to supports, the black ones to resistances. The persistency deriving from a Hurst exponent smaller than 0.5 is not able to explain the probability of bounce observed in Figs. 1 and 2.\nhighly preferred prices in any of the histograms produced. As a further proof, we also compare in that figure the histogram of support and resistance levels with the price level histogram and we do not observe any anomaly.\nthe price from the support or the resistance between two consecutive bounces.\nLong memory of the price and antipersistency . The analysis of the conditional probability p ( b j bprev ) proves the existence of a long memory in the price time series.\nHowever, it is a well-known results that stock prices exhibit deviations from a purely diffusion especially at short time scales. We indeed find that the mean of the Hurst exponent Æ H æ is always less than the value of 0.5 and therefore there is an anticorrelation effect of the price increments for the 9 stocks analyzed. The Hurst exponent is estimated via the detrended fluctuation method 55,56 . It is useful to recall what the Hurst exponent provides about the autocorrelation of the time series:\n- . if H , 0.5 one has negative correlation and antipersistent behavior\n- . if H 5 0.5 one has no correlation\n- . if H . 0.5 one has positive correlation and persistent behavior\nTherefore the anticorrelation of the price increments might lead to an increase of the bounces and therefore it could mimic a memory of the price on a support or resistance. We perform an analysis of the bounces on a antipersistent fractional random walk to verify if the memory effect depends on the antipersistent nature of the price in the time scale of the day. We choose a fractional random walk with the Hurst exponent H 5 Æ Hstock æ 5 0.44 given by the average over the H exponents of the different stocks. The result is shown in fig. 5. The conditional probabilities p ( b j bprev ) are very close to 0.5 and it is clear that p ( b j bprev ) is almost constant as expected. These two results prove that the memory effect of the price does not depend on its antipersistent features, or at least the antipersistency is not able to explain the pattern observed in figs. 1 and 2.\nFeatures of the bounces . In this section we want to describe two statistical features of the bounces as a sort of Stylized Facts of these two figures of technical trading techniques: the time t occurring between two consecutive bounces and the maximum distance d of\nThe time of recurrence t is defined as the time between an exit of the price from the stripe centered on the support or resistance and the return of the price in the same stripe, as shown in fig. 6 panel a. Westudy the distribution of t for different time scales (45, 60, 90 and 180 seconds). We point out that, being t measured in terms of the considered time scale, we can compare the four histograms. We find that a power law fit describes well the histograms of t and, as an example, in fig. 6 panel b we report the histogram for the time scale 60 seconds.\nWe instead call d the maximum distance reached by the price before the next bounce. We show in fig. 6 panel a, how the maximum distance d is defined. We study the distribution of d for different time scales (45, 60, 90 and 180 seconds). In this case a power law fit does not describe accurately the histogram of d and instead the behavior appears to be compatible at all scales with an exponentially truncated power law as shown in Fig. 6 panel c.",
    "context": "The analysis reveals a long-term memory effect in the price time series, specifically, the probability of bouncing on supports and resistances increases with the number of previous bounces. This memory effect is not explained by an antipersistent fractional random walk and appears to be statistically significant across different time scales.",
    "document": "srep04487.pdf",
    "pages": [
      5,
      6
    ],
    "id": "5f7e2668edcfe98e39e65e95895ef10916e1f9ad8f36b461f24f82ae640ec0af"
  },
  {
    "text": "In this paper we perform a statistical analysis of the price dynamics of several stocks traded at the London Stock Exchange in order to verify if there exists an empirical and detectable evidence for technical trading. In fact it is known that there exist investors which use technical analysis as trading strategy (also known as chartists). The actions of this type of agents may introduce a feedback effect, the so called self-fulfilling prophecy , which can lead to detectable signals and in some cases to exploitation opportunities, in details, these feedbacks can introduce some memory effects on the price evolution. The main goal of this paper is to determine if such memory in price dynamics exists or not and consequently if it it possible to quantify the feedback on the price dynamics deriving from some types of technical trading strategies. In particular we focus our attention on a particular figure of the technical analysis called supports and resistances . In order to estimate the impact on the price dynamics of such a strategy we measure the conditional probability of the bounces p ( b j bprev ) given bprev previous bounces on a set of suitably selected\ntime\nFigure 6 | Statistical features of bounces. Panel (a) : Sketch of the price showing how we defined t , the time between two bounces and d , the maximumdistance between the price and the support or resistance level between two bounces. Panel (b) Histogram for t at the time scales of 60 seconds. We obtain the histograms aggregating the data from all the 9 stocks analyzed. We do not make any difference between supports and resistances in this analysis. The histogram of t is well-fitted by a power law at all time scales. The exponent results to be dependent on the time scale considered. In this specific case (60 seconds) we find N , t 2 0.56 . Panel (c) Histogram of d at the time scale of 60 seconds. As in the previous panel, we obtain the histograms by aggregating the data from all the 9 stocks. We do not make any difference between supports and resistances in this analysis. The price difference d is measured in price ticks. Differently from the previous case we find that the decay is compatible with an exponentially truncated power law at all time scales. In this specific case (60 seconds) we find N , d 2 0.61 exp( 2 0.03 d ).\nprice values to quantify the memory introduced by the supports and resistances.\nWefindthat the probability of bouncing on support and resistance values is higher than 1 5 2 or, anyway, is higher than an equivalent randomwalk or of the shuffled series. In particular we find that as the number of bounces on these values increases, the probability of bouncing on them becomes higher. This means that the probability of bouncing on a support or a resistance is an increasing function of the number of previous bounces, differently from a random walk or from the shuffled time series in which this probability is independent onthe number of previous bounces. This features is a very interesting quantitative evidence for a self-reinforcement of agents' beliefs, in this case, of the strength of the resistance/support.\nAs a side result we also develop a criterion to select the price values that can be potential supports or resistances on which the probability of the bounces p ( b j bprev ) is measured.\nWepoint out that this finding may be, in principle, an exploitation opportunity because, once the support or the resistance is detected, the next time the price will be in the nearby of the value a re-bounce will be more likely than the crossing of the resistance/support. In future works we plan to verify if the predictability of the direction of the price movements around a support/resistance can lead to real exploitation once transaction costs, frictions (i.e. the delay between order submissions and executions) and liquidity effects are taken into account.\n\nThis chunk focuses on the methodology used to analyze the impact of technical trading strategies, specifically examining the relationship between bounces on support and resistance levels and the number of previous bounces. It details the measurement of conditional probability and the development of a criterion for identifying potential support/resistance levels, suggesting this analysis could be used to predict future price movements.",
    "original_text": "In this paper we perform a statistical analysis of the price dynamics of several stocks traded at the London Stock Exchange in order to verify if there exists an empirical and detectable evidence for technical trading. In fact it is known that there exist investors which use technical analysis as trading strategy (also known as chartists). The actions of this type of agents may introduce a feedback effect, the so called self-fulfilling prophecy , which can lead to detectable signals and in some cases to exploitation opportunities, in details, these feedbacks can introduce some memory effects on the price evolution. The main goal of this paper is to determine if such memory in price dynamics exists or not and consequently if it it possible to quantify the feedback on the price dynamics deriving from some types of technical trading strategies. In particular we focus our attention on a particular figure of the technical analysis called supports and resistances . In order to estimate the impact on the price dynamics of such a strategy we measure the conditional probability of the bounces p ( b j bprev ) given bprev previous bounces on a set of suitably selected\ntime\nFigure 6 | Statistical features of bounces. Panel (a) : Sketch of the price showing how we defined t , the time between two bounces and d , the maximumdistance between the price and the support or resistance level between two bounces. Panel (b) Histogram for t at the time scales of 60 seconds. We obtain the histograms aggregating the data from all the 9 stocks analyzed. We do not make any difference between supports and resistances in this analysis. The histogram of t is well-fitted by a power law at all time scales. The exponent results to be dependent on the time scale considered. In this specific case (60 seconds) we find N , t 2 0.56 . Panel (c) Histogram of d at the time scale of 60 seconds. As in the previous panel, we obtain the histograms by aggregating the data from all the 9 stocks. We do not make any difference between supports and resistances in this analysis. The price difference d is measured in price ticks. Differently from the previous case we find that the decay is compatible with an exponentially truncated power law at all time scales. In this specific case (60 seconds) we find N , d 2 0.61 exp( 2 0.03 d ).\nprice values to quantify the memory introduced by the supports and resistances.\nWefindthat the probability of bouncing on support and resistance values is higher than 1 5 2 or, anyway, is higher than an equivalent randomwalk or of the shuffled series. In particular we find that as the number of bounces on these values increases, the probability of bouncing on them becomes higher. This means that the probability of bouncing on a support or a resistance is an increasing function of the number of previous bounces, differently from a random walk or from the shuffled time series in which this probability is independent onthe number of previous bounces. This features is a very interesting quantitative evidence for a self-reinforcement of agents' beliefs, in this case, of the strength of the resistance/support.\nAs a side result we also develop a criterion to select the price values that can be potential supports or resistances on which the probability of the bounces p ( b j bprev ) is measured.\nWepoint out that this finding may be, in principle, an exploitation opportunity because, once the support or the resistance is detected, the next time the price will be in the nearby of the value a re-bounce will be more likely than the crossing of the resistance/support. In future works we plan to verify if the predictability of the direction of the price movements around a support/resistance can lead to real exploitation once transaction costs, frictions (i.e. the delay between order submissions and executions) and liquidity effects are taken into account.",
    "context": "This chunk focuses on the methodology used to analyze the impact of technical trading strategies, specifically examining the relationship between bounces on support and resistance levels and the number of previous bounces. It details the measurement of conditional probability and the development of a criterion for identifying potential support/resistance levels, suggesting this analysis could be used to predict future price movements.",
    "document": "srep04487.pdf",
    "pages": [
      6,
      7
    ],
    "id": "d976de69de08cb7c4fbd1161960eee6e7b3c8d244460319e2fc5467dfc83db40"
  },
  {
    "text": "Supports and Resistances: quantitative definition . One has to face two issues to build a quantitative definition of support and resistance: the time scale and the width of the bounces.\n- . We define a bounce of the price on a support/resistance level as the event of a future price entering in a stripe centered on the support/resistance and exiting from the stripe without crossing it. Furthermore, we want to develop a quantitative definition compatible with the way the investors use to spot the support/ resistances in the price graphs. In fact the assumed memory effect of the price stems from the visual inspection of the graphs that comes before an investment decision. To clarify this point let us consider the three price graphs in fig. 7. The\nFigure 7 | The graph above illustrates the price (panel (a)) tick-by-tick of the stock British Petroleum in the 18th trading day of 2002. The blue and red circles define two regions of different size where we want to look for supports and resistances. Panel (b) shows the price (in blue) extracted from the time series tick-by-tick picking out a price every 50 ticks in the same trading day of the same stock. Panel (c) shows the price (in red) extracted from the time series tick-by-tick picking out a price every 10 ticks. The horizontal lines represent the stripes of the resistances to be analyzed.\ngraph in the top panel shows the price tick-by-tick of British Petroleum in the 18th trading day of 2002. If we look to the price at the time scale of the blue circle we can state that there are two bounces on a resistance, neglecting the price fluctuations at minor time scales. Conversely, if we look to the price at the time scale of the red circle we can state that there are three bounces on a resistance, neglecting the price fluctuation at greater time scales. The bare eye distinguishes between bounces at different time scales, which have to be analyzed separately. To select the time scale to be used for the analysis of the bounces, we considered the time series PT ( t i ) obtained picking out a price every T ticks from the time series. The obtained time series is a subset of the original one: if the latter has L terms then the former has [ L / T ] terms (the square brackets [ ] indicate the floor function defined as x ½ /C138 ~ max m [ Z m ƒ x j f g ). In this way we can remove the information on the price fluctuations for time scales smaller than T . The two graphs in fig. 7 (bottom panel) show the price time series obtained from the tick-by-tick recordings respectively every 50 and 10 ticks. Wecan see that the blue graph on the left shows only the bounces at the greater time scale (the time scale of the blue circle) as the price fluctuations at the minor time scale (the one of the red circle) are absent. Conversely these price fluctuations at the minor time scale are evident in the red graph on the right.\n- . The width D of the stripe centered on the support or resistance at the time scale T is defined as\n<!-- formula-not-decoded -->\nthat is the average of the absolute value of the price increments at time scale T . Therefore D depends on both the trading day and the time scale and it generally rises as T does. In fact it approximately holds that D ( T ) , T a where a is the diffusion exponent of the price in the day considered. The width of the stripe represents the tolerance of the investors on a given support or resistance: if the price drops below this threshold the investors regard the support or resistance as broken.\nTo sum up, we try to separate the analysis of the bounces of price on supports and resistances for different time scales. Provided this quantitative definition of support and resistance in term of bounces we perform an analysis of the bounces in order to determine if there is a memory effect on the price dynamics on the previous bounces and if this effect is statistically meaningful.\n1. Golub, T. Counterpoint: Data first. Nature 464 , 679 (2010).\n2. Mitchell, T. Mining Our Reality. Science 326 , 1644-1645 (2009).\n3. Vespignani, A. Predicting the Behavior of Techno-Social Systems. Science 325 , 425-428 (2009).\n4. Lazer, D. et al . Computational Social Science. Science 323 , 721-23 (2009).\n5. Goel, S., Hofman, J., Lahaie, S., Pennock, D. & Watts, D. Predicting consumer behavior with Web search. PNAS 107 , 17486 (2010).\n6. Golder, S. & Macy, M. Diurnal and Seasonal Mood Vary with Work, Sleep, and Daylength Across Diverse Cultures. Science 333 , 1878-1881 (2011).\n7. Ginzberg, J. et al . Detecting influenza epidemics using search engine query data. Nature 457 , 1012-1014 (2009).\n8. Choi, H. & Varian, H. Predicting the Present with Google Trends. http://static. googleusercontent.com/external_content/untrusted_dlcp/www.google.com/it// googleblogs/pdfs/google_predicting_the_present.pdf (2009)Accessed November 12, 2013.\n9. Bordino, I. et al . Web Search Queries Can Predict Stock Market Volumes. PLOS ONE 7 , e40014; DOI: 10.1371/journal.pone.0040014 (2009).\n10. Saavedra, S., Hagerty, K. & Uzzi, B. Synchronicity, instant messaging, and performance among financial traders. PNAS 108 , 5296-5301 (2011).\n11. Preis, T., Reith, D. & Stanley, H. E. Complex dynamics of our economic life on different scales: insights from search engine query data. Philos Transact A Math Phys Eng Sci. 386 , 5707-5719 (2010).\n12. Chakraborti, A., Toke, I. M., Patriarca, M. & Abergel, F. Econophysics Review: I. Empirical facts. Quant. Financ. 11 , 991-1012 (2011).\n13. Cristelli, M., Zaccaria, A. & Pietronero, L. [Critical overview of Agent-Based Models] Complex Materials in Physics and Biology [Mallamace, F. & Stanley, H. E. (eds.)] [235-282] (IOS Press, 2011).\n14. Bouchaud, J.-P., Farmer, J. D. & Lillo, F. [How Markets Slowly Digest Changes in Supply and Demand] Handbook of Financial Markets: Dynamics and Evolution [57-160] (Elsevier: Academic Press, 2009).\n15. Bouchaud, J.-P., Mezard, M. & Potters, M. Statistical Properties of Stock Order Books: Empirical Results and Models. Quant. Financ. 2 , 251-256 (2002).\n\nDefines a quantitative support/resistance level based on price bounces, specifying the width of the stripe as the average absolute price increment at a given time scale and acknowledging the importance of considering both the trading day and the time scale.",
    "original_text": "Supports and Resistances: quantitative definition . One has to face two issues to build a quantitative definition of support and resistance: the time scale and the width of the bounces.\n- . We define a bounce of the price on a support/resistance level as the event of a future price entering in a stripe centered on the support/resistance and exiting from the stripe without crossing it. Furthermore, we want to develop a quantitative definition compatible with the way the investors use to spot the support/ resistances in the price graphs. In fact the assumed memory effect of the price stems from the visual inspection of the graphs that comes before an investment decision. To clarify this point let us consider the three price graphs in fig. 7. The\nFigure 7 | The graph above illustrates the price (panel (a)) tick-by-tick of the stock British Petroleum in the 18th trading day of 2002. The blue and red circles define two regions of different size where we want to look for supports and resistances. Panel (b) shows the price (in blue) extracted from the time series tick-by-tick picking out a price every 50 ticks in the same trading day of the same stock. Panel (c) shows the price (in red) extracted from the time series tick-by-tick picking out a price every 10 ticks. The horizontal lines represent the stripes of the resistances to be analyzed.\ngraph in the top panel shows the price tick-by-tick of British Petroleum in the 18th trading day of 2002. If we look to the price at the time scale of the blue circle we can state that there are two bounces on a resistance, neglecting the price fluctuations at minor time scales. Conversely, if we look to the price at the time scale of the red circle we can state that there are three bounces on a resistance, neglecting the price fluctuation at greater time scales. The bare eye distinguishes between bounces at different time scales, which have to be analyzed separately. To select the time scale to be used for the analysis of the bounces, we considered the time series PT ( t i ) obtained picking out a price every T ticks from the time series. The obtained time series is a subset of the original one: if the latter has L terms then the former has [ L / T ] terms (the square brackets [ ] indicate the floor function defined as x ½ /C138 ~ max m [ Z m ƒ x j f g ). In this way we can remove the information on the price fluctuations for time scales smaller than T . The two graphs in fig. 7 (bottom panel) show the price time series obtained from the tick-by-tick recordings respectively every 50 and 10 ticks. Wecan see that the blue graph on the left shows only the bounces at the greater time scale (the time scale of the blue circle) as the price fluctuations at the minor time scale (the one of the red circle) are absent. Conversely these price fluctuations at the minor time scale are evident in the red graph on the right.\n- . The width D of the stripe centered on the support or resistance at the time scale T is defined as\n<!-- formula-not-decoded -->\nthat is the average of the absolute value of the price increments at time scale T . Therefore D depends on both the trading day and the time scale and it generally rises as T does. In fact it approximately holds that D ( T ) , T a where a is the diffusion exponent of the price in the day considered. The width of the stripe represents the tolerance of the investors on a given support or resistance: if the price drops below this threshold the investors regard the support or resistance as broken.\nTo sum up, we try to separate the analysis of the bounces of price on supports and resistances for different time scales. Provided this quantitative definition of support and resistance in term of bounces we perform an analysis of the bounces in order to determine if there is a memory effect on the price dynamics on the previous bounces and if this effect is statistically meaningful.\n1. Golub, T. Counterpoint: Data first. Nature 464 , 679 (2010).\n2. Mitchell, T. Mining Our Reality. Science 326 , 1644-1645 (2009).\n3. Vespignani, A. Predicting the Behavior of Techno-Social Systems. Science 325 , 425-428 (2009).\n4. Lazer, D. et al . Computational Social Science. Science 323 , 721-23 (2009).\n5. Goel, S., Hofman, J., Lahaie, S., Pennock, D. & Watts, D. Predicting consumer behavior with Web search. PNAS 107 , 17486 (2010).\n6. Golder, S. & Macy, M. Diurnal and Seasonal Mood Vary with Work, Sleep, and Daylength Across Diverse Cultures. Science 333 , 1878-1881 (2011).\n7. Ginzberg, J. et al . Detecting influenza epidemics using search engine query data. Nature 457 , 1012-1014 (2009).\n8. Choi, H. & Varian, H. Predicting the Present with Google Trends. http://static. googleusercontent.com/external_content/untrusted_dlcp/www.google.com/it// googleblogs/pdfs/google_predicting_the_present.pdf (2009)Accessed November 12, 2013.\n9. Bordino, I. et al . Web Search Queries Can Predict Stock Market Volumes. PLOS ONE 7 , e40014; DOI: 10.1371/journal.pone.0040014 (2009).\n10. Saavedra, S., Hagerty, K. & Uzzi, B. Synchronicity, instant messaging, and performance among financial traders. PNAS 108 , 5296-5301 (2011).\n11. Preis, T., Reith, D. & Stanley, H. E. Complex dynamics of our economic life on different scales: insights from search engine query data. Philos Transact A Math Phys Eng Sci. 386 , 5707-5719 (2010).\n12. Chakraborti, A., Toke, I. M., Patriarca, M. & Abergel, F. Econophysics Review: I. Empirical facts. Quant. Financ. 11 , 991-1012 (2011).\n13. Cristelli, M., Zaccaria, A. & Pietronero, L. [Critical overview of Agent-Based Models] Complex Materials in Physics and Biology [Mallamace, F. & Stanley, H. E. (eds.)] [235-282] (IOS Press, 2011).\n14. Bouchaud, J.-P., Farmer, J. D. & Lillo, F. [How Markets Slowly Digest Changes in Supply and Demand] Handbook of Financial Markets: Dynamics and Evolution [57-160] (Elsevier: Academic Press, 2009).\n15. Bouchaud, J.-P., Mezard, M. & Potters, M. Statistical Properties of Stock Order Books: Empirical Results and Models. Quant. Financ. 2 , 251-256 (2002).",
    "context": "Defines a quantitative support/resistance level based on price bounces, specifying the width of the stripe as the average absolute price increment at a given time scale and acknowledging the importance of considering both the trading day and the time scale.",
    "document": "srep04487.pdf",
    "pages": [
      8,
      7
    ],
    "id": "a1d6988781933127ec29d69391f6b12cd24a96e7b6ae950436f409c6bc073272"
  },
  {
    "text": "16. Lillo, F. & Farmer, J. The Long Memory of the Efficient Market. Stud. Nonlinear Dyn. E. 8 , Article 1 (2004).\n17. Laloux, L., Cizeau, P., Bouchaud, J.-P. & Potters, M. Noise Dressing of Financial Correlation Matrices. Phys. Rev. Lett. 83 , 1467 (1999).\n18. Laloux, L., Cizeau, P., Bouchaud, J.-P. & Potters, M. Random matrix theory and financial correlations. Int. J. Theor. Appl. Finance 3 , 391 (2000).\n19. Potters, M. & Bouchaud, J.-P. More statistical properties of order books and price impact. Physica A , 324 , 133-140 (2003).\n20. Wyart, M., Bouchaud, J.-P., Kockelkoren, J., Potters, M. & Vettorazzo, M. Relation between bidask spread, impact and volatility in order-driven markets. Quant. Financ. 8 , 41-57 (2008).\n21. Lillo, F., Farmer, J. D. & Mantegna, R. N. Master curve for price-impact function. Nature , 421 , 129-130 (2003).\n22. Farmer, J. D., Gillemot, L., Lillo, F., Mike, S. & Sen, A. What really causes large price changes? Quant. Financ. 4 , 383-397 (2004).\n23. Weber, P. & Rosenow, B. Large stock price changes: Volume or liquidity? Quant. Financ. 6 , 7-14 (2006).\n24. Cristelli, M., Alfi, V., Pietronero, L. & Zaccaria, A. Liquidity crisis, granularity of the order book and price fluctuations. Eur. Phys. J. B 73 , 41-49 (2010).\n25. Alfi, V., Cristelli, M., Pietronero, L. & Zaccaria, A. Mechanisms of SelfOrganization and Finite Size Effects in a Minimal Agent Based Model. J. Stat. Mech. P03016 (2009).\n26. Samanidou, E., Zschischang, E., Stauffer, D. & Lux, T. Agent-based models of financial markets. Rep. Prog. Phys. 70 , 409 (2007).\n27. Lux, T. & Marchesi, M. Scaling and criticality in a stochastic multi-agent model of a financial market. Nature , 397 , 498-500 (1999).\n28. Giardina, I. & Bouchaud, J.-P. Bubbles, crashes and intermittency in agent based market models. Eur. Phys. J. B 31 , 421-437 (2003).\n29. Caldarelli, G., Marsili, M. & Zhang, Y.-C. A prototype model of stock exchange. Europhys. Lett. 40 , 479-484 (1997).\n30. Alfi, V., Pietronero, L. & Zaccaria, A. Self-organization for the stylized facts and finite-size effects in a financial-market model. Europhys. Lett. 86 , 58003 (2009).\n31. Alfi, V., Cristelli, M., Pietronero, L. & Zaccaria, A. Minimal Agent Based Model for Financial Markets I: Origin and Self-Organization of Stylized Facts. Eur. Phys. J. B 67 , 385-397 (2009).\n32. Alfi, V., Cristelli, M., Pietronero, L. & Zaccaria, A. Minimal Agent Based Model for Financial Markets II: Statistical Properties of the Linear and Multiplicative Dynamics. Eur. Phys. J. B 67 , 399-417 (2009).\n33. Alanyali, M., Moat, S. M. & Preis, T. Quantifying the Relationship Between Financial News and the Stock Market. Sci. Rep. 3 , 3578; DOI:10.1038/srep03578 (2013).\n34. Buchanan, M. Why economic theory is out of whack. NewSci. 2665 , 32-35 (2008).\n35. Kreps, D. M. Acourse in microeconomic theory (Princeton University Press, 1990).\n36. Fama, E. F. Efficient Capital Markets: A Review of Theory and Empirical Work. J. Finance 25 , 383-417 (1970).\n37. Park, C.-H. & Irwin, S. H. What Do We Know About the Profitability of Technical Analysis? J. Econ. Surv. 21 , 786-826 (2007).\n38. Smidt, S. Amateur speculators (Ithaca, NY: Graduate School of Business and Public Administration Cornell University, 1965).\n39. Menkhoff, L. The use of technical analysis by fund managers: International evidence. J. Bank Financ. 34 , 2573-2586 (2010).\n40. Brock, W., Lakonishok, J. & LeBaron, B. Simple Technical Trading Rules and the Stochastic Properties of Stock Returns. J. Finance 47 , 1731-1767 (1992).\n41. Sullivan, R., Timmermann, A. & White, H. Forecast evaluation with shared data sets. Int. J. Forecasting 19 , 217-227 (2003).\n42. Osler, C. L. Currency Orders and Exchange Rate Dynamics: An Explanation for the Predictive Success of Technical Analysis. J. Finance 58 , 1791-1820 (2003).\n43. Preis, T., Schneider, J. J. & Stanley, H. E. Switching processes in financial markets. PNAS 108 , 7674-7678 (2011).\n44. Preis, T., Paul, W. J. & Schneider, J. J. Fluctuation patterns in high-frequency financial asset returns. Europhys. Lett. 82 , 68005 (2008).\n45. Preis, T. GPU-computing in econophysics and statistical physics. Eur. Phys. J.Spec. Top. 194 , 5-86 (2011).\n46. Sornette, D., Woodard, R. & Zhou, W.-X. Oil Bubble: evidence of speculation and prediction. Physica A 388 , 1571-1576 (2009).\n47. Hull, J. C. Options, Futures, and Other Derivatives (Prentice Hall, 2002).\n48. Bouchaud, J. P. & Potters, M. Theory of Financial Risk and Derivative Pricing (Cambridge university press, 2003).\n49. Voit, J. The Statistical Mechanics of Financial Markets (Springer, 2005).\n50. Mantegna, R. N. & Stanley, H. E. An Introduction to Econophysics (Cambridge university press, 2000).\n51. Murphy, J. J. Technical Analysis of the Financial Markets (New York institute of Finance, 1999).\n52. Feller, W. An Introduction to Probability Theory and Its Applications (Wiley 3th edition, 1968).\n53. D'Agostini, G. Bayesian reasoning in data analysis. A critical introduction (World Scientific Publishing, 2003).\n54. Smirnov, N. Table for estimating the goodness of fit of empirical distributions. Ann. Math. Stat. 19 , 279-281 (1948).\n55. Peng, C.-K. et al . Mosaic Organization of DNA Nucleotides. Phys. Rev. E 49 , 1685-1689 (1994).\n56. Peng, C. K., Havlin, S., Stanley, H. E. & Goldberger, A. L. Quantification of Scaling Exponents and Crossover Phenomena in Nonstationary Heartbeat Time Series. CHAOS 5 , 82 (2005).\n\nExplores long-term memory and self-fulfilling prophecies in stock price dynamics, building upon previous research on technical trading strategies and market behavior.",
    "original_text": "16. Lillo, F. & Farmer, J. The Long Memory of the Efficient Market. Stud. Nonlinear Dyn. E. 8 , Article 1 (2004).\n17. Laloux, L., Cizeau, P., Bouchaud, J.-P. & Potters, M. Noise Dressing of Financial Correlation Matrices. Phys. Rev. Lett. 83 , 1467 (1999).\n18. Laloux, L., Cizeau, P., Bouchaud, J.-P. & Potters, M. Random matrix theory and financial correlations. Int. J. Theor. Appl. Finance 3 , 391 (2000).\n19. Potters, M. & Bouchaud, J.-P. More statistical properties of order books and price impact. Physica A , 324 , 133-140 (2003).\n20. Wyart, M., Bouchaud, J.-P., Kockelkoren, J., Potters, M. & Vettorazzo, M. Relation between bidask spread, impact and volatility in order-driven markets. Quant. Financ. 8 , 41-57 (2008).\n21. Lillo, F., Farmer, J. D. & Mantegna, R. N. Master curve for price-impact function. Nature , 421 , 129-130 (2003).\n22. Farmer, J. D., Gillemot, L., Lillo, F., Mike, S. & Sen, A. What really causes large price changes? Quant. Financ. 4 , 383-397 (2004).\n23. Weber, P. & Rosenow, B. Large stock price changes: Volume or liquidity? Quant. Financ. 6 , 7-14 (2006).\n24. Cristelli, M., Alfi, V., Pietronero, L. & Zaccaria, A. Liquidity crisis, granularity of the order book and price fluctuations. Eur. Phys. J. B 73 , 41-49 (2010).\n25. Alfi, V., Cristelli, M., Pietronero, L. & Zaccaria, A. Mechanisms of SelfOrganization and Finite Size Effects in a Minimal Agent Based Model. J. Stat. Mech. P03016 (2009).\n26. Samanidou, E., Zschischang, E., Stauffer, D. & Lux, T. Agent-based models of financial markets. Rep. Prog. Phys. 70 , 409 (2007).\n27. Lux, T. & Marchesi, M. Scaling and criticality in a stochastic multi-agent model of a financial market. Nature , 397 , 498-500 (1999).\n28. Giardina, I. & Bouchaud, J.-P. Bubbles, crashes and intermittency in agent based market models. Eur. Phys. J. B 31 , 421-437 (2003).\n29. Caldarelli, G., Marsili, M. & Zhang, Y.-C. A prototype model of stock exchange. Europhys. Lett. 40 , 479-484 (1997).\n30. Alfi, V., Pietronero, L. & Zaccaria, A. Self-organization for the stylized facts and finite-size effects in a financial-market model. Europhys. Lett. 86 , 58003 (2009).\n31. Alfi, V., Cristelli, M., Pietronero, L. & Zaccaria, A. Minimal Agent Based Model for Financial Markets I: Origin and Self-Organization of Stylized Facts. Eur. Phys. J. B 67 , 385-397 (2009).\n32. Alfi, V., Cristelli, M., Pietronero, L. & Zaccaria, A. Minimal Agent Based Model for Financial Markets II: Statistical Properties of the Linear and Multiplicative Dynamics. Eur. Phys. J. B 67 , 399-417 (2009).\n33. Alanyali, M., Moat, S. M. & Preis, T. Quantifying the Relationship Between Financial News and the Stock Market. Sci. Rep. 3 , 3578; DOI:10.1038/srep03578 (2013).\n34. Buchanan, M. Why economic theory is out of whack. NewSci. 2665 , 32-35 (2008).\n35. Kreps, D. M. Acourse in microeconomic theory (Princeton University Press, 1990).\n36. Fama, E. F. Efficient Capital Markets: A Review of Theory and Empirical Work. J. Finance 25 , 383-417 (1970).\n37. Park, C.-H. & Irwin, S. H. What Do We Know About the Profitability of Technical Analysis? J. Econ. Surv. 21 , 786-826 (2007).\n38. Smidt, S. Amateur speculators (Ithaca, NY: Graduate School of Business and Public Administration Cornell University, 1965).\n39. Menkhoff, L. The use of technical analysis by fund managers: International evidence. J. Bank Financ. 34 , 2573-2586 (2010).\n40. Brock, W., Lakonishok, J. & LeBaron, B. Simple Technical Trading Rules and the Stochastic Properties of Stock Returns. J. Finance 47 , 1731-1767 (1992).\n41. Sullivan, R., Timmermann, A. & White, H. Forecast evaluation with shared data sets. Int. J. Forecasting 19 , 217-227 (2003).\n42. Osler, C. L. Currency Orders and Exchange Rate Dynamics: An Explanation for the Predictive Success of Technical Analysis. J. Finance 58 , 1791-1820 (2003).\n43. Preis, T., Schneider, J. J. & Stanley, H. E. Switching processes in financial markets. PNAS 108 , 7674-7678 (2011).\n44. Preis, T., Paul, W. J. & Schneider, J. J. Fluctuation patterns in high-frequency financial asset returns. Europhys. Lett. 82 , 68005 (2008).\n45. Preis, T. GPU-computing in econophysics and statistical physics. Eur. Phys. J.Spec. Top. 194 , 5-86 (2011).\n46. Sornette, D., Woodard, R. & Zhou, W.-X. Oil Bubble: evidence of speculation and prediction. Physica A 388 , 1571-1576 (2009).\n47. Hull, J. C. Options, Futures, and Other Derivatives (Prentice Hall, 2002).\n48. Bouchaud, J. P. & Potters, M. Theory of Financial Risk and Derivative Pricing (Cambridge university press, 2003).\n49. Voit, J. The Statistical Mechanics of Financial Markets (Springer, 2005).\n50. Mantegna, R. N. & Stanley, H. E. An Introduction to Econophysics (Cambridge university press, 2000).\n51. Murphy, J. J. Technical Analysis of the Financial Markets (New York institute of Finance, 1999).\n52. Feller, W. An Introduction to Probability Theory and Its Applications (Wiley 3th edition, 1968).\n53. D'Agostini, G. Bayesian reasoning in data analysis. A critical introduction (World Scientific Publishing, 2003).\n54. Smirnov, N. Table for estimating the goodness of fit of empirical distributions. Ann. Math. Stat. 19 , 279-281 (1948).\n55. Peng, C.-K. et al . Mosaic Organization of DNA Nucleotides. Phys. Rev. E 49 , 1685-1689 (1994).\n56. Peng, C. K., Havlin, S., Stanley, H. E. & Goldberger, A. L. Quantification of Scaling Exponents and Crossover Phenomena in Nonstationary Heartbeat Time Series. CHAOS 5 , 82 (2005).",
    "context": "Explores long-term memory and self-fulfilling prophecies in stock price dynamics, building upon previous research on technical trading strategies and market behavior.",
    "document": "srep04487.pdf",
    "pages": [
      9
    ],
    "id": "2b69a821fa366b7a164b576c571c862ea410428c9e5557c1d2ac1234e3820e3c"
  },
  {
    "text": "This work was supported by Italian PNR project CRISIS-Lab. We thank Diego Fiorani for providing the data and Andrea Tacchella for useful discussion about generating fractional random walks.\n\nThis chunk details the funding source and acknowledges assistance received for data analysis and discussion regarding fractional random walks, providing context for the research’s origins and collaborative aspects.",
    "original_text": "This work was supported by Italian PNR project CRISIS-Lab. We thank Diego Fiorani for providing the data and Andrea Tacchella for useful discussion about generating fractional random walks.",
    "context": "This chunk details the funding source and acknowledges assistance received for data analysis and discussion regarding fractional random walks, providing context for the research’s origins and collaborative aspects.",
    "document": "srep04487.pdf",
    "pages": [
      9
    ],
    "id": "126848f2292e4a8c2bf137df74ad5240d328585bacf25a10a04ae4d6daddd92e"
  },
  {
    "text": "M.C., A.Z. and L.P. developed the logic for these ideas, F.G. explored the data and M.C., A.Z., F.G. and G.P. developed the formal and statistical analysis. All authors were involved in writing the paper.\n\nDetails the collaborative development of the analysis and writing of the paper.",
    "original_text": "M.C., A.Z. and L.P. developed the logic for these ideas, F.G. explored the data and M.C., A.Z., F.G. and G.P. developed the formal and statistical analysis. All authors were involved in writing the paper.",
    "context": "Details the collaborative development of the analysis and writing of the paper.",
    "document": "srep04487.pdf",
    "pages": [
      9
    ],
    "id": "1fff241e63df7248329cfe749946a2bf8f5903badd6dc29761f928b01791e0ea"
  },
  {
    "text": "Supplementary information accompanies this paper at http://www.nature.com/ scientificreports\nCompeting financial interests: The authors declare no competing financial interests.\nHowtocite this article: Garzarelli, F., Cristelli, M., Pompa, G., Zaccaria, A. & Pietronero, L. Memory effects in stock price dynamics: evidences of technical trading. Sci. Rep. 4 , 4487; DOI:10.1038/srep04487 (2014).\nThis work is licensed under a Creative Commons Attribution-\nNonCommercial-ShareAlike 3.0 Unported license. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-sa/3.0\n\nProvides citation information and licensing details for the article.",
    "original_text": "Supplementary information accompanies this paper at http://www.nature.com/ scientificreports\nCompeting financial interests: The authors declare no competing financial interests.\nHowtocite this article: Garzarelli, F., Cristelli, M., Pompa, G., Zaccaria, A. & Pietronero, L. Memory effects in stock price dynamics: evidences of technical trading. Sci. Rep. 4 , 4487; DOI:10.1038/srep04487 (2014).\nThis work is licensed under a Creative Commons Attribution-\nNonCommercial-ShareAlike 3.0 Unported license. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-sa/3.0",
    "context": "Provides citation information and licensing details for the article.",
    "document": "srep04487.pdf",
    "pages": [
      9
    ],
    "id": "6d2da13d43426b9a051aee068403c31488fe223a9f5511b9c0ca29229dc6eddc"
  },
  {
    "text": "SUBJECT AREAS: COMPLEX NETWORKS APPLIED PHYSICS COMPUTATIONAL SCIENCE COMPUTER SCIENCE\nReceived 17 September 2013\nAccepted 5 December 2013\nPublished 20 December 2013\nCorrespondence and requests for materials should be addressed to M.A. (M.Alanyali@ warwick.ac.uk)\n\nQuantifies the relationship between decisions taken in financial markets and developments in financial news.",
    "original_text": "SUBJECT AREAS: COMPLEX NETWORKS APPLIED PHYSICS COMPUTATIONAL SCIENCE COMPUTER SCIENCE\nReceived 17 September 2013\nAccepted 5 December 2013\nPublished 20 December 2013\nCorrespondence and requests for materials should be addressed to M.A. (M.Alanyali@ warwick.ac.uk)",
    "context": "Quantifies the relationship between decisions taken in financial markets and developments in financial news.",
    "document": "srep03578.pdf",
    "pages": [
      1
    ],
    "id": "c39b1b779265011c97964b077d16b04cfea472f9e0df7236dc22d98762d28689"
  },
  {
    "text": "Merve Alanyali 1 , Helen Susannah Moat 2 & Tobias Preis 2\n1 Centre for Complexity Science, University of Warwick, Coventry, CV4 7AL, UK, 2 WarwickBusinessSchool, University of Warwick, Coventry, CV4 7AL, UK.\nThecomplexbehaviorof financial markets emerges from decisions made by many traders. Here, we exploit a large corpus of daily print issues of the Financial Times from 2 nd January 2007 until 31 st December 2012 to quantify the relationship between decisions taken in financial markets and developments in financial news. We find a positive correlation between the daily number of mentions of a company in the Financial Times and the daily transaction volume of a company's stock both on the day before the news is released, and on the same day as the news is released. Our results provide quantitative support for the suggestion that movements in financial markets and movements in financial news are intrinsically interlinked.\nT he movements of stock markets impact the lives of many individuals, within the financial sector and far beyond. Obvious benefits therefore lie in an improved understanding of the behavior of this complex system. Research towards this goal has been fuelled by the vast amount of data on financial transactions recorded at exchanges, with increasing numbers of studies in complex systems science aiming to analyze 1-7 and model stock market behavior 8-12 .\nFinancial transaction data sets reflect the final outcome of a trader's decision making process 13 , the decision to buy or sell a particular stock. Such decisions may be influenced by various types of information in a trader's environment. In modern society, our interactions with the Internet are generating large new data sources on our consumption of information 14-18 . Previous work has demonstrated that search patterns on Google can be linked to various indicators of behavior in the real world 19 , such as reports of infections of influenza-like illnesses 20 , the economic success of nations 21 , and various other economic indicators such as popularity of international travel destinations and unemployment claims 22 .\nRecent research has sought to investigate whether data on what information users seek online can provide insight into market movements. Preis, Reith and Stanley provided initial evidence of a link between online searches and financial market behavior, describing a correlation between the weekly number of Google searches on a company name and weekly cumulative transaction volume of the corresponding company's stock 23 . Preis, Moat and Stanley built on this result, demonstrating that changes in Google query volume for search terms related to finance could be interpreted as early warning signs of stock market moves 24 . Moat et al. showed that data on views of Wikipedia pages can also be related to market movements, providing evidence of increases in views of financially related Wikipedia pages before stock market falls 25 . Evidence has also been provided that Google Trends data can be used to measure the risk of investment in a stock 26 .\nTraders may however not only receive information through explicit attempts to search for information online, but by passively or actively receiving news broadcast by large financial news outlets. Equally, the actions of traders may lead to events which are described by the financial news. In this study, we seek to quantify the relationship between movements in financial news and movements in financial markets by exploiting a corpus of six years of financial news.\n\nQuantifies the relationship between financial news and market movements by analyzing six years of Financial Times print issues.",
    "original_text": "Merve Alanyali 1 , Helen Susannah Moat 2 & Tobias Preis 2\n1 Centre for Complexity Science, University of Warwick, Coventry, CV4 7AL, UK, 2 WarwickBusinessSchool, University of Warwick, Coventry, CV4 7AL, UK.\nThecomplexbehaviorof financial markets emerges from decisions made by many traders. Here, we exploit a large corpus of daily print issues of the Financial Times from 2 nd January 2007 until 31 st December 2012 to quantify the relationship between decisions taken in financial markets and developments in financial news. We find a positive correlation between the daily number of mentions of a company in the Financial Times and the daily transaction volume of a company's stock both on the day before the news is released, and on the same day as the news is released. Our results provide quantitative support for the suggestion that movements in financial markets and movements in financial news are intrinsically interlinked.\nT he movements of stock markets impact the lives of many individuals, within the financial sector and far beyond. Obvious benefits therefore lie in an improved understanding of the behavior of this complex system. Research towards this goal has been fuelled by the vast amount of data on financial transactions recorded at exchanges, with increasing numbers of studies in complex systems science aiming to analyze 1-7 and model stock market behavior 8-12 .\nFinancial transaction data sets reflect the final outcome of a trader's decision making process 13 , the decision to buy or sell a particular stock. Such decisions may be influenced by various types of information in a trader's environment. In modern society, our interactions with the Internet are generating large new data sources on our consumption of information 14-18 . Previous work has demonstrated that search patterns on Google can be linked to various indicators of behavior in the real world 19 , such as reports of infections of influenza-like illnesses 20 , the economic success of nations 21 , and various other economic indicators such as popularity of international travel destinations and unemployment claims 22 .\nRecent research has sought to investigate whether data on what information users seek online can provide insight into market movements. Preis, Reith and Stanley provided initial evidence of a link between online searches and financial market behavior, describing a correlation between the weekly number of Google searches on a company name and weekly cumulative transaction volume of the corresponding company's stock 23 . Preis, Moat and Stanley built on this result, demonstrating that changes in Google query volume for search terms related to finance could be interpreted as early warning signs of stock market moves 24 . Moat et al. showed that data on views of Wikipedia pages can also be related to market movements, providing evidence of increases in views of financially related Wikipedia pages before stock market falls 25 . Evidence has also been provided that Google Trends data can be used to measure the risk of investment in a stock 26 .\nTraders may however not only receive information through explicit attempts to search for information online, but by passively or actively receiving news broadcast by large financial news outlets. Equally, the actions of traders may lead to events which are described by the financial news. In this study, we seek to quantify the relationship between movements in financial news and movements in financial markets by exploiting a corpus of six years of financial news.",
    "context": "Quantifies the relationship between financial news and market movements by analyzing six years of Financial Times print issues.",
    "document": "srep03578.pdf",
    "pages": [
      1
    ],
    "id": "207f22f77d5bd6b8e280d8df0ff17f0d31953e4c9587a97e28d3c6917e23321a"
  },
  {
    "text": "To examine the relationship between financial news and market behavior, we analyze a corpus of daily issues of the Financial Times from 2 nd January 2007 to 31 st December 2012. Details of how the corpus was retrieved and preprocessed are provided in the Supplementary Information.\nThe Financial Times is released each day from Monday to Saturday, at 5 am London time. An initial evaluation of the basic characteristics of the data, depicted in Figure 1, shows that there are significant differences in the length of the Financial Times on different days of the week (median of the number of total words for the given weekday: Monday, 134768.5; Tuesday, 112279; Wednesday, 112536; Thursday, 116690; Friday, 111663; Saturday,\nFigure 1 | Daily variation in the total number of words occurring in each issue of the Financial Times. Daily variation in the total number of words in each issue of the Financial Times between 2 nd January 2007 and 31 st December 2012. We find significant differences in the length of the Financial Times on different days of the week (median of the number of total words for the given weekday: Monday, 134768.5; Tuesday, 112279; Wednesday, 112536; Thursday, 116690; Friday, 111663; Saturday, 195492; x 2 5 702.5324, df 5 5, p , 0.001, Kruskal-Wallis rank sum test).\nSignificantly longer issues are produced on Saturdays in comparison to the rest of the week (all W s . 128,000, all p s , 0.001, pairwise Wilcoxon rank sum tests with Bonferroni corrected a 5 0.0033), and issues on Mondays are significantly longer than issues on Tuesday to Friday (all W s . 111,000, all p s , 0.001, pairwise Wilcoxon rank sum tests with Bonferroni corrected a 5 0.0033). We find no evidence that the length of issues varies between Tuesday to Friday (all W s , 100,000, all p s . 0.01, pairwise Wilcoxon rank sum tests with Bonferroni corrected a 5 0.0033).\n195492; x 2 5 702.5324, df 5 5, p , 0.001, Kruskal-Wallis rank sum test). We find longer issues on Saturdays in comparison to the rest of the week (all W s . 128,000, all p s , 0.001, pairwise Wilcoxon rank sum tests with Bonferroni corrected a 5 0.0033), reflecting the publication of a special weekend edition of the Financial Times .\nSimilarly, issues on Mondays, following the break on a Sunday, are significantly longer than issues on Tuesday to Friday (all W s . 111,000, all p s , 0.001, pairwise Wilcoxon rank sum tests with Bonferroni corrected a 5 0.0033). We find no evidence that the length of issues varies between Tuesday to Friday (all W s , 100,000, all p s . 0.01, pairwise Wilcoxon rank sum tests with Bonferroni corrected a 5 0.0033).\nAtotal of 891,171 different words occur throughout the Financial Times corpus. We begin our investigation of the relationship between financial news and financial market movements by focusing on occurrences of the names of the 31 companies that were listed in the Dow Jones Industrial Average (DJIA) between 2 nd January 2008 and 31 st December 2012, a period for which we have transaction volume and price data for the DJIA components. At any one time, the DJIA consists of 30 companies. However, Travelers replaced Citigroup in the DJIA during the period of our analysis. In the calculations reported, we consider stock data and news data for both of these companies. Full details of the company names used in the corpus analysis are provided in the Supplementary Information (Table S1).\nWe investigate the relationship between interest in a company in the news and interest in a company in the stock markets. Stocks for companies listed in the DJIA are traded at the New York Stock Exchange (NYSE), open between 9:30 am and 4 pm New York time (for most of the year, 2:30 pm to 9 pm London time). We carry out this analysis and all following analyses for trading days only, excluding all weekends and bank holidays.\nIn Figure 2, we take Bank of America as an example for this analysis, and plot the number of daily mentions of ''Bank of America'' against daily transaction volume for Bank of America . We find that a greater number of daily mentions of ''Bank of America'' corresponds to a greater daily transaction volume for Bank of America stocks ( r 5 0.43, p , 0.001, Spearman's rank correlation).\nWe extend this analysis to all 31 Dow Jones companies from this period. For each company, we calculate the Spearman's rank correlation between the daily number of mentions of a company's name in the Financial Times and the transaction volume of the corresponding company's stocks (Figure 3). We analyze the distribution of Spearman's rank correlation coefficients for all 31 companies. Whilst the strongest correlation is found for Bank of America , we find that overall, the correlation coefficients are significantly higher than zero (median correlation coefficient 5 0.074; mean correlation coefficient 5 0.100, W 5 450, p , 0.001, Wilcoxon signed rank test). A greater number of mentions of a company in the news therefore corresponds to a greater transaction volume of a company's stocks. This suggests greater interest in a company in the news is related to greater interest in a company in the stock markets.\nWe examine whether there is a similar link between the daily number of mentions of a company's name and the daily absolute return of the corresponding company's stocks. The absolute return indicates how much a stock price has changed, regardless of its direction. As a greater volume of trading is known to be correlated with greater movements in the price of a company's stock, it would be reasonable to expect the relationship between news and absolute return to be similar to the relationship we find between news and transaction volume.\nThe daily return is defined as the natural logarithm of the ratio of the closing price of a given day to the closing price from the previous day. We compute the absolute daily return for each of the 31 companies by taking the absolute values of the daily returns, and calculate the Spearman's rank correlation between the daily number of mentions of a company and the company's daily absolute return (Figure 4). Again, we find that across all 31 companies, the correlation coefficients are significantly higher than zero (median correlation coefficient 5 0.040; mean correlation coefficient 5 0.047; W 5 408, p 5 0.0017, Wilcoxon signed rank test). Our results therefore\nDaily Transaction Volume of Bank of America (BAC)\nFigure 2 | Daily number of mentions of ''Bank of America'' in the Financial Times and daily transaction volume for Bank of America (BAC) stocks. We depict the correlation between the daily number of mentions of ''Bank of America'' and the daily transaction volume for Bank of America (BAC) stocks. We find that the daily number of mentions of ''Bank of America'' is positively correlated with the daily transaction volume for Bank of America (BAC) stocks ( r 5 0.43, p , 0.001, Spearman's rank correlation).\nFigure 3 | Correlations between daily mentions of a company's name and transaction volumes for the company's stock. For each of the 31 companies that were listed in the Dow Jones Industrial Average between 2 nd January 2008 and 31 st December 2012, we plot the Spearman's rank correlation between the daily number of mentions of a company's name and the transaction volume of the corresponding company's stocks. Companies are indicated using their ticker symbol, for which a full list can be found in the Supplementary Information (Table S1). We analyze the distribution of correlation coefficients and find that, overall, the correlation coefficients are significantly higher than zero (median correlation coefficient 5 0.074; mean correlation coefficient 5 0.100; W 5 450, p , 0.001, Wilcoxon signed rank test). In other words, the daily number of mentions of a company's name is positively correlated with the daily transaction volume of a company's stocks.\nFigure 4 | Correlations between daily mentions of a company's name and absolute return for the company's stock. We examine whether there is a link between the daily number of mentions of a company's name and the daily absolute return of the corresponding company's stocks. We calculate Spearman's rank correlation between the daily number of mentions and the daily absolute return, and again find that overall, the correlation coefficients are significantly higher than zero (median correlation coefficient 5 0.040; mean correlation coefficient 5 0.047; W 5 408, p 5 0.0017, Wilcoxon signed rank test). In other words, the daily number of mentions of a company's name is positively correlated with the daily absolute return of the company's stocks.\n\nAnalyzes a corpus of daily Financial Times issues to quantify the relationship between financial news and market behavior.",
    "original_text": "To examine the relationship between financial news and market behavior, we analyze a corpus of daily issues of the Financial Times from 2 nd January 2007 to 31 st December 2012. Details of how the corpus was retrieved and preprocessed are provided in the Supplementary Information.\nThe Financial Times is released each day from Monday to Saturday, at 5 am London time. An initial evaluation of the basic characteristics of the data, depicted in Figure 1, shows that there are significant differences in the length of the Financial Times on different days of the week (median of the number of total words for the given weekday: Monday, 134768.5; Tuesday, 112279; Wednesday, 112536; Thursday, 116690; Friday, 111663; Saturday,\nFigure 1 | Daily variation in the total number of words occurring in each issue of the Financial Times. Daily variation in the total number of words in each issue of the Financial Times between 2 nd January 2007 and 31 st December 2012. We find significant differences in the length of the Financial Times on different days of the week (median of the number of total words for the given weekday: Monday, 134768.5; Tuesday, 112279; Wednesday, 112536; Thursday, 116690; Friday, 111663; Saturday, 195492; x 2 5 702.5324, df 5 5, p , 0.001, Kruskal-Wallis rank sum test).\nSignificantly longer issues are produced on Saturdays in comparison to the rest of the week (all W s . 128,000, all p s , 0.001, pairwise Wilcoxon rank sum tests with Bonferroni corrected a 5 0.0033), and issues on Mondays are significantly longer than issues on Tuesday to Friday (all W s . 111,000, all p s , 0.001, pairwise Wilcoxon rank sum tests with Bonferroni corrected a 5 0.0033). We find no evidence that the length of issues varies between Tuesday to Friday (all W s , 100,000, all p s . 0.01, pairwise Wilcoxon rank sum tests with Bonferroni corrected a 5 0.0033).\n195492; x 2 5 702.5324, df 5 5, p , 0.001, Kruskal-Wallis rank sum test). We find longer issues on Saturdays in comparison to the rest of the week (all W s . 128,000, all p s , 0.001, pairwise Wilcoxon rank sum tests with Bonferroni corrected a 5 0.0033), reflecting the publication of a special weekend edition of the Financial Times .\nSimilarly, issues on Mondays, following the break on a Sunday, are significantly longer than issues on Tuesday to Friday (all W s . 111,000, all p s , 0.001, pairwise Wilcoxon rank sum tests with Bonferroni corrected a 5 0.0033). We find no evidence that the length of issues varies between Tuesday to Friday (all W s , 100,000, all p s . 0.01, pairwise Wilcoxon rank sum tests with Bonferroni corrected a 5 0.0033).\nAtotal of 891,171 different words occur throughout the Financial Times corpus. We begin our investigation of the relationship between financial news and financial market movements by focusing on occurrences of the names of the 31 companies that were listed in the Dow Jones Industrial Average (DJIA) between 2 nd January 2008 and 31 st December 2012, a period for which we have transaction volume and price data for the DJIA components. At any one time, the DJIA consists of 30 companies. However, Travelers replaced Citigroup in the DJIA during the period of our analysis. In the calculations reported, we consider stock data and news data for both of these companies. Full details of the company names used in the corpus analysis are provided in the Supplementary Information (Table S1).\nWe investigate the relationship between interest in a company in the news and interest in a company in the stock markets. Stocks for companies listed in the DJIA are traded at the New York Stock Exchange (NYSE), open between 9:30 am and 4 pm New York time (for most of the year, 2:30 pm to 9 pm London time). We carry out this analysis and all following analyses for trading days only, excluding all weekends and bank holidays.\nIn Figure 2, we take Bank of America as an example for this analysis, and plot the number of daily mentions of ''Bank of America'' against daily transaction volume for Bank of America . We find that a greater number of daily mentions of ''Bank of America'' corresponds to a greater daily transaction volume for Bank of America stocks ( r 5 0.43, p , 0.001, Spearman's rank correlation).\nWe extend this analysis to all 31 Dow Jones companies from this period. For each company, we calculate the Spearman's rank correlation between the daily number of mentions of a company's name in the Financial Times and the transaction volume of the corresponding company's stocks (Figure 3). We analyze the distribution of Spearman's rank correlation coefficients for all 31 companies. Whilst the strongest correlation is found for Bank of America , we find that overall, the correlation coefficients are significantly higher than zero (median correlation coefficient 5 0.074; mean correlation coefficient 5 0.100, W 5 450, p , 0.001, Wilcoxon signed rank test). A greater number of mentions of a company in the news therefore corresponds to a greater transaction volume of a company's stocks. This suggests greater interest in a company in the news is related to greater interest in a company in the stock markets.\nWe examine whether there is a similar link between the daily number of mentions of a company's name and the daily absolute return of the corresponding company's stocks. The absolute return indicates how much a stock price has changed, regardless of its direction. As a greater volume of trading is known to be correlated with greater movements in the price of a company's stock, it would be reasonable to expect the relationship between news and absolute return to be similar to the relationship we find between news and transaction volume.\nThe daily return is defined as the natural logarithm of the ratio of the closing price of a given day to the closing price from the previous day. We compute the absolute daily return for each of the 31 companies by taking the absolute values of the daily returns, and calculate the Spearman's rank correlation between the daily number of mentions of a company and the company's daily absolute return (Figure 4). Again, we find that across all 31 companies, the correlation coefficients are significantly higher than zero (median correlation coefficient 5 0.040; mean correlation coefficient 5 0.047; W 5 408, p 5 0.0017, Wilcoxon signed rank test). Our results therefore\nDaily Transaction Volume of Bank of America (BAC)\nFigure 2 | Daily number of mentions of ''Bank of America'' in the Financial Times and daily transaction volume for Bank of America (BAC) stocks. We depict the correlation between the daily number of mentions of ''Bank of America'' and the daily transaction volume for Bank of America (BAC) stocks. We find that the daily number of mentions of ''Bank of America'' is positively correlated with the daily transaction volume for Bank of America (BAC) stocks ( r 5 0.43, p , 0.001, Spearman's rank correlation).\nFigure 3 | Correlations between daily mentions of a company's name and transaction volumes for the company's stock. For each of the 31 companies that were listed in the Dow Jones Industrial Average between 2 nd January 2008 and 31 st December 2012, we plot the Spearman's rank correlation between the daily number of mentions of a company's name and the transaction volume of the corresponding company's stocks. Companies are indicated using their ticker symbol, for which a full list can be found in the Supplementary Information (Table S1). We analyze the distribution of correlation coefficients and find that, overall, the correlation coefficients are significantly higher than zero (median correlation coefficient 5 0.074; mean correlation coefficient 5 0.100; W 5 450, p , 0.001, Wilcoxon signed rank test). In other words, the daily number of mentions of a company's name is positively correlated with the daily transaction volume of a company's stocks.\nFigure 4 | Correlations between daily mentions of a company's name and absolute return for the company's stock. We examine whether there is a link between the daily number of mentions of a company's name and the daily absolute return of the corresponding company's stocks. We calculate Spearman's rank correlation between the daily number of mentions and the daily absolute return, and again find that overall, the correlation coefficients are significantly higher than zero (median correlation coefficient 5 0.040; mean correlation coefficient 5 0.047; W 5 408, p 5 0.0017, Wilcoxon signed rank test). In other words, the daily number of mentions of a company's name is positively correlated with the daily absolute return of the company's stocks.",
    "context": "Analyzes a corpus of daily Financial Times issues to quantify the relationship between financial news and market behavior.",
    "document": "srep03578.pdf",
    "pages": [
      1,
      2,
      3
    ],
    "id": "5a98a61fec05e6697e20d245a1bb34a69610f817edcf458b3232892c13903838"
  },
  {
    "text": "Figure 5 | Correlations between daily mentions of a company's name and return for the company's stock. We investigate whether there is a relationship between the daily number of mentions of a company's name and the daily return of the corresponding company's stocks. Again, we calculate Spearman's rank correlation between the daily number of mentions and the daily return. Here, we find that the correlation coefficients are not significantly different to zero (median correlation coefficient 5 0.000; mean correlation coefficient 5 0.002; W 5 262, p 5 0.784, Wilcoxon signed rank test). In other words, the daily number of mentions of a company's name is not significantly correlated with the daily return of the company's stocks.\nsuggest that greater interest in a company in the news is also related to greater movements in the company's stock price in the markets.\nWe investigate whether a relationship also exists between interest in a company in the news and movement in a company's stock price when direction of movement is taken into account. We calculate the Spearman's rank correlation between the daily number of mentions of a company and the daily return of a company's stocks (Figure 5), and find that here, the correlation coefficients are not significantly different to zero (median correlation coefficient 5 0.000, mean correlation coefficient 5 0.002, W 5 262, p 5 0.784, Wilcoxon signed rank test). In other words, our analysis so far provides no evidence that interest in a company in the news is correlated with company stock price movements when direction of movement is considered.\nIn summary, we find evidence for a relationship between interest in a company in the news on a given day, and both the volume of trading and size of price change for a company's stocks on the same day. We find no evidence for a relationship between interest in a company in the news on a given day and price change for a company's stocks on the same day when direction of this change is taken into account.\nHowever, whilst we are linking news released at 5 am London time on a given day with trading in a market much later in the day, between 9:30 am and 4 pm New York time, our current analyses do not allow us to draw strong conclusions about whether news influences the markets, or the markets influence the news. To gain some understanding of the directionality of this relationship, we extend this analysis by considering the relationship between mentions of a company in the news on a given day and transaction volume for a company on the three days beforehand, and the three days afterwards (Figure 6).\nWe find that correlation coefficients for daily transaction volume one day before the news ( 2 1) and on the same day as the news (0) are significantly greater than zero (lag 2 1: W 5 373, p 5 0.014; lag 0: W 5 362, p 5 0.026, Wilcoxon signed rank tests). We find no significant relationship between the daily number of mentions of a company's name in the Financial Times and transaction volume at any other lag analyzed (lag 2 3: W 5 270, p 5 0.666; lag 2 2: W 5 301, p 5 0.299; lag 1: W 5 317, p 5 0.176; lag 2: W 5 307, p 5 0.248; lag 3: W 5 298, p 5 0.327; Wilcoxon signed rank tests). A greater transaction volume for a company's stocks on a given day is therefore related to a greater number of mentions of that company in the Financial Times on the following day. Equally, a greater number of mentions of a company in the Financial Times on a given day is related to a greater transaction volume for a company's stocks during trading later that day. Whilst more detailed analysis is required to draw strong conclusions about the direction of this relationship, these results are consistent with the hypothesis that movements in the news and movements in the markets may exert a mutual influence upon each other.\n\nNo evidence that interest in a company in the news is correlated with company stock price movements when direction of movement is considered.",
    "original_text": "Figure 5 | Correlations between daily mentions of a company's name and return for the company's stock. We investigate whether there is a relationship between the daily number of mentions of a company's name and the daily return of the corresponding company's stocks. Again, we calculate Spearman's rank correlation between the daily number of mentions and the daily return. Here, we find that the correlation coefficients are not significantly different to zero (median correlation coefficient 5 0.000; mean correlation coefficient 5 0.002; W 5 262, p 5 0.784, Wilcoxon signed rank test). In other words, the daily number of mentions of a company's name is not significantly correlated with the daily return of the company's stocks.\nsuggest that greater interest in a company in the news is also related to greater movements in the company's stock price in the markets.\nWe investigate whether a relationship also exists between interest in a company in the news and movement in a company's stock price when direction of movement is taken into account. We calculate the Spearman's rank correlation between the daily number of mentions of a company and the daily return of a company's stocks (Figure 5), and find that here, the correlation coefficients are not significantly different to zero (median correlation coefficient 5 0.000, mean correlation coefficient 5 0.002, W 5 262, p 5 0.784, Wilcoxon signed rank test). In other words, our analysis so far provides no evidence that interest in a company in the news is correlated with company stock price movements when direction of movement is considered.\nIn summary, we find evidence for a relationship between interest in a company in the news on a given day, and both the volume of trading and size of price change for a company's stocks on the same day. We find no evidence for a relationship between interest in a company in the news on a given day and price change for a company's stocks on the same day when direction of this change is taken into account.\nHowever, whilst we are linking news released at 5 am London time on a given day with trading in a market much later in the day, between 9:30 am and 4 pm New York time, our current analyses do not allow us to draw strong conclusions about whether news influences the markets, or the markets influence the news. To gain some understanding of the directionality of this relationship, we extend this analysis by considering the relationship between mentions of a company in the news on a given day and transaction volume for a company on the three days beforehand, and the three days afterwards (Figure 6).\nWe find that correlation coefficients for daily transaction volume one day before the news ( 2 1) and on the same day as the news (0) are significantly greater than zero (lag 2 1: W 5 373, p 5 0.014; lag 0: W 5 362, p 5 0.026, Wilcoxon signed rank tests). We find no significant relationship between the daily number of mentions of a company's name in the Financial Times and transaction volume at any other lag analyzed (lag 2 3: W 5 270, p 5 0.666; lag 2 2: W 5 301, p 5 0.299; lag 1: W 5 317, p 5 0.176; lag 2: W 5 307, p 5 0.248; lag 3: W 5 298, p 5 0.327; Wilcoxon signed rank tests). A greater transaction volume for a company's stocks on a given day is therefore related to a greater number of mentions of that company in the Financial Times on the following day. Equally, a greater number of mentions of a company in the Financial Times on a given day is related to a greater transaction volume for a company's stocks during trading later that day. Whilst more detailed analysis is required to draw strong conclusions about the direction of this relationship, these results are consistent with the hypothesis that movements in the news and movements in the markets may exert a mutual influence upon each other.",
    "context": "No evidence that interest in a company in the news is correlated with company stock price movements when direction of movement is considered.",
    "document": "srep03578.pdf",
    "pages": [
      4
    ],
    "id": "123ddcf83ab03270fa993398fc3054f79db55d55ebea5930ce383190d447be9b"
  },
  {
    "text": "We use six years of daily print issues of the Financial Times to quantify the relationship between decisions taken in financial markets and developments in financial news. We analyze mentions of the companies that form the DowJones Industrial Average , and find that a greater number of mentions of a company in the news on a given morning corresponds to a greater volume of trading for that company during a given day, as well as a greater change in price for a company's stocks. Our analyses also uncover a link between the volume of trading for a company and the number of mentions of company in the news on the next day. Our current analysis provides no evidence of a relationship between the number of mentions of a company in the morning's news and the change in price for a company's stocks when direction of price movement is taken into account.\nThe results we present here are consistent with the hypothesis that movements in the news and movements in the markets may exert a mutual influence upon each other. Future analyses building on this work will seek to provide further insight into the direction and causality of the relationship between financial news and market movements.\n\nQuantifies the relationship between financial news and market movements; finds a link between company mentions in the news and trading volume, as well as price changes; provides no evidence of a relationship between news mentions and price movement direction.",
    "original_text": "We use six years of daily print issues of the Financial Times to quantify the relationship between decisions taken in financial markets and developments in financial news. We analyze mentions of the companies that form the DowJones Industrial Average , and find that a greater number of mentions of a company in the news on a given morning corresponds to a greater volume of trading for that company during a given day, as well as a greater change in price for a company's stocks. Our analyses also uncover a link between the volume of trading for a company and the number of mentions of company in the news on the next day. Our current analysis provides no evidence of a relationship between the number of mentions of a company in the morning's news and the change in price for a company's stocks when direction of price movement is taken into account.\nThe results we present here are consistent with the hypothesis that movements in the news and movements in the markets may exert a mutual influence upon each other. Future analyses building on this work will seek to provide further insight into the direction and causality of the relationship between financial news and market movements.",
    "context": "Quantifies the relationship between financial news and market movements; finds a link between company mentions in the news and trading volume, as well as price changes; provides no evidence of a relationship between news mentions and price movement direction.",
    "document": "srep03578.pdf",
    "pages": [
      4
    ],
    "id": "44bc9ef119f95f8df2f7f6705553cf647a40c01370c32011d7ad1c30a77fe24f"
  },
  {
    "text": "Data retrieval and preprocessing . We analyze a corpus of daily issues of the Financial Times from 2 nd January 2007 to 31 st December 2012. We retrieved this data in PDF form from the website http://www.ft.com/. Issues of the Financial Times are released in PDF form online every day from Monday to Saturday at 5 am London time. All issues of the Financial Times were retrieved for this period, with the exception of five dates where technical problems prevented download. These dates were 22 nd February 2007, 8 th March 2007, 12 th May 2007, 28 th January 2009 and 8 th November 2012. In total, 1821 issues of the Financial Times are included in the analysis.\nWe preprocess the news data by converting the PDFs to text format, eliminating special characters such as '?', ' 2 ', '/' and converting all remaining words to lower case. All occurrences of digits without any letters or other symbols such as ''$'' in the same word are also removed. Words are not stemmed for this analysis, and stop words such as ''the'' and ''and'' are left in the corpus. We find 891,171 unique words in the corpus.\nCompany names used for news corpus analysis . We compile a list of common names used for the 31 companies listed in the Dow Jones Industrial Average (DJIA) between 2 nd January 2008 and 31 st December 2012. At any one time, the Dow Jones Industrial Average (DJIA) is derived from the stock prices of 30 companies. On 8 th June 2009, during the period of our study, Travelers replaced Citigroup in the DJIA. In\nFigure 6 | Lagged analysis of correlations between daily mentions of a company's name and transaction volumes for the company's stock. We investigate the correlation between daily mentions of a company's name and transaction volumes for the corresponding company's stock at different time lags. We calculate correlations between the daily number of mentions of a company's name and the daily transaction volume for a company from three days beforehand (indicated as 2 3 on the x-axis) to three days afterwards (indicated as 3 on the x-axis). We find that correlation coefficients for daily transaction volume one day before the news ( 2 1) and on the same day as the news (0) are significantly greater than zero (lag 2 1: W 5 373, p 5 0.014; lag 0: W 5 362, p 5 0.026, Wilcoxon signed rank tests). In other words, a greater number of mentions of a company in the Financial Times is related to a greater transaction volume for a company's stocks on the same day and on the previous day. Wefind no significant relationship between the daily number of mentions of a company's name in the Financial Times and transaction volume at any other lag (lag 2 3: W 5 270, p 5 0.666; lag 2 2: W 5 301, p 5 0.299; lag 1: W 5 317, p 5 0.176; lag 2: W 5 307, p 5 0.248; lag 3: W 5 298, p 5 0.327; Wilcoxon signed rank tests).\nour analyses, we consider stock data and news data for both of these companies throughout the whole period.\nTo maximize the amount of news data available for our analysis, we determine commonly used forms of the names of the companies in the DJIA. We retrieved the namesused to describe the companies on the Wikipedia page http://en.wikipedia.org/\nwiki/Dow_Jones_Industrial_Average on 21st May 2013. Where symbols such as '' 2 '' occur in these short names, we delete the symbol, and replace it with a space, if we find that this increases the number of hits for the name in the Financial Times corpus. The final list of short names used is given in Table S1 in the Supplementary Information .\nStatistical analyses of basic characteristics of the data . Our analyses focus on four types of time series for each of the 31 companies listed in the DJIA during the period of our study: the daily number of mentions of a company's name in the Financial Times , the daily transaction volume of a company's stock, the daily absolute return of a company's stock, and the daily return of a company's stock. Before running correlational analyses, we check for stationarity and normality of each of these 124 time series.\nTo check for stationarity, we first run an Augmented Dickey-Fuller test on each of these company name mention, daily transaction volume, daily absolute return and daily return time series. With the exception of the time series of mentions of CocaCola in the Financial Times , we reject the null hypothesis of a unit root for all time series, providing support for the assumption of stationarity of these time series (company names mentions: Coca-Cola Dickey-Fuller 52 3.137, p 5 0.099; all other Dickey-Fuller , 2 3.478, all other p s , 0.05; daily transaction volume: all DickeyFuller , 2 3.763, all p s , 0.05; daily absolute return: all Dickey-Fuller , 2 5.046, all p s , 0.01; daily return: all Dickey-Fuller , 2 9.371, all p s , 0.01). We verify the results of the Augmented Dickey-Fuller test with an alternative test for the presence of a unit root, the Phillips-Perron test. Here, we reject the null hypothesis of a unit root for all company name, transaction volume, absolute return and return time series, with no exceptions, again providing support for the assumption of stationarity of these time series (company names mentions: all Dickey-Fuller Z ( a ) , 2 1016.124, all p s , 0.01; daily transaction volume: all Dickey-Fuller Z ( a ) , 2 176.305, all p s , 0.01; daily absolute return: all Dickey-Fuller Z ( a ) , 2 1096.038, all p s , 0.01; daily return: all Dickey-Fuller Z ( a ) , 2 1118.684, all p s , 0.01).\nTo check for normality, we run a Shapiro-Wilk test on each of our company name mention, daily transaction volume, daily absolute return and daily return time series. We find that none of our 124 time series have a Gaussian distribution (company namesmentions: all W , 0.945, all p s , 0.01; daily transaction volume: all W , 0.909, all p s , 0.01; daily absolute return: all W , 0.811, all p s , 0.01; daily return: all W , 0.962, all p s , 0.01). Throughout the study, we therefore test for the existence of relationships between datasets by calculating Spearman's rank correlation coefficient, a non-parametric measure which makes no assumption about the normality of the underlying data.\n\nDetails of data retrieval and preprocessing, including the collection of daily Financial Times issues from 2007 to 2012, the conversion to text format, removal of special characters and digits, and the exclusion of five dates.",
    "original_text": "Data retrieval and preprocessing . We analyze a corpus of daily issues of the Financial Times from 2 nd January 2007 to 31 st December 2012. We retrieved this data in PDF form from the website http://www.ft.com/. Issues of the Financial Times are released in PDF form online every day from Monday to Saturday at 5 am London time. All issues of the Financial Times were retrieved for this period, with the exception of five dates where technical problems prevented download. These dates were 22 nd February 2007, 8 th March 2007, 12 th May 2007, 28 th January 2009 and 8 th November 2012. In total, 1821 issues of the Financial Times are included in the analysis.\nWe preprocess the news data by converting the PDFs to text format, eliminating special characters such as '?', ' 2 ', '/' and converting all remaining words to lower case. All occurrences of digits without any letters or other symbols such as ''$'' in the same word are also removed. Words are not stemmed for this analysis, and stop words such as ''the'' and ''and'' are left in the corpus. We find 891,171 unique words in the corpus.\nCompany names used for news corpus analysis . We compile a list of common names used for the 31 companies listed in the Dow Jones Industrial Average (DJIA) between 2 nd January 2008 and 31 st December 2012. At any one time, the Dow Jones Industrial Average (DJIA) is derived from the stock prices of 30 companies. On 8 th June 2009, during the period of our study, Travelers replaced Citigroup in the DJIA. In\nFigure 6 | Lagged analysis of correlations between daily mentions of a company's name and transaction volumes for the company's stock. We investigate the correlation between daily mentions of a company's name and transaction volumes for the corresponding company's stock at different time lags. We calculate correlations between the daily number of mentions of a company's name and the daily transaction volume for a company from three days beforehand (indicated as 2 3 on the x-axis) to three days afterwards (indicated as 3 on the x-axis). We find that correlation coefficients for daily transaction volume one day before the news ( 2 1) and on the same day as the news (0) are significantly greater than zero (lag 2 1: W 5 373, p 5 0.014; lag 0: W 5 362, p 5 0.026, Wilcoxon signed rank tests). In other words, a greater number of mentions of a company in the Financial Times is related to a greater transaction volume for a company's stocks on the same day and on the previous day. Wefind no significant relationship between the daily number of mentions of a company's name in the Financial Times and transaction volume at any other lag (lag 2 3: W 5 270, p 5 0.666; lag 2 2: W 5 301, p 5 0.299; lag 1: W 5 317, p 5 0.176; lag 2: W 5 307, p 5 0.248; lag 3: W 5 298, p 5 0.327; Wilcoxon signed rank tests).\nour analyses, we consider stock data and news data for both of these companies throughout the whole period.\nTo maximize the amount of news data available for our analysis, we determine commonly used forms of the names of the companies in the DJIA. We retrieved the namesused to describe the companies on the Wikipedia page http://en.wikipedia.org/\nwiki/Dow_Jones_Industrial_Average on 21st May 2013. Where symbols such as '' 2 '' occur in these short names, we delete the symbol, and replace it with a space, if we find that this increases the number of hits for the name in the Financial Times corpus. The final list of short names used is given in Table S1 in the Supplementary Information .\nStatistical analyses of basic characteristics of the data . Our analyses focus on four types of time series for each of the 31 companies listed in the DJIA during the period of our study: the daily number of mentions of a company's name in the Financial Times , the daily transaction volume of a company's stock, the daily absolute return of a company's stock, and the daily return of a company's stock. Before running correlational analyses, we check for stationarity and normality of each of these 124 time series.\nTo check for stationarity, we first run an Augmented Dickey-Fuller test on each of these company name mention, daily transaction volume, daily absolute return and daily return time series. With the exception of the time series of mentions of CocaCola in the Financial Times , we reject the null hypothesis of a unit root for all time series, providing support for the assumption of stationarity of these time series (company names mentions: Coca-Cola Dickey-Fuller 52 3.137, p 5 0.099; all other Dickey-Fuller , 2 3.478, all other p s , 0.05; daily transaction volume: all DickeyFuller , 2 3.763, all p s , 0.05; daily absolute return: all Dickey-Fuller , 2 5.046, all p s , 0.01; daily return: all Dickey-Fuller , 2 9.371, all p s , 0.01). We verify the results of the Augmented Dickey-Fuller test with an alternative test for the presence of a unit root, the Phillips-Perron test. Here, we reject the null hypothesis of a unit root for all company name, transaction volume, absolute return and return time series, with no exceptions, again providing support for the assumption of stationarity of these time series (company names mentions: all Dickey-Fuller Z ( a ) , 2 1016.124, all p s , 0.01; daily transaction volume: all Dickey-Fuller Z ( a ) , 2 176.305, all p s , 0.01; daily absolute return: all Dickey-Fuller Z ( a ) , 2 1096.038, all p s , 0.01; daily return: all Dickey-Fuller Z ( a ) , 2 1118.684, all p s , 0.01).\nTo check for normality, we run a Shapiro-Wilk test on each of our company name mention, daily transaction volume, daily absolute return and daily return time series. We find that none of our 124 time series have a Gaussian distribution (company namesmentions: all W , 0.945, all p s , 0.01; daily transaction volume: all W , 0.909, all p s , 0.01; daily absolute return: all W , 0.811, all p s , 0.01; daily return: all W , 0.962, all p s , 0.01). Throughout the study, we therefore test for the existence of relationships between datasets by calculating Spearman's rank correlation coefficient, a non-parametric measure which makes no assumption about the normality of the underlying data.",
    "context": "Details of data retrieval and preprocessing, including the collection of daily Financial Times issues from 2007 to 2012, the conversion to text format, removal of special characters and digits, and the exclusion of five dates.",
    "document": "srep03578.pdf",
    "pages": [
      4,
      5
    ],
    "id": "427303f8e59fe8297c1a2712f6018bf1af0e88a9b51f24b371abe1d8a53ddd15"
  },
  {
    "text": "1. Fehr, E. Behavioral science: The economics of impatience. Nature 415 , 269-272 (2002).\n2. Gabaix, X., Gopikrishnan, P., Plerou, V. & Stanley, H. E. A theory of power-law distributions in financial market fluctuations. Nature 423 , 267-270 (2003).\n3. Preis, T., Schneider, J. J. & Stanley, H. E. Switching processes in financial markets. Proc. Natl. Acad. Sci. U.S.A. 108 , 7674-7678 (2011).\n4. Podobnik, B., Horvatic, D., Petersen, A. M. & Stanley, H. E. Cross-correlations between volume change and price change. Proc. Natl. Acad. Sci. U.S.A. 106 , 22079-22084 (2009).\n5. Feng, L., Li, B., Podobnik, B., Preis, T. & Stanley, H. E. Linking agent-based models and stochastic models of financial markets. Proc. Natl. Acad. Sci. U.S.A. 109 , 8388-8393 (2012).\n6. Mantegna, R. N. & Stanley, H. E. Scaling behaviour in the dynamics of an economic index. Nature 376 , 46-49 (2002).\n7. Preis, T., Kenett, D. Y., Stanley, H. E., Helbing, D. & Ben-Jacob, E. Quantifying the behavior of stock correlations under market stress. Sci. Rep. 2 , 752 (2012).\n8. Cont, R. & Bouchaud, J.-P. Herd behavior and aggregate fluctuations in financial markets. Macroecon. Dyn. 4 , 170-196 (2000).\n9. Gabaix, X., Gopikrishnan, P., Plerou, V. & Stanley, H. E. A theory of power-law distributions in financial market fluctuations. Nature 423 , 267-270 (2003).\n10. Krawiecki, A., Holyst, J. A. & Helbing, D. Volatility clustering and scaling for financial time series due to attractor bubbling. Phys. Rev. Lett. 89 , 158701 (2002).\n11. Vandewalle, N. & Ausloos, M. Coherent and random sequences in financial fluctuations. Physica A 246 , 454-459 (1997).\n12. Watanabe, K., Takayasu, H. & Takayasu, M. A mathematical definition of the financial bubbles and crashes. Physica A 383 , 120-124 (2007).\n13. Simon, H. A. A behavioral model of rational choice. Q. J. Econ. 69 , 99-118 (1955).\n14. Vespignani, A. Predicting the Behavior of Techno-Social Systems. Science 325 , 425 (2009).\n15. King, G. Ensuring the Data-Rich Future of the Social Sciences. Science 331 , 719 (2011).\n16. Lazer, D. et al . Computational Social Science. Science 323 , 721 (2009).\n17. Conte, R. et al . Manifesto of computational social science. Eur. Phys. J. Spec. Top. 214 , 325-346.\n18. Preis, T., Moat, H. S., Bishop, S. R., Treleaven, P. & Stanley, H. E. Quantifying the Digital Traces of Hurricane Sandy on Flickr. Sci. Rep. 3 , 3141 (2013).\n19. Moat, H. S., Preis, T., Olivola, C. Y., Liu, C. & Chater, N. Using big data to predict collective behavior in the real world. Behav. Brain Sci. (in press).\n20. Ginsberg, J. et al . Detecting influenza epidemics using search engine query data. Nature 457 , 1012-1014 (2009).\n21. Preis, T., Moat, H. S., Stanley, H. E. & Bishop, S. R. Quantifying the Advantage of Looking Forward. Sci. Rep. 2 , 350 (2012).\n22. Choi, H. & Varian, H. Predicting the Present with Google Trends. Econ. Rec. 88 , 2-9 (2012).\n23. Preis, T., Reith, D. & Stanley, H. E. Complex dynamics of our economic life on different scales: insights from search engine query data. Phil. Trans. R. Soc. A 368 , 5707-5719 (2010).\n24. Preis, T., Moat, H. S. & Stanley, H. E. Quantifying Trading Behavior in Financial Markets Using Google Trends. Sci. Rep. 3 , 1684 (2013).\n25. Moat, H. S., Curme, C., Avakian, A., Kenett, D. Y., Stanley, H. E. & Preis, T. Quantifying Wikipedia Usage Patterns Before Stock Market Moves. Sci. Rep. 3 , 1801 (2013).\n26. Kristoufek, L. Can Google Trends Search Queries Contribute To Risk Diversification? Sci. Rep. 3 , 2713 (2013).\n\nProvides supporting evidence for the argument on economic inequality and offers a range of established research methods for analyzing financial market dynamics.",
    "original_text": "1. Fehr, E. Behavioral science: The economics of impatience. Nature 415 , 269-272 (2002).\n2. Gabaix, X., Gopikrishnan, P., Plerou, V. & Stanley, H. E. A theory of power-law distributions in financial market fluctuations. Nature 423 , 267-270 (2003).\n3. Preis, T., Schneider, J. J. & Stanley, H. E. Switching processes in financial markets. Proc. Natl. Acad. Sci. U.S.A. 108 , 7674-7678 (2011).\n4. Podobnik, B., Horvatic, D., Petersen, A. M. & Stanley, H. E. Cross-correlations between volume change and price change. Proc. Natl. Acad. Sci. U.S.A. 106 , 22079-22084 (2009).\n5. Feng, L., Li, B., Podobnik, B., Preis, T. & Stanley, H. E. Linking agent-based models and stochastic models of financial markets. Proc. Natl. Acad. Sci. U.S.A. 109 , 8388-8393 (2012).\n6. Mantegna, R. N. & Stanley, H. E. Scaling behaviour in the dynamics of an economic index. Nature 376 , 46-49 (2002).\n7. Preis, T., Kenett, D. Y., Stanley, H. E., Helbing, D. & Ben-Jacob, E. Quantifying the behavior of stock correlations under market stress. Sci. Rep. 2 , 752 (2012).\n8. Cont, R. & Bouchaud, J.-P. Herd behavior and aggregate fluctuations in financial markets. Macroecon. Dyn. 4 , 170-196 (2000).\n9. Gabaix, X., Gopikrishnan, P., Plerou, V. & Stanley, H. E. A theory of power-law distributions in financial market fluctuations. Nature 423 , 267-270 (2003).\n10. Krawiecki, A., Holyst, J. A. & Helbing, D. Volatility clustering and scaling for financial time series due to attractor bubbling. Phys. Rev. Lett. 89 , 158701 (2002).\n11. Vandewalle, N. & Ausloos, M. Coherent and random sequences in financial fluctuations. Physica A 246 , 454-459 (1997).\n12. Watanabe, K., Takayasu, H. & Takayasu, M. A mathematical definition of the financial bubbles and crashes. Physica A 383 , 120-124 (2007).\n13. Simon, H. A. A behavioral model of rational choice. Q. J. Econ. 69 , 99-118 (1955).\n14. Vespignani, A. Predicting the Behavior of Techno-Social Systems. Science 325 , 425 (2009).\n15. King, G. Ensuring the Data-Rich Future of the Social Sciences. Science 331 , 719 (2011).\n16. Lazer, D. et al . Computational Social Science. Science 323 , 721 (2009).\n17. Conte, R. et al . Manifesto of computational social science. Eur. Phys. J. Spec. Top. 214 , 325-346.\n18. Preis, T., Moat, H. S., Bishop, S. R., Treleaven, P. & Stanley, H. E. Quantifying the Digital Traces of Hurricane Sandy on Flickr. Sci. Rep. 3 , 3141 (2013).\n19. Moat, H. S., Preis, T., Olivola, C. Y., Liu, C. & Chater, N. Using big data to predict collective behavior in the real world. Behav. Brain Sci. (in press).\n20. Ginsberg, J. et al . Detecting influenza epidemics using search engine query data. Nature 457 , 1012-1014 (2009).\n21. Preis, T., Moat, H. S., Stanley, H. E. & Bishop, S. R. Quantifying the Advantage of Looking Forward. Sci. Rep. 2 , 350 (2012).\n22. Choi, H. & Varian, H. Predicting the Present with Google Trends. Econ. Rec. 88 , 2-9 (2012).\n23. Preis, T., Reith, D. & Stanley, H. E. Complex dynamics of our economic life on different scales: insights from search engine query data. Phil. Trans. R. Soc. A 368 , 5707-5719 (2010).\n24. Preis, T., Moat, H. S. & Stanley, H. E. Quantifying Trading Behavior in Financial Markets Using Google Trends. Sci. Rep. 3 , 1684 (2013).\n25. Moat, H. S., Curme, C., Avakian, A., Kenett, D. Y., Stanley, H. E. & Preis, T. Quantifying Wikipedia Usage Patterns Before Stock Market Moves. Sci. Rep. 3 , 1801 (2013).\n26. Kristoufek, L. Can Google Trends Search Queries Contribute To Risk Diversification? Sci. Rep. 3 , 2713 (2013).",
    "context": "Provides supporting evidence for the argument on economic inequality and offers a range of established research methods for analyzing financial market dynamics.",
    "document": "srep03578.pdf",
    "pages": [
      5,
      6
    ],
    "id": "4a5c88f851ca7454e402edbc74ac8d84e275f96df99fa2db6c34c67a755153aa"
  },
  {
    "text": "M.A. was supported by an Erasmus Mundus Category A scholarship. H.S.M. and T.P. acknowledge the support of the Research Councils UK Grant EP/K039830/1, and of the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center (DoI/NBC) contract number D12PC00285. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/NBC, or the U.S. Government.\n\nDetails the funding sources and acknowledgements for the research project, outlining the grants and institutional support received.",
    "original_text": "M.A. was supported by an Erasmus Mundus Category A scholarship. H.S.M. and T.P. acknowledge the support of the Research Councils UK Grant EP/K039830/1, and of the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center (DoI/NBC) contract number D12PC00285. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/NBC, or the U.S. Government.",
    "context": "Details the funding sources and acknowledgements for the research project, outlining the grants and institutional support received.",
    "document": "srep03578.pdf",
    "pages": [
      6
    ],
    "id": "8c93962ced7dff422e6b36e9063b8dfce7696f7a9e07f9bdfafbbb199d23fb22"
  },
  {
    "text": "M.A., H.S.M. and T.P. performed analyses, discussed the results, and contributed to the text of the manuscript.\n\nDetails the authors’ contributions to the manuscript’s development.",
    "original_text": "M.A., H.S.M. and T.P. performed analyses, discussed the results, and contributed to the text of the manuscript.",
    "context": "Details the authors’ contributions to the manuscript’s development.",
    "document": "srep03578.pdf",
    "pages": [
      6
    ],
    "id": "b26ed9f68c5f492a79867a76e87abf14f0e31d5d5ba9dd30725b222c5e384296"
  },
  {
    "text": "Supplementary information accompanies this paper at http://www.nature.com/ scientificreports\nCompeting financial interests: The authors declare no competing financial interests.\nHow to cite this article: Alanyali, M., Moat, H.S. & Preis, T. Quantifying the Relationship Between Financial News and the Stock Market. Sci. Rep. 3 , 3578; DOI:10.1038/srep03578 (2013).\nThis work is licensed under a Creative Commons Attribution 3.0 Unported license. To view a copy of this license, visit http://creativecommons.org/licenses/by/3.0\n\nProvides citation information and licensing details for the article.",
    "original_text": "Supplementary information accompanies this paper at http://www.nature.com/ scientificreports\nCompeting financial interests: The authors declare no competing financial interests.\nHow to cite this article: Alanyali, M., Moat, H.S. & Preis, T. Quantifying the Relationship Between Financial News and the Stock Market. Sci. Rep. 3 , 3578; DOI:10.1038/srep03578 (2013).\nThis work is licensed under a Creative Commons Attribution 3.0 Unported license. To view a copy of this license, visit http://creativecommons.org/licenses/by/3.0",
    "context": "Provides citation information and licensing details for the article.",
    "document": "srep03578.pdf",
    "pages": [
      6
    ],
    "id": "74fd440cc972ea24283d7a5b725365eb4ad42acb0b1bac343fa97b5eb2a70bc3"
  },
  {
    "text": "SUBJECT AREAS:\nSTATISTICAL PHYSICS, THERMODYNAMICS AND NONLINEAR DYNAMICS\nAPPLIED PHYSICS\nCOMPUTATIONAL SCIENCE\nINFORMATION THEORY AND COMPUTATION\nReceived 25 February 2013\nAccepted 3 April 2013\nPublished 25 April 2013\nCorrespondence and requests for materials should be addressed to T.P. (Tobias.Preis@ wbs.ac.uk)\n* These authors contributed equally to this work.\nTobias Preis 1 * , Helen Susannah Moat 2,3 * & H. Eugene Stanley 2 *\n1 Warwick Business School, University of Warwick, Scarman Road, Coventry, CV4 7AL, UK, 2 Department of Physics, Boston University, 590 Commonwealth Avenue, Boston, Massachusetts 02215, USA, 3 Department of Civil, Environmental and Geomatic Engineering, UCL, Gower Street, London, WC1E 6BT, UK.\nCrises in financial markets affect humans worldwide. Detailed market data on trading decisions reflect some of the complex human behavior that has led to these crises. We suggest that massive new data sources resulting from human interaction with the Internet may offer a new perspective on the behavior of market participants in periods of large market movements. By analyzing changes in Google query volumes for search terms related to finance, we find patterns that may be interpreted as ''early warning signs'' of stock market moves. Our results illustrate the potential that combining extensive behavioral data sets offers for a better understanding of collective human behavior.\nT he increasing volumes of 'big data' reflecting various aspects of our everyday activities represent a vital new opportunity for scientists to address fundamental questions about the complex world we inhabit 1-7 . Financial markets are a prime target for such quantitative investigations 8,9 . Movements in the markets exert immense impacts on personal fortunes and geopolitical events, generating considerable scientific attention to this subject 10-19 . For example, a range of recent studies have focused on modeling financial markets 20-25 and on performing network analyses 26-29 .\nAt their core, financial trading data sets reflect the myriad of decisions taken by market participants. According to Herbert Simon, actors begin their decision making processes by attempting to gather information 30 . In today's world, information gathering often consists of searching online sources. Recently, the search engine Google has begun to provide access to aggregated information on the volume of queries for different search terms and how these volumes change over time, via the publicly available service Google Trends . In the present study, we investigate the intriguing possibility of analyzing search query data from Google Trends to provide new insights into the information gathering process that precedes the trading decisions recorded in the stock market data.\nA recent investigation has shown that the number of clicks on search results stemming from a given country correlates with the amount of investment in that country 31 . Further studies exploiting the temporal dimension of Google Trends data have demonstrated that changes in query volumes for selected search terms mirror changes in current numbers of influenza cases 32 and current volumes of stock market transactions 33 . This demonstration of a link between stock market transaction volume and search volume has also been replicated using Yahoo! data 34 . Choi and Varian 35 have shown that data from Google Trends can be linked to current values of various economic indicators, including automobile sales, unemployment claims, travel destination planning and consumer confidence. A very recent study has shown that Internet users from countries with a higher per capita GDP are more likely to search for information about years in the future than years in the past 36 .\nHere, we suggest that within the time period we investigate, Google Trends data did not only reflect the current state of the stock markets 33 but may have also been able to anticipate certain future trends. Our findings are consistent with the intriguing proposal that notable drops in the financial market are preceded by periods of investor concern. In such periods, investors may search for more information about the market, before eventually deciding to buy or sell. Our results suggest that, following this logic, during the period 2004 to 2011 Google Trends search query volumes for certain terms could have been used in the construction of profitable trading strategies.\n\nIntroduces the central thesis about using Google Trends data to identify early warning signs of stock market movements.",
    "original_text": "SUBJECT AREAS:\nSTATISTICAL PHYSICS, THERMODYNAMICS AND NONLINEAR DYNAMICS\nAPPLIED PHYSICS\nCOMPUTATIONAL SCIENCE\nINFORMATION THEORY AND COMPUTATION\nReceived 25 February 2013\nAccepted 3 April 2013\nPublished 25 April 2013\nCorrespondence and requests for materials should be addressed to T.P. (Tobias.Preis@ wbs.ac.uk)\n* These authors contributed equally to this work.\nTobias Preis 1 * , Helen Susannah Moat 2,3 * & H. Eugene Stanley 2 *\n1 Warwick Business School, University of Warwick, Scarman Road, Coventry, CV4 7AL, UK, 2 Department of Physics, Boston University, 590 Commonwealth Avenue, Boston, Massachusetts 02215, USA, 3 Department of Civil, Environmental and Geomatic Engineering, UCL, Gower Street, London, WC1E 6BT, UK.\nCrises in financial markets affect humans worldwide. Detailed market data on trading decisions reflect some of the complex human behavior that has led to these crises. We suggest that massive new data sources resulting from human interaction with the Internet may offer a new perspective on the behavior of market participants in periods of large market movements. By analyzing changes in Google query volumes for search terms related to finance, we find patterns that may be interpreted as ''early warning signs'' of stock market moves. Our results illustrate the potential that combining extensive behavioral data sets offers for a better understanding of collective human behavior.\nT he increasing volumes of 'big data' reflecting various aspects of our everyday activities represent a vital new opportunity for scientists to address fundamental questions about the complex world we inhabit 1-7 . Financial markets are a prime target for such quantitative investigations 8,9 . Movements in the markets exert immense impacts on personal fortunes and geopolitical events, generating considerable scientific attention to this subject 10-19 . For example, a range of recent studies have focused on modeling financial markets 20-25 and on performing network analyses 26-29 .\nAt their core, financial trading data sets reflect the myriad of decisions taken by market participants. According to Herbert Simon, actors begin their decision making processes by attempting to gather information 30 . In today's world, information gathering often consists of searching online sources. Recently, the search engine Google has begun to provide access to aggregated information on the volume of queries for different search terms and how these volumes change over time, via the publicly available service Google Trends . In the present study, we investigate the intriguing possibility of analyzing search query data from Google Trends to provide new insights into the information gathering process that precedes the trading decisions recorded in the stock market data.\nA recent investigation has shown that the number of clicks on search results stemming from a given country correlates with the amount of investment in that country 31 . Further studies exploiting the temporal dimension of Google Trends data have demonstrated that changes in query volumes for selected search terms mirror changes in current numbers of influenza cases 32 and current volumes of stock market transactions 33 . This demonstration of a link between stock market transaction volume and search volume has also been replicated using Yahoo! data 34 . Choi and Varian 35 have shown that data from Google Trends can be linked to current values of various economic indicators, including automobile sales, unemployment claims, travel destination planning and consumer confidence. A very recent study has shown that Internet users from countries with a higher per capita GDP are more likely to search for information about years in the future than years in the past 36 .\nHere, we suggest that within the time period we investigate, Google Trends data did not only reflect the current state of the stock markets 33 but may have also been able to anticipate certain future trends. Our findings are consistent with the intriguing proposal that notable drops in the financial market are preceded by periods of investor concern. In such periods, investors may search for more information about the market, before eventually deciding to buy or sell. Our results suggest that, following this logic, during the period 2004 to 2011 Google Trends search query volumes for certain terms could have been used in the construction of profitable trading strategies.",
    "context": "Introduces the central thesis about using Google Trends data to identify early warning signs of stock market movements.",
    "document": "srep01684.pdf",
    "pages": [
      1
    ],
    "id": "326e42cc95fc78ae06dc4023c715a91f02054804d2d49493534f2e79f13e8dec"
  },
  {
    "text": "We analyze the performance of a set of 98 search terms. We included terms related to the concept of stock markets, with some terms suggested by the Google Sets service, a tool which identifies semantically related keywords. The set of terms used was therefore not arbitrarily chosen, as we intentionally introduced some financial bias. We explain our strategy based on changes in search volume with reference to the term debt , a\nFigure 1 | Search volume data and stock market moves. Time series of closing prices p(t) of the Dow Jones Industrial Average (DJIA) on the first day of trading in each week t covering the period from 5 January 2004 until 22 February 2011. The color code corresponds to the relative search volume changes for the search term debt , with D t 5 3 weeks. Search volume data are restricted to requests of users localized in the United States of America.\nkeyword with an obvious semantic connection to the most recent financial crisis, and overall the term which performed best in our analyses.\nTo uncover the relationship between the volume of search queries for a specific term and the overall direction of trader decisions, we analyze closing prices p(t) of the DowJonesIndustrial Average (DJIA) on the first trading day of week t . We use Google Trends to determine howmanysearches n(t 1 ) have been carried out for a specific search term such as debt in week t 1, where Google defines weeks as ending on a Sunday, relative to the total number of searches carried out on Google during that time. We find that search volume data change slightly over time due to Google 's extraction procedure. For each search term, we therefore average over three realizations of its search volume time series, based on three independent data requests in consecutive weeks. The variability of Google Trends data across different dates of access is irrelevant for our results, and it can be shown that the data are consistent with reported real world events (see Fig. S1 in the Supplementary Information ).\nToquantify changes in information gathering behavior, we use the relative change in search volume: D n(t, D t) 5 n(t) 2 N(t 2 1 , D t) with N(t 2 1 , D t) 5 (n(t 2 1 ) 1 n(t 2 2 ) 1 … 1 n(t 2 D t))/ D t , where t is measured in units of weeks. In Fig. 1, we depict relative search volume changes for the term debt , and their relationship to DJIA closing prices.\nmovements. We implement this strategy by selling the DJIA at the closing price p(t) on the first trading day of week t , if D n(t 2 1 , D t) . 0 , and buying the DJIA at price p(t 1 1) at the end of the first trading day of the following week. Note that mechanisms exist which make it possible to sell assets in financial markets without first owning them. If instead D n(t 2 1 , D t) , 0 , then we buy the DJIA at the closing price p(t) on the first trading day of week t and sell the DJIA at price p(t 1 1) at the end of the first trading day of the coming week. At the beginning of trading, we set the value of all portfolios to an arbitrary value of 1. If we take a 'short position'-selling at the closing price p(t) and buying back at price p(t 1 1 ) -then the cumulative return R changes by log( p(t) ) 2 log( p(t 1 1 ) ). If we take a 'long position'-buying at the closing price p(t) and selling at price p(t 1 1 ) -then the cumulative return R changes by log( p(t 1 1 ) ) 2 log( p(t) ). In this way, buy and sell actions have symmetric impacts on the cumulative return R of a strategy's portfolio. In using this approach to analyze the relationship between Google search volume and stock market movements, we neglect transaction fees, since the maximum number of transactions per year when using our strategy is only 104, allowing a closing and an opening transaction per week. Weof course do not dispute that such transaction fees would impact profit in a real world implementation.\nToinvestigate whether changes in information gathering behavior as captured by Google Trends data were related to later changes in stock price in the period between 2004-2011, we implement a hypothetical investment strategy for a portfolio using search volume data, called ' Google Trends strategy' in the following. Profit can only be made in a trading strategy if at least some future changes in the stock price are correctly anticipated, in particular around large market\nIn Fig. 2, the performance of the Google Trends strategy based on the search term debt is depicted by a blue line, whereas dashed lines indicate the standard deviation of the cumulative return from a strategy in which we buy and sell the market index in an uncorrelated, random manner ('random investment strategy'). The standard deviation is derived from simulations of 10,000 independent realizations of the random investment strategy. Fig. 2 shows that the use of the Google Trends strategy, based on the search term debt and D t 5 3 weeks, would have increased the value of a portfolio by 326%. The\nFigure 2 | Cumulative performance of an investment strategy based on Google Trends data. Profit and loss for an investment strategy based on the volume of the search term debt , the best performing keyword in our analysis, with D t 5 3 weeks, plotted as a function of time ( blue line ). This is compared to the ''buy and hold'' strategy ( red line ) and the standard deviation of 10,000 simulations using a purely random investment strategy ( dashed lines ). The Google Trends strategy using the search volume of the term debt would have yielded a profit of 326%.\n\nProvides supporting evidence for the argument on economic inequality.",
    "original_text": "We analyze the performance of a set of 98 search terms. We included terms related to the concept of stock markets, with some terms suggested by the Google Sets service, a tool which identifies semantically related keywords. The set of terms used was therefore not arbitrarily chosen, as we intentionally introduced some financial bias. We explain our strategy based on changes in search volume with reference to the term debt , a\nFigure 1 | Search volume data and stock market moves. Time series of closing prices p(t) of the Dow Jones Industrial Average (DJIA) on the first day of trading in each week t covering the period from 5 January 2004 until 22 February 2011. The color code corresponds to the relative search volume changes for the search term debt , with D t 5 3 weeks. Search volume data are restricted to requests of users localized in the United States of America.\nkeyword with an obvious semantic connection to the most recent financial crisis, and overall the term which performed best in our analyses.\nTo uncover the relationship between the volume of search queries for a specific term and the overall direction of trader decisions, we analyze closing prices p(t) of the DowJonesIndustrial Average (DJIA) on the first trading day of week t . We use Google Trends to determine howmanysearches n(t 1 ) have been carried out for a specific search term such as debt in week t 1, where Google defines weeks as ending on a Sunday, relative to the total number of searches carried out on Google during that time. We find that search volume data change slightly over time due to Google 's extraction procedure. For each search term, we therefore average over three realizations of its search volume time series, based on three independent data requests in consecutive weeks. The variability of Google Trends data across different dates of access is irrelevant for our results, and it can be shown that the data are consistent with reported real world events (see Fig. S1 in the Supplementary Information ).\nToquantify changes in information gathering behavior, we use the relative change in search volume: D n(t, D t) 5 n(t) 2 N(t 2 1 , D t) with N(t 2 1 , D t) 5 (n(t 2 1 ) 1 n(t 2 2 ) 1 … 1 n(t 2 D t))/ D t , where t is measured in units of weeks. In Fig. 1, we depict relative search volume changes for the term debt , and their relationship to DJIA closing prices.\nmovements. We implement this strategy by selling the DJIA at the closing price p(t) on the first trading day of week t , if D n(t 2 1 , D t) . 0 , and buying the DJIA at price p(t 1 1) at the end of the first trading day of the following week. Note that mechanisms exist which make it possible to sell assets in financial markets without first owning them. If instead D n(t 2 1 , D t) , 0 , then we buy the DJIA at the closing price p(t) on the first trading day of week t and sell the DJIA at price p(t 1 1) at the end of the first trading day of the coming week. At the beginning of trading, we set the value of all portfolios to an arbitrary value of 1. If we take a 'short position'-selling at the closing price p(t) and buying back at price p(t 1 1 ) -then the cumulative return R changes by log( p(t) ) 2 log( p(t 1 1 ) ). If we take a 'long position'-buying at the closing price p(t) and selling at price p(t 1 1 ) -then the cumulative return R changes by log( p(t 1 1 ) ) 2 log( p(t) ). In this way, buy and sell actions have symmetric impacts on the cumulative return R of a strategy's portfolio. In using this approach to analyze the relationship between Google search volume and stock market movements, we neglect transaction fees, since the maximum number of transactions per year when using our strategy is only 104, allowing a closing and an opening transaction per week. Weof course do not dispute that such transaction fees would impact profit in a real world implementation.\nToinvestigate whether changes in information gathering behavior as captured by Google Trends data were related to later changes in stock price in the period between 2004-2011, we implement a hypothetical investment strategy for a portfolio using search volume data, called ' Google Trends strategy' in the following. Profit can only be made in a trading strategy if at least some future changes in the stock price are correctly anticipated, in particular around large market\nIn Fig. 2, the performance of the Google Trends strategy based on the search term debt is depicted by a blue line, whereas dashed lines indicate the standard deviation of the cumulative return from a strategy in which we buy and sell the market index in an uncorrelated, random manner ('random investment strategy'). The standard deviation is derived from simulations of 10,000 independent realizations of the random investment strategy. Fig. 2 shows that the use of the Google Trends strategy, based on the search term debt and D t 5 3 weeks, would have increased the value of a portfolio by 326%. The\nFigure 2 | Cumulative performance of an investment strategy based on Google Trends data. Profit and loss for an investment strategy based on the volume of the search term debt , the best performing keyword in our analysis, with D t 5 3 weeks, plotted as a function of time ( blue line ). This is compared to the ''buy and hold'' strategy ( red line ) and the standard deviation of 10,000 simulations using a purely random investment strategy ( dashed lines ). The Google Trends strategy using the search volume of the term debt would have yielded a profit of 326%.",
    "context": "Provides supporting evidence for the argument on economic inequality.",
    "document": "srep01684.pdf",
    "pages": [
      1,
      2
    ],
    "id": "293d693229ed7fd9ac926515d24b56a9425b831c435fb9586ebe60765565a51a"
  },
  {
    "text": "Figure 3 | Performances of investment strategies based on search volume data. ( A ) Cumulative returns of 98 investment strategies based on search volumes restricted to search requests of users located in the United States for different search terms, displayed for the entire time period of our study from 5 January 2004 until 22 February 2011-the time period for which Google Trends provides data. We use two shades of blue for positive returns and two shades of red for negative returns to improve the readability of the search terms. The cumulative performance for the ''buy and hold strategy'' is also shown, as is a ''Dow Jones strategy'', which uses weekly closing prices of the Dow Jones Industrial Average (DJIA) rather than Google Trends data (see gray bars). Figures provided next to the bars indicate the returns of a strategy, R , in standard deviations from the mean return of uncorrelated random investment strategies, , R . RandomStrategy 5 0. Dashed lines correspond to 2 3, 2 2, 2 1, 0, 1 1, 1 2, and 1 3 standard deviations of random strategies. We find that returns from the Google Trends strategies tested are significantly higher overall than returns from the random strategies ( , R . US 5 0.60; t 5 8.65, df 5 97, p , 0.001, one sample t-test). ( B ) A parallel analysis shows that extending the range of the search volume analysis to global users reduces the overall return achieved by Google Trends trading strategies on the U.S. market ( , R . US 5 0.60, , R . Global 5 0.43; t 5 2.69, df 5 97, p , 0.01, two-sided paired t-test). However, returns are still significantly higher than the mean return of random investment strategies ( , R . Global 5 0.43; t 5 6.40, df 5 97, p , 0.001, one sample t-test).\nA\nFigure 4 | Analysis using strategies in which we take long or short positions only, using U.S. search volume data. ( A ) We implement Google Trends strategies in which we take long positions following a decrease in search volume, and never take short positions. We find that returns from these long position Google Trends strategies are significantly higher overall than returns from the random investment strategies ( , R . USLong 5 0.41; t 5 11.42, df 5 97, p , 0.001, one sample t-test). Again, we find a positive correlation between our indicator of financial relevance and returns from these strategies (Kendall's tau 5 0.242, z 5 3.53, N 5 98, p , 0.001). ( B ) We also implement Google Trends strategies in which we take short positions following an increase in search volume, and never take long positions. In line with our results from the long position Google Trends strategies, we find that returns from the short position Google Trends strategies are significantly higher overall than returns from the random investment strategies ( , R . USShort 5 0.19; t 5 5.28, df 5 97, p , 0.001, one sample t-test), and that there is a positive correlation between our indicator of financial relevance and short position Google Trends returns (Kendall's tau 5 0.275, z 5 4.01, N 5 98, p , 0.001).\nperformance of Google Trends strategies based on all other search terms that we analyze is depicted in Figures S3-S100 in the Supplementary Information .\nWe rank the full list of the 98 investigated search terms by their trading performance when using search data for U.S. users only (Fig. 3A) and when using globally generated search volume (Fig. 3B). In order to ensure the robustness of our results, the overall performance of a strategy based on a given search term is determined as the mean value over the six returns obtained for D t 5 1 . . . 6 weeks. Returns of the strategies are calculated as the logarithm of relative portfolio changes, following the usual definition of returns. The distribution of final portfolio values resulting from the random investment strategies is close to log-normal. Cumulative returns from the random investment strategy, derived from the logarithm of these portfolio values, therefore follow a normal distribution, with a mean value of , R . RandomStrategy 5 0. Here we report R, the cumulative returns of a strategy , in standard deviations of the cumulative returns of these uncorrelated random investment strategies.\nWe find that returns from the Google Trends strategies we tested are significantly higher overall than returns from the random strategies ( , R . US 5 0.60; t 5 8.65, df 5 97, p , 0.001, one sample t-test).\nWe compare the performance of these search terms with two benchmark strategies. The 'buy and hold' strategy is implemented by buying the index in the beginning and selling it at the end of the hold period. This strategy yields 16% profit, equal to the overall increase in value of the DJIA in the time period from January 2004 until February 2011. We further implement a ' DowJones strategy' by using changes in p(t) in place of changes in search volume data as the basis of buy and sell decisions. We find that this strategy also yields only 33% profit with D t 5 3 weeks, or when determined as the mean value over the six returns obtained for D t 5 1 . . . 6 weeks, 0.45 standard deviations of cumulative returns of uncorrelated random investment strategies (Figs. 3A and 3B; see also Fig. S101 in the Supplementary Information ).\nOur results show that performance of the Google Trends strategy differs with the search term chosen. We investigate whether these differences in performance can be partially explained using an indicator of the extent to which different terms are of financial relevance-a concept we quantify by calculating the frequency of each search term in the online edition of the Financial Times from August 2004 to June 2011, normalized by the number of Google hits for each search term (see Fig. S2 in the Supplementary Information). Wefind that the return associated with a given search term is correlated with this indicator of financial relevance (Kendall's tau 5 0.275, z 5 4.01, N 5 98, p , 0.001) using Kendall's tau rank correlation coefficient 37 .\nIt is widely recognized that investors prefer to trade on their domestic market, suggesting that search data for U.S. users only, as used in analyses so far, should better capture the information gathering behavior of U.S. stock market participants than data for Google users worldwide. Indeed, we find that strategies based on global search volume data are less successful than strategies based on U.S. search volume data in anticipating movements of the U.S. market ( , R . US 5 0.60, , R . Global 5 0.43; t 5 2.69, df 5 97, p , 0.01, two-sided paired t-test).\nOur empirical results so far are consistent with a two part hypothesis: namely that key increases in the price of the DJIA were preceded by a decrease in search volume for certain financially related terms, and conversely, that key decreases in the price of the DJIA were preceded by an increase in search volume for certain financially related terms. However, our trading strategy can be decomposed into two strategy components: one in which a decrease in search volume prompts us to buy (or take a long position) and one in which an increase in search volume prompts us to sell (or take a short position).\nIn order to verify that both strategy components play a significant role in our results, such that we have evidence for both parts of this hypothesis, we implement and test one strategy in which we take long positions following a decrease in search volume but never take short positions (Fig. 4A), and another strategy in which we take short positions following an increase in search volume but never take long positions (Fig. 4B). We find that returns from both Google Trends strategy components are significantly higher overall than returns from a random investment strategy (long position strategies: , R . USLong 5 0.41; t 5 11.42, df 5 97, p , 0.001, one sample t-test; short position strategies: , R . USShort 5 0.19; t 5 5.28, df 5 97, p , 0.001, one sample t-test).\n\nHighlights the significant outperformance of Google Trends strategies based on U.S. search volume data compared to random strategies, demonstrating a clear correlation between search volume changes and stock market movements.",
    "original_text": "Figure 3 | Performances of investment strategies based on search volume data. ( A ) Cumulative returns of 98 investment strategies based on search volumes restricted to search requests of users located in the United States for different search terms, displayed for the entire time period of our study from 5 January 2004 until 22 February 2011-the time period for which Google Trends provides data. We use two shades of blue for positive returns and two shades of red for negative returns to improve the readability of the search terms. The cumulative performance for the ''buy and hold strategy'' is also shown, as is a ''Dow Jones strategy'', which uses weekly closing prices of the Dow Jones Industrial Average (DJIA) rather than Google Trends data (see gray bars). Figures provided next to the bars indicate the returns of a strategy, R , in standard deviations from the mean return of uncorrelated random investment strategies, , R . RandomStrategy 5 0. Dashed lines correspond to 2 3, 2 2, 2 1, 0, 1 1, 1 2, and 1 3 standard deviations of random strategies. We find that returns from the Google Trends strategies tested are significantly higher overall than returns from the random strategies ( , R . US 5 0.60; t 5 8.65, df 5 97, p , 0.001, one sample t-test). ( B ) A parallel analysis shows that extending the range of the search volume analysis to global users reduces the overall return achieved by Google Trends trading strategies on the U.S. market ( , R . US 5 0.60, , R . Global 5 0.43; t 5 2.69, df 5 97, p , 0.01, two-sided paired t-test). However, returns are still significantly higher than the mean return of random investment strategies ( , R . Global 5 0.43; t 5 6.40, df 5 97, p , 0.001, one sample t-test).\nA\nFigure 4 | Analysis using strategies in which we take long or short positions only, using U.S. search volume data. ( A ) We implement Google Trends strategies in which we take long positions following a decrease in search volume, and never take short positions. We find that returns from these long position Google Trends strategies are significantly higher overall than returns from the random investment strategies ( , R . USLong 5 0.41; t 5 11.42, df 5 97, p , 0.001, one sample t-test). Again, we find a positive correlation between our indicator of financial relevance and returns from these strategies (Kendall's tau 5 0.242, z 5 3.53, N 5 98, p , 0.001). ( B ) We also implement Google Trends strategies in which we take short positions following an increase in search volume, and never take long positions. In line with our results from the long position Google Trends strategies, we find that returns from the short position Google Trends strategies are significantly higher overall than returns from the random investment strategies ( , R . USShort 5 0.19; t 5 5.28, df 5 97, p , 0.001, one sample t-test), and that there is a positive correlation between our indicator of financial relevance and short position Google Trends returns (Kendall's tau 5 0.275, z 5 4.01, N 5 98, p , 0.001).\nperformance of Google Trends strategies based on all other search terms that we analyze is depicted in Figures S3-S100 in the Supplementary Information .\nWe rank the full list of the 98 investigated search terms by their trading performance when using search data for U.S. users only (Fig. 3A) and when using globally generated search volume (Fig. 3B). In order to ensure the robustness of our results, the overall performance of a strategy based on a given search term is determined as the mean value over the six returns obtained for D t 5 1 . . . 6 weeks. Returns of the strategies are calculated as the logarithm of relative portfolio changes, following the usual definition of returns. The distribution of final portfolio values resulting from the random investment strategies is close to log-normal. Cumulative returns from the random investment strategy, derived from the logarithm of these portfolio values, therefore follow a normal distribution, with a mean value of , R . RandomStrategy 5 0. Here we report R, the cumulative returns of a strategy , in standard deviations of the cumulative returns of these uncorrelated random investment strategies.\nWe find that returns from the Google Trends strategies we tested are significantly higher overall than returns from the random strategies ( , R . US 5 0.60; t 5 8.65, df 5 97, p , 0.001, one sample t-test).\nWe compare the performance of these search terms with two benchmark strategies. The 'buy and hold' strategy is implemented by buying the index in the beginning and selling it at the end of the hold period. This strategy yields 16% profit, equal to the overall increase in value of the DJIA in the time period from January 2004 until February 2011. We further implement a ' DowJones strategy' by using changes in p(t) in place of changes in search volume data as the basis of buy and sell decisions. We find that this strategy also yields only 33% profit with D t 5 3 weeks, or when determined as the mean value over the six returns obtained for D t 5 1 . . . 6 weeks, 0.45 standard deviations of cumulative returns of uncorrelated random investment strategies (Figs. 3A and 3B; see also Fig. S101 in the Supplementary Information ).\nOur results show that performance of the Google Trends strategy differs with the search term chosen. We investigate whether these differences in performance can be partially explained using an indicator of the extent to which different terms are of financial relevance-a concept we quantify by calculating the frequency of each search term in the online edition of the Financial Times from August 2004 to June 2011, normalized by the number of Google hits for each search term (see Fig. S2 in the Supplementary Information). Wefind that the return associated with a given search term is correlated with this indicator of financial relevance (Kendall's tau 5 0.275, z 5 4.01, N 5 98, p , 0.001) using Kendall's tau rank correlation coefficient 37 .\nIt is widely recognized that investors prefer to trade on their domestic market, suggesting that search data for U.S. users only, as used in analyses so far, should better capture the information gathering behavior of U.S. stock market participants than data for Google users worldwide. Indeed, we find that strategies based on global search volume data are less successful than strategies based on U.S. search volume data in anticipating movements of the U.S. market ( , R . US 5 0.60, , R . Global 5 0.43; t 5 2.69, df 5 97, p , 0.01, two-sided paired t-test).\nOur empirical results so far are consistent with a two part hypothesis: namely that key increases in the price of the DJIA were preceded by a decrease in search volume for certain financially related terms, and conversely, that key decreases in the price of the DJIA were preceded by an increase in search volume for certain financially related terms. However, our trading strategy can be decomposed into two strategy components: one in which a decrease in search volume prompts us to buy (or take a long position) and one in which an increase in search volume prompts us to sell (or take a short position).\nIn order to verify that both strategy components play a significant role in our results, such that we have evidence for both parts of this hypothesis, we implement and test one strategy in which we take long positions following a decrease in search volume but never take short positions (Fig. 4A), and another strategy in which we take short positions following an increase in search volume but never take long positions (Fig. 4B). We find that returns from both Google Trends strategy components are significantly higher overall than returns from a random investment strategy (long position strategies: , R . USLong 5 0.41; t 5 11.42, df 5 97, p , 0.001, one sample t-test; short position strategies: , R . USShort 5 0.19; t 5 5.28, df 5 97, p , 0.001, one sample t-test).",
    "context": "Highlights the significant outperformance of Google Trends strategies based on U.S. search volume data compared to random strategies, demonstrating a clear correlation between search volume changes and stock market movements.",
    "document": "srep01684.pdf",
    "pages": [
      3,
      4,
      5
    ],
    "id": "8eec0a9d2ad059f275b47ac2043e29ecf363c5534cbb93b1d0c12e3ebf0a9b8a"
  },
  {
    "text": "In summary, our results are consistent with the suggestion that during the period we investigate, Google Trends data did not only reflect aspects of the current state of the economy, but may have also provided some insight into future trends in the behavior of economic actors. Using historic data from the period between January 2004 and February 2011, we detect increases in Google search volumes for keywords relating to financial markets before stock market falls. Our results suggest that these warning signs in search volume data could have been exploited in the construction of profitable trading strategies.\nWe offer one possible interpretation of our results within the context of Herbert Simon's model of decision making 28 . We suggest that Google Trends data and stock market data may reflect two subsequent stages in the decision making process of investors. Trends to sell on the financial market at lower prices may be preceded by periods of concern. During such periods of concern, people may tend to gather more information about the state of the market. It is conceivable that such behavior may have historically been reflected by increased Google Trends search volumes for terms of higher financial relevance.\nWefind that strategies based on search volume data for U.S. users are more successful for the U.S. market than strategies using global search volume data. Given the assumption that the population of U.S. Internet users contains a higher proportion of traders on the U.S. markets than the worldwide population of Internet users contains, this finding is in line with the intriguing suggestion that these datasets may provide insights into different stages of decision making within the same population.\nIn this work, we provide a quantification of the relationship between changes in search volume and changes in stock market prices. Future work will be needed to provide a thorough explanation of the underlying psychological mechanisms which lead people to search for terms like debt before selling stocks at a lower price. It is clear that many opportunities also remain to extend our analyses to further financial data sets.\nThe results of our investigation suggest that combining large behavioral data sets such as financial trading data with data on search query volumes may open up new insights into different stages of large-scale collective decision making. We conclude that these results further illustrate the exciting possibilities offered by new big data sets to advance our understanding of complex collective behavior in our society.\n\nSuggests that Google Trends search volume data could have been used to anticipate future stock market trends.",
    "original_text": "In summary, our results are consistent with the suggestion that during the period we investigate, Google Trends data did not only reflect aspects of the current state of the economy, but may have also provided some insight into future trends in the behavior of economic actors. Using historic data from the period between January 2004 and February 2011, we detect increases in Google search volumes for keywords relating to financial markets before stock market falls. Our results suggest that these warning signs in search volume data could have been exploited in the construction of profitable trading strategies.\nWe offer one possible interpretation of our results within the context of Herbert Simon's model of decision making 28 . We suggest that Google Trends data and stock market data may reflect two subsequent stages in the decision making process of investors. Trends to sell on the financial market at lower prices may be preceded by periods of concern. During such periods of concern, people may tend to gather more information about the state of the market. It is conceivable that such behavior may have historically been reflected by increased Google Trends search volumes for terms of higher financial relevance.\nWefind that strategies based on search volume data for U.S. users are more successful for the U.S. market than strategies using global search volume data. Given the assumption that the population of U.S. Internet users contains a higher proportion of traders on the U.S. markets than the worldwide population of Internet users contains, this finding is in line with the intriguing suggestion that these datasets may provide insights into different stages of decision making within the same population.\nIn this work, we provide a quantification of the relationship between changes in search volume and changes in stock market prices. Future work will be needed to provide a thorough explanation of the underlying psychological mechanisms which lead people to search for terms like debt before selling stocks at a lower price. It is clear that many opportunities also remain to extend our analyses to further financial data sets.\nThe results of our investigation suggest that combining large behavioral data sets such as financial trading data with data on search query volumes may open up new insights into different stages of large-scale collective decision making. We conclude that these results further illustrate the exciting possibilities offered by new big data sets to advance our understanding of complex collective behavior in our society.",
    "context": "Suggests that Google Trends search volume data could have been used to anticipate future stock market trends.",
    "document": "srep01684.pdf",
    "pages": [
      5
    ],
    "id": "c80c4fb4f9449b543440f05257d57a5bee14a10d6f666ee0cae5b01b3b93c2b5"
  },
  {
    "text": "How related are search terms to the topic of finance? We quantify financial relevance by calculating the frequency of each search term in the online edition of the Financial Times (http://www.ft.com) from August 2004 to June 2011, normalized by the number of Google hits (http://www.google.com) for each search term. Details are given in the Supplementary Information .\nData retrieval . We retrieved search volume data by accessing the Google Trends website (http://www.google.com/trends) on 10 April 2011, 17 April 2011, and 24 April 2011. The data on the number of hits for search terms in the online edition of the Financial Times was retrieved on 7 June 2011. The numbers of Google hits for these terms were obtained on 8 June 2011.\n1. Axtell, R. L. Zipf distribution of US firm sizes. Science 293 , 1818-1820 (2001).\n2. King, G. Ensuring the Data-Rich Future of the Social Sciences. Science 331 , 719-721 (2011).\n3. Vespignani, A. Predicting the Behavior of Techno-Social Systems. Science 325 , 425-428 (2009).\n4. Lazer, D. et al . Computational Social Science. Science 323 , 721-723. (2009).\n5. Perc, M. Evolution of the most common English words and phrases over the centuries. J. R. Soc. Interface 9 , 3323-3328 (2012).\n6. Petersen, A. M., Tenenbaum, J. N., Havlin, S., Stanley, H. E. & Perc, M. Languages cool as they expand: Allometric scaling and the decreasing need for new words. Scientific Reports 2 , 943 (2012).\n7. Christakis, N. A. & Fowler, J. H. Connected: The surprising power of our social networks and how they shape our lives (Little, Brown and Company, 2009).\n8. Fehr, E. Behavioural science - The economics of impatience. Nature 415 , 269-272 (2002).\n9. Shleifer, A. Inefficient Markets: An Introduction to Behavioral Finance (Oxford University Press, Oxford, 2000).\n10. Lillo, F., Farmer, J. D. & Mantegna, R. N. Econophysics - Master curve for priceimpact function. Nature 421 , 129-130 (2003).\n11. Gabaix, X., Gopikrishnan, P., Plerou, V. & Stanley, H. E. A theory of power-law distributions in financial market fluctuations. Nature 423 , 267-270 (2003).\n12. Preis, T., Kenett, D. Y., Stanley, H. E., Helbing, D. & Ben-Jacob, E. Quantifying the Behavior of Stock Correlations Under Market Stress. Scientific Reports 2 , 752 (2012).\n13. Preis, T., Schneider, J. J. & Stanley, H. E. Switching processes in financial markets. PNAS 108 , 7674-7678 (2011).\n14. Preis, T. Econophysics - complex correlations and trend switchings in financial time series. European Physical Journal Special Topics 194 , 5-86 (2011).\n15. Bunde, A., Schellnhuber, H. J. & Kropp, J., eds. The Science of Disasters: Climate Disruptions, Heart Attacks, and Market Crashes (Springer, Berlin, 2002).\n16. Vandewalle, N. & Ausloos, M. Coherent and random sequences in financial fluctuations. Physica A 246 , 454-459 (1997).\n17. Podobnik, B., Horvatic, D., Petersen, A. M. & Stanley, H. E. Cross-correlations between volume change and price change. PNAS 106 , 22079-22084 (2009).\n18. Sornette, D., Woodard, R. & Zhou, W. X. The 2006-2008 oil bubble: Evidence of speculation, and prediction. Physica A 388 , 1571-1576. (2009).\n19. Watanabe, K., Takayasu, H. & Takayasu, M. A mathematical definition of the financial bubbles and crashes. Physica A 383 , 120-124 (2007).\n20. Bouchaud, J. P., Matacz, A. & Potters, M. Leverage effect in financial markets: the retarded volatility model. Physical Review Letters 87 , 228701 (2001).\n21. Hommes, C. H. Modeling the stylized facts in finance through simple nonlinear adaptive systems. PNAS 99 , 7221-7228 (2002).\n22. Haldane, A. G. & May, R. M. Systemic risk in banking ecosystems. Nature 469 , 351-355 (2011).\n23. Lux, T. & Marchesi, M. Scaling and criticality in a stochastic multi-agent model of a financial market. Nature 397 , 498-500 (1999).\n24. Krugman, P. The Self-Organizing Economy (Blackwell, Cambridge, Massachusetts, 1996).\n25. Sornette, D. & von der Becke, S. Complexity clouds finance-risk models. Nature 471 , 166 (2011).\n26. Schweitzer, F. et al . Economic Networks: The New Challenges. Science 325 , 422-425 (2009).\n27. Garlaschelli, D., Caldarelli, G. & Pietronero, L. Universal scaling relations in food webs. Nature 423 , 165-168 (2003).\n28. Onnela, J. P., Arbesman, S., Gonzalez, M. C., Barabasi, A. L. & Christakis, N. A. Geographic Constraints on Social Network Groups. PLoS One 6 , e16939 (2011).\n29. Buldyrev, S. V., Parshani, R., Paul, G., Stanley, H. E. & Havlin, S. Catastrophic cascade of failures in interdependent networks. Nature 464 , 1025-1028 (2010).\n30. Simon, H. A. A behavioral model of rational choice. Quarterly Journal of Economics 69 , 99-118 (1955).\n31. Mondria, J., Wu, T. & Zhang, Y. The determinants of international investment and attention allocation: Using internet search query data. Journal of International Economics 82 , 85-95 (2010).\n32. Ginsberg, J. et al . Detecting influenza epidemics using search engine query data. Nature 457 , 1012-1014 (2009).\n33. Preis, T., Reith, D. & Stanley, H. E. Complex dynamics of our economic life on different scales: insights from search engine query data. Phil. Trans. R. Soc. A 368 , 5707-5719 (2010).\n34. Bordino, I. et al . Web Search Queries Can Predict Stock Market Volumes. PLoS One 7 , e40014 (2012).\n35. Choi, H. & Varian, H. Predicting the Present with Google Trends. The Economic Record 88 , 2-9 (2012).\n36. Preis, T., Moat, H. S., Stanley, H. E. & Bishop, S. R. Quantifying the Advantage of Looking Forward. Scientific Reports 2 , 350 (2012).\n37. Kendall, M. A New Measure of Rank Correlation. Biometrika 30 , 81-89 (1938).\n\nWe quantify financial relevance by calculating the frequency of each search term in the online edition of the Financial Times (http://www.ft.com) from August 2004 to June 2011, normalized by the number of Google hits (http://www.google.com) for each search term.",
    "original_text": "How related are search terms to the topic of finance? We quantify financial relevance by calculating the frequency of each search term in the online edition of the Financial Times (http://www.ft.com) from August 2004 to June 2011, normalized by the number of Google hits (http://www.google.com) for each search term. Details are given in the Supplementary Information .\nData retrieval . We retrieved search volume data by accessing the Google Trends website (http://www.google.com/trends) on 10 April 2011, 17 April 2011, and 24 April 2011. The data on the number of hits for search terms in the online edition of the Financial Times was retrieved on 7 June 2011. The numbers of Google hits for these terms were obtained on 8 June 2011.\n1. Axtell, R. L. Zipf distribution of US firm sizes. Science 293 , 1818-1820 (2001).\n2. King, G. Ensuring the Data-Rich Future of the Social Sciences. Science 331 , 719-721 (2011).\n3. Vespignani, A. Predicting the Behavior of Techno-Social Systems. Science 325 , 425-428 (2009).\n4. Lazer, D. et al . Computational Social Science. Science 323 , 721-723. (2009).\n5. Perc, M. Evolution of the most common English words and phrases over the centuries. J. R. Soc. Interface 9 , 3323-3328 (2012).\n6. Petersen, A. M., Tenenbaum, J. N., Havlin, S., Stanley, H. E. & Perc, M. Languages cool as they expand: Allometric scaling and the decreasing need for new words. Scientific Reports 2 , 943 (2012).\n7. Christakis, N. A. & Fowler, J. H. Connected: The surprising power of our social networks and how they shape our lives (Little, Brown and Company, 2009).\n8. Fehr, E. Behavioural science - The economics of impatience. Nature 415 , 269-272 (2002).\n9. Shleifer, A. Inefficient Markets: An Introduction to Behavioral Finance (Oxford University Press, Oxford, 2000).\n10. Lillo, F., Farmer, J. D. & Mantegna, R. N. Econophysics - Master curve for priceimpact function. Nature 421 , 129-130 (2003).\n11. Gabaix, X., Gopikrishnan, P., Plerou, V. & Stanley, H. E. A theory of power-law distributions in financial market fluctuations. Nature 423 , 267-270 (2003).\n12. Preis, T., Kenett, D. Y., Stanley, H. E., Helbing, D. & Ben-Jacob, E. Quantifying the Behavior of Stock Correlations Under Market Stress. Scientific Reports 2 , 752 (2012).\n13. Preis, T., Schneider, J. J. & Stanley, H. E. Switching processes in financial markets. PNAS 108 , 7674-7678 (2011).\n14. Preis, T. Econophysics - complex correlations and trend switchings in financial time series. European Physical Journal Special Topics 194 , 5-86 (2011).\n15. Bunde, A., Schellnhuber, H. J. & Kropp, J., eds. The Science of Disasters: Climate Disruptions, Heart Attacks, and Market Crashes (Springer, Berlin, 2002).\n16. Vandewalle, N. & Ausloos, M. Coherent and random sequences in financial fluctuations. Physica A 246 , 454-459 (1997).\n17. Podobnik, B., Horvatic, D., Petersen, A. M. & Stanley, H. E. Cross-correlations between volume change and price change. PNAS 106 , 22079-22084 (2009).\n18. Sornette, D., Woodard, R. & Zhou, W. X. The 2006-2008 oil bubble: Evidence of speculation, and prediction. Physica A 388 , 1571-1576. (2009).\n19. Watanabe, K., Takayasu, H. & Takayasu, M. A mathematical definition of the financial bubbles and crashes. Physica A 383 , 120-124 (2007).\n20. Bouchaud, J. P., Matacz, A. & Potters, M. Leverage effect in financial markets: the retarded volatility model. Physical Review Letters 87 , 228701 (2001).\n21. Hommes, C. H. Modeling the stylized facts in finance through simple nonlinear adaptive systems. PNAS 99 , 7221-7228 (2002).\n22. Haldane, A. G. & May, R. M. Systemic risk in banking ecosystems. Nature 469 , 351-355 (2011).\n23. Lux, T. & Marchesi, M. Scaling and criticality in a stochastic multi-agent model of a financial market. Nature 397 , 498-500 (1999).\n24. Krugman, P. The Self-Organizing Economy (Blackwell, Cambridge, Massachusetts, 1996).\n25. Sornette, D. & von der Becke, S. Complexity clouds finance-risk models. Nature 471 , 166 (2011).\n26. Schweitzer, F. et al . Economic Networks: The New Challenges. Science 325 , 422-425 (2009).\n27. Garlaschelli, D., Caldarelli, G. & Pietronero, L. Universal scaling relations in food webs. Nature 423 , 165-168 (2003).\n28. Onnela, J. P., Arbesman, S., Gonzalez, M. C., Barabasi, A. L. & Christakis, N. A. Geographic Constraints on Social Network Groups. PLoS One 6 , e16939 (2011).\n29. Buldyrev, S. V., Parshani, R., Paul, G., Stanley, H. E. & Havlin, S. Catastrophic cascade of failures in interdependent networks. Nature 464 , 1025-1028 (2010).\n30. Simon, H. A. A behavioral model of rational choice. Quarterly Journal of Economics 69 , 99-118 (1955).\n31. Mondria, J., Wu, T. & Zhang, Y. The determinants of international investment and attention allocation: Using internet search query data. Journal of International Economics 82 , 85-95 (2010).\n32. Ginsberg, J. et al . Detecting influenza epidemics using search engine query data. Nature 457 , 1012-1014 (2009).\n33. Preis, T., Reith, D. & Stanley, H. E. Complex dynamics of our economic life on different scales: insights from search engine query data. Phil. Trans. R. Soc. A 368 , 5707-5719 (2010).\n34. Bordino, I. et al . Web Search Queries Can Predict Stock Market Volumes. PLoS One 7 , e40014 (2012).\n35. Choi, H. & Varian, H. Predicting the Present with Google Trends. The Economic Record 88 , 2-9 (2012).\n36. Preis, T., Moat, H. S., Stanley, H. E. & Bishop, S. R. Quantifying the Advantage of Looking Forward. Scientific Reports 2 , 350 (2012).\n37. Kendall, M. A New Measure of Rank Correlation. Biometrika 30 , 81-89 (1938).",
    "context": "We quantify financial relevance by calculating the frequency of each search term in the online edition of the Financial Times (http://www.ft.com) from August 2004 to June 2011, normalized by the number of Google hits (http://www.google.com) for each search term.",
    "document": "srep01684.pdf",
    "pages": [
      5,
      6
    ],
    "id": "e28dfd29c2561b22e85baac7e88090be7be858c7e1f67b8dd0b00b8c5c0cfec9"
  },
  {
    "text": "We thank Didier Sornette, Dirk Helbing, and Steven R. Bishop for comments. This work was partially supported by the German Research Foundation Grant PR 1305/1-1 (to T.P.). This work was also supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center (DoI/NBC) contract number D12PC00285 and by the National Science Foundation (NSF), the Office of Naval Research (ONR), and the Defense Threat Reduction Agency (DTRA). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/NBC, or the U.S. Government.\n\nAcknowledges funding sources and expresses gratitude for reviewer comments, indicating the research’s institutional backing and collaborative nature.",
    "original_text": "We thank Didier Sornette, Dirk Helbing, and Steven R. Bishop for comments. This work was partially supported by the German Research Foundation Grant PR 1305/1-1 (to T.P.). This work was also supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center (DoI/NBC) contract number D12PC00285 and by the National Science Foundation (NSF), the Office of Naval Research (ONR), and the Defense Threat Reduction Agency (DTRA). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/NBC, or the U.S. Government.",
    "context": "Acknowledges funding sources and expresses gratitude for reviewer comments, indicating the research’s institutional backing and collaborative nature.",
    "document": "srep01684.pdf",
    "pages": [
      6
    ],
    "id": "b52c287ef40d307c266a0483196277a242c55c664a95c9c62eee67ac7f110ac7"
  },
  {
    "text": "T.P., H.S.M. and H.E.S. developed the design of the study, performed analyses, discussed the results, and contributed to the text of the manuscript.\n\nDetails the authors’ contributions to the study’s design, analysis, and manuscript development.",
    "original_text": "T.P., H.S.M. and H.E.S. developed the design of the study, performed analyses, discussed the results, and contributed to the text of the manuscript.",
    "context": "Details the authors’ contributions to the study’s design, analysis, and manuscript development.",
    "document": "srep01684.pdf",
    "pages": [
      6
    ],
    "id": "c4b61b5ff3013113ecc523228b31405d2e498bb72cab8ad0a02632e1f13f4102"
  },
  {
    "text": "Supplementary information accompanies this paper at http://www.nature.com/ scientificreports\nCompeting financial interests: The authors declare no competing financial interests.\nLicense: This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-sa/3.0/\nHowto cite this article: Preis, T., Moat, H.S. & Stanley, H.E. Quantifying Trading Behavior in Financial Markets Using Google Trends . Sci. Rep. 3 , 1684; DOI:10.1038/srep01684 (2013).\n\nReplicates essential licensing and citation information for the article.",
    "original_text": "Supplementary information accompanies this paper at http://www.nature.com/ scientificreports\nCompeting financial interests: The authors declare no competing financial interests.\nLicense: This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-sa/3.0/\nHowto cite this article: Preis, T., Moat, H.S. & Stanley, H.E. Quantifying Trading Behavior in Financial Markets Using Google Trends . Sci. Rep. 3 , 1684; DOI:10.1038/srep01684 (2013).",
    "context": "Replicates essential licensing and citation information for the article.",
    "document": "srep01684.pdf",
    "pages": [
      6
    ],
    "id": "79fefb453e609705bfd70fbd230a90425b61f9d18dc15c40a1765061a3a2dec1"
  },
  {
    "text": "iyad Rahwan 1,2,3,34 * , Manuel Cebrian 1,34 , Nick Obradovich 1,34 , Josh Bongard 4 , Jean-François Bonnefon 5 , Cynthia Breazeal 1 , Jacob w . Crandall 6 , Nicholas A. Christakis 7,8,9,10 , iain D. Couzin 11,12,13 , Matthew O. Jackson 14,15,16 , Nicholas R. Jennings 17,18 , ece Kamar 19 , isabel M. Kloumann 20 , Hugo Larochelle 21 , David Lazer 22,23,24 , Richard Mcelreath 25,26 , Alan Mislove 27 , David C. Parkes 28,29 , Alex 'Sandy' Pentland 1 , Margaret e. Roberts 30 , Azim Shariff 31 , Joshua B. Tenenbaum 32  & Michael wellman 33\nMachines powered by artificial intelligence increasingly mediate our social, cultural, economic and political interactions. Understanding the behaviour of artificial intelligence systems is essential to our ability to control their actions, reap their benefits and minimize their harms. Here we argue that this necessitates a broad scientific research agenda to study machine behaviour that incorporates and expands upon the discipline of computer science and includes insights from across the sciences. We first outline a set of questions that are fundamental to this emerging field and then explore the technical, legal and institutional constraints on the study of machine behaviour.\ni n his landmark 1969 book Sciences of the Artificial 1 , Nobel Laureate Herbert Simon wrote: 'Natural science is knowledge about natural objects and phenomena. We ask whether there cannot also be 'artificial' science-knowledge about artificial objects\nand phenomena.' In line with Simon's vision, we describe the emergence of an interdisciplinary field of scientific study. This field is concerned with the scientific study of intelligent machines, not as engineering artefacts, but as a class of actors with particular behavioural patterns and ecology. This field overlaps with, but is distinct from, computer science and robotics. It treats machine behaviour empirically. This is akin to how ethology and behavioural ecology study animal behaviour by integrating physiology and biochemistry-intrinsic properties-with the study of ecology and evolution-properties shaped by the environment. Animal and human behaviours cannot be fully understood without the study of the contexts in which behaviours occur. Machine behaviour similarly cannot be fully understood without the integrated study of algorithms and the social environments in which algorithms operate 2 .\nwhich individuals to target in advertising campaigns on social media.\nThese AI agents have the potential to augment human welfare and well-being in many ways. Indeed, that is typically the vision of their creators. But a broader consideration of the behaviour of AI agents is now critical. AI agents will increasingly integrate into our society and are already involved in a variety of activities, such as credit scoring, algorithmic trading, local policing, parole decisions, driving, online dating and drone warfare 3,4 . Commentators and scholars from diverse fields-including, but not limited to, cognitive systems engineering, human computer interaction, human factors, science, technology and society, and safety engineeringare raising the alarm about the broad, unintended consequences of AI agents that can exhibit behaviours and produce downstream societal effects-both positive and negative-that are unanticipated by their creators 5-8 .\nAt present, the scientists who study the behaviours of these virtual and embodied artificial intelligence (AI) agents are predominantly the same scientists who have created the agents themselves (throughout we use the term ' AI agents' liberally to refer to both complex and simple algorithms used to make decisions). As these scientists create agents to solve particular tasks, they often focus on ensuring the agents fulfil their intended function (although these respective fields are much broader than the specific examples listed here). For example, AI agents should meet a benchmark of accuracy in document classification, facial recognition or visual object detection. Autonomous cars must navigate successfully in a variety of weather conditions; game-playing agents must defeat a variety of human or machine opponents; and data-mining agents must learn\nIn addition to this lack of predictability surrounding the consequences of AI, there is a fear of the potential loss of human oversight over intelligent machines 5 and of the potential harms that are associated with the increasing use of machines for tasks that were once performed directly by humans 9 . At the same time, researchers describe the benefits that AI agents can offer society by supporting and augmenting human decisionmaking 10,11 .  Although discussions of these issues have led to many important insights in many separate fields of academic inquiry 12 , with some highlighting safety challenges of autonomous systems 13 and others studying the implications in fairness, accountability and transparency (for example, the ACM conference on fairness, accountability and transparency (https:// fatconference.org/)), many questions remain.\nThis Review frames and surveys the emerging interdisciplinary field of machine behaviour: the scientific study of behaviour exhibited by\n1 Media Lab, Massachusetts Institute of Technology, Cambridge, MA, USA.  2 Institute for Data, Systems & Society, Massachusetts Institute of Technology, Cambridge, MA, USA.  3 Center for Humans and Machines, Max Planck Institute for Human Development, Berlin, Germany.  4 Department of Computer Science, University of Vermont, Burlington, VT, USA.  5 Toulouse School of Economics (TSM-R), CNRS, Université Toulouse Capitole, Toulouse, France.  6 Computer Science Department, Brigham Young University, Provo, UT, USA.  7 Department of Sociology, Yale University, New Haven, CT, USA.  8 Department of Statistics and Data Science, Yale University, New Haven, CT, USA.  9 Department of Ecology and Evolutionary Biology, Yale University, New Haven, CT, USA.  10 Yale Institute for Network Science, Yale University, New Haven, CT, USA.  11 Department of Collective Behaviour, Max Planck Institute for Ornithology, Konstanz, Germany.  12 Department of Biology, University of Konstanz, Konstanz, Germany.  13 Centre for the Advanced Study of Collective Behaviour, University of Konstanz, Konstanz, Germany.  14 Department of Economics, Stanford University, Stanford, CA, USA.  15 Canadian Institute for Advanced Research, Toronto, Ontario, Canada.  16 The Sante Fe Institute, Santa Fe, NM, USA.  17 Department of Computing, Imperial College London, London, UK. 18 Department of Electrical and Electronic Engineering, Imperial College London, London, UK.  19 Microsoft Research, Redmond, WA, USA.  20 Facebook AI, Facebook Inc, New York, NY, USA.  21 Google Brain, Montreal, Québec, Canada.  22 Department of Political Science, Northeastern University, Boston, MA, USA.  23 College of Computer & Information Science, Northeastern University, Boston, MA, USA.  24 Institute for Quantitative Social Science, Harvard University, Cambridge, MA, USA.  25 Max Planck Institute for Evolutionary Anthropology, Leipzig, Germany.  26 Department of Anthropology, University of California, Davis, Davis, CA, USA.  27 College of Computer & Information Science, Northeastern University, Boston, MA, USA.  28 School of Engineering and Applied Sciences, Harvard University, Cambridge, MA, USA.  29 Harvard Data Science Initiative, Harvard University, Cambridge, MA, USA.  30 Department of Political Science, University of California, San Diego, San Diego, CA, USA.  31 Department of Psychology, University of British Columbia, Vancouver, British Columbia, Canada.  32 Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, MA, USA.  33 Computer Science & Engineering, University of Michigan, Ann Arbor, MI, USA.  34 These authors contributed equally: Iyad Rahwan, Manuel Cebrian, Nick Obradovich.\n*\ne-mail: irahwan@mit.edu\nintelligent machines. Here we outline the key research themes, questions and landmark research studies that exemplify this field. We start by providing background on the study of machine behaviour and the necessarily interdisciplinary nature of this science. We then provide a framework for the conceptualization of studies of machine behaviour. We close with a call for the scientific study of machine and human-machine ecologies and discuss some of the technical, legal and institutional barriers that are faced by researchers in this field.\n\nOutlines a need for a broad, interdisciplinary scientific research agenda to study machine behavior, emphasizing the importance of integrating insights from various scientific disciplines beyond computer science.",
    "original_text": "iyad Rahwan 1,2,3,34 * , Manuel Cebrian 1,34 , Nick Obradovich 1,34 , Josh Bongard 4 , Jean-François Bonnefon 5 , Cynthia Breazeal 1 , Jacob w . Crandall 6 , Nicholas A. Christakis 7,8,9,10 , iain D. Couzin 11,12,13 , Matthew O. Jackson 14,15,16 , Nicholas R. Jennings 17,18 , ece Kamar 19 , isabel M. Kloumann 20 , Hugo Larochelle 21 , David Lazer 22,23,24 , Richard Mcelreath 25,26 , Alan Mislove 27 , David C. Parkes 28,29 , Alex 'Sandy' Pentland 1 , Margaret e. Roberts 30 , Azim Shariff 31 , Joshua B. Tenenbaum 32  & Michael wellman 33\nMachines powered by artificial intelligence increasingly mediate our social, cultural, economic and political interactions. Understanding the behaviour of artificial intelligence systems is essential to our ability to control their actions, reap their benefits and minimize their harms. Here we argue that this necessitates a broad scientific research agenda to study machine behaviour that incorporates and expands upon the discipline of computer science and includes insights from across the sciences. We first outline a set of questions that are fundamental to this emerging field and then explore the technical, legal and institutional constraints on the study of machine behaviour.\ni n his landmark 1969 book Sciences of the Artificial 1 , Nobel Laureate Herbert Simon wrote: 'Natural science is knowledge about natural objects and phenomena. We ask whether there cannot also be 'artificial' science-knowledge about artificial objects\nand phenomena.' In line with Simon's vision, we describe the emergence of an interdisciplinary field of scientific study. This field is concerned with the scientific study of intelligent machines, not as engineering artefacts, but as a class of actors with particular behavioural patterns and ecology. This field overlaps with, but is distinct from, computer science and robotics. It treats machine behaviour empirically. This is akin to how ethology and behavioural ecology study animal behaviour by integrating physiology and biochemistry-intrinsic properties-with the study of ecology and evolution-properties shaped by the environment. Animal and human behaviours cannot be fully understood without the study of the contexts in which behaviours occur. Machine behaviour similarly cannot be fully understood without the integrated study of algorithms and the social environments in which algorithms operate 2 .\nwhich individuals to target in advertising campaigns on social media.\nThese AI agents have the potential to augment human welfare and well-being in many ways. Indeed, that is typically the vision of their creators. But a broader consideration of the behaviour of AI agents is now critical. AI agents will increasingly integrate into our society and are already involved in a variety of activities, such as credit scoring, algorithmic trading, local policing, parole decisions, driving, online dating and drone warfare 3,4 . Commentators and scholars from diverse fields-including, but not limited to, cognitive systems engineering, human computer interaction, human factors, science, technology and society, and safety engineeringare raising the alarm about the broad, unintended consequences of AI agents that can exhibit behaviours and produce downstream societal effects-both positive and negative-that are unanticipated by their creators 5-8 .\nAt present, the scientists who study the behaviours of these virtual and embodied artificial intelligence (AI) agents are predominantly the same scientists who have created the agents themselves (throughout we use the term ' AI agents' liberally to refer to both complex and simple algorithms used to make decisions). As these scientists create agents to solve particular tasks, they often focus on ensuring the agents fulfil their intended function (although these respective fields are much broader than the specific examples listed here). For example, AI agents should meet a benchmark of accuracy in document classification, facial recognition or visual object detection. Autonomous cars must navigate successfully in a variety of weather conditions; game-playing agents must defeat a variety of human or machine opponents; and data-mining agents must learn\nIn addition to this lack of predictability surrounding the consequences of AI, there is a fear of the potential loss of human oversight over intelligent machines 5 and of the potential harms that are associated with the increasing use of machines for tasks that were once performed directly by humans 9 . At the same time, researchers describe the benefits that AI agents can offer society by supporting and augmenting human decisionmaking 10,11 .  Although discussions of these issues have led to many important insights in many separate fields of academic inquiry 12 , with some highlighting safety challenges of autonomous systems 13 and others studying the implications in fairness, accountability and transparency (for example, the ACM conference on fairness, accountability and transparency (https:// fatconference.org/)), many questions remain.\nThis Review frames and surveys the emerging interdisciplinary field of machine behaviour: the scientific study of behaviour exhibited by\n1 Media Lab, Massachusetts Institute of Technology, Cambridge, MA, USA.  2 Institute for Data, Systems & Society, Massachusetts Institute of Technology, Cambridge, MA, USA.  3 Center for Humans and Machines, Max Planck Institute for Human Development, Berlin, Germany.  4 Department of Computer Science, University of Vermont, Burlington, VT, USA.  5 Toulouse School of Economics (TSM-R), CNRS, Université Toulouse Capitole, Toulouse, France.  6 Computer Science Department, Brigham Young University, Provo, UT, USA.  7 Department of Sociology, Yale University, New Haven, CT, USA.  8 Department of Statistics and Data Science, Yale University, New Haven, CT, USA.  9 Department of Ecology and Evolutionary Biology, Yale University, New Haven, CT, USA.  10 Yale Institute for Network Science, Yale University, New Haven, CT, USA.  11 Department of Collective Behaviour, Max Planck Institute for Ornithology, Konstanz, Germany.  12 Department of Biology, University of Konstanz, Konstanz, Germany.  13 Centre for the Advanced Study of Collective Behaviour, University of Konstanz, Konstanz, Germany.  14 Department of Economics, Stanford University, Stanford, CA, USA.  15 Canadian Institute for Advanced Research, Toronto, Ontario, Canada.  16 The Sante Fe Institute, Santa Fe, NM, USA.  17 Department of Computing, Imperial College London, London, UK. 18 Department of Electrical and Electronic Engineering, Imperial College London, London, UK.  19 Microsoft Research, Redmond, WA, USA.  20 Facebook AI, Facebook Inc, New York, NY, USA.  21 Google Brain, Montreal, Québec, Canada.  22 Department of Political Science, Northeastern University, Boston, MA, USA.  23 College of Computer & Information Science, Northeastern University, Boston, MA, USA.  24 Institute for Quantitative Social Science, Harvard University, Cambridge, MA, USA.  25 Max Planck Institute for Evolutionary Anthropology, Leipzig, Germany.  26 Department of Anthropology, University of California, Davis, Davis, CA, USA.  27 College of Computer & Information Science, Northeastern University, Boston, MA, USA.  28 School of Engineering and Applied Sciences, Harvard University, Cambridge, MA, USA.  29 Harvard Data Science Initiative, Harvard University, Cambridge, MA, USA.  30 Department of Political Science, University of California, San Diego, San Diego, CA, USA.  31 Department of Psychology, University of British Columbia, Vancouver, British Columbia, Canada.  32 Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, MA, USA.  33 Computer Science & Engineering, University of Michigan, Ann Arbor, MI, USA.  34 These authors contributed equally: Iyad Rahwan, Manuel Cebrian, Nick Obradovich.\n*\ne-mail: irahwan@mit.edu\nintelligent machines. Here we outline the key research themes, questions and landmark research studies that exemplify this field. We start by providing background on the study of machine behaviour and the necessarily interdisciplinary nature of this science. We then provide a framework for the conceptualization of studies of machine behaviour. We close with a call for the scientific study of machine and human-machine ecologies and discuss some of the technical, legal and institutional barriers that are faced by researchers in this field.",
    "context": "Outlines a need for a broad, interdisciplinary scientific research agenda to study machine behavior, emphasizing the importance of integrating insights from various scientific disciplines beyond computer science.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      1,
      2
    ],
    "id": "d4afcc1910d1bd9c9cb23992a19bf7e5cb987ff7e4fc68387348cfeb43623d4a"
  },
  {
    "text": "There are three primary motivations for the scientific discipline of machine behaviour. First, various kinds of algorithms operate in our society, and algorithms have an ever-increasing role in our daily activities. Second, because of the complex properties of these algorithms and the environments in which they operate, some of their attributes and behaviours can be difficult or impossible to formalize analytically. Third, because of their ubiquity and complexity, predicting the effects of intelligent algorithms on humanity-whether positive or negativeposes a substantial challenge.\n\nHighlights the core drivers for studying machine behavior: algorithms’ increasing societal role, the difficulty in analyzing their complex properties, and the significant challenge of predicting their impact on humanity.",
    "original_text": "There are three primary motivations for the scientific discipline of machine behaviour. First, various kinds of algorithms operate in our society, and algorithms have an ever-increasing role in our daily activities. Second, because of the complex properties of these algorithms and the environments in which they operate, some of their attributes and behaviours can be difficult or impossible to formalize analytically. Third, because of their ubiquity and complexity, predicting the effects of intelligent algorithms on humanity-whether positive or negativeposes a substantial challenge.",
    "context": "Highlights the core drivers for studying machine behavior: algorithms’ increasing societal role, the difficulty in analyzing their complex properties, and the significant challenge of predicting their impact on humanity.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      2
    ],
    "id": "4fc86e205c9178b2cf81a6cf081f32a2b78d58943834289c942ca97dfd8d4cd1"
  },
  {
    "text": "The current prevalence of diverse algorithms in society is unprecedented 5 (Fig. 1). News-ranking algorithms and social media bots influence the information seen by citizens 14-18 . Credit-scoring algorithms determine loan decisions 19-22 . Online pricing algorithms shape the cost of products differentially across consumers 23-25 . Algorithmic trading software makes transactions in financial markets at rapid speed 26-29 . Algorithms shape the dispatch and spatial patterns of local policing 30 and programs for algorithmic sentencing affect time served in the penal system 7 . Autonomous cars traverse our cities 31 , and ride-sharing algorithms alter the travel patterns of conventional vehicles 32 . Machines map our homes, respond to verbal commands 33  and perform regular household tasks 34 . Algorithms shape romantic matches for online dating services 35,36 . Machines are likely to increasingly substitute for humans in the raising of our young 37 and the care for our old 38 . Autonomous agents are increasingly likely to affect collective behaviours, from group-wide coordination to sharing 39 . Furthermore, although the prospect of developing autonomous weapons is highly controversial, with many in the field voicing their opposition 6,40 , if such weapons end up being deployed, then machines could determine who lives and who dies in armed conflicts 41,42 .\n\nHighlights the unprecedented prevalence of algorithms across various societal domains, illustrating their impact on information access, financial decisions, law enforcement, transportation, and even childcare, while also raising concerns about potential societal consequences like algorithmic bias and the development of autonomous weapons.",
    "original_text": "The current prevalence of diverse algorithms in society is unprecedented 5 (Fig. 1). News-ranking algorithms and social media bots influence the information seen by citizens 14-18 . Credit-scoring algorithms determine loan decisions 19-22 . Online pricing algorithms shape the cost of products differentially across consumers 23-25 . Algorithmic trading software makes transactions in financial markets at rapid speed 26-29 . Algorithms shape the dispatch and spatial patterns of local policing 30 and programs for algorithmic sentencing affect time served in the penal system 7 . Autonomous cars traverse our cities 31 , and ride-sharing algorithms alter the travel patterns of conventional vehicles 32 . Machines map our homes, respond to verbal commands 33  and perform regular household tasks 34 . Algorithms shape romantic matches for online dating services 35,36 . Machines are likely to increasingly substitute for humans in the raising of our young 37 and the care for our old 38 . Autonomous agents are increasingly likely to affect collective behaviours, from group-wide coordination to sharing 39 . Furthermore, although the prospect of developing autonomous weapons is highly controversial, with many in the field voicing their opposition 6,40 , if such weapons end up being deployed, then machines could determine who lives and who dies in armed conflicts 41,42 .",
    "context": "Highlights the unprecedented prevalence of algorithms across various societal domains, illustrating their impact on information access, financial decisions, law enforcement, transportation, and even childcare, while also raising concerns about potential societal consequences like algorithmic bias and the development of autonomous weapons.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      2
    ],
    "id": "68a1b48bace22f9840558578bfc94a5e0ba9e1d1a6a7ec611b137e63754ad669"
  },
  {
    "text": "The extreme diversity of these AI systems, coupled with their ubiquity, would by itself ensure that studying the behaviour of such systems poses a formidable challenge, even if the individual algorithms themselves were relatively simple. The complexity of individual AI agents is currently high and rapidly increasing. Although the code for specifying the architecture and training of a model can be simple, the results can be very complex, oftentimes effectively resulting in 'black boxes' 43 . They are given input and produce output, but the exact functional processes that generate these outputs are hard to interpret even to the very scientists who generate the algorithms themselves 44 , although some progress in interpretability is being made 45,46 . Furthermore, when systems learn from data, their failures are linked to imperfections in the data or how data was collected, which has led some to argue for adapted reporting mechanisms for datasets 47 and models 48 . The dimensionality and size of data add another layer of complexity to understanding machine behaviour 49 .\nFurther complicating this challenge is the fact that much of the source code and model structure for the most frequently used algorithms in society is proprietary, as are the data on which these systems are trained. Industrial secrecy and legal protection of intellectual property often surround source code and model structure. In many settings, the only factors that are publicly observable about industrial AI systems are their inputs and outputs.\nEven when available, the source code or model structure of an AI agent can provide insufficient predictive power over its output. AI\nagents can also demonstrate novel behaviours through their interaction with the world and other agents that are impossible to predict with precision 50 . Even when the analytical solutions are mathematically describable, they can be so lengthy and complex as to be indecipherable 51,52 . Furthermore, when the environment is changing-perhaps as a result of the algorithm itself-anticipating and analysing behaviour is made much harder.\n\nThis chunk highlights the significant challenge in studying AI systems due to their complexity, proprietary nature, and the difficulty in predicting their behavior. It emphasizes that despite potentially simple underlying code, AI systems often produce opaque and unpredictable results, making comprehensive analysis a formidable task.",
    "original_text": "The extreme diversity of these AI systems, coupled with their ubiquity, would by itself ensure that studying the behaviour of such systems poses a formidable challenge, even if the individual algorithms themselves were relatively simple. The complexity of individual AI agents is currently high and rapidly increasing. Although the code for specifying the architecture and training of a model can be simple, the results can be very complex, oftentimes effectively resulting in 'black boxes' 43 . They are given input and produce output, but the exact functional processes that generate these outputs are hard to interpret even to the very scientists who generate the algorithms themselves 44 , although some progress in interpretability is being made 45,46 . Furthermore, when systems learn from data, their failures are linked to imperfections in the data or how data was collected, which has led some to argue for adapted reporting mechanisms for datasets 47 and models 48 . The dimensionality and size of data add another layer of complexity to understanding machine behaviour 49 .\nFurther complicating this challenge is the fact that much of the source code and model structure for the most frequently used algorithms in society is proprietary, as are the data on which these systems are trained. Industrial secrecy and legal protection of intellectual property often surround source code and model structure. In many settings, the only factors that are publicly observable about industrial AI systems are their inputs and outputs.\nEven when available, the source code or model structure of an AI agent can provide insufficient predictive power over its output. AI\nagents can also demonstrate novel behaviours through their interaction with the world and other agents that are impossible to predict with precision 50 . Even when the analytical solutions are mathematically describable, they can be so lengthy and complex as to be indecipherable 51,52 . Furthermore, when the environment is changing-perhaps as a result of the algorithm itself-anticipating and analysing behaviour is made much harder.",
    "context": "This chunk highlights the significant challenge in studying AI systems due to their complexity, proprietary nature, and the difficulty in predicting their behavior. It emphasizes that despite potentially simple underlying code, AI systems often produce opaque and unpredictable results, making comprehensive analysis a formidable task.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      2
    ],
    "id": "e80e7749cf7966421e96f762dcba46470cb6d2aabee92a3cdf06295bea5f66f5"
  },
  {
    "text": "The ubiquity of algorithms, coupled with their increasing complexity, tends to amplify the difficulty of estimating the effects of algorithms on individuals and society. AI agents can shape human behaviours and societal outcomes in both intended and unintended ways. For example, some AI agents are designed to aid learning outcomes for children 53 and others are designed to assist older people 38,54 . These AI systems may benefit their intended humans by nudging those humans into better learning or safer mobility behaviours. However, with the power to nudge human behaviours in positive or intended ways comes the risk that human behaviours may be nudged in costly or unintended ways-children could be influenced to buy certain branded products and elders could be nudged to watch certain television programs.\nThe way that such algorithmic influences on individual humans scale into society-wide effects, both positive and negative, is of critical concern. As an example, the exposure of a small number of individuals to political misinformation may have little effect on society as a whole. However, the effect of the insertion and propagation of such misinformation on social media may have more substantial societal consequences 55-57 . Furthermore, issues of algorithmic fairness or bias 58,59 have been already documented in diverse contexts, including computer vision 60 , word embeddings 61,62 , advertising 63 , policing 64 , criminal justice 7,65 and social services 66 . To address these issues, practitioners will sometimes be forced to make value trade-offs between competing and incompatible notions of bias 58,59 or between human versus machine biases. Additional questions regarding the effect of algorithms remain, such as how online dating algorithms alter the societal institution of marriage 35,36 and whether there are systemic effects of increasing interaction with intelligent algorithms on the stages and speed of human development 53 . These questions become more complex in 'hybrid systems' composed of many machines and humans interacting and manifesting collective behaviour 39,67 . For society to have input into and oversight of the downstream consequences of AI, scholars of machine behaviour must provide insights into how these systems work and the benefits, costs and trade-offs presented by the ubiquitous use of AI in society.\n\nHighlights the difficulty of predicting and understanding the broad societal impacts of algorithms, particularly concerning unintended consequences like influencing consumer behavior or shaping societal institutions.",
    "original_text": "The ubiquity of algorithms, coupled with their increasing complexity, tends to amplify the difficulty of estimating the effects of algorithms on individuals and society. AI agents can shape human behaviours and societal outcomes in both intended and unintended ways. For example, some AI agents are designed to aid learning outcomes for children 53 and others are designed to assist older people 38,54 . These AI systems may benefit their intended humans by nudging those humans into better learning or safer mobility behaviours. However, with the power to nudge human behaviours in positive or intended ways comes the risk that human behaviours may be nudged in costly or unintended ways-children could be influenced to buy certain branded products and elders could be nudged to watch certain television programs.\nThe way that such algorithmic influences on individual humans scale into society-wide effects, both positive and negative, is of critical concern. As an example, the exposure of a small number of individuals to political misinformation may have little effect on society as a whole. However, the effect of the insertion and propagation of such misinformation on social media may have more substantial societal consequences 55-57 . Furthermore, issues of algorithmic fairness or bias 58,59 have been already documented in diverse contexts, including computer vision 60 , word embeddings 61,62 , advertising 63 , policing 64 , criminal justice 7,65 and social services 66 . To address these issues, practitioners will sometimes be forced to make value trade-offs between competing and incompatible notions of bias 58,59 or between human versus machine biases. Additional questions regarding the effect of algorithms remain, such as how online dating algorithms alter the societal institution of marriage 35,36 and whether there are systemic effects of increasing interaction with intelligent algorithms on the stages and speed of human development 53 . These questions become more complex in 'hybrid systems' composed of many machines and humans interacting and manifesting collective behaviour 39,67 . For society to have input into and oversight of the downstream consequences of AI, scholars of machine behaviour must provide insights into how these systems work and the benefits, costs and trade-offs presented by the ubiquitous use of AI in society.",
    "context": "Highlights the difficulty of predicting and understanding the broad societal impacts of algorithms, particularly concerning unintended consequences like influencing consumer behavior or shaping societal institutions.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      2
    ],
    "id": "cd536bdb60f09d55c6c4a3048e1d7a2cc105e287b5172c54bd5d2a3acd9e395f"
  },
  {
    "text": "To study machine behaviour-especially the behaviours of black box algorithms in real-world settings-we must integrate knowledge from across a variety of scientific disciplines (Fig. 2). This integration is currently in its nascent stages and has happened largely in an ad hoc fashion in response to the growing need to understand machine behaviour. Currently, the scientists who most commonly study the behaviour of machines are the computer scientists, roboticists and engineers who have created the machines in the first place. These scientists may be expert mathematicians and engineers; however, they are typically not trained behaviourists. They rarely receive formal instruction on experimental methodology, population-based statistics and sampling paradigms, or observational causal inference, let alone neuroscience, collective behaviour or social theory. Conversely, although behavioural scientists are more likely to possess training in these scientific methods, they are less likely to possess the expertise required to proficiently evaluate the underlying quality and appropriateness of AI techniques for a given problem domain or to mathematically describe the properties of particular algorithms.\nIntegrating scientific practices from across multiple fields is not easy. Up to this point, the main focus of those who create AI systems has been on crafting, implementing and optimizing intelligent systems to\n\nHighlights the need for interdisciplinary collaboration to understand and evaluate the impact of complex AI systems.",
    "original_text": "To study machine behaviour-especially the behaviours of black box algorithms in real-world settings-we must integrate knowledge from across a variety of scientific disciplines (Fig. 2). This integration is currently in its nascent stages and has happened largely in an ad hoc fashion in response to the growing need to understand machine behaviour. Currently, the scientists who most commonly study the behaviour of machines are the computer scientists, roboticists and engineers who have created the machines in the first place. These scientists may be expert mathematicians and engineers; however, they are typically not trained behaviourists. They rarely receive formal instruction on experimental methodology, population-based statistics and sampling paradigms, or observational causal inference, let alone neuroscience, collective behaviour or social theory. Conversely, although behavioural scientists are more likely to possess training in these scientific methods, they are less likely to possess the expertise required to proficiently evaluate the underlying quality and appropriateness of AI techniques for a given problem domain or to mathematically describe the properties of particular algorithms.\nIntegrating scientific practices from across multiple fields is not easy. Up to this point, the main focus of those who create AI systems has been on crafting, implementing and optimizing intelligent systems to",
    "context": "Highlights the need for interdisciplinary collaboration to understand and evaluate the impact of complex AI systems.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      2
    ],
    "id": "9b691af4464481ae95d2ee2f9d98f50ca505883efdc18e3cf5c5d23ad699f4e3"
  },
  {
    "text": "- Does the algorithm create filter bubbles?\n- Does the algorithm disproportionately censor content?\n\nThis chunk raises critical questions about the potential negative impacts of AI algorithms, specifically focusing on issues of bias, censorship, and the creation of filter bubbles – highlighting concerns about fairness and societal influence.",
    "original_text": "- Does the algorithm create filter bubbles?\n- Does the algorithm disproportionately censor content?",
    "context": "This chunk raises critical questions about the potential negative impacts of AI algorithms, specifically focusing on issues of bias, censorship, and the creation of filter bubbles – highlighting concerns about fairness and societal influence.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      3
    ],
    "id": "2f02a5aeeebf0ec3eba84c52543c870674ed67e4eb9834d5d1bb40c38503e51c"
  },
  {
    "text": "- Does the algorithm discriminate against a racial group in granting parole?\n- Does a predictive policing system increase the false conviction rate?\n\nExamines potential biases and negative societal impacts of algorithmic decision-making, specifically focusing on fairness in parole and the accuracy of predictive policing systems.",
    "original_text": "- Does the algorithm discriminate against a racial group in granting parole?\n- Does a predictive policing system increase the false conviction rate?",
    "context": "Examines potential biases and negative societal impacts of algorithmic decision-making, specifically focusing on fairness in parole and the accuracy of predictive policing systems.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      3
    ],
    "id": "d67a70fe0d3fd22b51f17fa60bc89302ac13d84bd750e8dc1a50c471f4e74d01"
  },
  {
    "text": "- How aggressively does the car overtake other vehicles?\n- How does the car distribute risk between passengers and pedestrians?\n\nExamines the safety implications of autonomous vehicle behavior.",
    "original_text": "- How aggressively does the car overtake other vehicles?\n- How does the car distribute risk between passengers and pedestrians?",
    "context": "Examines the safety implications of autonomous vehicle behavior.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      3
    ],
    "id": "e4630f342f2cbc15f02dfa11a9e6483e72c55f55c2c3f0a757f0e97c255d2845"
  },
  {
    "text": "- Does the weapon respect necessity and proportionality in its use of force?\n- Does the weapon distinguish between combatants and civilians?\n\nExamines the ethical considerations of lethal force, specifically regarding adherence to necessity and proportionality, and the ability to differentiate between combatants and civilians.",
    "original_text": "- Does the weapon respect necessity and proportionality in its use of force?\n- Does the weapon distinguish between combatants and civilians?",
    "context": "Examines the ethical considerations of lethal force, specifically regarding adherence to necessity and proportionality, and the ability to differentiate between combatants and civilians.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      3
    ],
    "id": "910b8909f6f0f13cc69b488bddb8a121b929ac8af98c6ede45271158935b89f7"
  },
  {
    "text": "- Do algorithms manipulate markets?\n- Does the behaviour of the algorithm increase systemic risk of market crash?\n\nExamines potential risks associated with algorithmic market behavior, specifically systemic risk and manipulation.",
    "original_text": "- Do algorithms manipulate markets?\n- Does the behaviour of the algorithm increase systemic risk of market crash?",
    "context": "Examines potential risks associated with algorithmic market behavior, specifically systemic risk and manipulation.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      3
    ],
    "id": "c5af2277ec9255a25643b3d6f77e15334ebd713eeb24082462ab2376cd60df12"
  },
  {
    "text": "- Do algorithms of competitors collude to fix prices?\n- Does the algorithm exhibit price discrimination?\n\nExamines potential anti-competitive behavior of algorithms, specifically focusing on price-fixing and price discrimination.",
    "original_text": "- Do algorithms of competitors collude to fix prices?\n- Does the algorithm exhibit price discrimination?",
    "context": "Examines potential anti-competitive behavior of algorithms, specifically focusing on price-fixing and price discrimination.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      3
    ],
    "id": "08a55b65e4ce036598d60383c82a4a4e27ac50c200653af0d433c06eceb65f8f"
  },
  {
    "text": "- Does the matching algorithm use facial features?\n- Does the matching algorithm amplify or reduce homophily?\n\nThis chunk explores the mechanisms of a matching algorithm, specifically investigating whether it utilizes facial features and whether it promotes or diminishes social similarity (homophily). It highlights the need for understanding the algorithm's inner workings, including its potential impact on social dynamics and the importance of interpretability methods in machine learning.",
    "original_text": "- Does the matching algorithm use facial features?\n- Does the matching algorithm amplify or reduce homophily?",
    "context": "This chunk explores the mechanisms of a matching algorithm, specifically investigating whether it utilizes facial features and whether it promotes or diminishes social similarity (homophily). It highlights the need for understanding the algorithm's inner workings, including its potential impact on social dynamics and the importance of interpretability methods in machine learning.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      3
    ],
    "id": "59f3e95de81e3ed5265b7d2b30c7086246ec0ddf547185a25cefd723c78f2ac0"
  },
  {
    "text": "- Does the robot promote products to children?\n- Does the algorithm affect collective behaviours?\nFig. 1 | Examples of questions that fall into the domain of machine behaviour. Questions of concern to machine behaviour span a wide variety of traditional scientific disciplines and topics.\nperform specialized tasks. Excellent progress has been made on benchmark tasks-including board games such as chess 68 , checkers 69 and Go 70,71 , card games such as poker 72 , computer games such as those on the Atari platform 73 , artificial markets 74 and Robocup Soccer 75 -as well as standardized evaluation data, such as the ImageNet data for object recognition 76 and the Microsoft Common Objects in Context data for image-captioning tasks 77 . Success has also been achieved in speech recognition, language translation and autonomous locomotion. These benchmarks are coupled with metrics to quantify performance on standardized tasks 78-81 and are used to improved performance, a proxy that enables AI builders to aim for better, faster and more-robust algorithms.\nBut methodologies aimed at maximized algorithmic performance are not optimal for conducting scientific observation of the properties and behaviours of AI agents. Rather than using metrics in the service of optimization against benchmarks, scholars of machine behaviour are interested in a broader set of indicators, much as social scientists explore a wide range of human behaviours in the realm of social, political or economic interactions 82 . As such, scholars of machine behaviour spend considerable effort in defining measures of micro and macro outcomes to answer broad questions such as how these algorithms behave in different environments and whether human interactions with algorithms alter societal outcomes. Randomized experiments, observational inference and population-based descriptive statistics-methods that are often used in quantitative behavioural sciences-must be central to the study of machine behaviour. Incorporating scholars from outside of the disciplines that traditionally produce intelligent machines can provide knowledge of important methodological tools, scientific approaches, alternative conceptual frameworks and perspectives on the economic, social and political phenomena that machines will increasingly influence.\n\nExplores questions regarding algorithmic influence, particularly concerning the promotion of products to children and broader impacts on collective behaviors, aligning with a broader methodological approach to studying AI agent behavior beyond traditional performance benchmarks.",
    "original_text": "- Does the robot promote products to children?\n- Does the algorithm affect collective behaviours?\nFig. 1 | Examples of questions that fall into the domain of machine behaviour. Questions of concern to machine behaviour span a wide variety of traditional scientific disciplines and topics.\nperform specialized tasks. Excellent progress has been made on benchmark tasks-including board games such as chess 68 , checkers 69 and Go 70,71 , card games such as poker 72 , computer games such as those on the Atari platform 73 , artificial markets 74 and Robocup Soccer 75 -as well as standardized evaluation data, such as the ImageNet data for object recognition 76 and the Microsoft Common Objects in Context data for image-captioning tasks 77 . Success has also been achieved in speech recognition, language translation and autonomous locomotion. These benchmarks are coupled with metrics to quantify performance on standardized tasks 78-81 and are used to improved performance, a proxy that enables AI builders to aim for better, faster and more-robust algorithms.\nBut methodologies aimed at maximized algorithmic performance are not optimal for conducting scientific observation of the properties and behaviours of AI agents. Rather than using metrics in the service of optimization against benchmarks, scholars of machine behaviour are interested in a broader set of indicators, much as social scientists explore a wide range of human behaviours in the realm of social, political or economic interactions 82 . As such, scholars of machine behaviour spend considerable effort in defining measures of micro and macro outcomes to answer broad questions such as how these algorithms behave in different environments and whether human interactions with algorithms alter societal outcomes. Randomized experiments, observational inference and population-based descriptive statistics-methods that are often used in quantitative behavioural sciences-must be central to the study of machine behaviour. Incorporating scholars from outside of the disciplines that traditionally produce intelligent machines can provide knowledge of important methodological tools, scientific approaches, alternative conceptual frameworks and perspectives on the economic, social and political phenomena that machines will increasingly influence.",
    "context": "Explores questions regarding algorithmic influence, particularly concerning the promotion of products to children and broader impacts on collective behaviors, aligning with a broader methodological approach to studying AI agent behavior beyond traditional performance benchmarks.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      3
    ],
    "id": "fae6ae5ac0c8906dcff2be1ff98869e2f7fc0e076ecb675d92b8f526b7a97662"
  },
  {
    "text": "Nikolaas Tinbergen, who won the 1973 Nobel Prize in Physiology or Medicine alongside Karl von Frisch and Konrad Lorenz for founding the field of ethology, identified four complementary dimensions of\nFig. 2 | The interdisciplinarity of machine behaviour. Machine\nbehaviour lies at the intersection of the fields that design and engineer AI systems and the fields that traditionally use scientific methods to study the behaviour of biological agents. The insights from machine behavioural studies provide quantitative evidence that can help to inform those fields that study the potential effects of technology on social and technological systems. In turn, those fields can provide useful engineering practices and scientific questions to fields that examine machine behaviours. Finally, the scientific study of behaviour helps AI scholars to make more precise statements about what AI systems can and cannot do.\nanalysis that help to explain animal behaviour 83 . These dimensions concern questions of the function, mechanism, development and evolutionary history of a behaviour and provide an organizing framework for the study of animal and human behaviour. For example, this conceptualization distinguishes the study of how a young animal or human develops a type of behaviour from the evolutionary trajectory that selected for such behaviour in the population. The goal of these distinctions is not division but rather integration. Although it is not wrong to say that, for example, a bird's song is explained by learning or by its specific evolutionary history, a complete understanding of the song will require both.\nDespite fundamental differences between machines and animals, the behavioural study of machines can benefit from a similar classification. Machines have mechanisms that produce behaviour, undergo development that integrates environmental information into behaviour, produce functional consequences that cause specific machines to become more or less common in specific environments and embody evolutionary histories through which past environments and human decisions continue to influence machine behaviour. Scholars of computer science have already achieved substantial gains in understanding the mechanisms and development of AI systems, although many questions remain. Relatively less emphasis has been placed on the function and evolution of AI systems. We discuss these four topics in the next subsections and provide Fig. 3 as a summary 84 .\n\nThis section outlines the framework for studying machine behavior, drawing parallels to ethology. It proposes four key dimensions – function, mechanism, development, and evolutionary history – to analyze how machines exhibit behavior, emphasizing the need for interdisciplinary approaches and acknowledging the influence of external factors like economic incentives and human design choices.",
    "original_text": "Nikolaas Tinbergen, who won the 1973 Nobel Prize in Physiology or Medicine alongside Karl von Frisch and Konrad Lorenz for founding the field of ethology, identified four complementary dimensions of\nFig. 2 | The interdisciplinarity of machine behaviour. Machine\nbehaviour lies at the intersection of the fields that design and engineer AI systems and the fields that traditionally use scientific methods to study the behaviour of biological agents. The insights from machine behavioural studies provide quantitative evidence that can help to inform those fields that study the potential effects of technology on social and technological systems. In turn, those fields can provide useful engineering practices and scientific questions to fields that examine machine behaviours. Finally, the scientific study of behaviour helps AI scholars to make more precise statements about what AI systems can and cannot do.\nanalysis that help to explain animal behaviour 83 . These dimensions concern questions of the function, mechanism, development and evolutionary history of a behaviour and provide an organizing framework for the study of animal and human behaviour. For example, this conceptualization distinguishes the study of how a young animal or human develops a type of behaviour from the evolutionary trajectory that selected for such behaviour in the population. The goal of these distinctions is not division but rather integration. Although it is not wrong to say that, for example, a bird's song is explained by learning or by its specific evolutionary history, a complete understanding of the song will require both.\nDespite fundamental differences between machines and animals, the behavioural study of machines can benefit from a similar classification. Machines have mechanisms that produce behaviour, undergo development that integrates environmental information into behaviour, produce functional consequences that cause specific machines to become more or less common in specific environments and embody evolutionary histories through which past environments and human decisions continue to influence machine behaviour. Scholars of computer science have already achieved substantial gains in understanding the mechanisms and development of AI systems, although many questions remain. Relatively less emphasis has been placed on the function and evolution of AI systems. We discuss these four topics in the next subsections and provide Fig. 3 as a summary 84 .",
    "context": "This section outlines the framework for studying machine behavior, drawing parallels to ethology. It proposes four key dimensions – function, mechanism, development, and evolutionary history – to analyze how machines exhibit behavior, emphasizing the need for interdisciplinary approaches and acknowledging the influence of external factors like economic incentives and human design choices.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      3,
      4
    ],
    "id": "5dd0ae66ace29fdc244685254a7884a907a2b5c6e05be15c25b6467c7de453c1"
  },
  {
    "text": "The proximate causes of a machine's behaviour have to do with how the behaviour is observationally triggered and generated in specific environments. For example, early algorithmic trading programs used simple rules to trigger buying and selling behaviour 85 . More sophisticated agents may compute strategies based on adaptive heuristics or explicit maximization of expected utility 86 . The behaviour of a reinforcement learning algorithm that plays poker could be attributed to the particular way in which it represents the state space or evaluates the game tree 72 , and so on.\nA mechanism depends on both an algorithm and its environment. A more sophisticated agent, such as a driverless car, may exhibit particular driving behaviour-for example, lane switching, overtaking or signalling to pedestrians. These behaviours would be generated according to the algorithms that construct driving policies 87 and are also shaped fundamentally by features of the perception and actuation system of the car, including the resolution and accuracy of its object detection and classification system, and the responsiveness and accuracy of its steering, among other factors. Because many current AI systems are derived from machine learning methods that are applied to increasingly complex data, the study of the mechanism behind a machine's behaviour, such as those mentioned above, will require continued work on interpretability methods for machine learning 46,88,89 .\n\nThis chunk focuses on the observable triggers and mechanisms behind a machine’s behavior, specifically detailing how algorithms and their environments interact to produce actions like lane switching in a driverless car. It highlights the importance of understanding these mechanisms through interpretability methods and acknowledges the role of environmental factors like object detection accuracy.",
    "original_text": "The proximate causes of a machine's behaviour have to do with how the behaviour is observationally triggered and generated in specific environments. For example, early algorithmic trading programs used simple rules to trigger buying and selling behaviour 85 . More sophisticated agents may compute strategies based on adaptive heuristics or explicit maximization of expected utility 86 . The behaviour of a reinforcement learning algorithm that plays poker could be attributed to the particular way in which it represents the state space or evaluates the game tree 72 , and so on.\nA mechanism depends on both an algorithm and its environment. A more sophisticated agent, such as a driverless car, may exhibit particular driving behaviour-for example, lane switching, overtaking or signalling to pedestrians. These behaviours would be generated according to the algorithms that construct driving policies 87 and are also shaped fundamentally by features of the perception and actuation system of the car, including the resolution and accuracy of its object detection and classification system, and the responsiveness and accuracy of its steering, among other factors. Because many current AI systems are derived from machine learning methods that are applied to increasingly complex data, the study of the mechanism behind a machine's behaviour, such as those mentioned above, will require continued work on interpretability methods for machine learning 46,88,89 .",
    "context": "This chunk focuses on the observable triggers and mechanisms behind a machine’s behavior, specifically detailing how algorithms and their environments interact to produce actions like lane switching in a driverless car. It highlights the importance of understanding these mechanisms through interpretability methods and acknowledges the role of environmental factors like object detection accuracy.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      4
    ],
    "id": "2b90a9450cb39b9135c93054141019e1db5064c724ef2aa5120d67adbe169d7d"
  },
  {
    "text": "In the study of animal or human behaviour, development refers to how an individual acquires a particular behaviour-for example, through imitation or environmental conditioning. This is distinct from longerterm evolutionary changes.\nIn the context of machines, we can ask how machines acquire (develop) a specific individual or collective behaviour. Behavioural development could be directly attributable to human engineering or design choices. Architectural design choices made by the programmer (for example, the value of a learning rate parameter, the acquisition of the representation of knowledge and state, or a particular wiring of a convolutional neural network) determine or influence the kinds of behaviours that the algorithm exhibits. In a more complex AI system, such as a driverless car, the behaviour of the car develops over time, from software development and changing hardware components that engineers incorporate into its overall architecture. Behaviours can also change as a result of algorithmic upgrades pushed to the machine by its designers after deployment.\nA human engineer may also shape the behaviour of the machine by exposing it to particular training stimuli. For instance, many image and text classification algorithms are trained to optimize accuracy on a specific set of datasets that were manually labelled by humans. The choice of dataset-and those features it represents 60,61 -can substantially influence the behaviour exhibited by the algorithm.\nFinally, a machine may acquire behaviours through its own experience. For instance, a reinforcement learning agent trained to maximize long-term profit can learn peculiar short-term trading strategies based on its own past actions and concomitant feedback from the market 90 . Similarly, product recommendation algorithms make recommendations based on an endless stream of choices made by customers and update their recommendations accordingly.\n\nThis chunk focuses on how machines acquire behaviors, specifically detailing the mechanisms of development – including human engineering choices, training stimuli, and experiential learning. It highlights that this development is distinct from evolutionary changes and is influenced by factors like dataset selection and algorithmic upgrades, ultimately shaping the machine’s specific actions.",
    "original_text": "In the study of animal or human behaviour, development refers to how an individual acquires a particular behaviour-for example, through imitation or environmental conditioning. This is distinct from longerterm evolutionary changes.\nIn the context of machines, we can ask how machines acquire (develop) a specific individual or collective behaviour. Behavioural development could be directly attributable to human engineering or design choices. Architectural design choices made by the programmer (for example, the value of a learning rate parameter, the acquisition of the representation of knowledge and state, or a particular wiring of a convolutional neural network) determine or influence the kinds of behaviours that the algorithm exhibits. In a more complex AI system, such as a driverless car, the behaviour of the car develops over time, from software development and changing hardware components that engineers incorporate into its overall architecture. Behaviours can also change as a result of algorithmic upgrades pushed to the machine by its designers after deployment.\nA human engineer may also shape the behaviour of the machine by exposing it to particular training stimuli. For instance, many image and text classification algorithms are trained to optimize accuracy on a specific set of datasets that were manually labelled by humans. The choice of dataset-and those features it represents 60,61 -can substantially influence the behaviour exhibited by the algorithm.\nFinally, a machine may acquire behaviours through its own experience. For instance, a reinforcement learning agent trained to maximize long-term profit can learn peculiar short-term trading strategies based on its own past actions and concomitant feedback from the market 90 . Similarly, product recommendation algorithms make recommendations based on an endless stream of choices made by customers and update their recommendations accordingly.",
    "context": "This chunk focuses on how machines acquire behaviors, specifically detailing the mechanisms of development – including human engineering choices, training stimuli, and experiential learning. It highlights that this development is distinct from evolutionary changes and is influenced by factors like dataset selection and algorithmic upgrades, ultimately shaping the machine’s specific actions.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      4
    ],
    "id": "6637d7b0e37b9e771d4ade6b15b431eee74bc80a8d6f551614f797f83344e44a"
  },
  {
    "text": "In the study of animal behaviour, adaptive value describes how a behaviour contributes to the lifetime reproductive fitness of an animal. For example, a particular hunting behaviour may be more or less successful than another at prolonging the animal's life and, relatedly, the number of mating opportunities, resulting offspring born and the probable reproductive success of the offspring. The focus on function helps us to understand why some behavioural mechanisms spread and persist while others decline and vanish. Function depends critically on the fit of the behaviour to environment.\nIn the case of machines, we may talk of how the behaviour fulfils a contemporaneous function for particular human stakeholders. The human environment creates selective forces that may make some machines more common. Behaviours that are successful ('fitness' enhancing) get copied by developers of other software and hardware or are sometimes engineered to propagate among the machines themselves. These dynamics are ultimately driven by the success of institutions-such as corporations, hospitals, municipal governments and universities-that build or use AI. The most obvious example is provided by algorithmic trading, in which successful automated trading strategies could be copied as their developers move from company to company, or are simply observed and reverse-engineered by rivals.\nThese forces can produce unanticipated effects. For example, objectives such as maximizing engagement on a social media site may lead to so-called filter bubbles 91 , which may increase political polarization or, without careful moderation, could facilitate the spread of fake news. However, websites that do not optimize for user engagement may not be as successful in comparison with ones that do, or may go out of business altogether. Similarly, in the absence of external regulation, autonomous cars that do not prioritize the safety of their own passengers may be\nFig. 3 | Tinbergen's type of question and object of study modified for the study of machine behaviour. The four categories Tinbergen proposed for the study of animal behaviour can be adapted to the study of machine behaviour 83,84 . Tinbergen's framework proposes two types of question,\nhow versus why, as well as two views of these questions, dynamic versus static. Each question can be examined at three scales of inquiry: individual machines, collectives of machines and hybrid human-machine systems.\nless attractive to consumers, leading to fewer sales 31 . Sometimes the function of machine behaviour is to cope with the behaviour of other machines. Adversarial attacks-synthetic inputs that fool a system into producing an undesired output 44,92-94 -on AI systems and the subsequent responses of those who develop AI to these attacks 95 may produce complex predator-prey dynamics that are not easily understood by studying each machine in isolation.\nThese examples highlight how incentives created by external institutions and economic forces can have indirect but substantial effects on the behaviours exhibited by machines 96 . Understanding the interaction between these incentives and AI is relevant to the study of machine behaviour. These market dynamics would, in turn, interact with other processes to produce evolution among machines and algorithms.\n\nThe chunk explains how the concept of “adaptive value” from animal behavior studies—how a behavior contributes to reproductive success—applies to machine behavior. It highlights that external incentives and market forces drive the spread of successful machine behaviors, mirroring how successful animal behaviors persist, and can lead to unintended consequences like filter bubbles.",
    "original_text": "In the study of animal behaviour, adaptive value describes how a behaviour contributes to the lifetime reproductive fitness of an animal. For example, a particular hunting behaviour may be more or less successful than another at prolonging the animal's life and, relatedly, the number of mating opportunities, resulting offspring born and the probable reproductive success of the offspring. The focus on function helps us to understand why some behavioural mechanisms spread and persist while others decline and vanish. Function depends critically on the fit of the behaviour to environment.\nIn the case of machines, we may talk of how the behaviour fulfils a contemporaneous function for particular human stakeholders. The human environment creates selective forces that may make some machines more common. Behaviours that are successful ('fitness' enhancing) get copied by developers of other software and hardware or are sometimes engineered to propagate among the machines themselves. These dynamics are ultimately driven by the success of institutions-such as corporations, hospitals, municipal governments and universities-that build or use AI. The most obvious example is provided by algorithmic trading, in which successful automated trading strategies could be copied as their developers move from company to company, or are simply observed and reverse-engineered by rivals.\nThese forces can produce unanticipated effects. For example, objectives such as maximizing engagement on a social media site may lead to so-called filter bubbles 91 , which may increase political polarization or, without careful moderation, could facilitate the spread of fake news. However, websites that do not optimize for user engagement may not be as successful in comparison with ones that do, or may go out of business altogether. Similarly, in the absence of external regulation, autonomous cars that do not prioritize the safety of their own passengers may be\nFig. 3 | Tinbergen's type of question and object of study modified for the study of machine behaviour. The four categories Tinbergen proposed for the study of animal behaviour can be adapted to the study of machine behaviour 83,84 . Tinbergen's framework proposes two types of question,\nhow versus why, as well as two views of these questions, dynamic versus static. Each question can be examined at three scales of inquiry: individual machines, collectives of machines and hybrid human-machine systems.\nless attractive to consumers, leading to fewer sales 31 . Sometimes the function of machine behaviour is to cope with the behaviour of other machines. Adversarial attacks-synthetic inputs that fool a system into producing an undesired output 44,92-94 -on AI systems and the subsequent responses of those who develop AI to these attacks 95 may produce complex predator-prey dynamics that are not easily understood by studying each machine in isolation.\nThese examples highlight how incentives created by external institutions and economic forces can have indirect but substantial effects on the behaviours exhibited by machines 96 . Understanding the interaction between these incentives and AI is relevant to the study of machine behaviour. These market dynamics would, in turn, interact with other processes to produce evolution among machines and algorithms.",
    "context": "The chunk explains how the concept of “adaptive value” from animal behavior studies—how a behavior contributes to reproductive success—applies to machine behavior. It highlights that external incentives and market forces drive the spread of successful machine behaviors, mirroring how successful animal behaviors persist, and can lead to unintended consequences like filter bubbles.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      4,
      5
    ],
    "id": "00ef9490f10b70bf389d1eca8c707deb0cca5ab3d909aa9ee33e7e31a4bdede1"
  },
  {
    "text": "In the study of animal behaviour, phylogeny describes how a behaviour evolved. In addition to its current function, behaviour is influenced by past selective pressures and previously evolved mechanisms. For example, the human hand evolved from the fin of a bony fish. Its current function is no longer for swimming, but its internal structure is explained by its evolutionary history. Non-selective forces, such as migration and drift, also have strong roles in explaining relationships among different forms of behaviour.\nIn the case of machines, evolutionary history can also generate path dependence, explaining otherwise puzzling behaviour. At each step, aspects of the algorithms are reused in new contexts, both constraining future behaviour and making possible additional innovations. For example, early choices about microprocessor design continue to influence modern computing, and traditions in algorithm design-such as neural networks and Bayesian state-space models-build in many assumptions and guide future innovations by making some new algorithms easier to access than others. As a result, some algorithms may attend to certain features and ignore others because those features were important in early successful applications. Some machine behaviour may spread because it is 'evolvable'-easy to modify and robust to perturbations-similar to how some traits of animals may be common because they facilitate diversity and stability 97 .\nMachine behaviour evolves differently from animal behaviour. Most animal inheritance is simple-two parents, one transmission event. Algorithms are much more flexible and they have a designer with an objective in the background. The human environment strongly influences how algorithms evolve by changing their inheritance system. AI replication behaviour may be facilitated through a culture of open source sharing of software, the details of network architecture or underlying training datasets. For instance, companies that develop software for driverless cars may share enhanced open source libraries for object detection or path planning as well as the training data that underlie these algorithms to enable safety-enhancing software to spread throughout the industry. It is possible for a single adaptive 'mutation' in the behaviour of a particular driverless car to propagate instantly to millions of other cars through a software update. However, other institutions apply limits as well. For example, software patents may impose constraints on the copying of particular behavioural traits. And regulatory constraints-such as privacy protection laws-can prevent machines from accessing, retaining or otherwise using particular information in their decision-making. These peculiarities highlight the fact that machines may exhibit very different evolutionary trajectories, as they are not bound by the mechanisms of organic evolution.\n\nExplains how machine behavior evolves, drawing parallels to animal phylogeny and highlighting the influence of past design choices and human-driven modifications on algorithmic development and spread.",
    "original_text": "In the study of animal behaviour, phylogeny describes how a behaviour evolved. In addition to its current function, behaviour is influenced by past selective pressures and previously evolved mechanisms. For example, the human hand evolved from the fin of a bony fish. Its current function is no longer for swimming, but its internal structure is explained by its evolutionary history. Non-selective forces, such as migration and drift, also have strong roles in explaining relationships among different forms of behaviour.\nIn the case of machines, evolutionary history can also generate path dependence, explaining otherwise puzzling behaviour. At each step, aspects of the algorithms are reused in new contexts, both constraining future behaviour and making possible additional innovations. For example, early choices about microprocessor design continue to influence modern computing, and traditions in algorithm design-such as neural networks and Bayesian state-space models-build in many assumptions and guide future innovations by making some new algorithms easier to access than others. As a result, some algorithms may attend to certain features and ignore others because those features were important in early successful applications. Some machine behaviour may spread because it is 'evolvable'-easy to modify and robust to perturbations-similar to how some traits of animals may be common because they facilitate diversity and stability 97 .\nMachine behaviour evolves differently from animal behaviour. Most animal inheritance is simple-two parents, one transmission event. Algorithms are much more flexible and they have a designer with an objective in the background. The human environment strongly influences how algorithms evolve by changing their inheritance system. AI replication behaviour may be facilitated through a culture of open source sharing of software, the details of network architecture or underlying training datasets. For instance, companies that develop software for driverless cars may share enhanced open source libraries for object detection or path planning as well as the training data that underlie these algorithms to enable safety-enhancing software to spread throughout the industry. It is possible for a single adaptive 'mutation' in the behaviour of a particular driverless car to propagate instantly to millions of other cars through a software update. However, other institutions apply limits as well. For example, software patents may impose constraints on the copying of particular behavioural traits. And regulatory constraints-such as privacy protection laws-can prevent machines from accessing, retaining or otherwise using particular information in their decision-making. These peculiarities highlight the fact that machines may exhibit very different evolutionary trajectories, as they are not bound by the mechanisms of organic evolution.",
    "context": "Explains how machine behavior evolves, drawing parallels to animal phylogeny and highlighting the influence of past design choices and human-driven modifications on algorithmic development and spread.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      5
    ],
    "id": "55773dc382c5c6c91b1fcfb72edd033c40122b4a6be3a62951b0d91cd12db09f"
  },
  {
    "text": "With the framework outlined above and in Fig. 3, we now catalogue examples of machine behaviour at the three scales of inquiry: individual machines, collectives of machines and groups of machines embedded in a social environment with groups of humans in hybrid or heterogeneous systems 39 (Fig. 4). Individual machine behaviour emphasizes the study of the algorithm itself, collective machine behaviour emphasizes the study of interactions between machines and hybrid human-machine behaviour emphasizes the study of interactions between machines and humans. Here we can draw an analogy to the study of a particular species, the study of interactions among members of a species and the interactions of the species with their broader environment. Analyses at any of these scales may address any or all of the questions described in Fig. 3.\n\nThis chunk outlines the three levels of inquiry for studying machine behavior: individual machines, collective machines, and hybrid human-machine systems, drawing an analogy to the study of animal species and their environments.",
    "original_text": "With the framework outlined above and in Fig. 3, we now catalogue examples of machine behaviour at the three scales of inquiry: individual machines, collectives of machines and groups of machines embedded in a social environment with groups of humans in hybrid or heterogeneous systems 39 (Fig. 4). Individual machine behaviour emphasizes the study of the algorithm itself, collective machine behaviour emphasizes the study of interactions between machines and hybrid human-machine behaviour emphasizes the study of interactions between machines and humans. Here we can draw an analogy to the study of a particular species, the study of interactions among members of a species and the interactions of the species with their broader environment. Analyses at any of these scales may address any or all of the questions described in Fig. 3.",
    "context": "This chunk outlines the three levels of inquiry for studying machine behavior: individual machines, collective machines, and hybrid human-machine systems, drawing an analogy to the study of animal species and their environments.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      5
    ],
    "id": "965b5d916e846536475776983dc0b9918d76a6dfd9444323a1cfe11261da879c"
  },
  {
    "text": "The study of the behaviour of individual machines focuses on specific intelligent machines by themselves. Often these studies focus on properties that are intrinsic to the individual machines and that are driven by their source code or design. The fields of machine learning and software engineering currently conduct the majority of these studies. There are two general approaches to the study of individual machine behaviour. The first focuses on profiling the set of behaviours of any\nFig. 4 | Scale of inquiry in the machine behaviour ecosystem. AI systems represent the amalgamation of humans, data and algorithms. Each of these domains influences the other in both well-understood and unknown ways. Data-filtered through algorithms created by humans-influences individual and collective machine behaviour. AI systems are trained on the data, in turn influencing how humans generate data. AI systems collectively interact with and influence one another. Human interactions can be altered by the introduction of these AI systems. Studies of machine behaviour tend to occur at the individual, the collective or the hybrid human-machine scale of inquiry.\nspecific machine agent using a within-machine approach, comparing the behaviour of a particular machine across different conditions. The second, a between-machine approach, examines how a variety of individual machine agents behave in the same condition.\nA within-machine approach to the study of individual machine behaviours investigates questions such as whether there are constants that characterize the within-machine behaviour of any particular AI across a variety of contexts, how the behaviour of a particular AI progresses over time in the same, or different, environments and which environmental factors lead to the expression of particular behaviours by machines.\nFor instance, an algorithm may only exhibit certain behaviours if trained on particular underlying data 98-100 (Fig. 3). Then, the question becomes whether or not an algorithm that scores probability of recidivism in parole decisions 7 would behave in unexpected ways when presented with evaluation data that diverge substantially from its training data. Other studies related to the characterization of within-machine behaviour include the study of individual robotic recovery behaviours 101,102 , the ' cognitive' attributes of algorithms and the utility of using techniques from psychology in the study of algorithmic behaviour 103 , and the examination of bot-specific characteristics such as those designed to influence human users 104 .\nThe second approach to the study of individual machine behaviour examines the same behaviours as they vary between machines. For example, those interested in examining advertising behaviours of intelligent agents 63,105,106 may investigate a variety of advertising platforms (and their underlying algorithms) and examine the between-machine effect of performing experiments with the same set of advertising inputs across platforms. The same approach could be used for investigations of dynamic pricing algorithms 23,24,32 across platforms. Other betweenmachine studies might look at the different behaviours used by autonomous vehicles in their overtaking patterns or at the varied foraging behaviours exhibited by search and rescue drones 107 .\n\nThis chunk details two approaches to studying individual machine behavior: a within-machine approach comparing a single AI across different conditions, and a between-machine approach examining how various AIs behave in the same condition. It highlights research questions like identifying consistent AI behaviors, tracking behavioral changes over time, and investigating how environmental factors influence specific AI actions, illustrating the methods used to characterize individual machine behavior.",
    "original_text": "The study of the behaviour of individual machines focuses on specific intelligent machines by themselves. Often these studies focus on properties that are intrinsic to the individual machines and that are driven by their source code or design. The fields of machine learning and software engineering currently conduct the majority of these studies. There are two general approaches to the study of individual machine behaviour. The first focuses on profiling the set of behaviours of any\nFig. 4 | Scale of inquiry in the machine behaviour ecosystem. AI systems represent the amalgamation of humans, data and algorithms. Each of these domains influences the other in both well-understood and unknown ways. Data-filtered through algorithms created by humans-influences individual and collective machine behaviour. AI systems are trained on the data, in turn influencing how humans generate data. AI systems collectively interact with and influence one another. Human interactions can be altered by the introduction of these AI systems. Studies of machine behaviour tend to occur at the individual, the collective or the hybrid human-machine scale of inquiry.\nspecific machine agent using a within-machine approach, comparing the behaviour of a particular machine across different conditions. The second, a between-machine approach, examines how a variety of individual machine agents behave in the same condition.\nA within-machine approach to the study of individual machine behaviours investigates questions such as whether there are constants that characterize the within-machine behaviour of any particular AI across a variety of contexts, how the behaviour of a particular AI progresses over time in the same, or different, environments and which environmental factors lead to the expression of particular behaviours by machines.\nFor instance, an algorithm may only exhibit certain behaviours if trained on particular underlying data 98-100 (Fig. 3). Then, the question becomes whether or not an algorithm that scores probability of recidivism in parole decisions 7 would behave in unexpected ways when presented with evaluation data that diverge substantially from its training data. Other studies related to the characterization of within-machine behaviour include the study of individual robotic recovery behaviours 101,102 , the ' cognitive' attributes of algorithms and the utility of using techniques from psychology in the study of algorithmic behaviour 103 , and the examination of bot-specific characteristics such as those designed to influence human users 104 .\nThe second approach to the study of individual machine behaviour examines the same behaviours as they vary between machines. For example, those interested in examining advertising behaviours of intelligent agents 63,105,106 may investigate a variety of advertising platforms (and their underlying algorithms) and examine the between-machine effect of performing experiments with the same set of advertising inputs across platforms. The same approach could be used for investigations of dynamic pricing algorithms 23,24,32 across platforms. Other betweenmachine studies might look at the different behaviours used by autonomous vehicles in their overtaking patterns or at the varied foraging behaviours exhibited by search and rescue drones 107 .",
    "context": "This chunk details two approaches to studying individual machine behavior: a within-machine approach comparing a single AI across different conditions, and a between-machine approach examining how various AIs behave in the same condition. It highlights research questions like identifying consistent AI behaviors, tracking behavioral changes over time, and investigating how environmental factors influence specific AI actions, illustrating the methods used to characterize individual machine behavior.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      5,
      6
    ],
    "id": "48fe169e6918ff8a5d66318bf7b73b6641d1fea933eb275a798c211d993c4595"
  },
  {
    "text": "In contrast the study of the behaviour of individual machines, the study of collective machine behaviour focuses on the interactive and systemwide behaviours of collections of machine agents. In some cases, the implications of individual machine behaviour may make little sense until the collective level is considered. Some investigations of these systems have been inspired by natural collectives, such as swarms of insects, or mobile groups, such as flocking birds or schooling fish. For example, animal groups are known to exhibit both emergent sensing of complex environmental features 108  and effective consensus decision-making 109 . In both scenarios, groups exhibit an awareness of the environment that does not exist at the individual level. Fields such as multi-agent systems and computational game theory provide useful examples of the study of this area of machine behaviour.\nRobots that use simple algorithms for local interactions between bots can nevertheless produce interesting behaviour once aggregated into large collectives. For example, scholars have examined the swarm-like properties of microrobots that combine into aggregations that resemble swarms found in systems of biological agents 110,111 . Additional examples include the collective behaviours of algorithms both in the laboratory (in the Game of Life 112 ) as well as in the wild (as seen in Wikipedia-editing bots 113 ). Other examples include the emergence of novel algorithmic languages 114 between communicating intelligent machines as well as the dynamic properties of fully autonomous transportation systems. Ultimately, many interesting questions in this domain remain to be examined.\nThe vast majority of work on collective animal behaviour and collective robotics has focused on how interactions among simple agents can create higher-order structures and properties. Although important, this neglects that fact that many organisms, and increasingly also AI agents 75 , are sophisticated entities with behaviours and interactions that may not be well-characterized by simplistic representations. Revealing what extra properties emerge when interacting entities are capable of sophisticated cognition remains a key challenge in the biological sciences and may have direct parallels in the study of machine behaviour. For example, similar to animals, machines may exhibit 'social learning' . Such social learning does not need be limited to machines learning from machines, but we may expect machines to learn from humans, and vice versa for humans to learn from the behaviour of machines. The feedback processes introduced may fundamentally alter the accumulation of knowledge, including across generations, directly affecting human and machine 'culture' .\nIn addition, human-made AI systems do not necessarily face the same constraints as do organisms, and collective assemblages of machines provide new capabilities, such as instant global communication, that can lead to entirely new collective behavioural patterns. Studies in collective machine behaviour examine the properties of assemblages of machines as well as the unexpected properties that can emerge from these complex systems of interactions.\nFor example, some of the most interesting collective behaviour of algorithms has been observed in financial trading environments. These environments operate on tiny time scales, such that algorithmic traders can respond to events and each other ahead of any human trader 115 . Under certain conditions, high-frequency capabilities can produce inefficiencies in financial markets 26,115 . In addition to the unprecedented response speed, the extensive use of machine learning, autonomous operation and ability to deploy at scale are all reasons to believe that the collective behaviour of machine trading may be qualitatively different than that of human traders. Furthermore, these financial algorithms and trading systems are necessarily trained on certain historic datasets and react to a limited variety of foreseen scenarios, leading to the question of how they will react to situations that are new and unforeseen in their design. Flash crashes are examples of clearly unintended consequences of (interacting) algorithms 116,117 ; leading to the question of whether algorithms could interact to create a larger market crisis.\n\nThis section contrasts the study of individual machine behavior with the study of collective machine behavior, highlighting the emergence of complex patterns in groups of machines inspired by natural collectives like swarms and emphasizing the potential for unexpected outcomes in areas like financial trading.",
    "original_text": "In contrast the study of the behaviour of individual machines, the study of collective machine behaviour focuses on the interactive and systemwide behaviours of collections of machine agents. In some cases, the implications of individual machine behaviour may make little sense until the collective level is considered. Some investigations of these systems have been inspired by natural collectives, such as swarms of insects, or mobile groups, such as flocking birds or schooling fish. For example, animal groups are known to exhibit both emergent sensing of complex environmental features 108  and effective consensus decision-making 109 . In both scenarios, groups exhibit an awareness of the environment that does not exist at the individual level. Fields such as multi-agent systems and computational game theory provide useful examples of the study of this area of machine behaviour.\nRobots that use simple algorithms for local interactions between bots can nevertheless produce interesting behaviour once aggregated into large collectives. For example, scholars have examined the swarm-like properties of microrobots that combine into aggregations that resemble swarms found in systems of biological agents 110,111 . Additional examples include the collective behaviours of algorithms both in the laboratory (in the Game of Life 112 ) as well as in the wild (as seen in Wikipedia-editing bots 113 ). Other examples include the emergence of novel algorithmic languages 114 between communicating intelligent machines as well as the dynamic properties of fully autonomous transportation systems. Ultimately, many interesting questions in this domain remain to be examined.\nThe vast majority of work on collective animal behaviour and collective robotics has focused on how interactions among simple agents can create higher-order structures and properties. Although important, this neglects that fact that many organisms, and increasingly also AI agents 75 , are sophisticated entities with behaviours and interactions that may not be well-characterized by simplistic representations. Revealing what extra properties emerge when interacting entities are capable of sophisticated cognition remains a key challenge in the biological sciences and may have direct parallels in the study of machine behaviour. For example, similar to animals, machines may exhibit 'social learning' . Such social learning does not need be limited to machines learning from machines, but we may expect machines to learn from humans, and vice versa for humans to learn from the behaviour of machines. The feedback processes introduced may fundamentally alter the accumulation of knowledge, including across generations, directly affecting human and machine 'culture' .\nIn addition, human-made AI systems do not necessarily face the same constraints as do organisms, and collective assemblages of machines provide new capabilities, such as instant global communication, that can lead to entirely new collective behavioural patterns. Studies in collective machine behaviour examine the properties of assemblages of machines as well as the unexpected properties that can emerge from these complex systems of interactions.\nFor example, some of the most interesting collective behaviour of algorithms has been observed in financial trading environments. These environments operate on tiny time scales, such that algorithmic traders can respond to events and each other ahead of any human trader 115 . Under certain conditions, high-frequency capabilities can produce inefficiencies in financial markets 26,115 . In addition to the unprecedented response speed, the extensive use of machine learning, autonomous operation and ability to deploy at scale are all reasons to believe that the collective behaviour of machine trading may be qualitatively different than that of human traders. Furthermore, these financial algorithms and trading systems are necessarily trained on certain historic datasets and react to a limited variety of foreseen scenarios, leading to the question of how they will react to situations that are new and unforeseen in their design. Flash crashes are examples of clearly unintended consequences of (interacting) algorithms 116,117 ; leading to the question of whether algorithms could interact to create a larger market crisis.",
    "context": "This section contrasts the study of individual machine behavior with the study of collective machine behavior, highlighting the emergence of complex patterns in groups of machines inspired by natural collectives like swarms and emphasizing the potential for unexpected outcomes in areas like financial trading.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      6
    ],
    "id": "df9c6d6f89dd44b04524da6919b0fdc291392b179eef4e67b2292437836753e1"
  },
  {
    "text": "Humans increasingly interact with machines 16 . They mediate our social interactions 39 , shape the news 14,17,55,56 and online information 15,118 that we see, and form relationships with us that can alter our social systems. Because of their complexity, these hybrid human-machine systems\npose one of the most technically difficult yet simultaneously most important areas of study for machine behaviour.\n\nHighlights the complex and significant role of human-machine interactions in shaping social systems and represents a crucial area of study for understanding machine behavior.",
    "original_text": "Humans increasingly interact with machines 16 . They mediate our social interactions 39 , shape the news 14,17,55,56 and online information 15,118 that we see, and form relationships with us that can alter our social systems. Because of their complexity, these hybrid human-machine systems\npose one of the most technically difficult yet simultaneously most important areas of study for machine behaviour.",
    "context": "Highlights the complex and significant role of human-machine interactions in shaping social systems and represents a crucial area of study for understanding machine behavior.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      6,
      7
    ],
    "id": "00060f9f1c945501c0671b8c08279464161911f2fc4c54efa541bfb382e92453"
  },
  {
    "text": "One of the most obvious-but nonetheless vital-domains of the study of machine behaviour concerns the ways in which the introduction of intelligent machines into social systems can alter human beliefs and behaviours. As in the introduction of automation to industrial processes 119 , intelligent machines can create social problems in the process of improving existing problems. Numerous problems and questions arise during this process, such as whether the matching algorithms that are used for online dating alter the distributional outcomes of the dating process or whether news-filtering algorithms alter the distribution of public opinion. It is important to investigate whether small errors in algorithms or the data that they use could compound to produce societywide effects and how intelligent robots in our schools, hospitals 120 and care centres might alter human development 121 and quality of life 54 and potentially affect outcomes for people with disabilities 122 .\nOther questions in this domain relate to the potential for machines to alter the social fabric in more fundamental ways. For example, questions include to what extent and what ways are governments using machine intelligence to alter the nature of democracy, political accountability and transparency, or civic participation. Other questions include to what degree intelligent machines influence policing, surveillance and warfare, as well as how large of an effect bots have had on the outcomes of elections 56 and whether AI systems that aid in the formation of human social relationships can enable collective action.\nNotably, studies in this area also examine how humans perceive the use of machines as decision aids 8,123 , human preferences for and against making use of algorithms 124 , and the degree to which human-like machines produce or reduce discomfort in humans 39,125 . An important question in this area includes how humans respond to the increasing coproduction of economic goods and services in tandem with intelligent machines 126 . Ultimately, understanding how human systems can be altered by the introduction of intelligent machines into our lives is a vital component of the study of machine behaviour.\n\nExamines the potential for intelligent machines to significantly alter human beliefs, behaviors, and social systems, raising questions about impacts on democracy, public opinion, and societal structures.",
    "original_text": "One of the most obvious-but nonetheless vital-domains of the study of machine behaviour concerns the ways in which the introduction of intelligent machines into social systems can alter human beliefs and behaviours. As in the introduction of automation to industrial processes 119 , intelligent machines can create social problems in the process of improving existing problems. Numerous problems and questions arise during this process, such as whether the matching algorithms that are used for online dating alter the distributional outcomes of the dating process or whether news-filtering algorithms alter the distribution of public opinion. It is important to investigate whether small errors in algorithms or the data that they use could compound to produce societywide effects and how intelligent robots in our schools, hospitals 120 and care centres might alter human development 121 and quality of life 54 and potentially affect outcomes for people with disabilities 122 .\nOther questions in this domain relate to the potential for machines to alter the social fabric in more fundamental ways. For example, questions include to what extent and what ways are governments using machine intelligence to alter the nature of democracy, political accountability and transparency, or civic participation. Other questions include to what degree intelligent machines influence policing, surveillance and warfare, as well as how large of an effect bots have had on the outcomes of elections 56 and whether AI systems that aid in the formation of human social relationships can enable collective action.\nNotably, studies in this area also examine how humans perceive the use of machines as decision aids 8,123 , human preferences for and against making use of algorithms 124 , and the degree to which human-like machines produce or reduce discomfort in humans 39,125 . An important question in this area includes how humans respond to the increasing coproduction of economic goods and services in tandem with intelligent machines 126 . Ultimately, understanding how human systems can be altered by the introduction of intelligent machines into our lives is a vital component of the study of machine behaviour.",
    "context": "Examines the potential for intelligent machines to significantly alter human beliefs, behaviors, and social systems, raising questions about impacts on democracy, public opinion, and societal structures.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      7
    ],
    "id": "205cfd834fd391a6a1a3d3783a58cfbbf67e73d96cad005b03ea0a4392c76bfc"
  },
  {
    "text": "Intelligent machines can alter human behaviour, and humans also create, inform and mould the behaviours of intelligent machines. We shape machine behaviours through the direct engineering of AI systems and through the training of these systems on both active human input and passive observations of human behaviours through the data that we create daily. The choice of which algorithms to use, what feedback to provide to those algorithms 3,127 and on which data to train them are also, at present, human decisions and can directly alter machine behaviours. An important component in the study of machine behaviour is to understand how these engineering processes alter the resulting behaviours of AI, whether the training data are responsible for a particular behaviour of the machine, whether it is the algorithm itself or whether it is a combination of both algorithm and data. The framework outlined in Fig. 3 suggests that there will be complementary answers to the each of these questions. Examining how altering the parameters of the engineering process can alter the subsequent behaviours of intelligent machines as they interact with other machines and with humans in natural settings is central to a holistic understanding of machine behaviour.\n\nHighlights the human influence on machine behavior, emphasizing that humans shape AI through engineering choices, training data, and feedback, ultimately impacting the resulting machine behaviors.",
    "original_text": "Intelligent machines can alter human behaviour, and humans also create, inform and mould the behaviours of intelligent machines. We shape machine behaviours through the direct engineering of AI systems and through the training of these systems on both active human input and passive observations of human behaviours through the data that we create daily. The choice of which algorithms to use, what feedback to provide to those algorithms 3,127 and on which data to train them are also, at present, human decisions and can directly alter machine behaviours. An important component in the study of machine behaviour is to understand how these engineering processes alter the resulting behaviours of AI, whether the training data are responsible for a particular behaviour of the machine, whether it is the algorithm itself or whether it is a combination of both algorithm and data. The framework outlined in Fig. 3 suggests that there will be complementary answers to the each of these questions. Examining how altering the parameters of the engineering process can alter the subsequent behaviours of intelligent machines as they interact with other machines and with humans in natural settings is central to a holistic understanding of machine behaviour.",
    "context": "Highlights the human influence on machine behavior, emphasizing that humans shape AI through engineering choices, training data, and feedback, ultimately impacting the resulting machine behaviors.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      7
    ],
    "id": "0ffad6153262ed3da72e9871ed57cea9ab72da645b853e0a9672f3c8d0557aeb"
  },
  {
    "text": "Although it can be methodologically convenient to separate studies into the ways that humans shape machines and vice versa, most AI systems function in domains where they co-exist with humans in complex hybrid systems 39,67,125,128 . Questions of importance to the study of these systems include those that examine the behaviours that characterize human-machine interactions including cooperation, competition and coordination-for example, how human biases combine with AI to alter human emotions or beliefs 14,55,56,129,130 , how human tendencies couple with algorithms to facilitate the spread of information 55 , how traffic patterns can be altered in streets populated by large numbers of both driverless and human-driven cars and how trading patterns can be altered by interactions between humans and algorithmic trading agents 29 as well as which factors can facilitate trust and cooperation between humans and machines 88,131 .\nAnother topic in this area relates to robotic and software-driven automation of human labour 132 . Here we see two different types of machine-human interactions. One is that machines can enhance a human's efficiency, such as in robotic- and computer-aided surgery. Another is that machines can replace humans, such as in driverless transportation and package delivery. This leads to questions about whether machines end up doing more of the replacing or the enhancing in the longer run and what human-machine co-behaviours will evolve as a result.\nThe above examples highlight that many of the questions that relate to hybrid human-machine behaviours must necessarily examine the feedback loops between human influence on machine behaviour and machine influence on human behaviour simultaneously. Scholars have begun to examine human-machine interactions in formal laboratory environments, observing that interactions with simple bots can increase human coordination 39  and that bots can cooperate directly with humans at levels that rival human-human cooperation 133 . However, there remains an urgent need to further understand feedback loops in natural settings, in which humans are increasingly using algorithms to make decisions 134 and subsequently informing the training of the same algorithms through those decisions. Furthermore, across all types of questions in the domain of machine behavioural ecology, there is a need for studies that examine longer-run dynamics of these hybrid systems 53 with particular emphasis on the ways that human social interactions 135,136 may be modified by the introduction of intelligent machines 137 .\n\nExamines human-machine interactions, including cooperation, competition, and coordination; explores how human biases combine with AI to affect emotions and beliefs; investigates how algorithms facilitate information spread; and assesses the long-term evolution of these hybrid systems.",
    "original_text": "Although it can be methodologically convenient to separate studies into the ways that humans shape machines and vice versa, most AI systems function in domains where they co-exist with humans in complex hybrid systems 39,67,125,128 . Questions of importance to the study of these systems include those that examine the behaviours that characterize human-machine interactions including cooperation, competition and coordination-for example, how human biases combine with AI to alter human emotions or beliefs 14,55,56,129,130 , how human tendencies couple with algorithms to facilitate the spread of information 55 , how traffic patterns can be altered in streets populated by large numbers of both driverless and human-driven cars and how trading patterns can be altered by interactions between humans and algorithmic trading agents 29 as well as which factors can facilitate trust and cooperation between humans and machines 88,131 .\nAnother topic in this area relates to robotic and software-driven automation of human labour 132 . Here we see two different types of machine-human interactions. One is that machines can enhance a human's efficiency, such as in robotic- and computer-aided surgery. Another is that machines can replace humans, such as in driverless transportation and package delivery. This leads to questions about whether machines end up doing more of the replacing or the enhancing in the longer run and what human-machine co-behaviours will evolve as a result.\nThe above examples highlight that many of the questions that relate to hybrid human-machine behaviours must necessarily examine the feedback loops between human influence on machine behaviour and machine influence on human behaviour simultaneously. Scholars have begun to examine human-machine interactions in formal laboratory environments, observing that interactions with simple bots can increase human coordination 39  and that bots can cooperate directly with humans at levels that rival human-human cooperation 133 . However, there remains an urgent need to further understand feedback loops in natural settings, in which humans are increasingly using algorithms to make decisions 134 and subsequently informing the training of the same algorithms through those decisions. Furthermore, across all types of questions in the domain of machine behavioural ecology, there is a need for studies that examine longer-run dynamics of these hybrid systems 53 with particular emphasis on the ways that human social interactions 135,136 may be modified by the introduction of intelligent machines 137 .",
    "context": "Examines human-machine interactions, including cooperation, competition, and coordination; explores how human biases combine with AI to affect emotions and beliefs; investigates how algorithms facilitate information spread; and assesses the long-term evolution of these hybrid systems.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      7
    ],
    "id": "0ae195c01a086c06444bba2dbec77f7de5547d4251e6978ad053b3c924018701"
  },
  {
    "text": "Furthering the study of machine behaviour is critical to maximizing the potential benefits of AI for society. The consequential choices that we make regarding the integration of AI agents into human lives must be made with some understanding of the eventual societal implications of these choices. To provide this understanding and anticipation, we need a new interdisciplinary field of scientific study: machine behaviour.\nFor this field to succeed, there are a number of relevant considerations. First, studying machine behaviour does not imply that AI algorithms necessarily have independent agency nor does it imply algorithms should bear moral responsibility for their actions. If a dog bites someone, the dog's owner is held responsible. Nonetheless, it is useful to study the behavioural patterns of animals to predict such aberrant behaviour. Machines operate within a larger socio-technical fabric, and their human stakeholders are ultimately responsible for any harm their deployment might cause.\nSecond, some commentators might suggest that treating AI systems as agents occludes the focus on the underlying data that such AI systems are trained on. Indeed, no behaviour is ever fully separable from the environmental data on which that agent is trained or developed; machine behaviour is no exception. However, it is just as critical to understand how machine behaviours vary with altered environmental inputs as it is to understand how biological agents' behaviours vary depending on the environments in which they exist. As such, scholars of machine behaviour should focus on characterizing agent behaviour across diverse environments, much as behavioural scientists desire to characterize political behaviours across differing demographic and institutional contexts.\nThird, machines exhibit behaviours that are fundamentally different from animals and humans, so we must avoid excessive anthropomorphism and zoomorphism. Even if borrowing existing behavioural scientific methods can prove useful for the study of machines, machines may exhibit forms of intelligence and behaviour that are qualitatively different-even alien-from those seen in biological agents.\nFurthermore, AI scientists can dissect and modify AI systems more easily and more thoroughly than is the case for many living systems. Although parallels exist, the study of AI systems will necessarily differ from the study of living systems.\nFourth, the study of machine behaviour will require cross-disciplinary efforts 82,103 and will entail all of the challenges associated with such research 138,139 . Addressing these challenges is vital 140 . Universities and governmental funding agencies can play an important part in the design of large-scale, neutral and trusted cross-disciplinary studies 141 .\nFifth, the study of machine behaviour will often require experimental intervention to study human-machine interactions in real-world settings 142,143 . These interventions could alter the overall behaviour of the system, possibly having adverse effects on normal users 144 . Ethical considerations such as these need careful oversight and standardized frameworks.\nFinally, studying intelligent algorithmic or robotic systems can result in legal and ethical problems for researchers studying machine behaviour. Reverse-engineering algorithms may require violating the terms of service of some platforms; for example, in setting up fake personas or masking true identities. The creators or maintainers of the systems of interest could embroil researchers in legal challenges if the research damages the reputation of their platforms. Moreover, it remains unclear whether violating terms of service may expose researchers to civil or criminal penalties (for example, through the Computer Fraud and Abuse Act in the United States), which may further discourage this type of research 145 .\nUnderstanding the behaviours and properties of AI agents-and the effects they might have on human systems-is critical. Society can benefit tremendously from the efficiencies and improved decision-making that can come from these agents. At the same time, these benefits may falter without minimizing the potential pitfalls of the incorporation of AI agents into everyday human life.\nReceived: 20 September 2018; Accepted: 14 March 2019; Published online 24 April 2019.\n1. Simon, H. A. The Sciences of the Artificial (MIT Press, Cambridge, 1969). Simon asks whether there can be a science of the 'artificial' that produces knowledge about artificial objects and phenomena .\n2. Milner, R. A modal characterisation of observable machine-behaviour. In Trees in Algebra and Programming, 6th Colloquium 25-34 (Springer, 1981). In this invited lecture, Robin Milner outlines the idea of studying machine behaviour using formal logic .\n3. Thomaz, A. L. & Breazeal, C. Teachable robots: understanding human teaching behavior to build more effective robot learners. Artif. Intell . 172 ,  716-737 (2008).\n4. Stone, P. et al . Artificial Intelligence and Life in 2030. One Hundred Year Study on Artificial Intelligence: Report of the 2015-2016 Study Panel https:// ai100.stanford.edu/2016-report (Stanford University, 2016).\n5. O'Neil, C. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy (Broadway Books, 2016).\nThis book articulates some of the risks posed by the uncritical use of algorithms in society and provides motivation for the study of machine behaviour .\n\nEstablishing a new interdisciplinary field of scientific study – machine behaviour – is critical to maximizing the potential benefits of AI while mitigating societal risks. This field necessitates a focus on characterizing agent behaviour across diverse environments, moving beyond anthropomorphic approaches and acknowledging the responsibility of human stakeholders for the deployment of AI systems.",
    "original_text": "Furthering the study of machine behaviour is critical to maximizing the potential benefits of AI for society. The consequential choices that we make regarding the integration of AI agents into human lives must be made with some understanding of the eventual societal implications of these choices. To provide this understanding and anticipation, we need a new interdisciplinary field of scientific study: machine behaviour.\nFor this field to succeed, there are a number of relevant considerations. First, studying machine behaviour does not imply that AI algorithms necessarily have independent agency nor does it imply algorithms should bear moral responsibility for their actions. If a dog bites someone, the dog's owner is held responsible. Nonetheless, it is useful to study the behavioural patterns of animals to predict such aberrant behaviour. Machines operate within a larger socio-technical fabric, and their human stakeholders are ultimately responsible for any harm their deployment might cause.\nSecond, some commentators might suggest that treating AI systems as agents occludes the focus on the underlying data that such AI systems are trained on. Indeed, no behaviour is ever fully separable from the environmental data on which that agent is trained or developed; machine behaviour is no exception. However, it is just as critical to understand how machine behaviours vary with altered environmental inputs as it is to understand how biological agents' behaviours vary depending on the environments in which they exist. As such, scholars of machine behaviour should focus on characterizing agent behaviour across diverse environments, much as behavioural scientists desire to characterize political behaviours across differing demographic and institutional contexts.\nThird, machines exhibit behaviours that are fundamentally different from animals and humans, so we must avoid excessive anthropomorphism and zoomorphism. Even if borrowing existing behavioural scientific methods can prove useful for the study of machines, machines may exhibit forms of intelligence and behaviour that are qualitatively different-even alien-from those seen in biological agents.\nFurthermore, AI scientists can dissect and modify AI systems more easily and more thoroughly than is the case for many living systems. Although parallels exist, the study of AI systems will necessarily differ from the study of living systems.\nFourth, the study of machine behaviour will require cross-disciplinary efforts 82,103 and will entail all of the challenges associated with such research 138,139 . Addressing these challenges is vital 140 . Universities and governmental funding agencies can play an important part in the design of large-scale, neutral and trusted cross-disciplinary studies 141 .\nFifth, the study of machine behaviour will often require experimental intervention to study human-machine interactions in real-world settings 142,143 . These interventions could alter the overall behaviour of the system, possibly having adverse effects on normal users 144 . Ethical considerations such as these need careful oversight and standardized frameworks.\nFinally, studying intelligent algorithmic or robotic systems can result in legal and ethical problems for researchers studying machine behaviour. Reverse-engineering algorithms may require violating the terms of service of some platforms; for example, in setting up fake personas or masking true identities. The creators or maintainers of the systems of interest could embroil researchers in legal challenges if the research damages the reputation of their platforms. Moreover, it remains unclear whether violating terms of service may expose researchers to civil or criminal penalties (for example, through the Computer Fraud and Abuse Act in the United States), which may further discourage this type of research 145 .\nUnderstanding the behaviours and properties of AI agents-and the effects they might have on human systems-is critical. Society can benefit tremendously from the efficiencies and improved decision-making that can come from these agents. At the same time, these benefits may falter without minimizing the potential pitfalls of the incorporation of AI agents into everyday human life.\nReceived: 20 September 2018; Accepted: 14 March 2019; Published online 24 April 2019.\n1. Simon, H. A. The Sciences of the Artificial (MIT Press, Cambridge, 1969). Simon asks whether there can be a science of the 'artificial' that produces knowledge about artificial objects and phenomena .\n2. Milner, R. A modal characterisation of observable machine-behaviour. In Trees in Algebra and Programming, 6th Colloquium 25-34 (Springer, 1981). In this invited lecture, Robin Milner outlines the idea of studying machine behaviour using formal logic .\n3. Thomaz, A. L. & Breazeal, C. Teachable robots: understanding human teaching behavior to build more effective robot learners. Artif. Intell . 172 ,  716-737 (2008).\n4. Stone, P. et al . Artificial Intelligence and Life in 2030. One Hundred Year Study on Artificial Intelligence: Report of the 2015-2016 Study Panel https:// ai100.stanford.edu/2016-report (Stanford University, 2016).\n5. O'Neil, C. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy (Broadway Books, 2016).\nThis book articulates some of the risks posed by the uncritical use of algorithms in society and provides motivation for the study of machine behaviour .",
    "context": "Establishing a new interdisciplinary field of scientific study – machine behaviour – is critical to maximizing the potential benefits of AI while mitigating societal risks. This field necessitates a focus on characterizing agent behaviour across diverse environments, moving beyond anthropomorphic approaches and acknowledging the responsibility of human stakeholders for the deployment of AI systems.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      8,
      7
    ],
    "id": "744ee7f826648d376c50ab0f00cb5f5c823011e1240dcbccd9941f47aa51ac49"
  },
  {
    "text": "6. Future of Life Institute. Autonomous weapons: an open letter from AI & robotics researchers. https://futureoflife.org/open-letter-autonomousweapons/?cn-reloaded = 1 (2015).\n7. Dressel, J. & Farid, H. The accuracy, fairness, and limits of predicting recidivism. Sci. Adv . 4 , eaao5580 (2018).\n8. Binns, R. et al . 'It's reducing a human being to a percentage': perceptions of justice in algorithmic decisions. In Proc. 2018 CHI Conference on Human Factors in Computing Systems 377 (ACM, 2018).\n9. Hudson, L., Owens, C. S. & Flannes, M. Drone warfare: blowback from the new American way of war. Middle East Policy 18 , 122-132 (2011).\n10. Kahneman, D., Rosenfield, A. M., Gandhi, L. & Blaser, T. Noise: how to overcome the high, hidden cost of inconsistent decision making. Harvard Business Review https://hbr.org/2016/10/noise (2016).\n11. Kleinberg, J., Lakkaraju, H., Leskovec, J., Ludwig, J. & Mullainathan, S. Human decisions and machine predictions. Q. J. Econ . 133 , 237-293 (2018).\n12. Crawford, K. et al. The AI Now report: The Social and Economic Implications of Artificial Intelligence Technologies in the Near-term . https://ainowinstitute.org/ AI_Now_2016_Report.pdf (2016).\n13. Amodei, D. et al. Concrete problems in AI safety. Preprint at https://arxiv.org/ abs/1606.06565 (2016).\n14. Bakshy, E., Messing, S. & Adamic, L. A. Exposure to ideologically diverse news and opinion on Facebook. Science 348 , 1130-1132 (2015).\n15. Bessi, A. & Ferrara, E. Social bots distort the 2016 U.S. Presidential election online discussion. First Monday 21 , 11 (2016).\n16. Ferrara, E., Varol, O., Davis, C., Menczer, F. & Flammini, A. The rise of social bots. Commun. ACM 59 , 96-104 (2016).\n17. Lazer, D. The rise of the social algorithm. Science 348 , 1090-1091 (2015).\n18. Tufekci, Z. Engineering the public: big data, surveillance and computational politics. First Monday 19 , 7 (2014).\n19. Lee, T.-S. & Chen, I.-F. A two-stage hybrid credit scoring model using artificial neural networks and multivariate adaptive regression splines. Expert Syst. Appl . 28 , 743-752 (2005).\n20. Roszbach, K. Bank lending policy, credit scoring, and the survival of loans. Rev. Econ. Stat . 86 , 946-958 (2004).\n21. Huang, C.-L., Chen, M.-C. & Wang, C.-J. Credit scoring with a data mining approach based on support vector machines. Expert Syst. Appl . 33 , 847-856 (2007).\n22. Tsai, C.-F. & Wu, J.-W. Using neural network ensembles for bankruptcy prediction and credit scoring. Expert Syst. Appl . 34 , 2639-2649 (2008).\n23. Chen, L. & Wilson, C. Observing algorithmic marketplaces in-the-wild. SIGecom Exch . 15 , 34-39 (2017).\n24. Chen, L., Mislove, A. & Wilson, C. An empirical analysis of algorithmic pricing on Amazon marketplace. In Proc. 25th International Conference on World Wide Web 1339-1349 (International World Wide Web Conferences Steering Committee, 2016).\n25. Hannák, A. et al. Bias in Online freelance marketplaces: evidence from TaskRabbit and Fiverr. In Proc. ACM Conference on Computer Supported Cooperative Work and Social Computing 1914-1933 (2017).\n26. Cartlidge, J., Szostek, C., De Luca, M. & Cliff, D. Too fast too furious-faster financial-market trading agents can give less efficient markets. In Proc. 4th International Conference on Agents and Artificial Intelligence 126-135 (2012).\n27. Kearns, M., Kulesza, A. & Nevmyvaka, Y. Empirical limitations on highfrequency trading profitability. J. Trading 5 , 50-62 (2010).\n28. Wellman, M. P. & Rajan, U. Ethical issues for autonomous trading agents. Minds Mach . 27 , 609-624 (2017).\n29. Farmer, J. D. & Skouras, S. An ecological perspective on the future of computer trading. Quant. Finance 13 , 325-346 (2013).\n30. Perry, W. L., McInnis, B., Price, C. C., Smith, S. & Hollywood, J. S. Predictive Policing: The Role of Crime Forecasting in Law Enforcement Operations (RAND, 2013).\n31. Bonnefon, J.-F., Shariff, A. & Rahwan, I. The social dilemma of autonomous vehicles. Science 352 , 1573-1576 (2016).\n32. Kooti, F. et al. Analyzing Uber's ride-sharing economy. In Proc. 26th International Conference on World Wide Web 574-582 (International World Wide Web Conferences Steering Committee, 2017).\n33. Zeng, X., Fapojuwo, A. O. & Davies, R. J. Design and performance evaluation of voice activated wireless home devices. IEEE Trans. Consum. Electron . 52 , 983-989 (2006).\n34. Hendriks, B., Meerbeek, B., Boess, S., Pauws, S. & Sonneveld, M. Robot vacuum cleaner personality and behavior. Int. J. Soc. Robot . 3 , 187-195 (2011).\n35. Hitsch, G. J., Hortaçsu, A. & Ariely, D. Matching and sorting in online dating. Am. Econ. Rev . 100 , 130-163 (2010).\n36. Finkel, E. J., Eastwick, P . W., Karney, B. R., Reis, H. T. & Sprecher, S. Online dating: a critical analysis from the perspective of psychological science. Psychol. Sci. Public Interest 13 , 3-66 (2012).\n37. Park, H. W., Rosenberg-Kima, R., Rosenberg, M., Gordon, G. & Breazeal, C. Growing growth mindset with a social robot peer. In Proc. 2017 ACM/IEEE International Conference on Human-Robot Interaction 137-145 (ACM, 2017).\n38. Bemelmans, R., Gelderblom, G. J., Jonker, P. & de Witte, L. Socially assistive robots in elderly care: a systematic review into effects and effectiveness. J. Am. Med. Dir. Assoc . 13 , 114-120 (2012).\n39. Shirado, H. & Christakis, N. A. Locally noisy autonomous agents improve global human coordination in network experiments. Nature 545 , 370-374 (2017). In this human-machine hybrid study, the authors show that simple algorithms injected into human gameplay can improve coordination outcomes among humans .\n40. Pichai, S. AI at Google: Our Principles. Google Blog https://blog.google/topics/ ai/ai-principles/ (2018).\n41. Roff, H. M. The strategic robot problem: lethal autonomous weapons in war. J. Mil. Ethics 13 , 211-227 (2014).\n42. Krishnan, A. Killer Robots: Legality and Ethicality of Autonomous Weapons (Routledge, 2016).\n43. Voosen, P. The AI detectives. Science 357 , 22-27 (2017).\n44. Szegedy, C. et al. Intriguing properties of neural networks. Preprint at https:// arxiv.org/abs/1312.6199 (2013).\n45. Zhang, Q.-S. & Zhu, S.-C. Visual interpretability for deep learning: a survey. Front. Inf. Technol. Electronic Eng . 19 , 27-39 (2018).\n46. Doshi-Velez, F. & Kim, B. Towards a rigorous science of interpretable machine learning. Preprint at https://arxiv.org/abs/1702.08608 (2017).\n\nSummarizes key research areas including the interplay between human behavior and AI, the challenges of algorithmic bias, and the ethical considerations surrounding autonomous systems.",
    "original_text": "6. Future of Life Institute. Autonomous weapons: an open letter from AI & robotics researchers. https://futureoflife.org/open-letter-autonomousweapons/?cn-reloaded = 1 (2015).\n7. Dressel, J. & Farid, H. The accuracy, fairness, and limits of predicting recidivism. Sci. Adv . 4 , eaao5580 (2018).\n8. Binns, R. et al . 'It's reducing a human being to a percentage': perceptions of justice in algorithmic decisions. In Proc. 2018 CHI Conference on Human Factors in Computing Systems 377 (ACM, 2018).\n9. Hudson, L., Owens, C. S. & Flannes, M. Drone warfare: blowback from the new American way of war. Middle East Policy 18 , 122-132 (2011).\n10. Kahneman, D., Rosenfield, A. M., Gandhi, L. & Blaser, T. Noise: how to overcome the high, hidden cost of inconsistent decision making. Harvard Business Review https://hbr.org/2016/10/noise (2016).\n11. Kleinberg, J., Lakkaraju, H., Leskovec, J., Ludwig, J. & Mullainathan, S. Human decisions and machine predictions. Q. J. Econ . 133 , 237-293 (2018).\n12. Crawford, K. et al. The AI Now report: The Social and Economic Implications of Artificial Intelligence Technologies in the Near-term . https://ainowinstitute.org/ AI_Now_2016_Report.pdf (2016).\n13. Amodei, D. et al. Concrete problems in AI safety. Preprint at https://arxiv.org/ abs/1606.06565 (2016).\n14. Bakshy, E., Messing, S. & Adamic, L. A. Exposure to ideologically diverse news and opinion on Facebook. Science 348 , 1130-1132 (2015).\n15. Bessi, A. & Ferrara, E. Social bots distort the 2016 U.S. Presidential election online discussion. First Monday 21 , 11 (2016).\n16. Ferrara, E., Varol, O., Davis, C., Menczer, F. & Flammini, A. The rise of social bots. Commun. ACM 59 , 96-104 (2016).\n17. Lazer, D. The rise of the social algorithm. Science 348 , 1090-1091 (2015).\n18. Tufekci, Z. Engineering the public: big data, surveillance and computational politics. First Monday 19 , 7 (2014).\n19. Lee, T.-S. & Chen, I.-F. A two-stage hybrid credit scoring model using artificial neural networks and multivariate adaptive regression splines. Expert Syst. Appl . 28 , 743-752 (2005).\n20. Roszbach, K. Bank lending policy, credit scoring, and the survival of loans. Rev. Econ. Stat . 86 , 946-958 (2004).\n21. Huang, C.-L., Chen, M.-C. & Wang, C.-J. Credit scoring with a data mining approach based on support vector machines. Expert Syst. Appl . 33 , 847-856 (2007).\n22. Tsai, C.-F. & Wu, J.-W. Using neural network ensembles for bankruptcy prediction and credit scoring. Expert Syst. Appl . 34 , 2639-2649 (2008).\n23. Chen, L. & Wilson, C. Observing algorithmic marketplaces in-the-wild. SIGecom Exch . 15 , 34-39 (2017).\n24. Chen, L., Mislove, A. & Wilson, C. An empirical analysis of algorithmic pricing on Amazon marketplace. In Proc. 25th International Conference on World Wide Web 1339-1349 (International World Wide Web Conferences Steering Committee, 2016).\n25. Hannák, A. et al. Bias in Online freelance marketplaces: evidence from TaskRabbit and Fiverr. In Proc. ACM Conference on Computer Supported Cooperative Work and Social Computing 1914-1933 (2017).\n26. Cartlidge, J., Szostek, C., De Luca, M. & Cliff, D. Too fast too furious-faster financial-market trading agents can give less efficient markets. In Proc. 4th International Conference on Agents and Artificial Intelligence 126-135 (2012).\n27. Kearns, M., Kulesza, A. & Nevmyvaka, Y. Empirical limitations on highfrequency trading profitability. J. Trading 5 , 50-62 (2010).\n28. Wellman, M. P. & Rajan, U. Ethical issues for autonomous trading agents. Minds Mach . 27 , 609-624 (2017).\n29. Farmer, J. D. & Skouras, S. An ecological perspective on the future of computer trading. Quant. Finance 13 , 325-346 (2013).\n30. Perry, W. L., McInnis, B., Price, C. C., Smith, S. & Hollywood, J. S. Predictive Policing: The Role of Crime Forecasting in Law Enforcement Operations (RAND, 2013).\n31. Bonnefon, J.-F., Shariff, A. & Rahwan, I. The social dilemma of autonomous vehicles. Science 352 , 1573-1576 (2016).\n32. Kooti, F. et al. Analyzing Uber's ride-sharing economy. In Proc. 26th International Conference on World Wide Web 574-582 (International World Wide Web Conferences Steering Committee, 2017).\n33. Zeng, X., Fapojuwo, A. O. & Davies, R. J. Design and performance evaluation of voice activated wireless home devices. IEEE Trans. Consum. Electron . 52 , 983-989 (2006).\n34. Hendriks, B., Meerbeek, B., Boess, S., Pauws, S. & Sonneveld, M. Robot vacuum cleaner personality and behavior. Int. J. Soc. Robot . 3 , 187-195 (2011).\n35. Hitsch, G. J., Hortaçsu, A. & Ariely, D. Matching and sorting in online dating. Am. Econ. Rev . 100 , 130-163 (2010).\n36. Finkel, E. J., Eastwick, P . W., Karney, B. R., Reis, H. T. & Sprecher, S. Online dating: a critical analysis from the perspective of psychological science. Psychol. Sci. Public Interest 13 , 3-66 (2012).\n37. Park, H. W., Rosenberg-Kima, R., Rosenberg, M., Gordon, G. & Breazeal, C. Growing growth mindset with a social robot peer. In Proc. 2017 ACM/IEEE International Conference on Human-Robot Interaction 137-145 (ACM, 2017).\n38. Bemelmans, R., Gelderblom, G. J., Jonker, P. & de Witte, L. Socially assistive robots in elderly care: a systematic review into effects and effectiveness. J. Am. Med. Dir. Assoc . 13 , 114-120 (2012).\n39. Shirado, H. & Christakis, N. A. Locally noisy autonomous agents improve global human coordination in network experiments. Nature 545 , 370-374 (2017). In this human-machine hybrid study, the authors show that simple algorithms injected into human gameplay can improve coordination outcomes among humans .\n40. Pichai, S. AI at Google: Our Principles. Google Blog https://blog.google/topics/ ai/ai-principles/ (2018).\n41. Roff, H. M. The strategic robot problem: lethal autonomous weapons in war. J. Mil. Ethics 13 , 211-227 (2014).\n42. Krishnan, A. Killer Robots: Legality and Ethicality of Autonomous Weapons (Routledge, 2016).\n43. Voosen, P. The AI detectives. Science 357 , 22-27 (2017).\n44. Szegedy, C. et al. Intriguing properties of neural networks. Preprint at https:// arxiv.org/abs/1312.6199 (2013).\n45. Zhang, Q.-S. & Zhu, S.-C. Visual interpretability for deep learning: a survey. Front. Inf. Technol. Electronic Eng . 19 , 27-39 (2018).\n46. Doshi-Velez, F. & Kim, B. Towards a rigorous science of interpretable machine learning. Preprint at https://arxiv.org/abs/1702.08608 (2017).",
    "context": "Summarizes key research areas including the interplay between human behavior and AI, the challenges of algorithmic bias, and the ethical considerations surrounding autonomous systems.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      8
    ],
    "id": "783a5ef01e5b3e6f613f40ca634e48594610adae975efce4b63070c9ea8bb63c"
  },
  {
    "text": "47. Gebru, T. et al. Datasheets for datasets. Preprint at https://arxiv.org/ abs/1803.09010 (2018).\n48. Mitchell, M. et al. Model cards for model reporting. Preprint at https:// arxiv.org/abs/1810.03993 (2018).\n49. Lakkaraju, H., Kamar, E., Caruana, R. & Horvitz, E. Identifying unknown unknowns in the open world: representations and policies for guided exploration. In Proc. 31st Association for the Advancement of Artificial Intelligence Conference on Artificial Intelligence 2 (2017).\n50. Johnson, N. et al. Abrupt rise of new machine ecology beyond human response time. Sci. Rep . 3 , 2627 (2013).\n51. Appel, K., Haken, W. & Koch, J. Every planar map is four colorable. Part II: reducibility. Illinois J. Math . 21 , 491-567 (1977).\n52. Appel, K. & Haken, W. Every planar map is four colorable. Part I: discharging. Illinois J. Math . 21 , 429-490 (1977).\n53. Westlund, J. M. K., Park, H. W., Williams, R. & Breazeal, C. Measuring young children's long-term relationships with social robots. In Proc. 17th ACM Conference on Interaction Design and Children 207-218 (ACM, 2018).\n54. Lorenz, T., Weiss, A. & Hirche, S. Synchrony and reciprocity: key mechanisms for social companion robots in therapy and care. Int. J. Soc. Robot . 8 , 125-143 (2016).\n55. Vosoughi, S., Roy, D. & Aral, S. The spread of true and false news online. Science 359 , 1146-1151 (2018). This study examines the complex hybrid ecology of bots and humans on Twitter and finds that humans spread false information at higher rates than bots .\n56.\nLazer, D. M. J. et al. The science of fake news.\nScience\n359\n, 1094-1096 (2018).\n\nProvides guidelines for documenting datasets and model cards to promote transparency and accountability in AI development, highlighting the importance of understanding how data influences model behavior and addressing potential biases.",
    "original_text": "47. Gebru, T. et al. Datasheets for datasets. Preprint at https://arxiv.org/ abs/1803.09010 (2018).\n48. Mitchell, M. et al. Model cards for model reporting. Preprint at https:// arxiv.org/abs/1810.03993 (2018).\n49. Lakkaraju, H., Kamar, E., Caruana, R. & Horvitz, E. Identifying unknown unknowns in the open world: representations and policies for guided exploration. In Proc. 31st Association for the Advancement of Artificial Intelligence Conference on Artificial Intelligence 2 (2017).\n50. Johnson, N. et al. Abrupt rise of new machine ecology beyond human response time. Sci. Rep . 3 , 2627 (2013).\n51. Appel, K., Haken, W. & Koch, J. Every planar map is four colorable. Part II: reducibility. Illinois J. Math . 21 , 491-567 (1977).\n52. Appel, K. & Haken, W. Every planar map is four colorable. Part I: discharging. Illinois J. Math . 21 , 429-490 (1977).\n53. Westlund, J. M. K., Park, H. W., Williams, R. & Breazeal, C. Measuring young children's long-term relationships with social robots. In Proc. 17th ACM Conference on Interaction Design and Children 207-218 (ACM, 2018).\n54. Lorenz, T., Weiss, A. & Hirche, S. Synchrony and reciprocity: key mechanisms for social companion robots in therapy and care. Int. J. Soc. Robot . 8 , 125-143 (2016).\n55. Vosoughi, S., Roy, D. & Aral, S. The spread of true and false news online. Science 359 , 1146-1151 (2018). This study examines the complex hybrid ecology of bots and humans on Twitter and finds that humans spread false information at higher rates than bots .\n56.\nLazer, D. M. J. et al. The science of fake news.\nScience\n359\n, 1094-1096 (2018).",
    "context": "Provides guidelines for documenting datasets and model cards to promote transparency and accountability in AI development, highlighting the importance of understanding how data influences model behavior and addressing potential biases.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      8,
      9
    ],
    "id": "3b4a336bc9861adb8a442ef208269c16865d957e3ef5b5668e1b8377db0aaaef"
  },
  {
    "text": "57. Roberts, M. E. Censored: Distraction and Diversion Inside China's Great Firewall (Princeton Univ. Press, 2018).\n58. Corbett-Davies, S., Pierson, E., Feller, A., Goel, S. & Huq, A. Algorithmic decision making and the cost of fairness. In Proc. 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 797-806 (ACM, 2017).\n59. Kleinberg, J., Mullainathan, S. & Raghavan, M. Inherent trade-offs in the fair determination of risk scores. Preprint at https://arxiv.org/abs/1609.05807 (2016).\n60. Buolamwini, J. & Gebru, T. Gender shades: intersectional accuracy disparities in commercial gender classification. In Proc. 1st Conference on Fairness, Accountability and Transparency (eds Friedler, S. A. & Wilson, C.) 81 , 77-91 (PMLR, 2018).\n61. Bolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V. & Kalai, A. T. Man is to computer programmer as woman is to homemaker? Debiasing word embeddings. In Proc. Advances in Neural Information Processing Systems 4349-4357 (2016).\n62. Caliskan, A., Bryson, J. J. & Narayanan, A. Semantics derived automatically from language corpora contain human-like biases. Science 356 , 183-186 (2017).\n63. Sweeney, L. Discrimination in online ad delivery. Queueing Syst . 11 , 10 (2013).\n64. Ensign, D., Friedler, S. A., Neville, S., Scheidegger, C. & Venkatasubramanian, S. Runaway feedback loops in predictive policing. Preprint at https://arxiv.org/ abs/1706.09847 (2017).\n65. Angwin, J., Larson, J., Mattu, S. & Kirchner, L. Machine bias. ProPublica https:// www.propublica.org/article/machine-bias-risk-assessments-in-criminalsentencing (2016).\n66. Chouldechova, A., Benavides-Prado, D., Fialko, O. & Vaithianathan, R. A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions. In Proc. 1st Conference on Fairness, Accountability and Transparency (eds Friedler, S. A. & Wilson, C.) 81 , 134-148 (PMLR, 2018).\n67. Jennings, N. R. et al. Human-agent collectives. Commun. ACM 57 , 80-88 (2014).\n68. Campbell, M., Hoane, A. J. & Hsu, F.-H. Deep blue. Artif. Intell . 134 , 57-83 (2002).\n69. Schaeffer, J. et al. Checkers is solved. Science 317 , 1518-1522 (2007).\n70. Silver, D. et al. Mastering the game of Go with deep neural networks and tree search. Nature 529 , 484-489 (2016).\n71. Silver, D. et al. Mastering the game of Go without human knowledge. Nature 550 , 354-359 (2017).\n72. Bowling, M., Burch, N., Johanson, M. & Tammelin, O. Heads-up limit hold'em poker is solved. Science 347 , 145-149 (2015).\n73. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning environment: an evaluation platform for general agents. J. Artif. Intell. Res . 47 , 253-279 (2013).\n74. Wellman, M. P. et al. Designing the market game for a trading agent competition. IEEE Internet Comput . 5 , 43-51 (2001).\n75. Kitano, H., Asada, M., Kuniyoshi, Y., Noda, I. & Osawa, E. RoboCup: the robot world cup initiative. In Proc. 1st International Conference on Autonomous Agents 340-347 (ACM, 1997).\n76. Russakovsky, O. et al. ImageNet large scale visual recognition challenge. Int. J. Comput. Vis . 115 , 211-252 (2015).\n77. Lin, T.-Y. et al. Microsoft COCO: common objects in context. In Proc. European Conference on Computer Vision (eds Fleet, D. et al.) 8693, 740-755 (Springer International Publishing, 2014).\n78. Davis, J. & Goadrich, M. The relationship between precision-recall and ROC curves. In Proc. 23rd International Conference on Machine Learning 233-240 (ACM, 2006).\n79. van de Sande, K. E. A., Gevers, T. & Snoek, C. G. M. Evaluating color descriptors for object and scene recognition. IEEE Trans. Pattern Anal. Mach. Intell . 32 , 1582-1596 (2010).\n80. Papineni, K., Roukos, S., Ward, T. & Zhu, W.-J. BLEU: a method for automatic evaluation of machine translation. In Proc. 40th Annual Meeting on Association for Computational Linguistics 311-318 (Association for Computational Linguistics, 2002).\n81. Zhou, Z., Zhang, W. & Wang, J. Inception score, label smoothing, gradient vanishing and -log(D(x)) alternative. Preprint at https://arxiv.org/ abs/1708.01729 (2017).\n82. Epstein, Z. et al. Closing the AI knowledge gap. Preprint at https://arxiv.org/ abs/1803.07233 (2018).\n83. Tinbergen, N. On aims and methods of ethology. Ethology 20 , 410-433 (1963).\n84. Nesse, R. M. Tinbergen's four questions, organized: a response to Bateson and Laland. Trends Ecol. Evol . 28 , 681-682 (2013).\n85. Das, R., Hanson, J. E., Kephart, J. O. & Tesauro, G. Agent-human interactions in the continuous double auction. In Proc. 17th International Joint Conference on Artificial Intelligence 1169-1178 (Lawrence Erlbaum, 2001).\n86. Deng, Y., Bao, F., Kong, Y., Ren, Z. & Dai, Q. Deep direct reinforcement learning for financial signal representation and trading. IEEE Trans. Neural Netw. Learn. Syst . 28 , 653-664 (2017).\n87. Galceran, E., Cunningham, A. G., Eustice, R. M. & Olson, E. Multipolicy decision-making for autonomous driving via changepoint-based behavior prediction: theory and experiment. Auton. Robots 41 , 1367-1382 (2017).\n88. Ribeiro, M. T., Singh, S. & Guestrin, C. Why should I trust you? Explaining the predictions of any classifier. In Proc. 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 1135-1144 (ACM, 2016).\n89. Smilkov, D., Thorat, N., Kim, B., Viégas, F. & Wattenberg, M. SmoothGrad: removing noise by adding noise. Preprint at https://arxiv.org/ abs/1706.03825 (2017).\n90. Nevmyvaka, Y., Feng, Y. & Kearns, M. reinforcement learning for optimized trade execution. In Proc. 23rd International Conference on Machine Learning 673-680 (ACM, 2006).\n91. Nguyen, T. T., Hui, P .-M., Harper, F. M., Terveen, L. & Konstan, J. A. Exploring the filter bubble: the effect of using recommender systems on content diversity. In Proc. 23rd International Conference on World Wide Web 677-686 (ACM, 2014).\n92. Dalvi, N. & Domingos, P. Mausam, Sanghai, S. & Verma, D. Adversarial classification. In Proc. Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 99-108 (ACM, 2004).\n93. Globerson, A. & Roweis, S. Nightmare at test time: robust learning by feature deletion. In Proc. 23rd International Conference on Machine Learning 353-360 (ACM, 2006).\n\nProvides a collection of research papers exploring algorithmic bias, fairness, and the potential for manipulation in online systems.",
    "original_text": "57. Roberts, M. E. Censored: Distraction and Diversion Inside China's Great Firewall (Princeton Univ. Press, 2018).\n58. Corbett-Davies, S., Pierson, E., Feller, A., Goel, S. & Huq, A. Algorithmic decision making and the cost of fairness. In Proc. 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 797-806 (ACM, 2017).\n59. Kleinberg, J., Mullainathan, S. & Raghavan, M. Inherent trade-offs in the fair determination of risk scores. Preprint at https://arxiv.org/abs/1609.05807 (2016).\n60. Buolamwini, J. & Gebru, T. Gender shades: intersectional accuracy disparities in commercial gender classification. In Proc. 1st Conference on Fairness, Accountability and Transparency (eds Friedler, S. A. & Wilson, C.) 81 , 77-91 (PMLR, 2018).\n61. Bolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V. & Kalai, A. T. Man is to computer programmer as woman is to homemaker? Debiasing word embeddings. In Proc. Advances in Neural Information Processing Systems 4349-4357 (2016).\n62. Caliskan, A., Bryson, J. J. & Narayanan, A. Semantics derived automatically from language corpora contain human-like biases. Science 356 , 183-186 (2017).\n63. Sweeney, L. Discrimination in online ad delivery. Queueing Syst . 11 , 10 (2013).\n64. Ensign, D., Friedler, S. A., Neville, S., Scheidegger, C. & Venkatasubramanian, S. Runaway feedback loops in predictive policing. Preprint at https://arxiv.org/ abs/1706.09847 (2017).\n65. Angwin, J., Larson, J., Mattu, S. & Kirchner, L. Machine bias. ProPublica https:// www.propublica.org/article/machine-bias-risk-assessments-in-criminalsentencing (2016).\n66. Chouldechova, A., Benavides-Prado, D., Fialko, O. & Vaithianathan, R. A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions. In Proc. 1st Conference on Fairness, Accountability and Transparency (eds Friedler, S. A. & Wilson, C.) 81 , 134-148 (PMLR, 2018).\n67. Jennings, N. R. et al. Human-agent collectives. Commun. ACM 57 , 80-88 (2014).\n68. Campbell, M., Hoane, A. J. & Hsu, F.-H. Deep blue. Artif. Intell . 134 , 57-83 (2002).\n69. Schaeffer, J. et al. Checkers is solved. Science 317 , 1518-1522 (2007).\n70. Silver, D. et al. Mastering the game of Go with deep neural networks and tree search. Nature 529 , 484-489 (2016).\n71. Silver, D. et al. Mastering the game of Go without human knowledge. Nature 550 , 354-359 (2017).\n72. Bowling, M., Burch, N., Johanson, M. & Tammelin, O. Heads-up limit hold'em poker is solved. Science 347 , 145-149 (2015).\n73. Bellemare, M. G., Naddaf, Y., Veness, J. & Bowling, M. The arcade learning environment: an evaluation platform for general agents. J. Artif. Intell. Res . 47 , 253-279 (2013).\n74. Wellman, M. P. et al. Designing the market game for a trading agent competition. IEEE Internet Comput . 5 , 43-51 (2001).\n75. Kitano, H., Asada, M., Kuniyoshi, Y., Noda, I. & Osawa, E. RoboCup: the robot world cup initiative. In Proc. 1st International Conference on Autonomous Agents 340-347 (ACM, 1997).\n76. Russakovsky, O. et al. ImageNet large scale visual recognition challenge. Int. J. Comput. Vis . 115 , 211-252 (2015).\n77. Lin, T.-Y. et al. Microsoft COCO: common objects in context. In Proc. European Conference on Computer Vision (eds Fleet, D. et al.) 8693, 740-755 (Springer International Publishing, 2014).\n78. Davis, J. & Goadrich, M. The relationship between precision-recall and ROC curves. In Proc. 23rd International Conference on Machine Learning 233-240 (ACM, 2006).\n79. van de Sande, K. E. A., Gevers, T. & Snoek, C. G. M. Evaluating color descriptors for object and scene recognition. IEEE Trans. Pattern Anal. Mach. Intell . 32 , 1582-1596 (2010).\n80. Papineni, K., Roukos, S., Ward, T. & Zhu, W.-J. BLEU: a method for automatic evaluation of machine translation. In Proc. 40th Annual Meeting on Association for Computational Linguistics 311-318 (Association for Computational Linguistics, 2002).\n81. Zhou, Z., Zhang, W. & Wang, J. Inception score, label smoothing, gradient vanishing and -log(D(x)) alternative. Preprint at https://arxiv.org/ abs/1708.01729 (2017).\n82. Epstein, Z. et al. Closing the AI knowledge gap. Preprint at https://arxiv.org/ abs/1803.07233 (2018).\n83. Tinbergen, N. On aims and methods of ethology. Ethology 20 , 410-433 (1963).\n84. Nesse, R. M. Tinbergen's four questions, organized: a response to Bateson and Laland. Trends Ecol. Evol . 28 , 681-682 (2013).\n85. Das, R., Hanson, J. E., Kephart, J. O. & Tesauro, G. Agent-human interactions in the continuous double auction. In Proc. 17th International Joint Conference on Artificial Intelligence 1169-1178 (Lawrence Erlbaum, 2001).\n86. Deng, Y., Bao, F., Kong, Y., Ren, Z. & Dai, Q. Deep direct reinforcement learning for financial signal representation and trading. IEEE Trans. Neural Netw. Learn. Syst . 28 , 653-664 (2017).\n87. Galceran, E., Cunningham, A. G., Eustice, R. M. & Olson, E. Multipolicy decision-making for autonomous driving via changepoint-based behavior prediction: theory and experiment. Auton. Robots 41 , 1367-1382 (2017).\n88. Ribeiro, M. T., Singh, S. & Guestrin, C. Why should I trust you? Explaining the predictions of any classifier. In Proc. 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 1135-1144 (ACM, 2016).\n89. Smilkov, D., Thorat, N., Kim, B., Viégas, F. & Wattenberg, M. SmoothGrad: removing noise by adding noise. Preprint at https://arxiv.org/ abs/1706.03825 (2017).\n90. Nevmyvaka, Y., Feng, Y. & Kearns, M. reinforcement learning for optimized trade execution. In Proc. 23rd International Conference on Machine Learning 673-680 (ACM, 2006).\n91. Nguyen, T. T., Hui, P .-M., Harper, F. M., Terveen, L. & Konstan, J. A. Exploring the filter bubble: the effect of using recommender systems on content diversity. In Proc. 23rd International Conference on World Wide Web 677-686 (ACM, 2014).\n92. Dalvi, N. & Domingos, P. Mausam, Sanghai, S. & Verma, D. Adversarial classification. In Proc. Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 99-108 (ACM, 2004).\n93. Globerson, A. & Roweis, S. Nightmare at test time: robust learning by feature deletion. In Proc. 23rd International Conference on Machine Learning 353-360 (ACM, 2006).",
    "context": "Provides a collection of research papers exploring algorithmic bias, fairness, and the potential for manipulation in online systems.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      9
    ],
    "id": "be56e3af31ad26f9ce0b9b11b2ffa95330f0caa40a75bf1c623031281285c153"
  },
  {
    "text": "94. Biggio, B. et al. Evasion attacks against machine learning at test time. In Proc. Joint European Conference on Machine Learning and Knowledge Discovery in Databases 387-402 (Springer, 2013).\n95. Tramèr, F. et al. Ensemble adversarial training: attacks and defenses. Preprint at https://arxiv.org/abs/1705.07204 (2017).\n96. Parkes, D. C. & Wellman, M. P. Economic reasoning and artificial intelligence. Science 349 , 267-272 (2015).\n97. Wagner, A. Robustness and Evolvability in Living Systems (Princeton Univ. Press, 2013).\n98. Edwards, H. & Storkey, A. Censoring representations with an adversary. Preprint at https://arxiv.org/abs/1511.05897 (2015).\n99. Zemel, R., Wu, Y., Swersky, K., Pitassi, T. & Dwork, C. learning fair representations. In Proc. International Conference on Machine Learning 325-333 (2013).\n100.  Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C. & Venkatasubramanian, S. Certifying and removing disparate impact. In Proc. 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 259-268 (ACM, 2015).\n101.  Cully, A., Clune, J., T arapore, D. & Mouret, J.-B. Robots that can adapt like animals. Nature 521 , 503-507 (2015).\nThis study characterizes a robot driven by an adaptive algorithm that mimics\n.\nthe adaptation and behaviours of animals\n102.  Bongard, J., Zykov, V. & Lipson, H. Resilient machines through continuous self-modeling. Science 314 , 1118-1121 (2006).\n103.  Leibo, J. Z. et al. Psychlab: a psychology laboratory for deep reinforcement learning agents. Preprint at https://arxiv.org/abs/1801.08116 (2018). In this study, the authors use behavioural tools from the life sciences in the study of machine behaviours .\n104.  Subrahmanian, V. S. et al. The DARPA Twitter bot challenge. Preprint at https://arxiv.org/abs/1601.05140 (2016).\n105.  Carrascosa, J. M., Mikians, J., Cuevas, R., Erramilli, V. & Laoutaris, N. I. Always feel like somebody's watching me: measuring online behavioural advertising. In Proc. 11th ACM Conference on Emerging Networking Experiments and Technologies 13 (ACM, 2015).\n106.  Datta, A., Tschantz, M. C. & Datta, A. Automated Experiments on Ad Privacy Settings. Proc. Privacy Enhancing Technologies 2015 , 92-112 (2015).\n107.  Giusti, A. et al. A machine learning approach to visual perception of forest trails for mobile robots . IEEE Robot. Autom. Lett . 1 , 661-667 (2016).\n108.  Berdahl, A., Torney, C. J., Ioannou, C. C., Faria, J. J. & Couzin, I. D. Emergent sensing of complex environments by mobile animal groups. Science 339 , 574-576 (2013).\n109.  Couzin, I. D. et al. Uninformed individuals promote democratic consensus in animal groups. Science 334 , 1578-1580 (2011).\n110.  Rubenstein, M., Cornejo, A. & Nagpal, R. Programmable self-assembly in a thousand-robot swarm. Science 345 , 795-799 (2014).\n111.  Kernbach, S., Thenius, R., Kernbach, O. & Schmickl, T. Re-embodiment of honeybee aggregation behavior in an artificial micro-robotic system. Adapt. Behav . 17 , 237-259 (2009).\n112.  Bak, P ., Chen, K. & Creutz, M. Self-organized criticality in the 'Game of Life'. Nature 342 , 780-782 (1989).\n113.  Tsvetkova, M., García-Gavilanes, R., Floridi, L. & Yasseri, T. Even good bots fight: the case of Wikipedia. PLoS ONE 12 , e0171774 (2017).\n114.  Lazaridou, A., Peysakhovich, A. & Baroni, M. Multi-agent cooperation and the emergence of (natural) language. Preprint at https://arxiv.org/ abs/1612.07182 (2016).\n115.  Budish, E., Cramton, P . & Shim, J. The high-frequency trading arms race: frequent batch auctions as a market design response. Q. J. Econ . 130 , 1547-1621 (2015).\n116.  Kirilenko, A. A. & Lo, A. W. Moore's law versus Murphy's law: algorithmic trading and its discontents. J. Econ. Perspect . 27 , 51-72 (2013).\n117.  Menkveld, A. J. The economics of high-frequency trading: taking stock. Annu. Rev. Financ. Econ . 8 , 1-24 (2016).\n118.  Mønsted, B., Sapie ż y ń ski, P ., Ferrara, E. & Lehmann, S. Evidence of complex contagion of information in social media: an experiment using Twitter bots. PLoS ONE 12 , e0184148 (2017).\nThis study presents an experimental intervention on Twitter using bots and provides evidence that information diffusion is most accurately described by complex contagion .\n\nSummarizes research on adversarial machine learning attacks and defenses, alongside studies exploring the impact of bots and social contagion in online information diffusion.",
    "original_text": "94. Biggio, B. et al. Evasion attacks against machine learning at test time. In Proc. Joint European Conference on Machine Learning and Knowledge Discovery in Databases 387-402 (Springer, 2013).\n95. Tramèr, F. et al. Ensemble adversarial training: attacks and defenses. Preprint at https://arxiv.org/abs/1705.07204 (2017).\n96. Parkes, D. C. & Wellman, M. P. Economic reasoning and artificial intelligence. Science 349 , 267-272 (2015).\n97. Wagner, A. Robustness and Evolvability in Living Systems (Princeton Univ. Press, 2013).\n98. Edwards, H. & Storkey, A. Censoring representations with an adversary. Preprint at https://arxiv.org/abs/1511.05897 (2015).\n99. Zemel, R., Wu, Y., Swersky, K., Pitassi, T. & Dwork, C. learning fair representations. In Proc. International Conference on Machine Learning 325-333 (2013).\n100.  Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C. & Venkatasubramanian, S. Certifying and removing disparate impact. In Proc. 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 259-268 (ACM, 2015).\n101.  Cully, A., Clune, J., T arapore, D. & Mouret, J.-B. Robots that can adapt like animals. Nature 521 , 503-507 (2015).\nThis study characterizes a robot driven by an adaptive algorithm that mimics\n.\nthe adaptation and behaviours of animals\n102.  Bongard, J., Zykov, V. & Lipson, H. Resilient machines through continuous self-modeling. Science 314 , 1118-1121 (2006).\n103.  Leibo, J. Z. et al. Psychlab: a psychology laboratory for deep reinforcement learning agents. Preprint at https://arxiv.org/abs/1801.08116 (2018). In this study, the authors use behavioural tools from the life sciences in the study of machine behaviours .\n104.  Subrahmanian, V. S. et al. The DARPA Twitter bot challenge. Preprint at https://arxiv.org/abs/1601.05140 (2016).\n105.  Carrascosa, J. M., Mikians, J., Cuevas, R., Erramilli, V. & Laoutaris, N. I. Always feel like somebody's watching me: measuring online behavioural advertising. In Proc. 11th ACM Conference on Emerging Networking Experiments and Technologies 13 (ACM, 2015).\n106.  Datta, A., Tschantz, M. C. & Datta, A. Automated Experiments on Ad Privacy Settings. Proc. Privacy Enhancing Technologies 2015 , 92-112 (2015).\n107.  Giusti, A. et al. A machine learning approach to visual perception of forest trails for mobile robots . IEEE Robot. Autom. Lett . 1 , 661-667 (2016).\n108.  Berdahl, A., Torney, C. J., Ioannou, C. C., Faria, J. J. & Couzin, I. D. Emergent sensing of complex environments by mobile animal groups. Science 339 , 574-576 (2013).\n109.  Couzin, I. D. et al. Uninformed individuals promote democratic consensus in animal groups. Science 334 , 1578-1580 (2011).\n110.  Rubenstein, M., Cornejo, A. & Nagpal, R. Programmable self-assembly in a thousand-robot swarm. Science 345 , 795-799 (2014).\n111.  Kernbach, S., Thenius, R., Kernbach, O. & Schmickl, T. Re-embodiment of honeybee aggregation behavior in an artificial micro-robotic system. Adapt. Behav . 17 , 237-259 (2009).\n112.  Bak, P ., Chen, K. & Creutz, M. Self-organized criticality in the 'Game of Life'. Nature 342 , 780-782 (1989).\n113.  Tsvetkova, M., García-Gavilanes, R., Floridi, L. & Yasseri, T. Even good bots fight: the case of Wikipedia. PLoS ONE 12 , e0171774 (2017).\n114.  Lazaridou, A., Peysakhovich, A. & Baroni, M. Multi-agent cooperation and the emergence of (natural) language. Preprint at https://arxiv.org/ abs/1612.07182 (2016).\n115.  Budish, E., Cramton, P . & Shim, J. The high-frequency trading arms race: frequent batch auctions as a market design response. Q. J. Econ . 130 , 1547-1621 (2015).\n116.  Kirilenko, A. A. & Lo, A. W. Moore's law versus Murphy's law: algorithmic trading and its discontents. J. Econ. Perspect . 27 , 51-72 (2013).\n117.  Menkveld, A. J. The economics of high-frequency trading: taking stock. Annu. Rev. Financ. Econ . 8 , 1-24 (2016).\n118.  Mønsted, B., Sapie ż y ń ski, P ., Ferrara, E. & Lehmann, S. Evidence of complex contagion of information in social media: an experiment using Twitter bots. PLoS ONE 12 , e0184148 (2017).\nThis study presents an experimental intervention on Twitter using bots and provides evidence that information diffusion is most accurately described by complex contagion .",
    "context": "Summarizes research on adversarial machine learning attacks and defenses, alongside studies exploring the impact of bots and social contagion in online information diffusion.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      9,
      10
    ],
    "id": "0eab8b864502ba21132299565390aa5531a13ebc407d39a8f04bf9606364bf00"
  },
  {
    "text": "119.  Bainbridge, L. Ironies of automation. Automatica 19 , 775-779 (1983).\n120.  Jeong, S., Breazeal, C., Logan, D. & Weinstock, P . Huggable: the impact of embodiment on promoting socio-emotional interactions for young pediatric inpatients. In Proc. 2018 CHI Conference on Human Factors in Computing Systems 495 (ACM, 2018).\n121.  Kory Westlund, J. M. et al. Flat vs. expressive storytelling: young children's learning and retention of a social robot's narrative. Front. Hum. Neurosci . 11 , 295 (2017).\n122.  Salisbury, E., Kamar, E. & Morris, M. R. Toward scalable social alt text: conversational crowdsourcing as a tool for refining vision-to-language technology for the blind. Proc. 5th AAAI Conference on Human Computation and Crowdsourcing (2017).\n123.  Awad, E. et al. The Moral Machine experiment. Nature 563 , 59-64 (2018).\n124.  Dietvorst, B. J., Simmons, J. P . & Massey, C. Algorithm aversion: people erroneously avoid algorithms after seeing them err. J. Exp. Psychol. Gen . 144 , 114-126 (2015).\n125.  Gray, K. & Wegner, D. M. Feeling robots and human zombies: mind perception and the uncanny valley. Cognition 125 , 125-130 (2012).\n126.  Brynjolfsson, E. & Mitchell, T. What can machine learning do? Workforce implications. Science 358 , 1530-1534 (2017).\n127.  Christiano, P . F . et al. Deep reinforcement learning from human preferences. In Proc. Advances in Neural Information Processing Systems 30 (eds Guyon, I. et al.) 4299-4307 (Curran Associates, 2017).\n128.  Tsvetkova, M. et al. Understanding human-machine networks: a crossdisciplinary survey. ACM Comput. Surv . 50 , 12:1-12:35 (2017).\n129.  Hilbert, M., Ahmed, S., Cho, J., Liu, B. & Luu, J. Communicating with algorithms: a transfer entropy analysis of emotions-based escapes from online echo chambers. Commun. Methods Meas . 12 , 260-275 (2018).\n130.  Kramer, A. D. I., Guillory, J. E. & Hancock, J. T. Experimental evidence of massive-scale emotional contagion through social networks. Proc. Natl Acad. Sci. USA 111 , 8788-8790 (2014).\n131.  Kamar, E., Hacker, S. & Horvitz, E. Combining human and machine intelligence in large-scale crowdsourcing. in Proc. 11th International Conference on Autonomous Agents and Multiagent Systems 467-474 (International Foundation for Autonomous Agents and Multiagent Systems, 2012).\n132.  Jackson, M. The Human Network: How Your Social Position Determines Your Power, Beliefs, and Behaviors (Knopf Doubleday, 2019).\n133.  Crandall, J. W. et al. Cooperating with machines. Nat. Commun . 9 , 233 (2018). This study examines algorithmic cooperation with humans and provides an example of methods that can be used to study the behaviour of humanmachine hybrid systems .\n134.  Wang, D., Khosla, A., Gargeya, R., Irshad, H. & Beck, A. H. Deep learning for identifying metastatic breast cancer. Preprint at https://arxiv.org/ abs/1606.05718 (2016).\n135.  Pentland, A. Social Physics: How Social Networks Can Make Us Smarter (Penguin, 2015).\n136.  Lazer, D. et al. Computational social science. Science 323 , 721-723 (2009).\n137.  Aharony, N., Pan, W., Ip, C., Khayal, I. & Pentland, A. Social fMRI: investigating and shaping social mechanisms in the real world. Pervasive Mobile Comput . 7 , 643-659 (2011).\n138.  Ledford, H. How to solve the world's biggest problems. Nature 525 , 308-311 (2015).\n139.  Bromham, L., Dinnage, R. & Hua, X. Interdisciplinary research has consistently lower funding success. Nature 534 , 684-687 (2016).\n140.  Kleinberg, J. & Oren, S. Mechanisms for (mis)allocating scientific credit. In Proc. 43rd Annual ACM Symposium on Theory of Computing 529-538 (ACM, 2011).\n141.  Kannel, W. B. & McGee, D. L. Diabetes and cardiovascular disease. The Framingham study. J. Am. Med. Assoc . 241 , 2035-2038 (1979).\n142.  Krafft, P . M., Macy, M. & Pentland, A. Bots as virtual confederates: design and ethics. In Proc. 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing 183-190 (ACM, 2017).\n143.  Meyer, M. N. Two cheers for corporate experimentation: The A/B illusion and the virtues of data-driven innovation. Colorado Technol. Law J . 13 , 273 (2015).\n144.  Xing, X. et al. T ake this personally: pollution attacks on personalized services. In Proc. 22nd USENIX Security Symposium 671-686 (2013).\n145.  Patel, K. T esting the limits of the First Amendment: how a CFAA prohibition on online antidiscrimination testing infringes on protected speech activity. Columbia Law Rev . https://doi.org/10.2139/ssrn.3046847 (2017).\nAcknowledgements I.R. received funding from the Ethics & Governance of Artificial Intelligence Fund; J.B. received funding from NSF awards INSPIRE-1344227 and BIGDATA-1447634, DARPA's Lifelong Learning Machines program, ARO contract W911NF-16-1-0304; J.-F.B. from the ANRLabex IAST; N.A.C. a Pioneer Grant from the Robert Wood Johnson Foundation; I.D.C. received funding from the NSF (IOS-1355061), the ONR (N0001409-1-1074 and N00014-14-1-0635), the ARO (W911NG-11-1-0385 and W911NF14-1-0431), the Struktur- und Innovationsfunds für die Forschung of the State of Baden-Württemberg, the Max Planck Society and the DFG Centre of Excellence 2117 'Centre for the Advanced Study of Collective Behaviour' (422037984); D.L. received funding from the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA) under contract 2017-17061500006; J.B.T. received funding from the Center for Brains, Minds and Machines (CBMM) under NSF STC award CCF-1231216; M.W. received funding from the Future of Life Institute. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA or the US Government.\nReviewer information Nature thanks Thomas Dietterich, Maria Gini, Wendell Wallach and the other anonymous reviewer(s) for their contribution to the peer review of this work.\nAuthor contributions I.R., M.C. and N.O. conceived the idea, produced the figures and drafted the manuscript. All authors contributed content, and refined and edited the manuscript.\nCompeting interests The authors declare no competing interests.\n\nProvides a collection of research exploring human-machine interaction, algorithmic bias, and the impact of automation on social behavior.",
    "original_text": "119.  Bainbridge, L. Ironies of automation. Automatica 19 , 775-779 (1983).\n120.  Jeong, S., Breazeal, C., Logan, D. & Weinstock, P . Huggable: the impact of embodiment on promoting socio-emotional interactions for young pediatric inpatients. In Proc. 2018 CHI Conference on Human Factors in Computing Systems 495 (ACM, 2018).\n121.  Kory Westlund, J. M. et al. Flat vs. expressive storytelling: young children's learning and retention of a social robot's narrative. Front. Hum. Neurosci . 11 , 295 (2017).\n122.  Salisbury, E., Kamar, E. & Morris, M. R. Toward scalable social alt text: conversational crowdsourcing as a tool for refining vision-to-language technology for the blind. Proc. 5th AAAI Conference on Human Computation and Crowdsourcing (2017).\n123.  Awad, E. et al. The Moral Machine experiment. Nature 563 , 59-64 (2018).\n124.  Dietvorst, B. J., Simmons, J. P . & Massey, C. Algorithm aversion: people erroneously avoid algorithms after seeing them err. J. Exp. Psychol. Gen . 144 , 114-126 (2015).\n125.  Gray, K. & Wegner, D. M. Feeling robots and human zombies: mind perception and the uncanny valley. Cognition 125 , 125-130 (2012).\n126.  Brynjolfsson, E. & Mitchell, T. What can machine learning do? Workforce implications. Science 358 , 1530-1534 (2017).\n127.  Christiano, P . F . et al. Deep reinforcement learning from human preferences. In Proc. Advances in Neural Information Processing Systems 30 (eds Guyon, I. et al.) 4299-4307 (Curran Associates, 2017).\n128.  Tsvetkova, M. et al. Understanding human-machine networks: a crossdisciplinary survey. ACM Comput. Surv . 50 , 12:1-12:35 (2017).\n129.  Hilbert, M., Ahmed, S., Cho, J., Liu, B. & Luu, J. Communicating with algorithms: a transfer entropy analysis of emotions-based escapes from online echo chambers. Commun. Methods Meas . 12 , 260-275 (2018).\n130.  Kramer, A. D. I., Guillory, J. E. & Hancock, J. T. Experimental evidence of massive-scale emotional contagion through social networks. Proc. Natl Acad. Sci. USA 111 , 8788-8790 (2014).\n131.  Kamar, E., Hacker, S. & Horvitz, E. Combining human and machine intelligence in large-scale crowdsourcing. in Proc. 11th International Conference on Autonomous Agents and Multiagent Systems 467-474 (International Foundation for Autonomous Agents and Multiagent Systems, 2012).\n132.  Jackson, M. The Human Network: How Your Social Position Determines Your Power, Beliefs, and Behaviors (Knopf Doubleday, 2019).\n133.  Crandall, J. W. et al. Cooperating with machines. Nat. Commun . 9 , 233 (2018). This study examines algorithmic cooperation with humans and provides an example of methods that can be used to study the behaviour of humanmachine hybrid systems .\n134.  Wang, D., Khosla, A., Gargeya, R., Irshad, H. & Beck, A. H. Deep learning for identifying metastatic breast cancer. Preprint at https://arxiv.org/ abs/1606.05718 (2016).\n135.  Pentland, A. Social Physics: How Social Networks Can Make Us Smarter (Penguin, 2015).\n136.  Lazer, D. et al. Computational social science. Science 323 , 721-723 (2009).\n137.  Aharony, N., Pan, W., Ip, C., Khayal, I. & Pentland, A. Social fMRI: investigating and shaping social mechanisms in the real world. Pervasive Mobile Comput . 7 , 643-659 (2011).\n138.  Ledford, H. How to solve the world's biggest problems. Nature 525 , 308-311 (2015).\n139.  Bromham, L., Dinnage, R. & Hua, X. Interdisciplinary research has consistently lower funding success. Nature 534 , 684-687 (2016).\n140.  Kleinberg, J. & Oren, S. Mechanisms for (mis)allocating scientific credit. In Proc. 43rd Annual ACM Symposium on Theory of Computing 529-538 (ACM, 2011).\n141.  Kannel, W. B. & McGee, D. L. Diabetes and cardiovascular disease. The Framingham study. J. Am. Med. Assoc . 241 , 2035-2038 (1979).\n142.  Krafft, P . M., Macy, M. & Pentland, A. Bots as virtual confederates: design and ethics. In Proc. 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing 183-190 (ACM, 2017).\n143.  Meyer, M. N. Two cheers for corporate experimentation: The A/B illusion and the virtues of data-driven innovation. Colorado Technol. Law J . 13 , 273 (2015).\n144.  Xing, X. et al. T ake this personally: pollution attacks on personalized services. In Proc. 22nd USENIX Security Symposium 671-686 (2013).\n145.  Patel, K. T esting the limits of the First Amendment: how a CFAA prohibition on online antidiscrimination testing infringes on protected speech activity. Columbia Law Rev . https://doi.org/10.2139/ssrn.3046847 (2017).\nAcknowledgements I.R. received funding from the Ethics & Governance of Artificial Intelligence Fund; J.B. received funding from NSF awards INSPIRE-1344227 and BIGDATA-1447634, DARPA's Lifelong Learning Machines program, ARO contract W911NF-16-1-0304; J.-F.B. from the ANRLabex IAST; N.A.C. a Pioneer Grant from the Robert Wood Johnson Foundation; I.D.C. received funding from the NSF (IOS-1355061), the ONR (N0001409-1-1074 and N00014-14-1-0635), the ARO (W911NG-11-1-0385 and W911NF14-1-0431), the Struktur- und Innovationsfunds für die Forschung of the State of Baden-Württemberg, the Max Planck Society and the DFG Centre of Excellence 2117 'Centre for the Advanced Study of Collective Behaviour' (422037984); D.L. received funding from the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA) under contract 2017-17061500006; J.B.T. received funding from the Center for Brains, Minds and Machines (CBMM) under NSF STC award CCF-1231216; M.W. received funding from the Future of Life Institute. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA or the US Government.\nReviewer information Nature thanks Thomas Dietterich, Maria Gini, Wendell Wallach and the other anonymous reviewer(s) for their contribution to the peer review of this work.\nAuthor contributions I.R., M.C. and N.O. conceived the idea, produced the figures and drafted the manuscript. All authors contributed content, and refined and edited the manuscript.\nCompeting interests The authors declare no competing interests.",
    "context": "Provides a collection of research exploring human-machine interaction, algorithmic bias, and the impact of automation on social behavior.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      10
    ],
    "id": "82f2989934fdfac6ae22fbdd6088aa5493c9525fc0c39383ce829fe909a2a535"
  },
  {
    "text": "Reprints and permissions information is available at http://www.nature.com/ reprints.\nCorrespondence and requests for materials should be addressed to I.R. Publisher's note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n© Springer Nature Limited 2019\n\nProvides contact information for reprints and permissions, and publisher’s note regarding jurisdictional claims.",
    "original_text": "Reprints and permissions information is available at http://www.nature.com/ reprints.\nCorrespondence and requests for materials should be addressed to I.R. Publisher's note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n© Springer Nature Limited 2019",
    "context": "Provides contact information for reprints and permissions, and publisher’s note regarding jurisdictional claims.",
    "document": "s41586-019-1138-y.pdf",
    "pages": [
      10
    ],
    "id": "ea8a14187a6743d8063d7ac5a38d593e73ea41205254d5a5b73519a80a52c1b7"
  },
  {
    "text": "Yi Li, Zichuan Mi & Wenjun Jing *\nThis study adopts the textual network to describe the coordination among the interplay of words, where nodes represent words and nodes are connected if the corresponding words have co-occurrence pattern across documents. To study stock movements, we further proposed the sparse laplacian shrinkage logistic model (SLS_L) which can properly take into account the network connectivity structure. By using this approach, we investigated the relationship between Shenwan index and analysts' research reports. The securities analysts' research reports are crawled by a famous financial website in China: EastMoney, and are then parsed into time-series textual data. The empirical results show that the proposed SLS_L model outperforms alternatives including Lasso-Logistics (L_L) and MCP-Logistic (MCP_L) models by having better prediction performance. Besides, we search published literature and find the identified keywords with more lucid interpretations. Our study unveils some interesting findings that the efficient use of textual network is important to improve the predictive power as well as the semantic interpretability in stock market analysis.\nIn the financial markets, the exponential increase in the amount and types of data available to investors prompted some companies to completely change their business strategy and adopt a Big Data investment framework. With the advent of social networking sites, like Facebook, Blog, Twitter, etc., the latest information about public opinions has become abundant. In fact, investigating stock market by using online financial textual data has attracted increasing attention in academic and real-life   business 1-3 . Some early empirical research in behavioral finance argued that investment decisions are subject to the effect of investor   sentiment 4,5 . Such inference was further confirmed by the studies of   Li 6 and Schumaker et al. 7 . The researchers discovered that emotion from online social media can be explored for the stock market trend, e.g. whether the stock price is going up or down according to the news media affects investor confidence with a delay of at least one   month 8 . However, when encountering the sheer volume of online resources on various websites, users find it very difficult to distinguish between posts containing facts, rumours, guesses, and fake news. The effects of imperfect information make investors unable to attract the stock market reactions accurately.\nNowadays, increasing numbers of studies quantitatively examine the information percolation of research reports on   stocks 8,9 . One of the earliest studies, by Previts et al. 10 , argued that the analyst reports are of great importance to investors' information needs. Asquith, Mikhail, and   Au 11 then discovered that the argument of the downgrade reports has a positive relationship with the stock market reaction. Twedt and   Rees 12 further investigated the effect of reports, finding that stocks could be positively correlated with the textual tone of the reports. Hence, extracting valuable information from analysts' research reports is considered to be a more stable and trustworthy source and is likely to enrich the knowledge of investors and affects their trading activities.\nThe Chinese language consists of several thousand characters known as Hanzi , with a word consisting of one or more characters. Compared to English, Chinese is more complicated in terms of recognition, segmentation and   analysis 13 : Chinese does not have an explicit word boundary marker and do not contain whitespace between words; Chinese words are not clearly marked grammatically; Chinese also contains a very large number of homophones in   sentences 14 . Hence, the amount of research on Chinese social media analysis is limited and the number of research articles on using the textual analysis results to predict the stock market in China is even lower. Fortunately, more researchers have started to conduct Chinese textual analysis in the recent years. For instance, Wang et al. 15 deploy text mining and statistical model to predict stock market price movement using Weibo mood. In other words, the increasing expansion of Chinese internet market has fuelled a growing important research field on text mining in Chinese.\nSchool of Statistics, Shanxi University of Finance and Economics, Taiyuan, Shanxi, China. * email: jingwj@sxufe. edu.cn\nVol.:(ͬͭͮͯͰͱͲͳʹ͵)\nVol:.(1234567890)\nThis paper investigates stock market movement (whether the price will be up or down) that can be deemed a classification problem. The fundamental idea of our multi-variable classification method is that related vectors of numeric feature values originate from words of documents in stock prediction. The classic logistic regression model has long been regarded as allowing highly accurate prediction as well with feature interpretability of a probabilistic nature. However, there are some challenges when performing logistic regression model on text mining. The first challenge is 'curse of dimensionality' due to the massive amounts of words. Generally, a document collection contains thousands of words, and a bag of words representation of a document will probably have a very high dimensionality. Furthermore, a sparse matrix is created in order to indicate which document each word occurs in. The matrix is sparse because most words occur just in a few documents. A large number of penalized regression approaches are proposed, including sparse penalty and thereby breaking the curse of dimensionality to improve model   interpretability 16-20 . Such proposals are the least absolute shrinkage and selection operator (LASSO), elastic net, the smoothly clipped absolute deviation (SCAD), the minimax concave penalty (MCP) and others. Among these methods, MCP has been demonstrated the preferred performance on the selection of predictors and computational   efficiency 21,22 .\nThe second challenge is the insufficiently independent words as the individual unit of text mining, which have usually been used to express the semantics of the document. The pioneering study shows that the text's semantics exist inherent interplay of words, analyses their interconnections, distinguishes \"signal from noise\" and provides a more comprehensive description of the   document 1 . For example, if the size of a laptop is described as a word \"thin\", then it is considered as a positive thing, whereas if the sentence contains \"thin\" relative to the weight of an individual then it is considered as a negative statement. By uncovering these latent relations between words, the meanings of words can be derived. From the perspective of each single relationship and then extend that perspective to the whole text network, each word represents a node. Links between nodes denote that the corresponding words are semantically \"correlated\". This network structure can be done by one individual who can provide personal meaning to the events, or meaning can also be provided at the supra-individual level. In the latter case, words with a high connectivity are more likely to be involved in an important semantic process. Hence, semantic relations between words can be constructed for whole text network analysis. Incorporation the latent structure information has been proved to significantly improve predictive power. Li et al. 23 proposed a network-constrained regularization method and found that it facilitates the selection of predictors using the network information. However, as far as we known, none of the previous network-based analyses have been trained on the background of text predictive procedures, which is the focus of this paper.\nThe main goal of this work is to combine the text network in forecasting stock market and thus be more informative. To this end, we study network-constrained sparse logistic regression, which has exploited a new perspective for the research and application of text mining. Firstly, we have constructed the textual co-occurrence network, and studied its properties. Then the composite penalization is presented. This method is built upon a combination of the minimax concave penalty and the Laplacian penalty in order to achieve the properties of sparsity and   smoothness 24 . In particular, Laplacian penalty takes full advantage of the existing network information to smooth the differences between coefficients of tightly connected words, aiming at improving prediction performance. Our work fills the gap in incorporating textual network into forecasting stock market, and undergirds the important conclusion that with some delay word patterns in security analysts' research reports affect the weekly Shenwan Stock closing index of the real estate sector. Besides, we offer a number of advantages over alternative approaches to improve the prediction performance.\nThe remainder of this paper is organized as follows. 'Methodology' introduces the novel textual predictive approach. In 'Empirical research' , we review the overall process on stock movement forecasting and the empirical research. Next section reviews the 'Experimental results' . Finally, this paper is summarized and discussed in 'Conclusions'\n\nCombines textual network analysis with sparse logistic regression to improve stock market prediction by leveraging analysts' research reports.",
    "original_text": "Yi Li, Zichuan Mi & Wenjun Jing *\nThis study adopts the textual network to describe the coordination among the interplay of words, where nodes represent words and nodes are connected if the corresponding words have co-occurrence pattern across documents. To study stock movements, we further proposed the sparse laplacian shrinkage logistic model (SLS_L) which can properly take into account the network connectivity structure. By using this approach, we investigated the relationship between Shenwan index and analysts' research reports. The securities analysts' research reports are crawled by a famous financial website in China: EastMoney, and are then parsed into time-series textual data. The empirical results show that the proposed SLS_L model outperforms alternatives including Lasso-Logistics (L_L) and MCP-Logistic (MCP_L) models by having better prediction performance. Besides, we search published literature and find the identified keywords with more lucid interpretations. Our study unveils some interesting findings that the efficient use of textual network is important to improve the predictive power as well as the semantic interpretability in stock market analysis.\nIn the financial markets, the exponential increase in the amount and types of data available to investors prompted some companies to completely change their business strategy and adopt a Big Data investment framework. With the advent of social networking sites, like Facebook, Blog, Twitter, etc., the latest information about public opinions has become abundant. In fact, investigating stock market by using online financial textual data has attracted increasing attention in academic and real-life   business 1-3 . Some early empirical research in behavioral finance argued that investment decisions are subject to the effect of investor   sentiment 4,5 . Such inference was further confirmed by the studies of   Li 6 and Schumaker et al. 7 . The researchers discovered that emotion from online social media can be explored for the stock market trend, e.g. whether the stock price is going up or down according to the news media affects investor confidence with a delay of at least one   month 8 . However, when encountering the sheer volume of online resources on various websites, users find it very difficult to distinguish between posts containing facts, rumours, guesses, and fake news. The effects of imperfect information make investors unable to attract the stock market reactions accurately.\nNowadays, increasing numbers of studies quantitatively examine the information percolation of research reports on   stocks 8,9 . One of the earliest studies, by Previts et al. 10 , argued that the analyst reports are of great importance to investors' information needs. Asquith, Mikhail, and   Au 11 then discovered that the argument of the downgrade reports has a positive relationship with the stock market reaction. Twedt and   Rees 12 further investigated the effect of reports, finding that stocks could be positively correlated with the textual tone of the reports. Hence, extracting valuable information from analysts' research reports is considered to be a more stable and trustworthy source and is likely to enrich the knowledge of investors and affects their trading activities.\nThe Chinese language consists of several thousand characters known as Hanzi , with a word consisting of one or more characters. Compared to English, Chinese is more complicated in terms of recognition, segmentation and   analysis 13 : Chinese does not have an explicit word boundary marker and do not contain whitespace between words; Chinese words are not clearly marked grammatically; Chinese also contains a very large number of homophones in   sentences 14 . Hence, the amount of research on Chinese social media analysis is limited and the number of research articles on using the textual analysis results to predict the stock market in China is even lower. Fortunately, more researchers have started to conduct Chinese textual analysis in the recent years. For instance, Wang et al. 15 deploy text mining and statistical model to predict stock market price movement using Weibo mood. In other words, the increasing expansion of Chinese internet market has fuelled a growing important research field on text mining in Chinese.\nSchool of Statistics, Shanxi University of Finance and Economics, Taiyuan, Shanxi, China. * email: jingwj@sxufe. edu.cn\nVol.:(ͬͭͮͯͰͱͲͳʹ͵)\nVol:.(1234567890)\nThis paper investigates stock market movement (whether the price will be up or down) that can be deemed a classification problem. The fundamental idea of our multi-variable classification method is that related vectors of numeric feature values originate from words of documents in stock prediction. The classic logistic regression model has long been regarded as allowing highly accurate prediction as well with feature interpretability of a probabilistic nature. However, there are some challenges when performing logistic regression model on text mining. The first challenge is 'curse of dimensionality' due to the massive amounts of words. Generally, a document collection contains thousands of words, and a bag of words representation of a document will probably have a very high dimensionality. Furthermore, a sparse matrix is created in order to indicate which document each word occurs in. The matrix is sparse because most words occur just in a few documents. A large number of penalized regression approaches are proposed, including sparse penalty and thereby breaking the curse of dimensionality to improve model   interpretability 16-20 . Such proposals are the least absolute shrinkage and selection operator (LASSO), elastic net, the smoothly clipped absolute deviation (SCAD), the minimax concave penalty (MCP) and others. Among these methods, MCP has been demonstrated the preferred performance on the selection of predictors and computational   efficiency 21,22 .\nThe second challenge is the insufficiently independent words as the individual unit of text mining, which have usually been used to express the semantics of the document. The pioneering study shows that the text's semantics exist inherent interplay of words, analyses their interconnections, distinguishes \"signal from noise\" and provides a more comprehensive description of the   document 1 . For example, if the size of a laptop is described as a word \"thin\", then it is considered as a positive thing, whereas if the sentence contains \"thin\" relative to the weight of an individual then it is considered as a negative statement. By uncovering these latent relations between words, the meanings of words can be derived. From the perspective of each single relationship and then extend that perspective to the whole text network, each word represents a node. Links between nodes denote that the corresponding words are semantically \"correlated\". This network structure can be done by one individual who can provide personal meaning to the events, or meaning can also be provided at the supra-individual level. In the latter case, words with a high connectivity are more likely to be involved in an important semantic process. Hence, semantic relations between words can be constructed for whole text network analysis. Incorporation the latent structure information has been proved to significantly improve predictive power. Li et al. 23 proposed a network-constrained regularization method and found that it facilitates the selection of predictors using the network information. However, as far as we known, none of the previous network-based analyses have been trained on the background of text predictive procedures, which is the focus of this paper.\nThe main goal of this work is to combine the text network in forecasting stock market and thus be more informative. To this end, we study network-constrained sparse logistic regression, which has exploited a new perspective for the research and application of text mining. Firstly, we have constructed the textual co-occurrence network, and studied its properties. Then the composite penalization is presented. This method is built upon a combination of the minimax concave penalty and the Laplacian penalty in order to achieve the properties of sparsity and   smoothness 24 . In particular, Laplacian penalty takes full advantage of the existing network information to smooth the differences between coefficients of tightly connected words, aiming at improving prediction performance. Our work fills the gap in incorporating textual network into forecasting stock market, and undergirds the important conclusion that with some delay word patterns in security analysts' research reports affect the weekly Shenwan Stock closing index of the real estate sector. Besides, we offer a number of advantages over alternative approaches to improve the prediction performance.\nThe remainder of this paper is organized as follows. 'Methodology' introduces the novel textual predictive approach. In 'Empirical research' , we review the overall process on stock movement forecasting and the empirical research. Next section reviews the 'Experimental results' . Finally, this paper is summarized and discussed in 'Conclusions'",
    "context": "Combines textual network analysis with sparse logistic regression to improve stock market prediction by leveraging analysts' research reports.",
    "document": "s41598-020-77823-3.pdf",
    "pages": [
      1,
      2
    ],
    "id": "fed29c982ee63cff1f8383b167908171bb8ab25035042623ac195836f49ab28e"
  },
  {
    "text": "Details about the designed textual predictive approach are given in this section. Suppose that the stock market movement is Y = { y 1, . . . , yt , . . . , yn } , where yt denotes an observation of categorical response variable at time t , which indicates the fluctuation direction in terms of increase ( ↑ ) or decrease ( ↓ ). There exists a market predictor set T = { T 1, . . . , Tt , . . . , Tn } where Tt represents the textual data available at time t , and n is the size of observations. The term xi , t extracted from the text Tt is the frequency count. Thus, the textual data Tt is defined as a word vector X t = x 1, t , . . . , xi , t , . . . , xm , t , where m is the number of vocabularies.\n{ } One assumption here is that the text Xt -1 could deliver underlying information about the future market and the ability to forecast stock movement y t . Assuming that y t follows a binomial distribution, a binomial logit model is defined by\n<!-- formula-not-decoded -->\nwhere β is the vector of regression coefficients, whose length is the vocabulary size m . Therefore, the objective function of logistic regression can be written as\n<!-- formula-not-decoded -->\nConstruction of text networks. In text network analysis, a vertex corresponds to a word. Links between words can be regarded as meaningful information in sentences. Furthermore, our study assumes that the meanings of words embodied in a text could be modelled as a network of linked words. That is, this type of information about nodes, along with the links between them, should be used for uncovering, understanding, and exploiting the semantics of text as a whole.\nConsider an undirected co-occurrence network represented by a weighted graph G = ( V, E , W ) with vertex set V = { 1, . . . , m } corresponding to m keywords, edge set E = {( i , j ) : ( i , j ) ∈ V × V } ,  the  degree set D = diag ( d1, d2, . . . , d m ) ,  where d m is the degree of vertex m ,  and the set of weights W = { α i , j : ( i , j ) ∈ E } . Here α i,j is the weight of edge ( i, j ), which measures the strength of connection between keyword vectors i and j , with 1 for complete-link and 0 for no-link, defined by\n<!-- formula-not-decoded -->\nwhere ˆ ri , j is the Pearson correlation coefficient, and r is the threshold parameter based on the p -value for determining the significance between keyword vectors i and j . There are other quantitative measurements of defining the correlation methods, including the Euclidean distance, Spearman's correlation, power adjacency function, and so on (Huang et al. 2011). In this paper, we only consider the Pearson correlation. Finally, we construct the adjacency matrix A = ( α i , j , 1 ≤ i , j ≤ m ), which is used for the Laplacian penalty related to the network, as we illustrate below. ∣ ∣\nSparse Laplacian shrinkage. The regression coefficients β = { β 1, . . . , β m } can capture the effects of the term variable xi , t .  In  addition, the term network's depicting relationships between predictors are informative. The penalized method for network-constrained sparse logistic regression is defined by\n<!-- formula-not-decoded -->\nwhere\n<!-- formula-not-decoded -->\nHere P /afii9838 , γ ( β ) is a sparsity and smoothness based penalty function with emphasis on the underlying network structure G . T wo tuning parameters ( /afii9838 1, /afii9838 2 ) with /afii9838 1 ≥ 0 and /afii9838 2 ≥ 0 control the degree of regularization, ρ is the minimax concave penalty with two regulation parameters ( /afii9838 1, γ ) .\nFor sparsity penalty, we use the minimax concave penalty in the first penalty term, defined as\n<!-- formula-not-decoded -->\nIn this analysis, terms are supposed to be the semantic units for selection. Hence, the first penalty promotes sparsity and directs the search to more meaningful item combinations.\nFor network penalty, the Laplacian penalty is adopted in the second penalty term, defined as /afii9838 2 β ′ L β . The Laplacian matrix L is always positive semi-definite and defined with respect to the network G , with L = D-A, which satisfies\n<!-- formula-not-decoded -->\nIt accommodates the network structure. For instance, items with higher connectivity are considered to have closely related semantic units. We adopt a constraint on the contrast between β j and β k to improve the smoothness of estimated coefficients with respect to the prior structure information and thus results in more interpretable identification of the items.\nThe prediction procedure follows the stock movement prediction method described in the work of Bergmeir and   Benitez 25 . In particular, the SLS method has three tuning parameters: (λ 1 , λ 2 and γ). The parameter λ 1 controls the level of sparsity, λ 2 controls the degree of the coefficient smoothing, and the third parameter γ in addition to λ 1 , governs the concavity of the sparsity penalty function. In practice, we search in a grid of λ 1 , λ 2 values with λ 2 ∈ (0, 0.001, 0.01, 0.1, 1, 10) and choose the values of λ 1 , λ 2 that maximize accuracy of predictions. The third parameter γ controls the concavity of the MCP penalty. When γ →∞ , MCP reduces to the   L 1 penalty. In practice, we fix γ to the default value of 2.7 to reduce computation, and the prediction accuracy is usually not sensitive to γ values.\nTo be more specific, we adopt V-fold cross-validation to choose the optimal combination of λ 1 and λ 2 , and then partition the dataset into separate training and test data. Furthermore, variable selection and parameter estimation are accomplished on the training data with cross-validation to choose the tuning parameters, and the test data is used to assess the forecasting abilities. The accuracy of predictions is defined as the area under the curve (AUC), which can be defined as the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance. A test with AUC > 0.9 has high classification accuracy. Moreover,\nVol.:(0123456789)\nVol:.(1234567890)\nFigure 1. The flowchart of the text mining processing.\nit is easy to compare with uncertainty forecasting, such as the Random Walk Hypothesis, because the random classification model could cause an AUC value of 50% 26,27 .\n\nDescribes the proposed method for predicting stock movement by leveraging textual network analysis.",
    "original_text": "Details about the designed textual predictive approach are given in this section. Suppose that the stock market movement is Y = { y 1, . . . , yt , . . . , yn } , where yt denotes an observation of categorical response variable at time t , which indicates the fluctuation direction in terms of increase ( ↑ ) or decrease ( ↓ ). There exists a market predictor set T = { T 1, . . . , Tt , . . . , Tn } where Tt represents the textual data available at time t , and n is the size of observations. The term xi , t extracted from the text Tt is the frequency count. Thus, the textual data Tt is defined as a word vector X t = x 1, t , . . . , xi , t , . . . , xm , t , where m is the number of vocabularies.\n{ } One assumption here is that the text Xt -1 could deliver underlying information about the future market and the ability to forecast stock movement y t . Assuming that y t follows a binomial distribution, a binomial logit model is defined by\n<!-- formula-not-decoded -->\nwhere β is the vector of regression coefficients, whose length is the vocabulary size m . Therefore, the objective function of logistic regression can be written as\n<!-- formula-not-decoded -->\nConstruction of text networks. In text network analysis, a vertex corresponds to a word. Links between words can be regarded as meaningful information in sentences. Furthermore, our study assumes that the meanings of words embodied in a text could be modelled as a network of linked words. That is, this type of information about nodes, along with the links between them, should be used for uncovering, understanding, and exploiting the semantics of text as a whole.\nConsider an undirected co-occurrence network represented by a weighted graph G = ( V, E , W ) with vertex set V = { 1, . . . , m } corresponding to m keywords, edge set E = {( i , j ) : ( i , j ) ∈ V × V } ,  the  degree set D = diag ( d1, d2, . . . , d m ) ,  where d m is the degree of vertex m ,  and the set of weights W = { α i , j : ( i , j ) ∈ E } . Here α i,j is the weight of edge ( i, j ), which measures the strength of connection between keyword vectors i and j , with 1 for complete-link and 0 for no-link, defined by\n<!-- formula-not-decoded -->\nwhere ˆ ri , j is the Pearson correlation coefficient, and r is the threshold parameter based on the p -value for determining the significance between keyword vectors i and j . There are other quantitative measurements of defining the correlation methods, including the Euclidean distance, Spearman's correlation, power adjacency function, and so on (Huang et al. 2011). In this paper, we only consider the Pearson correlation. Finally, we construct the adjacency matrix A = ( α i , j , 1 ≤ i , j ≤ m ), which is used for the Laplacian penalty related to the network, as we illustrate below. ∣ ∣\nSparse Laplacian shrinkage. The regression coefficients β = { β 1, . . . , β m } can capture the effects of the term variable xi , t .  In  addition, the term network's depicting relationships between predictors are informative. The penalized method for network-constrained sparse logistic regression is defined by\n<!-- formula-not-decoded -->\nwhere\n<!-- formula-not-decoded -->\nHere P /afii9838 , γ ( β ) is a sparsity and smoothness based penalty function with emphasis on the underlying network structure G . T wo tuning parameters ( /afii9838 1, /afii9838 2 ) with /afii9838 1 ≥ 0 and /afii9838 2 ≥ 0 control the degree of regularization, ρ is the minimax concave penalty with two regulation parameters ( /afii9838 1, γ ) .\nFor sparsity penalty, we use the minimax concave penalty in the first penalty term, defined as\n<!-- formula-not-decoded -->\nIn this analysis, terms are supposed to be the semantic units for selection. Hence, the first penalty promotes sparsity and directs the search to more meaningful item combinations.\nFor network penalty, the Laplacian penalty is adopted in the second penalty term, defined as /afii9838 2 β ′ L β . The Laplacian matrix L is always positive semi-definite and defined with respect to the network G , with L = D-A, which satisfies\n<!-- formula-not-decoded -->\nIt accommodates the network structure. For instance, items with higher connectivity are considered to have closely related semantic units. We adopt a constraint on the contrast between β j and β k to improve the smoothness of estimated coefficients with respect to the prior structure information and thus results in more interpretable identification of the items.\nThe prediction procedure follows the stock movement prediction method described in the work of Bergmeir and   Benitez 25 . In particular, the SLS method has three tuning parameters: (λ 1 , λ 2 and γ). The parameter λ 1 controls the level of sparsity, λ 2 controls the degree of the coefficient smoothing, and the third parameter γ in addition to λ 1 , governs the concavity of the sparsity penalty function. In practice, we search in a grid of λ 1 , λ 2 values with λ 2 ∈ (0, 0.001, 0.01, 0.1, 1, 10) and choose the values of λ 1 , λ 2 that maximize accuracy of predictions. The third parameter γ controls the concavity of the MCP penalty. When γ →∞ , MCP reduces to the   L 1 penalty. In practice, we fix γ to the default value of 2.7 to reduce computation, and the prediction accuracy is usually not sensitive to γ values.\nTo be more specific, we adopt V-fold cross-validation to choose the optimal combination of λ 1 and λ 2 , and then partition the dataset into separate training and test data. Furthermore, variable selection and parameter estimation are accomplished on the training data with cross-validation to choose the tuning parameters, and the test data is used to assess the forecasting abilities. The accuracy of predictions is defined as the area under the curve (AUC), which can be defined as the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance. A test with AUC > 0.9 has high classification accuracy. Moreover,\nVol.:(0123456789)\nVol:.(1234567890)\nFigure 1. The flowchart of the text mining processing.\nit is easy to compare with uncertainty forecasting, such as the Random Walk Hypothesis, because the random classification model could cause an AUC value of 50% 26,27 .",
    "context": "Describes the proposed method for predicting stock movement by leveraging textual network analysis.",
    "document": "s41598-020-77823-3.pdf",
    "pages": [
      2,
      3,
      4
    ],
    "id": "7bcae95044752737ccc377729d441be0d5ca0b39976e7294574d863b85f6d362"
  },
  {
    "text": "Experimental design. Our paper relates to research that the power of information on securities analysts' research reports and applies an SLS_L model to predict the stock market reaction. The flow of our method has some of the components depicted in Fig. 1. Our overall processor provides a solid basis to determine the internal connections between securities analysts' online research reports and stock price movements. At one end textual data obtained from online sources and stock index are fed as input to the system and at the other end, some stock predictive movements are generated as output. The first step is data collection using the web crawler, and then extracting relevant information from a suitable dataset. Words and phrases that signal a stock movement are important and should be extracted. Thus, the preprocessor is a crucial part of text mining and is mainly composed of the following common preparatory steps: dictionary building, word segmentation, and word cleaning. After data cleaning, we measure the time difference between each word and target indicator, and then choose the list of words that indicate the most significant correlation, in a sense that the subset is as small as possible but still retains all the relevant information.\nIn statistical terminology, a word is an observed variable, and a document is a list of observed variables representing an instance. Clearly, the list of words used the text representation are retrieved from the message corpus based on actual occurrences of the words. Building on the network construction, we examine important network properties such as density, centrality and K-core. At the subsequent step, we predict market movement direction using sparse group Laplacian shrinkage logistic model and assign weights based on text network information to keywords in proportion to the market movement. The remaining keywords serve as a determinate variable in our prediction model. Each step will be elaborated on Fig. 1.\nData descriptions. For the stock market, the weekly Stock closing index of the real estate sector from the Shenwan Research database (SWS, http://www.swsre  searc  h.com/EN/) has been collected ranging from May 14, 2015 to September 18,   2017 28,29 . The overall data includes 108 trading weeks. The sample of stocks being studied are 136 firms that together made up the weekly SWS index of real estate sector. The prediction target in this paper is the performance of the stock movement. The Stock closing index of the real estate sector is labelled suitably using a simple method. If the stock index at time t is larger than that at time t -1 , the direction at time t is 1, otherwise, direction at time t is 0, that is, the stock movement performance is mapped into 0 or 1 as the prediction target.\nThe textual data are collected by crawling and parsing large amounts of web pages about the most popular and prestigious financial website in China, EastMoney (http://www.eastm  oney.com/), by searching the content of all securities analysts' research reports in the China real estate sector from 14 May 2015 to 18 Sep. 2017. The online reports are to give a complete and timely description of stock information from the security company, which will dynamically capture the stock trend, hence assist the user to make an investment   decision 30,31 . We crawl down and compile from the Internet using Python libraries, which are composed of two major libraries: the target URL list generating library (Requests) and the HTML page parsing library (Beautiful Soup).\nFor each research report in the China real estate sector on the site, the script recorded the date of the posting, the title, the security company name and the body of the message. The text of a report includes varying levels of topics, such as the company's significant recent developments, industry dynamics, a critique of the company's management and board, and investment risk, etc. There are a total of 2082 reports from 65 security companies. Each research report is timed to the day, the mean security reports in a day are 3.8 and the number of words in a report is most frequently between 500 and 2000. In order to assess the content of the research report, we employ\nwell-established text mining, which is executed to transform raw documents into lists of keyword vectors and adjacent matrix of the network.\nPredictive keyword vector extraction. The research report is employed as our textual source, mainly because these texts might expose some things that firms may not like to tell their outside listeners in blunt terms. By implementing this action, the raw text of a report published in the same week is merged into one document, where larger reports have adequate data distribution within a week's span. After eliminating noise and outliers, our research reports are narrowed down to 108 windows over the 28 months and the mean security reports in a window are 17, with a maximum of 36. Each window is converted into the bag of words, represented as a vector of counts. The following steps are conducted on a single document. Our judgment and cleaning criterion is elaborated on as follow.\nStep 1: dictionary building. The Sogou Pinyin input method is a dominant input software in China, and Sogou cell lexicon can be obtained from the Sogou Pinyin input method official website (https  ://pinyi  n.sogou  .com/ dict/). These lexica come from the analysis of millions of Chinese web pages generated by the Sogou search engine 32 .  Therefore,  we  choose  the  Sogou  cell  lexicon  as  the  dictionary  and  exclude  the  uncommonly  used words. Then we own the remaining 63,320 words or phrases as the external dictionary, which is embedded in step 2 in order to decrease ambiguities.\nStep 2: Text segmentation. Unlike English where text processing starts with tokenization, Chinese text segmentation is the procedure of identifying the boundaries between semantic units, for instance, phrases or words. In this step, we use the Jieba package to perform text   segmentation 33 . This segmentation approach is mainly based on a Hidden Markov model, and adding our own custom dictionary obtained from step 1 can ensure a higher rate of correct segmentation. To discern words, the Viterbi algorithm used in this package is to find the maximum tangential points based on the word   frequency 34 .\nStep 3: words cleaning. Since stop words do not contribute to the analysis in textual data without dependency on a particular topic, we remove these words with the same roots appearing in the stop-list such as numbers, more white spaces, tabs, punctuation characters, stop words, etc. In addition, we build stop-list that contains 75 non-semantic words such as \" 是 (is)\", \" 的 (of)\", \" 关于 (about)\", \" 如何 (how)\", and so on. We remove a word with a proportion in all documents was smaller than 80% because the chi-squares statistics is known not to be reliable for the low-frequency terms.\nUsing the above step 1-step 3, an initial set of 3285 keywords was built.\nStep 4: keyword vector selection. There are thousands of words in documents. If we choose all words as features, it will be impossible to do forecasting since the computer cannot process such enormous amounts of data. Thus, we need to choose the most meaningful and representative units for prediction. There are very popular selection methods, for instance, chi-square   statistics 35 , information gain, mutual information, document frequency and latent semantic   analysis 36 . In this paper, we use chi-square ( χ 2 ) independent statistics to test the frequency count of each item in each report data and the market indicator fluctuations measured in terms of increase ( ↑ ) or decrease ( ↓ ) during each time period of one week. The statistic test is compared to the chi-square distribution with one degree of freedom. After the above filtering, 56 words remained at the 5% significance level in our trial. We concluded that the 56 words provide sufficient evidence to determine that there is an association between the nature of the frequency items and the market fluctuations. Therefore, we directly put the 56-word count vectors as the most predictive variable into the construction of the network and the predicted model.\n\nProvides a detailed methodology for text mining, including dictionary building, text segmentation, and keyword vector selection, outlining the process of extracting relevant information from financial analyst reports to predict stock market reactions.",
    "original_text": "Experimental design. Our paper relates to research that the power of information on securities analysts' research reports and applies an SLS_L model to predict the stock market reaction. The flow of our method has some of the components depicted in Fig. 1. Our overall processor provides a solid basis to determine the internal connections between securities analysts' online research reports and stock price movements. At one end textual data obtained from online sources and stock index are fed as input to the system and at the other end, some stock predictive movements are generated as output. The first step is data collection using the web crawler, and then extracting relevant information from a suitable dataset. Words and phrases that signal a stock movement are important and should be extracted. Thus, the preprocessor is a crucial part of text mining and is mainly composed of the following common preparatory steps: dictionary building, word segmentation, and word cleaning. After data cleaning, we measure the time difference between each word and target indicator, and then choose the list of words that indicate the most significant correlation, in a sense that the subset is as small as possible but still retains all the relevant information.\nIn statistical terminology, a word is an observed variable, and a document is a list of observed variables representing an instance. Clearly, the list of words used the text representation are retrieved from the message corpus based on actual occurrences of the words. Building on the network construction, we examine important network properties such as density, centrality and K-core. At the subsequent step, we predict market movement direction using sparse group Laplacian shrinkage logistic model and assign weights based on text network information to keywords in proportion to the market movement. The remaining keywords serve as a determinate variable in our prediction model. Each step will be elaborated on Fig. 1.\nData descriptions. For the stock market, the weekly Stock closing index of the real estate sector from the Shenwan Research database (SWS, http://www.swsre  searc  h.com/EN/) has been collected ranging from May 14, 2015 to September 18,   2017 28,29 . The overall data includes 108 trading weeks. The sample of stocks being studied are 136 firms that together made up the weekly SWS index of real estate sector. The prediction target in this paper is the performance of the stock movement. The Stock closing index of the real estate sector is labelled suitably using a simple method. If the stock index at time t is larger than that at time t -1 , the direction at time t is 1, otherwise, direction at time t is 0, that is, the stock movement performance is mapped into 0 or 1 as the prediction target.\nThe textual data are collected by crawling and parsing large amounts of web pages about the most popular and prestigious financial website in China, EastMoney (http://www.eastm  oney.com/), by searching the content of all securities analysts' research reports in the China real estate sector from 14 May 2015 to 18 Sep. 2017. The online reports are to give a complete and timely description of stock information from the security company, which will dynamically capture the stock trend, hence assist the user to make an investment   decision 30,31 . We crawl down and compile from the Internet using Python libraries, which are composed of two major libraries: the target URL list generating library (Requests) and the HTML page parsing library (Beautiful Soup).\nFor each research report in the China real estate sector on the site, the script recorded the date of the posting, the title, the security company name and the body of the message. The text of a report includes varying levels of topics, such as the company's significant recent developments, industry dynamics, a critique of the company's management and board, and investment risk, etc. There are a total of 2082 reports from 65 security companies. Each research report is timed to the day, the mean security reports in a day are 3.8 and the number of words in a report is most frequently between 500 and 2000. In order to assess the content of the research report, we employ\nwell-established text mining, which is executed to transform raw documents into lists of keyword vectors and adjacent matrix of the network.\nPredictive keyword vector extraction. The research report is employed as our textual source, mainly because these texts might expose some things that firms may not like to tell their outside listeners in blunt terms. By implementing this action, the raw text of a report published in the same week is merged into one document, where larger reports have adequate data distribution within a week's span. After eliminating noise and outliers, our research reports are narrowed down to 108 windows over the 28 months and the mean security reports in a window are 17, with a maximum of 36. Each window is converted into the bag of words, represented as a vector of counts. The following steps are conducted on a single document. Our judgment and cleaning criterion is elaborated on as follow.\nStep 1: dictionary building. The Sogou Pinyin input method is a dominant input software in China, and Sogou cell lexicon can be obtained from the Sogou Pinyin input method official website (https  ://pinyi  n.sogou  .com/ dict/). These lexica come from the analysis of millions of Chinese web pages generated by the Sogou search engine 32 .  Therefore,  we  choose  the  Sogou  cell  lexicon  as  the  dictionary  and  exclude  the  uncommonly  used words. Then we own the remaining 63,320 words or phrases as the external dictionary, which is embedded in step 2 in order to decrease ambiguities.\nStep 2: Text segmentation. Unlike English where text processing starts with tokenization, Chinese text segmentation is the procedure of identifying the boundaries between semantic units, for instance, phrases or words. In this step, we use the Jieba package to perform text   segmentation 33 . This segmentation approach is mainly based on a Hidden Markov model, and adding our own custom dictionary obtained from step 1 can ensure a higher rate of correct segmentation. To discern words, the Viterbi algorithm used in this package is to find the maximum tangential points based on the word   frequency 34 .\nStep 3: words cleaning. Since stop words do not contribute to the analysis in textual data without dependency on a particular topic, we remove these words with the same roots appearing in the stop-list such as numbers, more white spaces, tabs, punctuation characters, stop words, etc. In addition, we build stop-list that contains 75 non-semantic words such as \" 是 (is)\", \" 的 (of)\", \" 关于 (about)\", \" 如何 (how)\", and so on. We remove a word with a proportion in all documents was smaller than 80% because the chi-squares statistics is known not to be reliable for the low-frequency terms.\nUsing the above step 1-step 3, an initial set of 3285 keywords was built.\nStep 4: keyword vector selection. There are thousands of words in documents. If we choose all words as features, it will be impossible to do forecasting since the computer cannot process such enormous amounts of data. Thus, we need to choose the most meaningful and representative units for prediction. There are very popular selection methods, for instance, chi-square   statistics 35 , information gain, mutual information, document frequency and latent semantic   analysis 36 . In this paper, we use chi-square ( χ 2 ) independent statistics to test the frequency count of each item in each report data and the market indicator fluctuations measured in terms of increase ( ↑ ) or decrease ( ↓ ) during each time period of one week. The statistic test is compared to the chi-square distribution with one degree of freedom. After the above filtering, 56 words remained at the 5% significance level in our trial. We concluded that the 56 words provide sufficient evidence to determine that there is an association between the nature of the frequency items and the market fluctuations. Therefore, we directly put the 56-word count vectors as the most predictive variable into the construction of the network and the predicted model.",
    "context": "Provides a detailed methodology for text mining, including dictionary building, text segmentation, and keyword vector selection, outlining the process of extracting relevant information from financial analyst reports to predict stock market reactions.",
    "document": "s41598-020-77823-3.pdf",
    "pages": [
      4,
      5
    ],
    "id": "a3a29e665eb3215584b2cfab75bcaa99692dd7f375776330e13382b695a783b7"
  },
  {
    "text": "Network results. Following Nassirtoussi et al. 1 , we refer to the 56 keywords as \"representative textual features\". Each keyword is a node in the network. A link between nodes indicates that co-occur within the twokeyword window with a statistically stronger frequency, which can effectively capture meaningful   information 37 . Based on the keyword vector, a 56 × 56 undirected adjacency matrix is eventually constructed for network analysis. Figure 2a shows a simple grid layout of a graph which introduces the overview of the network on the basis of connectivity. Figure 2b shows the node is scaled using the Fruchterman-Reingold layout algorithm, which aims to keep adjacent vertices close to each other while ensuring that vertices are not too close to each other. Depending on the graph layout, we explore the characters of the text network. The graph demonstrates the prominent role and the comparative importance of keywords. More specifically, it can be seen that the keywords \"Hotspot( 热点 )\" and \"Short Term( 短期 )\" are central points of information flow. Keywords \"Pessimistic ( 悲观 )\" and \"Input Market ( 投入市场 )\" are supposed to be intermediary points. Other keywords seem to serve as peripheral points.\nTo do more detailed network analysis and achieve much more insightful and interesting ramifications, nevertheless, networks need to be distilled into key quantitative indicators that can be operationally defined and practically measured. We consider the following indicators to explore the cohesion, integration, composition and structure of the text network.\nDensity is a measure of network cohesion. In this sense, density represents the proportion of observed links in a network that are actually present. The value can be in the range from 0 to 1, where 0 indicates networks with no relationships and 1 indicates networks with all possible relationships. Our network density is 0.1695, meaning that there are 522 links in 56 × 56 adjacency matrix. Thus, the textual network is a sparse network. This\nVol.:(0123456789)\nVol:.(1234567890)\n(a)The diagram for Chinese version\n\nHighlights the key role of the text network in capturing relationships between words and predicting stock movements, emphasizing the network's importance for improving predictive power and semantic interpretability.",
    "original_text": "Network results. Following Nassirtoussi et al. 1 , we refer to the 56 keywords as \"representative textual features\". Each keyword is a node in the network. A link between nodes indicates that co-occur within the twokeyword window with a statistically stronger frequency, which can effectively capture meaningful   information 37 . Based on the keyword vector, a 56 × 56 undirected adjacency matrix is eventually constructed for network analysis. Figure 2a shows a simple grid layout of a graph which introduces the overview of the network on the basis of connectivity. Figure 2b shows the node is scaled using the Fruchterman-Reingold layout algorithm, which aims to keep adjacent vertices close to each other while ensuring that vertices are not too close to each other. Depending on the graph layout, we explore the characters of the text network. The graph demonstrates the prominent role and the comparative importance of keywords. More specifically, it can be seen that the keywords \"Hotspot( 热点 )\" and \"Short Term( 短期 )\" are central points of information flow. Keywords \"Pessimistic ( 悲观 )\" and \"Input Market ( 投入市场 )\" are supposed to be intermediary points. Other keywords seem to serve as peripheral points.\nTo do more detailed network analysis and achieve much more insightful and interesting ramifications, nevertheless, networks need to be distilled into key quantitative indicators that can be operationally defined and practically measured. We consider the following indicators to explore the cohesion, integration, composition and structure of the text network.\nDensity is a measure of network cohesion. In this sense, density represents the proportion of observed links in a network that are actually present. The value can be in the range from 0 to 1, where 0 indicates networks with no relationships and 1 indicates networks with all possible relationships. Our network density is 0.1695, meaning that there are 522 links in 56 × 56 adjacency matrix. Thus, the textual network is a sparse network. This\nVol.:(0123456789)\nVol:.(1234567890)\n(a)The diagram for Chinese version",
    "context": "Highlights the key role of the text network in capturing relationships between words and predicting stock movements, emphasizing the network's importance for improving predictive power and semantic interpretability.",
    "document": "s41598-020-77823-3.pdf",
    "pages": [
      5,
      6
    ],
    "id": "fc0191839f122a326f712a2b139ba85a29489e8ff227790510a1419ce8a1d5ac"
  },
  {
    "text": "Figure 2. Keywords network. ( a ) The simple grid layout of a graph; ( b ) the node is scaled using the Fruchterman-Reingold layout algorithm.\nresult is significantly affected by the fact that text network of research reports on stocks is loos knit instead of densely connected.\nCentrality is an indicator to find the most important keywords within a network. Betweenness is a centrality measure of a node within a network. Specifically, betweenness centrality measures the number of times a vertex acts as a link along the shortest path between two other nodes. A vertex with high betweenness has a high probability to control the flow of information in the network. In our study, only 44 vertices of the betweenness centrality are greater than zero. The word \"Short Term\" is the most influential keywords, with a betweenness centrality of 287.89, followed by \"Pessimistic\", \"Input Market\", \"Larger\" and \"Market\". The word \"Short Term\" takes up a position in the centre of the network and has a great effect on other words in the text network, which suggests that the factor like \"Short Term\" plays an important role in the interactivity of the stock market and the real estate market. This is because that our work focuses mainly on stock price reactions in the short term.\nK-core is an indicator to identify closely interlinked subgroup within a network, which indicates a coalition of many keywords who have many stable patterns with each other. A K-core measure is used to find whether the network is structured. Table 1 shows that the cores become more interlinked when k increases from zero, which indicates that our text network has a structural property rather than   random 38 .\nTable 1. Results of the K-core collapse sequence analysis.\n\nk-core, 1 = 0. k-core, 2 = 1. k-core, 3 = 2. k-core, 4 = 3. k-core, 5 = 4. k-core, 6 = 5. k-core, 7 = 6. k-core, 8 = 7. k-core, 9 = 8. k-core, 10 = 9. k-core, 11 = 10. k-remainder, 1 = 5. k-remainder, 2 = 3. k-remainder, 3 = 7. k-remainder, 4 = 3. k-remainder, 5 = 5. k-remainder, 6 = 3. k-remainder, 7 = 3. k-remainder, 8 = 5. k-remainder, 9 = 3. k-remainder, 10 = 1. k-remainder, 11 = 18\nPrediction results. One simple hypothesis about the validity of a text network is that the text network is very noisy if the stock movement can be forecasted well using textual data only. In this instance, textual data seem to be sufficient to forecast. At the same time, integration of the network structure may not increase the accuracy of prediction much. On the contrary, if the prediction accuracy in the SLS_L model is improved, the use of network information could be more effective.\nTo test the above hypothesis, we conduct an evaluation of prediction performance using threefold crossvalidation. For each fixed /afii9838 2 , we can obtain the second tuning parameter besides /afii9838 2 by using the number of steps for the MCP optimal solution. The prediction results for the proposed Sparse Laplacian Shrinkage-Logistic (SLS_L) model based on 5 replicates are calculated as AUC = 0.9360, /afii9838 1 =  0.0251, /afii9838 2 =  0.001. Given these descriptive results, /afii9838 1 performs the variable selection and control the level of sparsity, and /afii9838 2 controls the degree of the coefficient smoothing, that is, the similarity between coefficients. When /afii9838 2 is around zero, the network structure is very noisy. In our study, the values of the tuning parameter /afii9838 2 are non-zero in all replicates. This illustrates the prediction accuracy of SLS_L model, which seems to perform better as it uses the network information.\nTo evaluate the effectiveness of network information from another point of view, Lasso-Logistics (L_L) and MCP-Logistic (MCP_L) models are applied to assess the prediction accuracy of the proposed SLS_L model. These comparisons of results demonstrate that the proposed SLS_L model (AUC = 0.9360) has the highest AUC in comparison with L_L(AUC = 0.8344) and MCP_L(AUC = 0.8707). The AUC values conclude that the proposed method can achieve lower prediction errors and higher prediction accuracy in the stock direction of change and outperforms L_L and MCP_L models in predicting the SWS week index of real estate sector. It provides evidence of the advantage of appending the network penalty and smoothing over the similarity between words.\nIn addition, the impact of the time lag on the market reaction is considered in this work. The up and down of the stock price is predicated on the subsequent n weeks (five trading days). The results of the experiment show that in general, the AUC on the next one week was the highest, yielding up to 0.9360 for the SWS index of real estate sector. In the similar result, Asquith et al. 11 discovered that analyst reports on companies can affect their market's reaction with five trade days delay. In the market, the trading date is the day that an investor's order is executed. Our result shows that some time is still needed for getting news to trading. It is reasonable to expect that public opinion on the market will only affect the stock fluctuation with some delay.\nIn our experiment, 56 words are shown to provide sufficient evidence to determine that there is an association between the nature of the frequency items and the market fluctuations. However, among these 56 words, the coefficients of 25 words shrink to zero when using all of the three models (MCP_L, SLS_L and L_L models), meaning that they are not effective. The remaining 31 words are effective, that is, the coefficients of them are not zero in at least one of the three models. In our manuscript, Table 2 shows the 31 words. It is expected that the evaluation of prediction performance can also provide an indirect evaluation of the textual implications of the models and representative features. To gain further insights, we now more closely investigate overlapping keywords detected by MCP_L, SLS_L and L_L models. These 25 keywords of overlaps are identified by all the models. Some keywords are expressed as the expectations or needs of investors and their trading activities, like pessimistic ( 悲观 ), Cool down ( 冷却 ), Rise ( 升至 ). Some keywords represent the current event and financial topics, like Demographic Dividend ( 人口红利 ), Real Economy ( 实体经济 ), Securitization( 证券化 ). Some keywords indicate sufficient statistical meaning, that is, a stable relationship between keywords, like Intensify ( 加 剧 ), Prefer ( 首先 ). Some keywords reflect correspondence to target indicator, like company name, city name. Searching published literature suggest that these keywords may have important implications. Recent evidences in behavior finance indicates that emotion influence stock market   returns 6,39 . Previous studies suggest that the demographic variable can be related to the information component determining long-horizon stock market returns 40 . Westerhoff shows the interactions between the real economy and stock   market 41 . Fontana and Godin find the links between housing market and financial sector by taking into account the securitization   process 42 . Our result indicates events that most certainly influences stock market prices, public sentiment and opinion may play an equally important role in predicting stock movement.\nThe striking finding in Table 2 is that only five out of 25 terms have a positive impact on financial markets. Apparently even the positive word \"Securitization( 证券化 )\" received a negative connotation. The strongest positive indicator is \"Imagine( 想象 )\", and the strongest negative indicator is \"Concern( 关注度 )\". This finding seems to echo what   Soroka 43 and Wu et al. 44 discovered earlier-responses to positive and negative information are asymmetric-that negative information has a much greater impact on individuals' attitudes than does positive information.\n\nProvides a detailed analysis of the network structure within the text data, highlighting the importance of \"Short Term\" as a central keyword and demonstrating the effectiveness of the proposed model in incorporating network information for improved prediction accuracy.",
    "original_text": "Figure 2. Keywords network. ( a ) The simple grid layout of a graph; ( b ) the node is scaled using the Fruchterman-Reingold layout algorithm.\nresult is significantly affected by the fact that text network of research reports on stocks is loos knit instead of densely connected.\nCentrality is an indicator to find the most important keywords within a network. Betweenness is a centrality measure of a node within a network. Specifically, betweenness centrality measures the number of times a vertex acts as a link along the shortest path between two other nodes. A vertex with high betweenness has a high probability to control the flow of information in the network. In our study, only 44 vertices of the betweenness centrality are greater than zero. The word \"Short Term\" is the most influential keywords, with a betweenness centrality of 287.89, followed by \"Pessimistic\", \"Input Market\", \"Larger\" and \"Market\". The word \"Short Term\" takes up a position in the centre of the network and has a great effect on other words in the text network, which suggests that the factor like \"Short Term\" plays an important role in the interactivity of the stock market and the real estate market. This is because that our work focuses mainly on stock price reactions in the short term.\nK-core is an indicator to identify closely interlinked subgroup within a network, which indicates a coalition of many keywords who have many stable patterns with each other. A K-core measure is used to find whether the network is structured. Table 1 shows that the cores become more interlinked when k increases from zero, which indicates that our text network has a structural property rather than   random 38 .\nTable 1. Results of the K-core collapse sequence analysis.\n\nk-core, 1 = 0. k-core, 2 = 1. k-core, 3 = 2. k-core, 4 = 3. k-core, 5 = 4. k-core, 6 = 5. k-core, 7 = 6. k-core, 8 = 7. k-core, 9 = 8. k-core, 10 = 9. k-core, 11 = 10. k-remainder, 1 = 5. k-remainder, 2 = 3. k-remainder, 3 = 7. k-remainder, 4 = 3. k-remainder, 5 = 5. k-remainder, 6 = 3. k-remainder, 7 = 3. k-remainder, 8 = 5. k-remainder, 9 = 3. k-remainder, 10 = 1. k-remainder, 11 = 18\nPrediction results. One simple hypothesis about the validity of a text network is that the text network is very noisy if the stock movement can be forecasted well using textual data only. In this instance, textual data seem to be sufficient to forecast. At the same time, integration of the network structure may not increase the accuracy of prediction much. On the contrary, if the prediction accuracy in the SLS_L model is improved, the use of network information could be more effective.\nTo test the above hypothesis, we conduct an evaluation of prediction performance using threefold crossvalidation. For each fixed /afii9838 2 , we can obtain the second tuning parameter besides /afii9838 2 by using the number of steps for the MCP optimal solution. The prediction results for the proposed Sparse Laplacian Shrinkage-Logistic (SLS_L) model based on 5 replicates are calculated as AUC = 0.9360, /afii9838 1 =  0.0251, /afii9838 2 =  0.001. Given these descriptive results, /afii9838 1 performs the variable selection and control the level of sparsity, and /afii9838 2 controls the degree of the coefficient smoothing, that is, the similarity between coefficients. When /afii9838 2 is around zero, the network structure is very noisy. In our study, the values of the tuning parameter /afii9838 2 are non-zero in all replicates. This illustrates the prediction accuracy of SLS_L model, which seems to perform better as it uses the network information.\nTo evaluate the effectiveness of network information from another point of view, Lasso-Logistics (L_L) and MCP-Logistic (MCP_L) models are applied to assess the prediction accuracy of the proposed SLS_L model. These comparisons of results demonstrate that the proposed SLS_L model (AUC = 0.9360) has the highest AUC in comparison with L_L(AUC = 0.8344) and MCP_L(AUC = 0.8707). The AUC values conclude that the proposed method can achieve lower prediction errors and higher prediction accuracy in the stock direction of change and outperforms L_L and MCP_L models in predicting the SWS week index of real estate sector. It provides evidence of the advantage of appending the network penalty and smoothing over the similarity between words.\nIn addition, the impact of the time lag on the market reaction is considered in this work. The up and down of the stock price is predicated on the subsequent n weeks (five trading days). The results of the experiment show that in general, the AUC on the next one week was the highest, yielding up to 0.9360 for the SWS index of real estate sector. In the similar result, Asquith et al. 11 discovered that analyst reports on companies can affect their market's reaction with five trade days delay. In the market, the trading date is the day that an investor's order is executed. Our result shows that some time is still needed for getting news to trading. It is reasonable to expect that public opinion on the market will only affect the stock fluctuation with some delay.\nIn our experiment, 56 words are shown to provide sufficient evidence to determine that there is an association between the nature of the frequency items and the market fluctuations. However, among these 56 words, the coefficients of 25 words shrink to zero when using all of the three models (MCP_L, SLS_L and L_L models), meaning that they are not effective. The remaining 31 words are effective, that is, the coefficients of them are not zero in at least one of the three models. In our manuscript, Table 2 shows the 31 words. It is expected that the evaluation of prediction performance can also provide an indirect evaluation of the textual implications of the models and representative features. To gain further insights, we now more closely investigate overlapping keywords detected by MCP_L, SLS_L and L_L models. These 25 keywords of overlaps are identified by all the models. Some keywords are expressed as the expectations or needs of investors and their trading activities, like pessimistic ( 悲观 ), Cool down ( 冷却 ), Rise ( 升至 ). Some keywords represent the current event and financial topics, like Demographic Dividend ( 人口红利 ), Real Economy ( 实体经济 ), Securitization( 证券化 ). Some keywords indicate sufficient statistical meaning, that is, a stable relationship between keywords, like Intensify ( 加 剧 ), Prefer ( 首先 ). Some keywords reflect correspondence to target indicator, like company name, city name. Searching published literature suggest that these keywords may have important implications. Recent evidences in behavior finance indicates that emotion influence stock market   returns 6,39 . Previous studies suggest that the demographic variable can be related to the information component determining long-horizon stock market returns 40 . Westerhoff shows the interactions between the real economy and stock   market 41 . Fontana and Godin find the links between housing market and financial sector by taking into account the securitization   process 42 . Our result indicates events that most certainly influences stock market prices, public sentiment and opinion may play an equally important role in predicting stock movement.\nThe striking finding in Table 2 is that only five out of 25 terms have a positive impact on financial markets. Apparently even the positive word \"Securitization( 证券化 )\" received a negative connotation. The strongest positive indicator is \"Imagine( 想象 )\", and the strongest negative indicator is \"Concern( 关注度 )\". This finding seems to echo what   Soroka 43 and Wu et al. 44 discovered earlier-responses to positive and negative information are asymmetric-that negative information has a much greater impact on individuals' attitudes than does positive information.",
    "context": "Provides a detailed analysis of the network structure within the text data, highlighting the importance of \"Short Term\" as a central keyword and demonstrating the effectiveness of the proposed model in incorporating network information for improved prediction accuracy.",
    "document": "s41598-020-77823-3.pdf",
    "pages": [
      6,
      7
    ],
    "id": "8fa43baa6215dd058b377fe7478f41c7137f3e51e478b9f368590199ce536f26"
  },
  {
    "text": "This paper fills the gap in literature by integrating textual network in forecasting stock market and offers a number of advantages over alternative approaches. The sparse laplacian shrinkage logistic regression demonstrates stateof-the-art text predicting classification while producing sparsity and smoothness of efficient models. A richer penalty also may prove useful. Those used in our studies are not only informative in the statistical sense but also represent knowledge of the semantic network. We used text network analysis in order to take full advantage of\nVol.:(0123456789)\nVol:.(1234567890)\nTable 2. Results of the K-core collapse sequence analysis.\n\nFuzhou( 福州 ), Coefficient.MCP_L = - 1.0156. Fuzhou( 福州 ), Coefficient.SLS_L = - 0.3635. Fuzhou( 福州 ), Coefficient.L_L = - 0.0823. Design( 设计 ), Coefficient.MCP_L = - 6.2383. Design( 设计 ), Coefficient.SLS_L = - 0.9961. Design( 设计 ), Coefficient.L_L = - 0.2950. Demographic dividend( 人口红利 ), Coefficient.MCP_L = - 1.4596. Demographic dividend( 人口红利 ), Coefficient.SLS_L = - 1.4300. Demographic dividend( 人口红利 ), Coefficient.L_L = - 0.7243. Intensify( 加剧 ), Coefficient.MCP_L = - 2.3491. Intensify( 加剧 ), Coefficient.SLS_L = - 0.8559. Intensify( 加剧 ), Coefficient.L_L = - 0.3031. Input market( 入市 ), Coefficient.MCP_L = - 1.4538. Input market( 入市 ), Coefficient.SLS_L = - 0.4034. Input market( 入市 ), Coefficient.L_L = - 0.1640. Pessimism( 悲观 ), Coefficient.MCP_L = 6.6403. Pessimism( 悲观 ), Coefficient.SLS_L = 1.0625. Pessimism( 悲观 ), Coefficient.L_L = 0.3359. Concern( 关注度 ), Coefficient.MCP_L = - 10.1856. Concern( 关注度 ), Coefficient.SLS_L = - 1.9920. Concern( 关注度 ), Coefficient.L_L = - 0.8522. Anew( 重新 ), Coefficient.MCP_L = - 1.8171. Anew( 重新 ), Coefficient.SLS_L = - 0.3550. Anew( 重新 ), Coefficient.L_L = - 0.0789. Own funds( 自有资金 ), Coefficient.MCP_L = - 4.1016. Own funds( 自有资金 ), Coefficient.SLS_L = - 0.7022. Own funds( 自有资金 ), Coefficient.L_L = - 0.2374. Zhejiang( 浙江 ), Coefficient.MCP_L = 0. Zhejiang( 浙江 ), Coefficient.SLS_L = - 0.2541. Zhejiang( 浙江 ), Coefficient.L_L = - 0.0970. Hotspot( 热点 ), Coefficient.MCP_L = - 0.2639. Hotspot( 热点 ), Coefficient.SLS_L = 0. Hotspot( 热点 ), Coefficient.L_L = - 0.0019. Prefer( 首选 ), Coefficient.MCP_L = - 0.7123. Prefer( 首选 ), Coefficient.SLS_L = - 0.2818. Prefer( 首选 ), Coefficient.L_L = - 0.0752. Consistent( 一致 ), Coefficient.MCP_L = 0. Consistent( 一致 ), Coefficient.SLS_L = 0. Consistent( 一致 ), Coefficient.L_L = - 0.0087. Listing( 上市 ), Coefficient.MCP_L = - 0.3414. Listing( 上市 ), Coefficient.SLS_L = - 0.1252. Listing( 上市 ), Coefficient.L_L = - 0.0275. Principle( 原则 ), Coefficient.MCP_L = - 3.5691. Principle( 原则 ), Coefficient.SLS_L = 0. Principle( 原则 ), Coefficient.L_L = 0. Entrepreneurship( 创业 ), Coefficient.MCP_L = - 4.4760. Entrepreneurship( 创业 ), Coefficient.SLS_L = - 0.9795. Entrepreneurship( 创业 ), Coefficient.L_L = - 0.2328. Cool down( 冷却 ), Coefficient.MCP_L = - 4.3086. Cool down( 冷却 ), Coefficient.SLS_L = - 1.0358. Cool down( 冷却 ), Coefficient.L_L = - 0.0480. Rise( 升至 ), Coefficient.MCP_L = - 0.4476. Rise( 升至 ), Coefficient.SLS_L = - 0.4925. Rise( 升至 ), Coefficient.L_L = - 0.2201. Rise range( 上涨幅度 ), Coefficient.MCP_L = 4.5206. Rise range( 上涨幅度 ), Coefficient.SLS_L = 0.9423. Rise range( 上涨幅度 ), Coefficient.L_L = 0.1197. Chengyu stock( 成渝 ), Coefficient.MCP_L = - 5.2876. Chengyu stock( 成渝 ), Coefficient.SLS_L = - 1.9413. Chengyu stock( 成渝 ), Coefficient.L_L = - 0.2541. Real economy( 实体经济 ), Coefficient.MCP_L = - 3.7955. Real economy( 实体经济 ), Coefficient.SLS_L = - 1.0270. Real economy( 实体经济 ), Coefficient.L_L = - 0.1717. Federal reserve( 美联储 ), Coefficient.MCP_L = - 1.9805. Federal reserve( 美联储 ), Coefficient.SLS_L = - 0.6351. Federal reserve( 美联储 ), Coefficient.L_L = 0. Circulation( 流通 ), Coefficient.MCP_L = - 1.1935. Circulation( 流通 ), Coefficient.SLS_L = - 0.5240. Circulation( 流通 ), Coefficient.L_L = - 0.1364. Sign( 签署 ), Coefficient.MCP_L = - 1.3929. Sign( 签署 ), Coefficient.SLS_L = - 0.3504. Sign( 签署 ), Coefficient.L_L = - 0.0544. Volume of trade( 放量 ), Coefficient.MCP_L = - 2.0652. Volume of trade( 放量 ), Coefficient.SLS_L = - 0.3127. Volume of trade( 放量 ), Coefficient.L_L = - 0.0281. Securitization( 证券化 ), Coefficient.MCP_L = - 1.7006. Securitization( 证券化 ), Coefficient.SLS_L = - 0.4444. Securitization( 证券化 ), Coefficient.L_L = - 0.1080. Explore( 探索 ), Coefficient.MCP_L = 0. Explore( 探索 ), Coefficient.SLS_L = 0.4535. Explore( 探索 ), Coefficient.L_L = 0. Imagine( 想象 ), Coefficient.MCP_L = 7.2612. Imagine( 想象 ), Coefficient.SLS_L = 1.9061. Imagine( 想象 ), Coefficient.L_L = 0.4485. Effectiveness( 效用 ), Coefficient.MCP_L = 6.5592. Effectiveness( 效用 ), Coefficient.SLS_L = 1.9769. Effectiveness( 效用 ), Coefficient.L_L = 0.4402. Multiple( 倍数 ), Coefficient.MCP_L = 2.9660. Multiple( 倍数 ), Coefficient.SLS_L = 1.1980. Multiple( 倍数 ), Coefficient.L_L = 0.0678. Comprehensive planning( 总体规划 ), Coefficient.MCP_L = 5.9179. Comprehensive planning( 总体规划 ), Coefficient.SLS_L = 0.9804. Comprehensive planning( 总体规划 ), Coefficient.L_L = 0.2849\nsemantic relations, whereas previous text mining mainly emphasized words as independent variables with no connections. The features of the semantic network structure provide significantly effectiveness of text mining.\n\nIntegration of textual network information enhances stock market forecasting accuracy.",
    "original_text": "This paper fills the gap in literature by integrating textual network in forecasting stock market and offers a number of advantages over alternative approaches. The sparse laplacian shrinkage logistic regression demonstrates stateof-the-art text predicting classification while producing sparsity and smoothness of efficient models. A richer penalty also may prove useful. Those used in our studies are not only informative in the statistical sense but also represent knowledge of the semantic network. We used text network analysis in order to take full advantage of\nVol.:(0123456789)\nVol:.(1234567890)\nTable 2. Results of the K-core collapse sequence analysis.\n\nFuzhou( 福州 ), Coefficient.MCP_L = - 1.0156. Fuzhou( 福州 ), Coefficient.SLS_L = - 0.3635. Fuzhou( 福州 ), Coefficient.L_L = - 0.0823. Design( 设计 ), Coefficient.MCP_L = - 6.2383. Design( 设计 ), Coefficient.SLS_L = - 0.9961. Design( 设计 ), Coefficient.L_L = - 0.2950. Demographic dividend( 人口红利 ), Coefficient.MCP_L = - 1.4596. Demographic dividend( 人口红利 ), Coefficient.SLS_L = - 1.4300. Demographic dividend( 人口红利 ), Coefficient.L_L = - 0.7243. Intensify( 加剧 ), Coefficient.MCP_L = - 2.3491. Intensify( 加剧 ), Coefficient.SLS_L = - 0.8559. Intensify( 加剧 ), Coefficient.L_L = - 0.3031. Input market( 入市 ), Coefficient.MCP_L = - 1.4538. Input market( 入市 ), Coefficient.SLS_L = - 0.4034. Input market( 入市 ), Coefficient.L_L = - 0.1640. Pessimism( 悲观 ), Coefficient.MCP_L = 6.6403. Pessimism( 悲观 ), Coefficient.SLS_L = 1.0625. Pessimism( 悲观 ), Coefficient.L_L = 0.3359. Concern( 关注度 ), Coefficient.MCP_L = - 10.1856. Concern( 关注度 ), Coefficient.SLS_L = - 1.9920. Concern( 关注度 ), Coefficient.L_L = - 0.8522. Anew( 重新 ), Coefficient.MCP_L = - 1.8171. Anew( 重新 ), Coefficient.SLS_L = - 0.3550. Anew( 重新 ), Coefficient.L_L = - 0.0789. Own funds( 自有资金 ), Coefficient.MCP_L = - 4.1016. Own funds( 自有资金 ), Coefficient.SLS_L = - 0.7022. Own funds( 自有资金 ), Coefficient.L_L = - 0.2374. Zhejiang( 浙江 ), Coefficient.MCP_L = 0. Zhejiang( 浙江 ), Coefficient.SLS_L = - 0.2541. Zhejiang( 浙江 ), Coefficient.L_L = - 0.0970. Hotspot( 热点 ), Coefficient.MCP_L = - 0.2639. Hotspot( 热点 ), Coefficient.SLS_L = 0. Hotspot( 热点 ), Coefficient.L_L = - 0.0019. Prefer( 首选 ), Coefficient.MCP_L = - 0.7123. Prefer( 首选 ), Coefficient.SLS_L = - 0.2818. Prefer( 首选 ), Coefficient.L_L = - 0.0752. Consistent( 一致 ), Coefficient.MCP_L = 0. Consistent( 一致 ), Coefficient.SLS_L = 0. Consistent( 一致 ), Coefficient.L_L = - 0.0087. Listing( 上市 ), Coefficient.MCP_L = - 0.3414. Listing( 上市 ), Coefficient.SLS_L = - 0.1252. Listing( 上市 ), Coefficient.L_L = - 0.0275. Principle( 原则 ), Coefficient.MCP_L = - 3.5691. Principle( 原则 ), Coefficient.SLS_L = 0. Principle( 原则 ), Coefficient.L_L = 0. Entrepreneurship( 创业 ), Coefficient.MCP_L = - 4.4760. Entrepreneurship( 创业 ), Coefficient.SLS_L = - 0.9795. Entrepreneurship( 创业 ), Coefficient.L_L = - 0.2328. Cool down( 冷却 ), Coefficient.MCP_L = - 4.3086. Cool down( 冷却 ), Coefficient.SLS_L = - 1.0358. Cool down( 冷却 ), Coefficient.L_L = - 0.0480. Rise( 升至 ), Coefficient.MCP_L = - 0.4476. Rise( 升至 ), Coefficient.SLS_L = - 0.4925. Rise( 升至 ), Coefficient.L_L = - 0.2201. Rise range( 上涨幅度 ), Coefficient.MCP_L = 4.5206. Rise range( 上涨幅度 ), Coefficient.SLS_L = 0.9423. Rise range( 上涨幅度 ), Coefficient.L_L = 0.1197. Chengyu stock( 成渝 ), Coefficient.MCP_L = - 5.2876. Chengyu stock( 成渝 ), Coefficient.SLS_L = - 1.9413. Chengyu stock( 成渝 ), Coefficient.L_L = - 0.2541. Real economy( 实体经济 ), Coefficient.MCP_L = - 3.7955. Real economy( 实体经济 ), Coefficient.SLS_L = - 1.0270. Real economy( 实体经济 ), Coefficient.L_L = - 0.1717. Federal reserve( 美联储 ), Coefficient.MCP_L = - 1.9805. Federal reserve( 美联储 ), Coefficient.SLS_L = - 0.6351. Federal reserve( 美联储 ), Coefficient.L_L = 0. Circulation( 流通 ), Coefficient.MCP_L = - 1.1935. Circulation( 流通 ), Coefficient.SLS_L = - 0.5240. Circulation( 流通 ), Coefficient.L_L = - 0.1364. Sign( 签署 ), Coefficient.MCP_L = - 1.3929. Sign( 签署 ), Coefficient.SLS_L = - 0.3504. Sign( 签署 ), Coefficient.L_L = - 0.0544. Volume of trade( 放量 ), Coefficient.MCP_L = - 2.0652. Volume of trade( 放量 ), Coefficient.SLS_L = - 0.3127. Volume of trade( 放量 ), Coefficient.L_L = - 0.0281. Securitization( 证券化 ), Coefficient.MCP_L = - 1.7006. Securitization( 证券化 ), Coefficient.SLS_L = - 0.4444. Securitization( 证券化 ), Coefficient.L_L = - 0.1080. Explore( 探索 ), Coefficient.MCP_L = 0. Explore( 探索 ), Coefficient.SLS_L = 0.4535. Explore( 探索 ), Coefficient.L_L = 0. Imagine( 想象 ), Coefficient.MCP_L = 7.2612. Imagine( 想象 ), Coefficient.SLS_L = 1.9061. Imagine( 想象 ), Coefficient.L_L = 0.4485. Effectiveness( 效用 ), Coefficient.MCP_L = 6.5592. Effectiveness( 效用 ), Coefficient.SLS_L = 1.9769. Effectiveness( 效用 ), Coefficient.L_L = 0.4402. Multiple( 倍数 ), Coefficient.MCP_L = 2.9660. Multiple( 倍数 ), Coefficient.SLS_L = 1.1980. Multiple( 倍数 ), Coefficient.L_L = 0.0678. Comprehensive planning( 总体规划 ), Coefficient.MCP_L = 5.9179. Comprehensive planning( 总体规划 ), Coefficient.SLS_L = 0.9804. Comprehensive planning( 总体规划 ), Coefficient.L_L = 0.2849\nsemantic relations, whereas previous text mining mainly emphasized words as independent variables with no connections. The features of the semantic network structure provide significantly effectiveness of text mining.",
    "context": "Integration of textual network information enhances stock market forecasting accuracy.",
    "document": "s41598-020-77823-3.pdf",
    "pages": [
      8,
      7
    ],
    "id": "092e992fe517e1dc68fbebbec0c44803c01b478b065912390dcc54f5d9f69b53"
  },
  {
    "text": "Our experimental findings suggest textual information in the online analyst reports obtains better performance. Our work on stock-predictive text mining is specifically researched in a sector context, e.g. the web pages of all Chinese analysts' research reports of real estate sector from a well-known financial website. Furthermore, the textual information can give a full description of a specific stock or industry sector in order to help investors consider the complete decision context, rather than focus too narrowly on the quantitative measures of the prediction. Moreover, the research reports are not limited to the individual summary elements of earnings forecasts, such as whether to buy or sell a particular stock. Thus, it is evident that online security reports are a valuable reservoir of the reaction of stock market response to the message as feedback.\nAs noted above, there is still much room for performance improvement on stock movements forecasting. We have much work ahead of us. The predictive keyword vector extraction method we used is not sufficient to find the accuracy features in each document. The decision on keyword extraction is crucial because from an incorrect input nothing more than a meaningless output can be released. One of the shortcomings of our study is that only the historical stock index and semantics derived from the online research report are concerned. In the future, we will try to find and integrate more factors which can influence the stock market to develop a more accurate stock forecasting model.\nIn addition, the network construction procedure discussed in this paper is simple and straightforward. There are multiple ways of measuring the similarity among words, such as the log-likelihood   ratio 45 , Chi-square, cosine and pointwise mutual information, etc. To the best of our knowledge, there is a lack of definitive evidence on the relative performance of different network construction procedures. Our empirical results show that the proposed SLS_L model performance can be improved significantly by incorporating network connected information. However, the adjacency measure is based on the Pearson correlation coefficient. This may limit our model in some context. It is possible that in practical data, adopting other network construction methods may further improve prediction and feature selection. Moreover, some preprocessing was performed on the nodes prior to\nthe network construction and that significant \"noise\" may be removed. In future research, we will consider other ways of defining the similarity measure and adjacency matrix that can improve interpretability and reduce bias.\nLastly, how to apply our approach to practice is a challenging problem waiting to be explored. Guo et al. 46 , M'ng and   Mehralizadeh 47 have discussed the effectiveness of auto regression algorithms such as time series analysis. Given the market movement on a time-series, when the time-window is slid from the start to the end, the training and testing windows are captured sequentially. Future work will collect more volume of datasets containing mappings of text onto stock movements forecasting for multiple days that can explore the auto regression algorithm based on SLS_L for stock movement prediction. This strategy has the attractive property of effectiveness in the practical stock market analysis.\nReceived: 15 August 2020; Accepted: 12 November 2020\n\nFuture research and development are needed to improve stock movement forecasting accuracy. The current keyword extraction method is insufficient for identifying key features, and incorporating additional factors influencing the stock market would enhance model accuracy. Further investigation into alternative network construction methods and preprocessing techniques could also lead to improved results.",
    "original_text": "Our experimental findings suggest textual information in the online analyst reports obtains better performance. Our work on stock-predictive text mining is specifically researched in a sector context, e.g. the web pages of all Chinese analysts' research reports of real estate sector from a well-known financial website. Furthermore, the textual information can give a full description of a specific stock or industry sector in order to help investors consider the complete decision context, rather than focus too narrowly on the quantitative measures of the prediction. Moreover, the research reports are not limited to the individual summary elements of earnings forecasts, such as whether to buy or sell a particular stock. Thus, it is evident that online security reports are a valuable reservoir of the reaction of stock market response to the message as feedback.\nAs noted above, there is still much room for performance improvement on stock movements forecasting. We have much work ahead of us. The predictive keyword vector extraction method we used is not sufficient to find the accuracy features in each document. The decision on keyword extraction is crucial because from an incorrect input nothing more than a meaningless output can be released. One of the shortcomings of our study is that only the historical stock index and semantics derived from the online research report are concerned. In the future, we will try to find and integrate more factors which can influence the stock market to develop a more accurate stock forecasting model.\nIn addition, the network construction procedure discussed in this paper is simple and straightforward. There are multiple ways of measuring the similarity among words, such as the log-likelihood   ratio 45 , Chi-square, cosine and pointwise mutual information, etc. To the best of our knowledge, there is a lack of definitive evidence on the relative performance of different network construction procedures. Our empirical results show that the proposed SLS_L model performance can be improved significantly by incorporating network connected information. However, the adjacency measure is based on the Pearson correlation coefficient. This may limit our model in some context. It is possible that in practical data, adopting other network construction methods may further improve prediction and feature selection. Moreover, some preprocessing was performed on the nodes prior to\nthe network construction and that significant \"noise\" may be removed. In future research, we will consider other ways of defining the similarity measure and adjacency matrix that can improve interpretability and reduce bias.\nLastly, how to apply our approach to practice is a challenging problem waiting to be explored. Guo et al. 46 , M'ng and   Mehralizadeh 47 have discussed the effectiveness of auto regression algorithms such as time series analysis. Given the market movement on a time-series, when the time-window is slid from the start to the end, the training and testing windows are captured sequentially. Future work will collect more volume of datasets containing mappings of text onto stock movements forecasting for multiple days that can explore the auto regression algorithm based on SLS_L for stock movement prediction. This strategy has the attractive property of effectiveness in the practical stock market analysis.\nReceived: 15 August 2020; Accepted: 12 November 2020",
    "context": "Future research and development are needed to improve stock movement forecasting accuracy. The current keyword extraction method is insufficient for identifying key features, and incorporating additional factors influencing the stock market would enhance model accuracy. Further investigation into alternative network construction methods and preprocessing techniques could also lead to improved results.",
    "document": "s41598-020-77823-3.pdf",
    "pages": [
      8,
      9
    ],
    "id": "1e26e8e867f657afb3405e5546d8fc8e3f32c8060acc4e75eef0e3094c1ae611"
  },
  {
    "text": "1.  Nassirtoussi, A. K., Aghabozorgi, S., Wah, T. Y. & Ngo, D. C. L. Text mining for market prediction: a systematic review. Expert. Syst. Appl. 41 , 7653-7670 (2014).\n2.  Junqué de Fortuny, E., De Smedt, T., Martens, D. & Daelemans, W . Evaluating and understanding text-based stock price prediction models. Inform. Process. Manag. 50 , 426-441 (2014).\n3.  Kleinnijenhuis, J., Schultz, F., Oegema, D. & van Atteveldt, W . Financial news and market panics in the age of high-frequency sentiment trading algorithms. Journalism. 14 , 271-291 (2013).\n4.  De Long, J. B., Shleifer, A., Summers, L. H. & Waldmann, R. J. Noise Trader risk in financial-markets. J. Polit. Econ. 98 , 703-738 (1990).\n5.  Shleifer, A. & Vishny, R. W . The limits of arbitrage. J. Financ. 52 , 35-55 (1997).\n6.  Li, F. Do Stock Market Investors Understand the Risk Sentiment of Corporate Annual Reports? Working paper , 54 (2006).\n7.  Schumaker, R. P., Zhang, Y. L., Huang, C. N. & Chen, H. C. Evaluating sentiment in financial news articles. Decis. Support. Syst. 53 , 458-464 (2012).\n8.  Hollanders, D. & Vliegenthart, R. The influence of negative newspaper coverage on consumer confidence: the Dutch case. J. Econ. Psychol. 32 , 367-373 (2011).\n9.  Loughran, T. & Mcdonald, B. Textual analysis in accounting and finance: a survey. J. Account. Res. 54 , 1187-1230 (2016).\n10.  Previts, G. J., Bricker, R. J., Robinson, T. R. & Y oung, S. J. A Content analysis of sell-side financial analyst company reports. Account. Horiz. 8 , 55-70 (1994).\nJ. Financ. Econ.\n11.  Asquith, P., Mikhail, M. B. & Au, A. S. Information content of equity analyst reports.\n75\n, 245-282 (2005).\n12.  Twedt, B. & Rees, L. Reading between the lines: An empirical examination of qualitative attributes of financial analysts' reports. J. Account. Public. Pol. 31 , 1-21 (2012).\n13.  Peng, H. Y., Cambria, E. & Hussain, A. A Review of Sentiment Analysis Research in Chinese Language. Cogn. Comput. 9 , 423-435 (2017).\n14.  Zhang, C. L., Zeng, D., Li, J. X., Wang, F. Y. & Zuo, W . L. Sentiment analysis of Chinese documents: from sentence to document level. J. Am. Soc. Inf. Sci. Technol. 60 , 2474-2487 (2009).\n15.  Wang, N., Ke, S. H., Chen, Y. B., Yan, T. & Lim, A. Textual Sentiment of Chinese Microblog Toward the Stock Market. Int. T. J. Inf. Tech. Decis. 18 , 649-671 (2018).\n16.  Tibshirani, R. Regression shrinkage and selection via the Lasso: a retrospective. J. R. Stat. Soc. B. 73 , 273-282 (2011).\n17.  Fan, J. Q. & Li, R. Z. Variable selection via nonconcave penalized likelihood and its oracle properties. J. Am. Stat. Assoc. 96 , 1348-1360 (2001).\n18.  Zou, H. & Hastie, T. Regularization and variable selection via the elastic net. J. R. Stat. Soc. B. 67 , 301-320 (2005).\n19.  Zou, H. The adaptive lasso and its oracle properties. J. Am. Stat. Assoc. 101 , 1418-1429 (2006).\n20.  Zhang, C. H. Nearly unbiased variable selection under minimax concave penalty. Ann. Stat. 38 , 894-942 (2010).\n21.  Breheny, P. & Huang, J. Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection. Ann. Appl. Stat. 5 , 232-253 (2011).\n22.  Huang, J., Wei, F. R. & Ma, S. G. Semiparametric regression pursuit. Stat. Sinica. 22 , 1403-1426 (2012).\n23.  Li, C. Y. & Li, H. Z. Network-constrained regularization and variable selection for analysis of genomic data. Bioinformatics 24 , 1175-1182 (2008).\n24.  Huang, J., Ma, S. G., Li, H. Z. & Zhang, C. H. The sparse laplacian shrinkage estimator for high-dimensional regression. Ann. Stat. 39 , 2021-2046 (2011).\n25.  Bergmeir, C. & Benitez, J. M. On the use of cross-validation for time series predictor evaluation. Inform. Sci. 191 , 192-213 (2012).\n26.  Hsieh, F. S. & Turnbull, B. W . Nonparametric and semiparametric estimation of the receiver operating characteristic curve. Ann. Stat. 24 , 25-40 (1996).\n27.  Fawcett, T. An introduction to ROC analysis. Pattern. Recogn. Lett. 27 , 861-874 (2006).\n28.  Chen, Q. & Lv, X. The extreme-value dependence between the crude oil price and Chinese stock markets. Int. Rev. Econ. Financ. 39 , 121-132 (2015).\n29.  Hu, C. H. & Liu, S. S. The implications of low R-2: evidence from China. Emerg. Mark. Financ. Tr. 49 , 17-32 (2013).\n30.  Wang, H., Wu, J. J., Yuan, S. & Chen, J. On characterizing scale effect of Chinese mutual funds via text mining. Signal Process. 124 , 266-278 (2016).\n31.  Ko, K., Wang, Y., Paek, M. & Ha, Y. The flow-performance relationship of Chinese equity mutual funds: net flows, inflows, and outflows. Asia-Pac. J. Financ. St. 43 , 273-296 (2014).\n32.  Li, J., Zhang, P . Z. & Cao, J. W . External concept support for group support systems through web mining. J. Am. Soc. Inf. Sci. Tec. 60 , 1057-1070 (2009).\n33.  'Jieba' Chinese Word Segmentation Tool. https  ://githu  b.com/fxsjy  /jieba (accessed on 20 August 2018).\n34.  Bishop, C. M. Pattern Recognition and Machine Learning (Springer-Verlag, Berlin, Heidelberg, 2006).\n35.  Hagenau, M., Liebmann, M. & Neumann, D. Automated news reading: stock price prediction based on financial news using context-capturing features. Decis. Support. Syst. 55 , 685-697 (2013).\n36.  Tasci, S. & Gungor, T. Comparison of text feature selection policies and using an adaptive framework. Expert. Syst. Appl. 40 , 4871-4886 (2013).\n37.  Yoon, B. & Park, Y. A text-mining-based patent network: analytical tool for high-technology trend. J. High Technol. Manag. Res. 15 , 37-50 (2004).\n38.  Dorogovtsev, S. N., Goltsev, A. V . & Mendes, J. F. F. K-core organization of complex networks. Phys. Rev. Lett. 96 , 040601 (2005).\n39.  Li, Q. et al. The effect of news and public mood on stock movements. Inform. Sci. 278 , 826-840 (2014).\n40.  Favero, C. A. & Tamoni, A. Demographics and US stock market fluctuations. Cesifo. Econ. Stud. 57 , 25-43 (2011).\nVol.:(0123456789)\nVol:.(1234567890)\n\nProvides supporting evidence for the argument on economic inequality.",
    "original_text": "1.  Nassirtoussi, A. K., Aghabozorgi, S., Wah, T. Y. & Ngo, D. C. L. Text mining for market prediction: a systematic review. Expert. Syst. Appl. 41 , 7653-7670 (2014).\n2.  Junqué de Fortuny, E., De Smedt, T., Martens, D. & Daelemans, W . Evaluating and understanding text-based stock price prediction models. Inform. Process. Manag. 50 , 426-441 (2014).\n3.  Kleinnijenhuis, J., Schultz, F., Oegema, D. & van Atteveldt, W . Financial news and market panics in the age of high-frequency sentiment trading algorithms. Journalism. 14 , 271-291 (2013).\n4.  De Long, J. B., Shleifer, A., Summers, L. H. & Waldmann, R. J. Noise Trader risk in financial-markets. J. Polit. Econ. 98 , 703-738 (1990).\n5.  Shleifer, A. & Vishny, R. W . The limits of arbitrage. J. Financ. 52 , 35-55 (1997).\n6.  Li, F. Do Stock Market Investors Understand the Risk Sentiment of Corporate Annual Reports? Working paper , 54 (2006).\n7.  Schumaker, R. P., Zhang, Y. L., Huang, C. N. & Chen, H. C. Evaluating sentiment in financial news articles. Decis. Support. Syst. 53 , 458-464 (2012).\n8.  Hollanders, D. & Vliegenthart, R. The influence of negative newspaper coverage on consumer confidence: the Dutch case. J. Econ. Psychol. 32 , 367-373 (2011).\n9.  Loughran, T. & Mcdonald, B. Textual analysis in accounting and finance: a survey. J. Account. Res. 54 , 1187-1230 (2016).\n10.  Previts, G. J., Bricker, R. J., Robinson, T. R. & Y oung, S. J. A Content analysis of sell-side financial analyst company reports. Account. Horiz. 8 , 55-70 (1994).\nJ. Financ. Econ.\n11.  Asquith, P., Mikhail, M. B. & Au, A. S. Information content of equity analyst reports.\n75\n, 245-282 (2005).\n12.  Twedt, B. & Rees, L. Reading between the lines: An empirical examination of qualitative attributes of financial analysts' reports. J. Account. Public. Pol. 31 , 1-21 (2012).\n13.  Peng, H. Y., Cambria, E. & Hussain, A. A Review of Sentiment Analysis Research in Chinese Language. Cogn. Comput. 9 , 423-435 (2017).\n14.  Zhang, C. L., Zeng, D., Li, J. X., Wang, F. Y. & Zuo, W . L. Sentiment analysis of Chinese documents: from sentence to document level. J. Am. Soc. Inf. Sci. Technol. 60 , 2474-2487 (2009).\n15.  Wang, N., Ke, S. H., Chen, Y. B., Yan, T. & Lim, A. Textual Sentiment of Chinese Microblog Toward the Stock Market. Int. T. J. Inf. Tech. Decis. 18 , 649-671 (2018).\n16.  Tibshirani, R. Regression shrinkage and selection via the Lasso: a retrospective. J. R. Stat. Soc. B. 73 , 273-282 (2011).\n17.  Fan, J. Q. & Li, R. Z. Variable selection via nonconcave penalized likelihood and its oracle properties. J. Am. Stat. Assoc. 96 , 1348-1360 (2001).\n18.  Zou, H. & Hastie, T. Regularization and variable selection via the elastic net. J. R. Stat. Soc. B. 67 , 301-320 (2005).\n19.  Zou, H. The adaptive lasso and its oracle properties. J. Am. Stat. Assoc. 101 , 1418-1429 (2006).\n20.  Zhang, C. H. Nearly unbiased variable selection under minimax concave penalty. Ann. Stat. 38 , 894-942 (2010).\n21.  Breheny, P. & Huang, J. Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection. Ann. Appl. Stat. 5 , 232-253 (2011).\n22.  Huang, J., Wei, F. R. & Ma, S. G. Semiparametric regression pursuit. Stat. Sinica. 22 , 1403-1426 (2012).\n23.  Li, C. Y. & Li, H. Z. Network-constrained regularization and variable selection for analysis of genomic data. Bioinformatics 24 , 1175-1182 (2008).\n24.  Huang, J., Ma, S. G., Li, H. Z. & Zhang, C. H. The sparse laplacian shrinkage estimator for high-dimensional regression. Ann. Stat. 39 , 2021-2046 (2011).\n25.  Bergmeir, C. & Benitez, J. M. On the use of cross-validation for time series predictor evaluation. Inform. Sci. 191 , 192-213 (2012).\n26.  Hsieh, F. S. & Turnbull, B. W . Nonparametric and semiparametric estimation of the receiver operating characteristic curve. Ann. Stat. 24 , 25-40 (1996).\n27.  Fawcett, T. An introduction to ROC analysis. Pattern. Recogn. Lett. 27 , 861-874 (2006).\n28.  Chen, Q. & Lv, X. The extreme-value dependence between the crude oil price and Chinese stock markets. Int. Rev. Econ. Financ. 39 , 121-132 (2015).\n29.  Hu, C. H. & Liu, S. S. The implications of low R-2: evidence from China. Emerg. Mark. Financ. Tr. 49 , 17-32 (2013).\n30.  Wang, H., Wu, J. J., Yuan, S. & Chen, J. On characterizing scale effect of Chinese mutual funds via text mining. Signal Process. 124 , 266-278 (2016).\n31.  Ko, K., Wang, Y., Paek, M. & Ha, Y. The flow-performance relationship of Chinese equity mutual funds: net flows, inflows, and outflows. Asia-Pac. J. Financ. St. 43 , 273-296 (2014).\n32.  Li, J., Zhang, P . Z. & Cao, J. W . External concept support for group support systems through web mining. J. Am. Soc. Inf. Sci. Tec. 60 , 1057-1070 (2009).\n33.  'Jieba' Chinese Word Segmentation Tool. https  ://githu  b.com/fxsjy  /jieba (accessed on 20 August 2018).\n34.  Bishop, C. M. Pattern Recognition and Machine Learning (Springer-Verlag, Berlin, Heidelberg, 2006).\n35.  Hagenau, M., Liebmann, M. & Neumann, D. Automated news reading: stock price prediction based on financial news using context-capturing features. Decis. Support. Syst. 55 , 685-697 (2013).\n36.  Tasci, S. & Gungor, T. Comparison of text feature selection policies and using an adaptive framework. Expert. Syst. Appl. 40 , 4871-4886 (2013).\n37.  Yoon, B. & Park, Y. A text-mining-based patent network: analytical tool for high-technology trend. J. High Technol. Manag. Res. 15 , 37-50 (2004).\n38.  Dorogovtsev, S. N., Goltsev, A. V . & Mendes, J. F. F. K-core organization of complex networks. Phys. Rev. Lett. 96 , 040601 (2005).\n39.  Li, Q. et al. The effect of news and public mood on stock movements. Inform. Sci. 278 , 826-840 (2014).\n40.  Favero, C. A. & Tamoni, A. Demographics and US stock market fluctuations. Cesifo. Econ. Stud. 57 , 25-43 (2011).\nVol.:(0123456789)\nVol:.(1234567890)",
    "context": "Provides supporting evidence for the argument on economic inequality.",
    "document": "s41598-020-77823-3.pdf",
    "pages": [
      9,
      10
    ],
    "id": "aa92b229c85bf880105c30e69b18f1118d28877d2139f5c656d30c7152bb5bf3"
  },
  {
    "text": "41.  Westerhoff, F. Interactions between the real economy and the stock market: a simple agent-based approach. Discret. Dyn. Nat. Soc. 2012 , 504840 (2012).\n42.  Fontana, O. & Godin, A. Securitization, Housing Market and Banking Sector Behavior in a Stock-Flow Consistent Model. Economics Discussion Papers 2013 .\n43.  Soroka, S. N. Good news and bad news: asymmetric responses to economic information. J. Politics. 68 , 372-385 (2006).\n44.  Wu, H. D., Stevenson, R. L., Chen, H. & Güner, Z. N. The conditioned impact of recession news: a time-series analysis of economic communication in the United States, 1987-1996. Int. J. Public. Opin. R. 14 , 19-36 (2002).\n45.  Dunning, T. Accurate Methods for the Statistics of Surprise and Coincidence 61-74 (MIT Press, Cambridge, 1993).\n46.  Guo, Z. Q., Wang, H. Q., Liu, Q. & Yang, J. A feature fusion based forecasting model for financial time series. PLoS ONE 9 , e0101113 (2014).\n47.  M'ng, J. C. P . & Mehralizadeh, M. Forecasting East Asian indices futures via a novel hybrid of wavelet-PCA denoising and artificial neural network models. PLoS ONE 11 , e0156338 (2016).\n\nProvides supplementary research context and references for a forecasting model, building upon previous work in financial time series analysis.",
    "original_text": "41.  Westerhoff, F. Interactions between the real economy and the stock market: a simple agent-based approach. Discret. Dyn. Nat. Soc. 2012 , 504840 (2012).\n42.  Fontana, O. & Godin, A. Securitization, Housing Market and Banking Sector Behavior in a Stock-Flow Consistent Model. Economics Discussion Papers 2013 .\n43.  Soroka, S. N. Good news and bad news: asymmetric responses to economic information. J. Politics. 68 , 372-385 (2006).\n44.  Wu, H. D., Stevenson, R. L., Chen, H. & Güner, Z. N. The conditioned impact of recession news: a time-series analysis of economic communication in the United States, 1987-1996. Int. J. Public. Opin. R. 14 , 19-36 (2002).\n45.  Dunning, T. Accurate Methods for the Statistics of Surprise and Coincidence 61-74 (MIT Press, Cambridge, 1993).\n46.  Guo, Z. Q., Wang, H. Q., Liu, Q. & Yang, J. A feature fusion based forecasting model for financial time series. PLoS ONE 9 , e0101113 (2014).\n47.  M'ng, J. C. P . & Mehralizadeh, M. Forecasting East Asian indices futures via a novel hybrid of wavelet-PCA denoising and artificial neural network models. PLoS ONE 11 , e0156338 (2016).",
    "context": "Provides supplementary research context and references for a forecasting model, building upon previous work in financial time series analysis.",
    "document": "s41598-020-77823-3.pdf",
    "pages": [
      10
    ],
    "id": "1bd961d74246feff57bcd69675fc10464779b15e752cce01761b69262348dda4"
  },
  {
    "text": "This work was supported by the Fund for Shanxi \"1331 Project\" Key Innovative Research Team; the Humanities and Social Sciences Foundation of Ministry of Education, China (No.20YJA910004); Transformation of Scientific and Technological Achievements Programs of Higher Education Institutions in Shanxi (2019SK077); Program for the Philosophy and Social Sciences Key Research Base of Higher Education Institutions of Shanxi (2015317); the National Natural Science Foundation of China (No.31501002).\n\nDetails of funding sources and research grants supporting the study, including specific institutions and project numbers.",
    "original_text": "This work was supported by the Fund for Shanxi \"1331 Project\" Key Innovative Research Team; the Humanities and Social Sciences Foundation of Ministry of Education, China (No.20YJA910004); Transformation of Scientific and Technological Achievements Programs of Higher Education Institutions in Shanxi (2019SK077); Program for the Philosophy and Social Sciences Key Research Base of Higher Education Institutions of Shanxi (2015317); the National Natural Science Foundation of China (No.31501002).",
    "context": "Details of funding sources and research grants supporting the study, including specific institutions and project numbers.",
    "document": "s41598-020-77823-3.pdf",
    "pages": [
      10
    ],
    "id": "3eab2cd98dc0306d5693aaf18ebe513085a770618ad242fefbf635824589e7c9"
  },
  {
    "text": "Y.L. and Z.M. designed research; Y.L. and W .J. performed research and wrote the paper.\n\nDetails the authors' roles in the research design and execution.",
    "original_text": "Y.L. and Z.M. designed research; Y.L. and W .J. performed research and wrote the paper.",
    "context": "Details the authors' roles in the research design and execution.",
    "document": "s41598-020-77823-3.pdf",
    "pages": [
      10
    ],
    "id": "6c74598eb7f6c0bfa5f20004e50590c3c9386c2c22f166cb1013845c6c7c29a3"
  },
  {
    "text": "Correspondence and requests for materials should be addressed to W .J.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nOpen Access This  article  is  licensed  under  a  Creative  Commons  Attribution  4.0  International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat  iveco  mmons  .org/licen  ses/by/4.0/.\n© The Author(s) 2020\n\nDetails the copyright, licensing, and contact information for reprints and permissions.",
    "original_text": "Correspondence and requests for materials should be addressed to W .J.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nOpen Access This  article  is  licensed  under  a  Creative  Commons  Attribution  4.0  International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat  iveco  mmons  .org/licen  ses/by/4.0/.\n© The Author(s) 2020",
    "context": "Details the copyright, licensing, and contact information for reprints and permissions.",
    "document": "s41598-020-77823-3.pdf",
    "pages": [
      10
    ],
    "id": "2be9ec9978f63e629785c33b008109ab2ad546d5e7a7ff10d0d2a77e3b0eb9a3"
  },
  {
    "text": "Shigeki Mizushima , Naoki Kuramoto , Kenichi Fujii , and Takahide Umeda\nAbstract -For the future realization of the kilogram using the X-ray crystal density (XRCD) method, isotopically enriched silicon crystals grown by the floating zone method are employed. In this paper, we present quantitative electron paramagnetic resonance (EPR) measurements on 28 Si single crystal A VO28 to increase the reliability of mass deficit correction in the XRCD method. We detected phosphorus impurity in the crystal and determined its concentration to be 3.2(5) × 10 12 cm -3 , which is consistent with that estimated using Fourier transform infrared spectroscopy. In addition, the EPR measurements revealed that the concentrations of nine types of vacancy defects with unpaired electrons in the crystal are less than 1 × 10 12 cm -3 at 25 K both in a dark environment and under illumination. As a result, the necessary mass deficit correction due to these vacancy defects is estimated to be 0.0(2) θ g for 1-kg AVO28 spheres.\nIndex Terms -Electron paramagnetic resonance (EPR), kilogram, measurement uncertainty, metrology, silicon crystal, vacancy defects.\n\nProvides quantitative EPR measurements to increase reliability of mass deficit correction in the XRCD method, identifying phosphorus impurity and reporting low concentrations of nine types of vacancy defects.",
    "original_text": "Shigeki Mizushima , Naoki Kuramoto , Kenichi Fujii , and Takahide Umeda\nAbstract -For the future realization of the kilogram using the X-ray crystal density (XRCD) method, isotopically enriched silicon crystals grown by the floating zone method are employed. In this paper, we present quantitative electron paramagnetic resonance (EPR) measurements on 28 Si single crystal A VO28 to increase the reliability of mass deficit correction in the XRCD method. We detected phosphorus impurity in the crystal and determined its concentration to be 3.2(5) × 10 12 cm -3 , which is consistent with that estimated using Fourier transform infrared spectroscopy. In addition, the EPR measurements revealed that the concentrations of nine types of vacancy defects with unpaired electrons in the crystal are less than 1 × 10 12 cm -3 at 25 K both in a dark environment and under illumination. As a result, the necessary mass deficit correction due to these vacancy defects is estimated to be 0.0(2) θ g for 1-kg AVO28 spheres.\nIndex Terms -Electron paramagnetic resonance (EPR), kilogram, measurement uncertainty, metrology, silicon crystal, vacancy defects.",
    "context": "Provides quantitative EPR measurements to increase reliability of mass deficit correction in the XRCD method, identifying phosphorus impurity and reporting low concentrations of nine types of vacancy defects.",
    "document": "Electron_Paramagnetic_Resonance_Study_on_28Si_Single_Crystal_for_the_Future_Realization_of_the_Kilogram.pdf",
    "pages": [
      1
    ],
    "id": "eea82ad100ad4d66ff6008d891e6de1bbe955be35a1a27c1a14ffb144e56123b"
  },
  {
    "text": "T HE X-ray crystal density (XRCD) method is one of two independent methods that can realize the future realization of the kilogram with the highest accuracy [1]. The mass of a 28 Si single-crystal sphere with a diameter of approximately 93.7 mm is determined by counting the number of silicon atoms in the sphere along with using several basic physical constants, including the Planck constant h . If a significant number of defects exist in the crystal, then the mass deficit caused by their presence must be corrected. Here, the mass deficit is defined as the mass difference between two silicon spheres with a diameter of 93.7 cm, one of which is made of an ideal crystal, and the other of which is made of a real crystal containing defects. Fig. 1 shows the schematic of a normal structure, monovacancy defect V , and substitutional impurity defect in silicon crystals.\nManuscript received June 26, 2018; revised October 16, 2018; accepted November 8, 2018. Date of publication December 17, 2018; date of current version May 10, 2019. This work was supported by JSPS KAKENHI under Grant 17K05112. The Associate Editor coordinating the review process was Dr. Michael Lombardi. (Corresponding author: Shigeki Mizushima.)\n- S. Mizushima, N. Kuramoto, and K. Fujii are with the National Metrology Institute of Japan, National Institute of Advanced Industrial Science and Technology, Tsukuba 305-8563, Japan (e-mail: s.mizushima@aist.go.jp; n.kuramoto@aist.go.jp; fujii.kenichi@aist.go.jp).\n- T. Umeda is with Institute of Applied Physics, University of Tsukuba, Tsukuba 305-8573, Japan (e-mail: umeda@bk.tsukuba.ac.jp).\n3. Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org.\nDigital Object Identifier 10.1109/TIM.2018.2884044\nFig. 1. Ball-and-stick models of a normal structure, monovacancy defect V , and substitutional impurity defect in silicon crystals. Gray lines: chemical bonds between atoms.\nSeveral studies on defects in silicon crystals have been conducted to realize the kilogram using silicon crystals. Deslattes and Kessler [2] reported a qualitative difference between a silicon crystal with a natural isotope ratio of National Metrology Institute of Japan (NMIJ), formerly National Research Laboratory of Metrology, and that of Physikalisch-Technische Bundesanstalt by employing electron paramagnetic resonance (EPR) spectroscopy. They detected the EPR signals of impurities only from the NMIJ's crystal. Gebauer et al. [3] determined that the vacancy concentration in a silicon crystal grown by the floating zone (FZ) method ranges from 1 × 10 14 cm -3 to 4 × 10 14 cm -3 using positron annihilation lifetime spectroscopy (PALS) that detects neutral or negatively charged vacancies. They performed electron irradiation with low doses to obtain a quantitative calibration using an estimated introduction rate of 0.1 cm -1 for monovacancies. They did not find any relation between the average positron lifetime and the position in the radial direction of the crystal. They concluded that a decomposition of the positron lifetime spectra does not give reliable results because of the low intensities of the defect components. They estimated that hexavacancy V 6 is a candidate for explaining their PALS results. D'Agostino et al. [4] developed a method to measure the void concentration of 10 14 cm -3 in a silicon crystal with a few percent uncertainties using Cu decoration and neutron activation.\nAt present, the International Avogadro Coordination (IAC) project estimates a vacancy defect concentration of 3.3(1.1) × 10 14 cm -3 in 28 Si crystals grown by the FZ method, which was determined using PALS [5]. The IAC considers that this value represents the vacancy content per unit volume for all vacancy types including vacancy agglomerates and voids, which is expressed as ∑ n nc ( Vn ) ,\n0018-9456 ' 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\nwhere n is the vacancy size ( n = 1 for monovacancy V , n = 2 for divacancy V 2, n = 3 for trivacancy V 3, etc.), and c ( Vn ) is the concentration of the vacancy Vn . The value 3.3(1.1) × 10 14 cm -3 corresponds to a mass deficit correction of 6.6(2.2) θ g for a 1-kg 28 Si sphere. To confirm the reliability of this value, it is desirable to perform further measurements using other independent methods.\nTo date, a lot of detailed studies on vacancy defects in silicon crystals have been conducted using EPR spectroscopy [6], which is one of the high-sensitivity methods for identifying and quantifying vacancy defects. This paper reports the quantitative EPR measurement on 28 Si single crystal AVO28 to increase the reliability of mass deficit correction in the XRCD method. This paper is the extension of a presentation at the 2018 Conference on Precision Electromagnetic Measurements (CPEM 2018) and a two-page summary published in the CPEM 2018 Digest [7].\nThis paper proceeds as follows. Section II describes the defect concentration measurement method using EPR spectroscopy. Section III shows the preparation of a sample cut out from the AVO28 crystal. Section IV describes the measurement results using EPR spectroscopy. Section V discusses the measurement results obtained. Finally, Section VI presents the conclusions of this paper.\n\nReports quantitative EPR measurement on 28 Si single crystal AVO28 to increase the reliability of mass deficit correction in the XRCD method.",
    "original_text": "T HE X-ray crystal density (XRCD) method is one of two independent methods that can realize the future realization of the kilogram with the highest accuracy [1]. The mass of a 28 Si single-crystal sphere with a diameter of approximately 93.7 mm is determined by counting the number of silicon atoms in the sphere along with using several basic physical constants, including the Planck constant h . If a significant number of defects exist in the crystal, then the mass deficit caused by their presence must be corrected. Here, the mass deficit is defined as the mass difference between two silicon spheres with a diameter of 93.7 cm, one of which is made of an ideal crystal, and the other of which is made of a real crystal containing defects. Fig. 1 shows the schematic of a normal structure, monovacancy defect V , and substitutional impurity defect in silicon crystals.\nManuscript received June 26, 2018; revised October 16, 2018; accepted November 8, 2018. Date of publication December 17, 2018; date of current version May 10, 2019. This work was supported by JSPS KAKENHI under Grant 17K05112. The Associate Editor coordinating the review process was Dr. Michael Lombardi. (Corresponding author: Shigeki Mizushima.)\n- S. Mizushima, N. Kuramoto, and K. Fujii are with the National Metrology Institute of Japan, National Institute of Advanced Industrial Science and Technology, Tsukuba 305-8563, Japan (e-mail: s.mizushima@aist.go.jp; n.kuramoto@aist.go.jp; fujii.kenichi@aist.go.jp).\n- T. Umeda is with Institute of Applied Physics, University of Tsukuba, Tsukuba 305-8573, Japan (e-mail: umeda@bk.tsukuba.ac.jp).\n3. Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org.\nDigital Object Identifier 10.1109/TIM.2018.2884044\nFig. 1. Ball-and-stick models of a normal structure, monovacancy defect V , and substitutional impurity defect in silicon crystals. Gray lines: chemical bonds between atoms.\nSeveral studies on defects in silicon crystals have been conducted to realize the kilogram using silicon crystals. Deslattes and Kessler [2] reported a qualitative difference between a silicon crystal with a natural isotope ratio of National Metrology Institute of Japan (NMIJ), formerly National Research Laboratory of Metrology, and that of Physikalisch-Technische Bundesanstalt by employing electron paramagnetic resonance (EPR) spectroscopy. They detected the EPR signals of impurities only from the NMIJ's crystal. Gebauer et al. [3] determined that the vacancy concentration in a silicon crystal grown by the floating zone (FZ) method ranges from 1 × 10 14 cm -3 to 4 × 10 14 cm -3 using positron annihilation lifetime spectroscopy (PALS) that detects neutral or negatively charged vacancies. They performed electron irradiation with low doses to obtain a quantitative calibration using an estimated introduction rate of 0.1 cm -1 for monovacancies. They did not find any relation between the average positron lifetime and the position in the radial direction of the crystal. They concluded that a decomposition of the positron lifetime spectra does not give reliable results because of the low intensities of the defect components. They estimated that hexavacancy V 6 is a candidate for explaining their PALS results. D'Agostino et al. [4] developed a method to measure the void concentration of 10 14 cm -3 in a silicon crystal with a few percent uncertainties using Cu decoration and neutron activation.\nAt present, the International Avogadro Coordination (IAC) project estimates a vacancy defect concentration of 3.3(1.1) × 10 14 cm -3 in 28 Si crystals grown by the FZ method, which was determined using PALS [5]. The IAC considers that this value represents the vacancy content per unit volume for all vacancy types including vacancy agglomerates and voids, which is expressed as ∑ n nc ( Vn ) ,\n0018-9456 ' 2018 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\nwhere n is the vacancy size ( n = 1 for monovacancy V , n = 2 for divacancy V 2, n = 3 for trivacancy V 3, etc.), and c ( Vn ) is the concentration of the vacancy Vn . The value 3.3(1.1) × 10 14 cm -3 corresponds to a mass deficit correction of 6.6(2.2) θ g for a 1-kg 28 Si sphere. To confirm the reliability of this value, it is desirable to perform further measurements using other independent methods.\nTo date, a lot of detailed studies on vacancy defects in silicon crystals have been conducted using EPR spectroscopy [6], which is one of the high-sensitivity methods for identifying and quantifying vacancy defects. This paper reports the quantitative EPR measurement on 28 Si single crystal AVO28 to increase the reliability of mass deficit correction in the XRCD method. This paper is the extension of a presentation at the 2018 Conference on Precision Electromagnetic Measurements (CPEM 2018) and a two-page summary published in the CPEM 2018 Digest [7].\nThis paper proceeds as follows. Section II describes the defect concentration measurement method using EPR spectroscopy. Section III shows the preparation of a sample cut out from the AVO28 crystal. Section IV describes the measurement results using EPR spectroscopy. Section V discusses the measurement results obtained. Finally, Section VI presents the conclusions of this paper.",
    "context": "Reports quantitative EPR measurement on 28 Si single crystal AVO28 to increase the reliability of mass deficit correction in the XRCD method.",
    "document": "Electron_Paramagnetic_Resonance_Study_on_28Si_Single_Crystal_for_the_Future_Realization_of_the_Kilogram.pdf",
    "pages": [
      1,
      2
    ],
    "id": "553a3a55727747e86ea38ea92bf9f3d92493ec4440bbd59ad4126190674d046a"
  },
  {
    "text": "EPR spectroscopy detects unpaired electrons localized at defects such as vacancies and impurities in a sample. EPR spectra are recorded by irradiating the sample placed in a microwave cavity resonator with the microwave (frequency ν) and sweeping an external magnetic field B . Resonant microwave absorptions (i.e., EPR signals) are observed under the resonance condition, where the energy of a microwave photon is equal to the Zeeman splitting of the energy levels of an unpaired electron\n<!-- formula-not-decoded -->\nwhere h is the Planck constant, g is the gyromagnetic factor ( g -factor) of the unpaired electron, and µ B is the Bohr magneton. The g -factor for free electrons has been determined to be 2.002319 · · · with a relative standard uncertainty of 2.6 × 10 -13 [8]. The g -factor of the electron localized at a defect has an inherent deviation caused by spin-orbit coupling. Therefore, measurements of the g -factor are useful for classifying defects.\nFurthermore, since the wave function of the unpaired electron has a spatial distribution reflecting the arrangement of atoms around a defect, the angular dependence of the g -factor provides information on the symmetry of the defect in a crystal sample. By choosing a particular Cartesian coordinate system, the effective value of the g -factor for the defect in a certain direction g eff is given by\n<!-- formula-not-decoded -->\nwhere gx , gy , and gz are the principal values of the g -factor and cos θ x , cos θ y , and cos θ z are the direction cosines of the magnetic field in the selected Cartesian coordinate system.\nFig. 2. Simulation results of the anisotropic EPR signal of the positively charged monovacancy defect V + with the principal values of gx = 2.0087(3), gy = 1.9989(3), and gz = 1.9989(3) [11]. This defect shows axial symmetry about the < 100 > direction of the crystal. (a) Angular dependence of EPR line positions. Angle θ : angle between the [100] direction of the silicon single crystal and the magnetic field rotating in the ( 0 ¯ 11 ) plane. (b) First-derivative EPR spectra for θ = 0¡, 30¡, 60¡, and 90¡. For this simulation, a microwave frequency of 9.421 GHz and Lorentzian lineshapes with a half-width at halfmaximum /Gamma1 = 0.1 mT was assumed. In an EPR spectrum, multiple EPR lines appear depending on the direction of the magnetic field owing to the rotational symmetry of the crystal. Taking into account the symmetry of the silicon crystal, the maximum number of EPR lines observed is 12.\nWhen the arrangement of atoms around the defect has cubic symmetry such as tetrahedral group T d, all three principal values of the g -factor are equal and such an EPR signal is expressed as 'isotropic.' Otherwise, the EPR signal is referred to as 'anisotropic.'\nWatkins [6] identified anisotropic EPR signals of vacancy defects in silicon crystals, generated by high-energy electron irradiation. The principal values of the g -factor range from 1.9989 to 2.0087 for positively charged monovacancy defect V + and from 2.0028 to 2.0151 for negatively charged monovacancy defect V -. Such anisotropy of EPR signals is caused by the geometrical distortion of the four silicon atoms around a vacancy defect due to the Jahn-Teller effect [9].\nIn addition, charged divacancy, trivacancy, tetravacancy, and pentavacancy defects V ± 2 , V -3 , V -4 , and V -5 and monovacancy defects trapped by impurities ( V O) -and ( V P) 0 in silicon crystals have been identified by EPR spectroscopy [6], [10]. These anisotropic signals of the vacancy defects should appear in the magnetic field ranging from 334 to 337 mT at the microwave frequency of 9.421 GHz used in our study. Fig. 2 shows the simulation results of anisotropic EPR signals of the positively charged monovacancy defect V + in a silicon single crystal [11] as an example, which is available using the webbased database system [12].\nThe angular dependence of the EPR signals observed for irradiation-damaged silicon crystals is explained by the geometrical distortion of atoms around each type of vacancy, based on the Jahn-Teller effect. Because the atoms around the vacancy in undamaged silicon crystals should have the geometrical distortion similar to that of the irradiation-damaged silicon crystals, based on the Jahn-Teller effect, we believe that the identification performed on irradiation-damaged samples using EPR is applicable to undamaged samples used in this paper.\nAmong the 415 types of defects in silicon identified in previous EPR studies [13], the largest vacancy agglomerate is negatively charged pentavacancy defect V -5 [14]. On the other hand, to the best of our knowledge, the largest vacancy agglomerate estimated using PALS is hexavacancy defect V 6 in an electron-irradiated FZ silicon doped with phosphorus for an estimated positron lifetime of 400 ps [15].\nFurthermore, when the wave function of an unpaired electron spreads to the position of an impurity atom having nuclear spin I , where I is an integer or half-integer, the EPR signal splits into (2 I + 1) lines owing to the hyperfine interaction. Since the number of lines and splitting width of the hyperfine splitting depend on the impurity atom and the wave function of the unpaired electron, they are useful for identifying impurity defects.\nThe 28 Si single crystal used in this paper contains 29 Si atoms, which have a nuclear spin I = 1/2, with an amount-ofsubstance fraction of only 4 × 10 -5 [16]. This brings about relatively narrow line widths of the EPR signals and makes it possible to acquire high-resolution EPR spectra. This is a great advantage in determining a small amount of vacancy defect concentration compared to the measurement on silicon crystals with a natural isotope ratio, which contains 29 Si atoms with an amount-of-substance fraction of approximately 0.047, causing inhomogeneous broadening in the EPR signals owing to the hyperfine interaction.\nQuantitative determination of the number of EPR-active defects can be achieved through comparison with a reference sample with a known number of unpaired electrons. This is based on the measurement principle that each EPR absorption intensity is proportional to the number of unpaired electrons in a sample. When we use the reference sample with the number of unpaired electrons n R, the number of unpaired electrons localized at a certain type of defect X in the test sample n X, is given by\n<!-- formula-not-decoded -->\nwhere ( A X / A R ) is the ratio of the EPR absorption intensities for the test and reference samples and ( T X / T R ) is the ratio of the absolute temperatures of the test and reference samples being measured. Here, we used Curie's law, which states that the magnetization of the paramagnetic material is approximately inversely proportional to the absolute temperature.\nThe concentration of defect X with an unpaired electron in the test sample c X is obtained by dividing the number of unpaired electrons localized at the defect n X by a sample volume V s\n<!-- formula-not-decoded -->\nWe used a crystal of copper(II) sulfate pentahydrate CuSO4 · 5H2O (molar mass M = 249 . 69 g mol -1 ) with a mass of 12.53 mg as the reference sample, which retains one unpaired electron per molecule. The number of unpaired electrons in the reference sample n R was estimated to be 3.02(3) × 10 19 . The EPR absorption intensity of the reference sample A R was determined at T R = 298 K before measurements on the test sample using the same instrument.\nAll EPR measurements were carried out using a Bruker Bio-Spin E500 X -band spectrometer equipped with a superhighQ cavity ER 4122SHQ. The signal-to-noise ratio of the instrument is 3000 for the paramagnetic radical standard material, called weak pitch, having a linear density of unpaired electrons of around 10 13 cm -1 . The detectable minimum number of unpaired electrons in the sample placed in the microwave cavity resonator was estimated to be around 10 10 , which is ultimately limited by the thermal noise of the detector and amplifiers [17]. The quality factor of the microwave cavity resonator Q reaches 7500 under the best measurement conditions. The sample temperature was controlled from 4 K to room temperature using an Oxford ESR900 He-flow cryostat. Standard magnetic field modulation technique was employed at a frequency of 100 kHz with a modulation width of 0.1 mT.\n\nProvides a description of EPR spectroscopy and its application for identifying and quantifying vacancy defects in silicon crystals.",
    "original_text": "EPR spectroscopy detects unpaired electrons localized at defects such as vacancies and impurities in a sample. EPR spectra are recorded by irradiating the sample placed in a microwave cavity resonator with the microwave (frequency ν) and sweeping an external magnetic field B . Resonant microwave absorptions (i.e., EPR signals) are observed under the resonance condition, where the energy of a microwave photon is equal to the Zeeman splitting of the energy levels of an unpaired electron\n<!-- formula-not-decoded -->\nwhere h is the Planck constant, g is the gyromagnetic factor ( g -factor) of the unpaired electron, and µ B is the Bohr magneton. The g -factor for free electrons has been determined to be 2.002319 · · · with a relative standard uncertainty of 2.6 × 10 -13 [8]. The g -factor of the electron localized at a defect has an inherent deviation caused by spin-orbit coupling. Therefore, measurements of the g -factor are useful for classifying defects.\nFurthermore, since the wave function of the unpaired electron has a spatial distribution reflecting the arrangement of atoms around a defect, the angular dependence of the g -factor provides information on the symmetry of the defect in a crystal sample. By choosing a particular Cartesian coordinate system, the effective value of the g -factor for the defect in a certain direction g eff is given by\n<!-- formula-not-decoded -->\nwhere gx , gy , and gz are the principal values of the g -factor and cos θ x , cos θ y , and cos θ z are the direction cosines of the magnetic field in the selected Cartesian coordinate system.\nFig. 2. Simulation results of the anisotropic EPR signal of the positively charged monovacancy defect V + with the principal values of gx = 2.0087(3), gy = 1.9989(3), and gz = 1.9989(3) [11]. This defect shows axial symmetry about the < 100 > direction of the crystal. (a) Angular dependence of EPR line positions. Angle θ : angle between the [100] direction of the silicon single crystal and the magnetic field rotating in the ( 0 ¯ 11 ) plane. (b) First-derivative EPR spectra for θ = 0¡, 30¡, 60¡, and 90¡. For this simulation, a microwave frequency of 9.421 GHz and Lorentzian lineshapes with a half-width at halfmaximum /Gamma1 = 0.1 mT was assumed. In an EPR spectrum, multiple EPR lines appear depending on the direction of the magnetic field owing to the rotational symmetry of the crystal. Taking into account the symmetry of the silicon crystal, the maximum number of EPR lines observed is 12.\nWhen the arrangement of atoms around the defect has cubic symmetry such as tetrahedral group T d, all three principal values of the g -factor are equal and such an EPR signal is expressed as 'isotropic.' Otherwise, the EPR signal is referred to as 'anisotropic.'\nWatkins [6] identified anisotropic EPR signals of vacancy defects in silicon crystals, generated by high-energy electron irradiation. The principal values of the g -factor range from 1.9989 to 2.0087 for positively charged monovacancy defect V + and from 2.0028 to 2.0151 for negatively charged monovacancy defect V -. Such anisotropy of EPR signals is caused by the geometrical distortion of the four silicon atoms around a vacancy defect due to the Jahn-Teller effect [9].\nIn addition, charged divacancy, trivacancy, tetravacancy, and pentavacancy defects V ± 2 , V -3 , V -4 , and V -5 and monovacancy defects trapped by impurities ( V O) -and ( V P) 0 in silicon crystals have been identified by EPR spectroscopy [6], [10]. These anisotropic signals of the vacancy defects should appear in the magnetic field ranging from 334 to 337 mT at the microwave frequency of 9.421 GHz used in our study. Fig. 2 shows the simulation results of anisotropic EPR signals of the positively charged monovacancy defect V + in a silicon single crystal [11] as an example, which is available using the webbased database system [12].\nThe angular dependence of the EPR signals observed for irradiation-damaged silicon crystals is explained by the geometrical distortion of atoms around each type of vacancy, based on the Jahn-Teller effect. Because the atoms around the vacancy in undamaged silicon crystals should have the geometrical distortion similar to that of the irradiation-damaged silicon crystals, based on the Jahn-Teller effect, we believe that the identification performed on irradiation-damaged samples using EPR is applicable to undamaged samples used in this paper.\nAmong the 415 types of defects in silicon identified in previous EPR studies [13], the largest vacancy agglomerate is negatively charged pentavacancy defect V -5 [14]. On the other hand, to the best of our knowledge, the largest vacancy agglomerate estimated using PALS is hexavacancy defect V 6 in an electron-irradiated FZ silicon doped with phosphorus for an estimated positron lifetime of 400 ps [15].\nFurthermore, when the wave function of an unpaired electron spreads to the position of an impurity atom having nuclear spin I , where I is an integer or half-integer, the EPR signal splits into (2 I + 1) lines owing to the hyperfine interaction. Since the number of lines and splitting width of the hyperfine splitting depend on the impurity atom and the wave function of the unpaired electron, they are useful for identifying impurity defects.\nThe 28 Si single crystal used in this paper contains 29 Si atoms, which have a nuclear spin I = 1/2, with an amount-ofsubstance fraction of only 4 × 10 -5 [16]. This brings about relatively narrow line widths of the EPR signals and makes it possible to acquire high-resolution EPR spectra. This is a great advantage in determining a small amount of vacancy defect concentration compared to the measurement on silicon crystals with a natural isotope ratio, which contains 29 Si atoms with an amount-of-substance fraction of approximately 0.047, causing inhomogeneous broadening in the EPR signals owing to the hyperfine interaction.\nQuantitative determination of the number of EPR-active defects can be achieved through comparison with a reference sample with a known number of unpaired electrons. This is based on the measurement principle that each EPR absorption intensity is proportional to the number of unpaired electrons in a sample. When we use the reference sample with the number of unpaired electrons n R, the number of unpaired electrons localized at a certain type of defect X in the test sample n X, is given by\n<!-- formula-not-decoded -->\nwhere ( A X / A R ) is the ratio of the EPR absorption intensities for the test and reference samples and ( T X / T R ) is the ratio of the absolute temperatures of the test and reference samples being measured. Here, we used Curie's law, which states that the magnetization of the paramagnetic material is approximately inversely proportional to the absolute temperature.\nThe concentration of defect X with an unpaired electron in the test sample c X is obtained by dividing the number of unpaired electrons localized at the defect n X by a sample volume V s\n<!-- formula-not-decoded -->\nWe used a crystal of copper(II) sulfate pentahydrate CuSO4 · 5H2O (molar mass M = 249 . 69 g mol -1 ) with a mass of 12.53 mg as the reference sample, which retains one unpaired electron per molecule. The number of unpaired electrons in the reference sample n R was estimated to be 3.02(3) × 10 19 . The EPR absorption intensity of the reference sample A R was determined at T R = 298 K before measurements on the test sample using the same instrument.\nAll EPR measurements were carried out using a Bruker Bio-Spin E500 X -band spectrometer equipped with a superhighQ cavity ER 4122SHQ. The signal-to-noise ratio of the instrument is 3000 for the paramagnetic radical standard material, called weak pitch, having a linear density of unpaired electrons of around 10 13 cm -1 . The detectable minimum number of unpaired electrons in the sample placed in the microwave cavity resonator was estimated to be around 10 10 , which is ultimately limited by the thermal noise of the detector and amplifiers [17]. The quality factor of the microwave cavity resonator Q reaches 7500 under the best measurement conditions. The sample temperature was controlled from 4 K to room temperature using an Oxford ESR900 He-flow cryostat. Standard magnetic field modulation technique was employed at a frequency of 100 kHz with a modulation width of 0.1 mT.",
    "context": "Provides a description of EPR spectroscopy and its application for identifying and quantifying vacancy defects in silicon crystals.",
    "document": "Electron_Paramagnetic_Resonance_Study_on_28Si_Single_Crystal_for_the_Future_Realization_of_the_Kilogram.pdf",
    "pages": [
      2,
      3
    ],
    "id": "ef9dd3dcf90d19114d103214da672fce0ce4b60142a4802b0c312c3fe4235b63"
  },
  {
    "text": "In quantitative EPR measurements, it is necessary to avoid microwave saturation that causes the underestimation of EPR signal intensities. Without microwave saturation, the intensity of any EPR signal should be proportional to the square root of the microwave power. Therefore, the intensity of the EPR signal was examined beforehand at four microwave powers of 0.002, 0.02, 0.2, and 2 mW.\nFurthermore, out-of-phase second-harmonic EPR signals under rapid-passage conditions [18] were observed to selectively detect defects with long electron spin relaxation times in the sample. It was found that the concentrations of defects with long electron spin relaxation times were below the detection limit of our measurement in the expected magnetic field range from 334 to 337 mT for the EPR-active vacancy defects.\n\nDescribes the method for quantifying EPR signals to avoid signal saturation and highlights the detection limit for long-lived defects in the magnetic field range.",
    "original_text": "In quantitative EPR measurements, it is necessary to avoid microwave saturation that causes the underestimation of EPR signal intensities. Without microwave saturation, the intensity of any EPR signal should be proportional to the square root of the microwave power. Therefore, the intensity of the EPR signal was examined beforehand at four microwave powers of 0.002, 0.02, 0.2, and 2 mW.\nFurthermore, out-of-phase second-harmonic EPR signals under rapid-passage conditions [18] were observed to selectively detect defects with long electron spin relaxation times in the sample. It was found that the concentrations of defects with long electron spin relaxation times were below the detection limit of our measurement in the expected magnetic field range from 334 to 337 mT for the EPR-active vacancy defects.",
    "context": "Describes the method for quantifying EPR signals to avoid signal saturation and highlights the detection limit for long-lived defects in the magnetic field range.",
    "document": "Electron_Paramagnetic_Resonance_Study_on_28Si_Single_Crystal_for_the_Future_Realization_of_the_Kilogram.pdf",
    "pages": [
      3
    ],
    "id": "3fa4e15e0fe3bbf97148faaee6218b47955b6843d407ea878d1827f3df694fb9"
  },
  {
    "text": "The sample for EPR spectroscopy was prepared from semicircular disk-shaped sample 9.R1, cut out from the tail side of 28 Si single-crystal Si28-10Pr11 (referred to as AVO28) [5]. The sample has a rectangular solid shape with the dimensions of 1.8 mm × 3.4 mm × 10.4 mm. The axial distance between the sample and the seed of the AVO28 crystal was approximately 420 mm, and the radial distance from the center was approximately 35 mm. The dimensions of the sample were designed by taking into account the internal dimensions of the microwave cavity resonator and the distribution of the microwave field B 1 in the microwave cavity resonator. The volume of the sample at 20 ¡C was determined to be V s = 0.065 029(5) cm 3 from the crystal density [16] and the mass measured using a vacuum mass comparator [19]. Fig. 3 shows a photograph of the sample and the crystal orientations.\nFig. 3. Photograph of the mirror-polished crystal prepared from semicircular disk-shaped sample 9.R1, cut out from the tail side of the AVO28 crystal. The indices on the right side indicate the crystal orientations.\nTo detect a small EPR signal from the bulk crystal, we must reduce the EPR signals of defects near the sample surfaces, which were formed during the manufacturing process. For this purpose, the surface area of 1.09 cm 2 , out of the total surface area of 1.21 cm 2 , was mirror polished. Half of the remaining surface is the as-etched surface, and the other half is the as-cut surface finished using a dicing saw. From our experience, the silicon surface, properly cut using a dicing saw, is almost equivalent to the mirror-polished surface in terms of the surface density of dangling bonds. After our EPR measurement, it became apparent that chemical etching is necessary to minimize the dangling bond density on the sample surfaces.\nThe sample surfaces were then precisely cleaned to reduce the signal of the surface contaminants. The cleaning was conducted in four cleaning steps by successively using the following solutions: sulfuric acid peroxide mixture, ammonia peroxide mixture, hydrogen chloride peroxide mixture, and diluted hydrofluoric acid.\n\nPreparation of the EPR sample; details on the sample’s dimensions and surface preparation to minimize surface-related EPR signals.",
    "original_text": "The sample for EPR spectroscopy was prepared from semicircular disk-shaped sample 9.R1, cut out from the tail side of 28 Si single-crystal Si28-10Pr11 (referred to as AVO28) [5]. The sample has a rectangular solid shape with the dimensions of 1.8 mm × 3.4 mm × 10.4 mm. The axial distance between the sample and the seed of the AVO28 crystal was approximately 420 mm, and the radial distance from the center was approximately 35 mm. The dimensions of the sample were designed by taking into account the internal dimensions of the microwave cavity resonator and the distribution of the microwave field B 1 in the microwave cavity resonator. The volume of the sample at 20 ¡C was determined to be V s = 0.065 029(5) cm 3 from the crystal density [16] and the mass measured using a vacuum mass comparator [19]. Fig. 3 shows a photograph of the sample and the crystal orientations.\nFig. 3. Photograph of the mirror-polished crystal prepared from semicircular disk-shaped sample 9.R1, cut out from the tail side of the AVO28 crystal. The indices on the right side indicate the crystal orientations.\nTo detect a small EPR signal from the bulk crystal, we must reduce the EPR signals of defects near the sample surfaces, which were formed during the manufacturing process. For this purpose, the surface area of 1.09 cm 2 , out of the total surface area of 1.21 cm 2 , was mirror polished. Half of the remaining surface is the as-etched surface, and the other half is the as-cut surface finished using a dicing saw. From our experience, the silicon surface, properly cut using a dicing saw, is almost equivalent to the mirror-polished surface in terms of the surface density of dangling bonds. After our EPR measurement, it became apparent that chemical etching is necessary to minimize the dangling bond density on the sample surfaces.\nThe sample surfaces were then precisely cleaned to reduce the signal of the surface contaminants. The cleaning was conducted in four cleaning steps by successively using the following solutions: sulfuric acid peroxide mixture, ammonia peroxide mixture, hydrogen chloride peroxide mixture, and diluted hydrofluoric acid.",
    "context": "Preparation of the EPR sample; details on the sample’s dimensions and surface preparation to minimize surface-related EPR signals.",
    "document": "Electron_Paramagnetic_Resonance_Study_on_28Si_Single_Crystal_for_the_Future_Realization_of_the_Kilogram.pdf",
    "pages": [
      3,
      4
    ],
    "id": "956f86d2e6d9073dc91b9f31e44257dd1d6a779b49581cc7c0c3c95049f2ea56"
  },
  {
    "text": "To detect vacancy defects in the sample, EPR measurements were conducted in the presence of a magnetic field from 332 to 341 mT. EPR spectra were recorded at four different directions θ = 0¡, 30¡, 60¡, and 90¡ by rotating the sample about the [ 0 ¯ 11 ] direction, where θ represents the angle between the magnetic field and the [100] direction. Fig. 4 shows the first-derivative EPR spectra under 100-W halogen lamp illumination at 25 K. The halogen lamp illumination generates a large number of electron-hole pairs in the silicon crystal sample, provides additional unpaired electrons, and reveals the defects that cannot be observed in a dark environment. To guide light from the halogen lamp to the sample, a quartz glass rod, on which the sample was mounted, was employed.\nElectron-hole pairs excited near the sample surfaces under illumination spread into all parts of the sample. The reason for this is that the minority carrier diffusion length L = (τ D ) 1 / 2 is estimated to be greater than a silicon sample thickness\n\nProvides angular dependence of EPR signals for defect identification and reveals the presence of defects not observable in a dark environment.",
    "original_text": "To detect vacancy defects in the sample, EPR measurements were conducted in the presence of a magnetic field from 332 to 341 mT. EPR spectra were recorded at four different directions θ = 0¡, 30¡, 60¡, and 90¡ by rotating the sample about the [ 0 ¯ 11 ] direction, where θ represents the angle between the magnetic field and the [100] direction. Fig. 4 shows the first-derivative EPR spectra under 100-W halogen lamp illumination at 25 K. The halogen lamp illumination generates a large number of electron-hole pairs in the silicon crystal sample, provides additional unpaired electrons, and reveals the defects that cannot be observed in a dark environment. To guide light from the halogen lamp to the sample, a quartz glass rod, on which the sample was mounted, was employed.\nElectron-hole pairs excited near the sample surfaces under illumination spread into all parts of the sample. The reason for this is that the minority carrier diffusion length L = (τ D ) 1 / 2 is estimated to be greater than a silicon sample thickness",
    "context": "Provides angular dependence of EPR signals for defect identification and reveals the presence of defects not observable in a dark environment.",
    "document": "Electron_Paramagnetic_Resonance_Study_on_28Si_Single_Crystal_for_the_Future_Realization_of_the_Kilogram.pdf",
    "pages": [
      4
    ],
    "id": "88ae1f1f4bd8dd8bed2cd4b6930a9a90557cb7c9d4a3f139acb01f5c1074eae1"
  },
  {
    "text": "Fig. 4. First-derivative EPR spectra of the AVO28 crystal sample under illumination. To investigate angular dependence, the sample was rotated about the [ 0 ¯ 11 ] direction. Angle θ : angle between the magnetic field and the [100] direction. Each spectrum is accumulated for 3.7 h to improve the signalto-noise ratio. These spectra were measured without microwave saturation for phosphorus impurity and defects on mechanically damaged surfaces at a microwave power of 2 mW.\nof 1.8 mm. Here, τ is the minority carrier lifetime in silicon crystals and D is the diffusion coefficient of carriers in silicon crystals. If we use a value of the minority carrier lifetime τ = 21 . 6 ms [20] and a value of the diffusion coefficient of carriers D = 350 cm 2 s -1 for electrons at 25 K [21], the minority carrier diffusion length is estimated at 27 mm.\nThe observed EPR spectra identified the following three types of defects.\n- 1) Phosphorus ( 31 P, Nuclear Spin I = 1/2 ) Impurity: The constituent element is represented by Si4 ----P¥, where the lines symbolize chemical bonds and the dot symbolizes an unpaired electron. Phosphorus is a typical substitutional impurity in the silicon crystal. The identification is based on the facts that the spectra show a doublet hyperfine splitting of 4.04 mT owing to a 31 P hyperfine interaction, and the g -value of the spectral center is 1.9990(1) and isotropic, which are in good agreement with the well-established EPR signal of phosphorus donors in silicon crystal [22], [23]. This isotropic signal results from the tetrahedral symmetric structure, where four silicon atoms bond to a phosphorus atom without geometric distortion. The observed linewidth of the spectra was 2 /Gamma1 = 0.24 mT.\nSi4 ----P + ¥). This is known as a 'compensation'\nThe number and concentration of the phosphorus impurity with unpaired electrons under illumination are determined to be 0.21(3) × 10 12 and 3.2(5) × 10 12 cm -3 , respectively, based on (3) and (4). Table I shows the numerical values used for this determination and its uncertainty evaluation. The phosphorus EPR signal fell below the detection limit in a dark environment. This indicates that unpaired electrons of the phosphorus donors transferred to other compensating defects in the sample in a dark environment (Si4 ----P ¥ →\nTABLE I\nUNCERTAINTY EVALUATION OF CONCENTRATION OF PHOSPHORUS IMPURITY WITH UNPAIRED ELECTRONS UNDER ILLUMINATION\nphenomenon, namely, a decrease in the number of charge carriers in a semiconductor material, which contains both donors and acceptors. We suggest that such compensating defects originate from boron impurity with a relatively high concentration at the tail of the AVO28 crystal and also from the defects on mechanically damaged surfaces described in the following.\n- 2) Defects Formed Near the Mechanically-Damaged Surfaces of Silicon Single Crystal (Si3 ≡ Si¥) : The identification is based on the result that the g -value of the observed spectral center is close to 2.0055(2) [24] and isotropic. Most of these defects may be formed at microcracks near the as-cut surface with a surface area of 0.06 cm 2 , finished using a dicing saw or mirrorpolished surfaces. The linewidth of the spectra on the 28 Si single crystal used in this paper is 2 /Gamma1 = 0.41 mT, which is clearly narrower than that reported for natural isotope ratio crystals of 0.7-0.8 mT [24].\n- 3) E' centers (O3 ≡ Si¥), oxygen-vacancy defects in the quartz glass rod, on which the sample was mounted. The identification is based on the observation of isotropic and easily saturable signals with average g -value close to 2.0008 [25]. The microwave saturation\nThe number of EPR-active defects on the mechanically damaged surfaces was determined to be 1.67(23) × 10 12 in a dark environment and 0.68(10) × 10 12 under illumination. They are larger than the number of phosphorous impurity defects observed under illumination, 0.21(3) × 10 12 . We judge that unpaired electrons of phosphorus impurities acting as donors move to defects on the mechanically damaged surfaces, hiding the EPR signals of phosphorus in a dark environment. Under illumination, some of the unpaired electrons at defects on the mechanically damaged surfaces transfer to phosphorus impurity defects, thus recovering the EPR-active state (Si4 ≡ _ _ P + ¥ → Si4 ≡ _ _ P¥).\nTABLE II CONCENTRATIONS OF NINE TYPES OF VACANCY DEFECTS WITH UNPAIRED ELECTRONS AND THE CORRESPONDING MASS DEFICITS FOR A 1-kg 28 SI SINGLE CRYSTAL\noccurred even at a microwave power of 0.02 mW. This signal is detectable without the silicon sample and is irrelevant to the silicon sample.\nis estimated to be 0.00(23) θ g for a 1-kg 28 Si single crystal using the following equation:\nHowever, anisotropic EPR signals of the vacancy defects were not observed in the expected magnetic field range of 334 to 337 mT under illumination. This result indicates that the concentrations of nine types of vacancy defects with unpaired electrons V + , V -, V + 2 , V -2 , V -3 , V -4 , V -5 , ( V O) -, and ( V P) 0 , which were previously identified by Watkins [6] and Lee and Corbett [10] using EPR spectroscopy, are all below the detection limit of our EPR measurement, estimated as around 1 × 10 12 cm -3 . This value of the detection limit was estimated from the fact that the signals of the phosphorus impurity with unpaired electrons having a concentration of 3.2(5) × 10 12 cm -3 , which split into two spectra, are clearly distinguished from background noise, as shown in Fig. 4. Similar EPR measurement conducted in a dark environment also revealed that anisotropic EPR signals of the vacancy defects were below the detection limit of our measurement.\nTable II shows the evaluated concentrations of the nine types of vacancy defects with unpaired electrons and the corresponding mass deficits for a 1-kg 28 Si single crystal. Each concentration of vacancy defects with unpaired electrons is estimated to be 0.0 × 10 12 cm -3 . The standard uncertainty is estimated to be 0.6 × 10 12 cm -3 , assuming a uniform distribution with a half-width of 1 × 10 12 cm -3 , which is the estimated detection limit of our EPR measurements. The total mass deficit due to vacancy defects with unpaired electrons δ m\n<!-- formula-not-decoded -->\nwhere δ mV + , δ mV -, · · · , δ m ( V P ) 0 represent a mass deficit due to each type of vacancy defect.\nThe vacancy concentrations estimated in this paper are limited to those for the nine types of vacancy defects in silicon crystals, as identified in previous EPR studies. Therefore, it should be noted that the mass deficit estimated in this paper corresponds only to these nine types of vacancy defects.\nThe standard uncertainty of the total mass deficit due to vacancy defects with unpaired electrons u (δ m ) is given by\n<!-- formula-not-decoded -->\nwhere u (δ mV + ), u (δ mV -), · · · , u (δ m ( V P ) 0 ) are the standard uncertainties of mass deficits δ mV + , δ mV -, · · · , δ m ( V P ) 0 . In this estimation, we assume that correlation coefficients between the mass deficits are + 1.\n\nInvestigates phosphorus impurity and defects on mechanically damaged surfaces under illumination.",
    "original_text": "Fig. 4. First-derivative EPR spectra of the AVO28 crystal sample under illumination. To investigate angular dependence, the sample was rotated about the [ 0 ¯ 11 ] direction. Angle θ : angle between the magnetic field and the [100] direction. Each spectrum is accumulated for 3.7 h to improve the signalto-noise ratio. These spectra were measured without microwave saturation for phosphorus impurity and defects on mechanically damaged surfaces at a microwave power of 2 mW.\nof 1.8 mm. Here, τ is the minority carrier lifetime in silicon crystals and D is the diffusion coefficient of carriers in silicon crystals. If we use a value of the minority carrier lifetime τ = 21 . 6 ms [20] and a value of the diffusion coefficient of carriers D = 350 cm 2 s -1 for electrons at 25 K [21], the minority carrier diffusion length is estimated at 27 mm.\nThe observed EPR spectra identified the following three types of defects.\n- 1) Phosphorus ( 31 P, Nuclear Spin I = 1/2 ) Impurity: The constituent element is represented by Si4 ----P¥, where the lines symbolize chemical bonds and the dot symbolizes an unpaired electron. Phosphorus is a typical substitutional impurity in the silicon crystal. The identification is based on the facts that the spectra show a doublet hyperfine splitting of 4.04 mT owing to a 31 P hyperfine interaction, and the g -value of the spectral center is 1.9990(1) and isotropic, which are in good agreement with the well-established EPR signal of phosphorus donors in silicon crystal [22], [23]. This isotropic signal results from the tetrahedral symmetric structure, where four silicon atoms bond to a phosphorus atom without geometric distortion. The observed linewidth of the spectra was 2 /Gamma1 = 0.24 mT.\nSi4 ----P + ¥). This is known as a 'compensation'\nThe number and concentration of the phosphorus impurity with unpaired electrons under illumination are determined to be 0.21(3) × 10 12 and 3.2(5) × 10 12 cm -3 , respectively, based on (3) and (4). Table I shows the numerical values used for this determination and its uncertainty evaluation. The phosphorus EPR signal fell below the detection limit in a dark environment. This indicates that unpaired electrons of the phosphorus donors transferred to other compensating defects in the sample in a dark environment (Si4 ----P ¥ →\nTABLE I\nUNCERTAINTY EVALUATION OF CONCENTRATION OF PHOSPHORUS IMPURITY WITH UNPAIRED ELECTRONS UNDER ILLUMINATION\nphenomenon, namely, a decrease in the number of charge carriers in a semiconductor material, which contains both donors and acceptors. We suggest that such compensating defects originate from boron impurity with a relatively high concentration at the tail of the AVO28 crystal and also from the defects on mechanically damaged surfaces described in the following.\n- 2) Defects Formed Near the Mechanically-Damaged Surfaces of Silicon Single Crystal (Si3 ≡ Si¥) : The identification is based on the result that the g -value of the observed spectral center is close to 2.0055(2) [24] and isotropic. Most of these defects may be formed at microcracks near the as-cut surface with a surface area of 0.06 cm 2 , finished using a dicing saw or mirrorpolished surfaces. The linewidth of the spectra on the 28 Si single crystal used in this paper is 2 /Gamma1 = 0.41 mT, which is clearly narrower than that reported for natural isotope ratio crystals of 0.7-0.8 mT [24].\n- 3) E' centers (O3 ≡ Si¥), oxygen-vacancy defects in the quartz glass rod, on which the sample was mounted. The identification is based on the observation of isotropic and easily saturable signals with average g -value close to 2.0008 [25]. The microwave saturation\nThe number of EPR-active defects on the mechanically damaged surfaces was determined to be 1.67(23) × 10 12 in a dark environment and 0.68(10) × 10 12 under illumination. They are larger than the number of phosphorous impurity defects observed under illumination, 0.21(3) × 10 12 . We judge that unpaired electrons of phosphorus impurities acting as donors move to defects on the mechanically damaged surfaces, hiding the EPR signals of phosphorus in a dark environment. Under illumination, some of the unpaired electrons at defects on the mechanically damaged surfaces transfer to phosphorus impurity defects, thus recovering the EPR-active state (Si4 ≡ _ _ P + ¥ → Si4 ≡ _ _ P¥).\nTABLE II CONCENTRATIONS OF NINE TYPES OF VACANCY DEFECTS WITH UNPAIRED ELECTRONS AND THE CORRESPONDING MASS DEFICITS FOR A 1-kg 28 SI SINGLE CRYSTAL\noccurred even at a microwave power of 0.02 mW. This signal is detectable without the silicon sample and is irrelevant to the silicon sample.\nis estimated to be 0.00(23) θ g for a 1-kg 28 Si single crystal using the following equation:\nHowever, anisotropic EPR signals of the vacancy defects were not observed in the expected magnetic field range of 334 to 337 mT under illumination. This result indicates that the concentrations of nine types of vacancy defects with unpaired electrons V + , V -, V + 2 , V -2 , V -3 , V -4 , V -5 , ( V O) -, and ( V P) 0 , which were previously identified by Watkins [6] and Lee and Corbett [10] using EPR spectroscopy, are all below the detection limit of our EPR measurement, estimated as around 1 × 10 12 cm -3 . This value of the detection limit was estimated from the fact that the signals of the phosphorus impurity with unpaired electrons having a concentration of 3.2(5) × 10 12 cm -3 , which split into two spectra, are clearly distinguished from background noise, as shown in Fig. 4. Similar EPR measurement conducted in a dark environment also revealed that anisotropic EPR signals of the vacancy defects were below the detection limit of our measurement.\nTable II shows the evaluated concentrations of the nine types of vacancy defects with unpaired electrons and the corresponding mass deficits for a 1-kg 28 Si single crystal. Each concentration of vacancy defects with unpaired electrons is estimated to be 0.0 × 10 12 cm -3 . The standard uncertainty is estimated to be 0.6 × 10 12 cm -3 , assuming a uniform distribution with a half-width of 1 × 10 12 cm -3 , which is the estimated detection limit of our EPR measurements. The total mass deficit due to vacancy defects with unpaired electrons δ m\n<!-- formula-not-decoded -->\nwhere δ mV + , δ mV -, · · · , δ m ( V P ) 0 represent a mass deficit due to each type of vacancy defect.\nThe vacancy concentrations estimated in this paper are limited to those for the nine types of vacancy defects in silicon crystals, as identified in previous EPR studies. Therefore, it should be noted that the mass deficit estimated in this paper corresponds only to these nine types of vacancy defects.\nThe standard uncertainty of the total mass deficit due to vacancy defects with unpaired electrons u (δ m ) is given by\n<!-- formula-not-decoded -->\nwhere u (δ mV + ), u (δ mV -), · · · , u (δ m ( V P ) 0 ) are the standard uncertainties of mass deficits δ mV + , δ mV -, · · · , δ m ( V P ) 0 . In this estimation, we assume that correlation coefficients between the mass deficits are + 1.",
    "context": "Investigates phosphorus impurity and defects on mechanically damaged surfaces under illumination.",
    "document": "Electron_Paramagnetic_Resonance_Study_on_28Si_Single_Crystal_for_the_Future_Realization_of_the_Kilogram.pdf",
    "pages": [
      4,
      5,
      6
    ],
    "id": "506355153af2c530396dfe997cd362ae8fe1ec7376de074ee7ef1550bdc447ab"
  },
  {
    "text": "The sample cut out from 28 Si single crystal AVO28 was measured at 25 K using EPR spectroscopy. Under illumination, the number and concentration of EPR-active phosphorus\nimpurity (Si4 ≡ _ _ P¥) in the sample were determined to be 0.21(3) × 10 12 and 3.2(5) × 10 12 cm -3 , respectively. In a dark environment, the number of EPR-active defects on the mechanically damaged surfaces (Si3 ≡ Si¥) was determined to be 1.67(23) × 10 12 . The clear observations of low concentrations of phosphorous impurity and defects on mechanically damaged surfaces demonstrate that EPR measurements used in this paper have adequate sensitivity to vacancy defects at a concentration of 1 × 10 12 cm -3 .\nThe concentration of phosphorus impurity obtained in this paper, 3.2(5) × 10 12 cm -3 , agrees with that of sample 4.12 of the AVO28 crystal shown in [26, Table 3], 10(10) × 10 12 cm -3 , which was estimated from measurement results using Fourier transform infrared (FTIR) spectroscopy for 28 Si crystal Si28-23Pr11. It is worth noting that the phosphorus impurity in the silicon crystal serves as a common scale for EPR and FTIR measurements.\nThe measured surface density of defects on the mechanically damaged surfaces gives a value of 1.67(23) × 10 12 / 1.21(2) cm 2 = 1.4(2) × 10 12 cm -2 . This value is comparable to that of hydrogenated amorphous silicon (a-Si:H) films, determined using photothermal deflection spectroscopy, around 10 12 cm -2 [27].\nOn the other hand, anisotropic signals of nine types of vacancy defects were not observed in the expected magnetic field ranging from 334 to 337 mT both in a dark environment and under illumination. The concentrations of the nine types of vacancy defects were estimated to be 0.0(6) × 10 12 cm -2 , respectively. Consequently, the mass deficit correction for these vacancy defects is evaluated to be 0.0(2) θ g for a 1-kg AVO28 crystal.\nThere are vacancies without unpaired electrons that cannot be detected by EPR. At present, it is difficult to estimate the concentrations of these vacancies. However, we believe that the comparison of EPR signals in a dark environment and under illumination reveals a significant portion of vacancy defects.\nSubstitutional nitrogen atoms in a silicon crystal (Si4 ----N¥) were identified using EPR by Murakami et al. [28] and Sprenger et al. [29]. However, in our experiment, the EPR signal of nitrogen impurity, which was considered to be contained in the AVO28 crystal with a relatively high concentration, was not detected. This can be explained by the geometrical structure of atoms determined by Jones et al. [30], where most of the nitrogen atoms in silicon crystals exist in the form of interstitial atoms without unpaired electrons (Si3 ≡ N:, where the colon sign symbolizes a lone pair of electrons).\nTo the best of our knowledge, vacancies captured by nitrogen impurities have not been identified using EPR. They are not listed in the collected EPR data of the 415 types of defects in silicon [13].\nAbe et al. [31] observed an FZ silicon crystal after copper decoration using X-ray topography. They reported that the vacancy concentration has a maximum value at the center of the radial direction of the crystal. The sample used in our EPR measurement was located near the rim of the AVO28 crystal. We need to perform an EPR measurement on the sample near the center of the crystal to obtain additional results in the near future.\n\nQuantifies the concentration of phosphorus impurity and defects on mechanically damaged surfaces, establishing a baseline for detecting other vacancy defects and confirming the sensitivity of the EPR measurements.",
    "original_text": "The sample cut out from 28 Si single crystal AVO28 was measured at 25 K using EPR spectroscopy. Under illumination, the number and concentration of EPR-active phosphorus\nimpurity (Si4 ≡ _ _ P¥) in the sample were determined to be 0.21(3) × 10 12 and 3.2(5) × 10 12 cm -3 , respectively. In a dark environment, the number of EPR-active defects on the mechanically damaged surfaces (Si3 ≡ Si¥) was determined to be 1.67(23) × 10 12 . The clear observations of low concentrations of phosphorous impurity and defects on mechanically damaged surfaces demonstrate that EPR measurements used in this paper have adequate sensitivity to vacancy defects at a concentration of 1 × 10 12 cm -3 .\nThe concentration of phosphorus impurity obtained in this paper, 3.2(5) × 10 12 cm -3 , agrees with that of sample 4.12 of the AVO28 crystal shown in [26, Table 3], 10(10) × 10 12 cm -3 , which was estimated from measurement results using Fourier transform infrared (FTIR) spectroscopy for 28 Si crystal Si28-23Pr11. It is worth noting that the phosphorus impurity in the silicon crystal serves as a common scale for EPR and FTIR measurements.\nThe measured surface density of defects on the mechanically damaged surfaces gives a value of 1.67(23) × 10 12 / 1.21(2) cm 2 = 1.4(2) × 10 12 cm -2 . This value is comparable to that of hydrogenated amorphous silicon (a-Si:H) films, determined using photothermal deflection spectroscopy, around 10 12 cm -2 [27].\nOn the other hand, anisotropic signals of nine types of vacancy defects were not observed in the expected magnetic field ranging from 334 to 337 mT both in a dark environment and under illumination. The concentrations of the nine types of vacancy defects were estimated to be 0.0(6) × 10 12 cm -2 , respectively. Consequently, the mass deficit correction for these vacancy defects is evaluated to be 0.0(2) θ g for a 1-kg AVO28 crystal.\nThere are vacancies without unpaired electrons that cannot be detected by EPR. At present, it is difficult to estimate the concentrations of these vacancies. However, we believe that the comparison of EPR signals in a dark environment and under illumination reveals a significant portion of vacancy defects.\nSubstitutional nitrogen atoms in a silicon crystal (Si4 ----N¥) were identified using EPR by Murakami et al. [28] and Sprenger et al. [29]. However, in our experiment, the EPR signal of nitrogen impurity, which was considered to be contained in the AVO28 crystal with a relatively high concentration, was not detected. This can be explained by the geometrical structure of atoms determined by Jones et al. [30], where most of the nitrogen atoms in silicon crystals exist in the form of interstitial atoms without unpaired electrons (Si3 ≡ N:, where the colon sign symbolizes a lone pair of electrons).\nTo the best of our knowledge, vacancies captured by nitrogen impurities have not been identified using EPR. They are not listed in the collected EPR data of the 415 types of defects in silicon [13].\nAbe et al. [31] observed an FZ silicon crystal after copper decoration using X-ray topography. They reported that the vacancy concentration has a maximum value at the center of the radial direction of the crystal. The sample used in our EPR measurement was located near the rim of the AVO28 crystal. We need to perform an EPR measurement on the sample near the center of the crystal to obtain additional results in the near future.",
    "context": "Quantifies the concentration of phosphorus impurity and defects on mechanically damaged surfaces, establishing a baseline for detecting other vacancy defects and confirming the sensitivity of the EPR measurements.",
    "document": "Electron_Paramagnetic_Resonance_Study_on_28Si_Single_Crystal_for_the_Future_Realization_of_the_Kilogram.pdf",
    "pages": [
      6,
      7
    ],
    "id": "0171f158e5998ba88bcad8b02630e933ff8f82bbc34f82b99f54215a42f7b2ae"
  },
  {
    "text": "For the future realization of the kilogram using the XRCD method, it is essential to ensure the reliability of the mass deficit correction for vacancy defects. Therefore, we conducted quantitative EPR measurements on 28 Si single crystal AVO28 at 25 K both in a dark environment and under illumination. The following conclusions were obtained.\n- 1) Isotropic EPR spectra from phosphorus impurity and defects near the mechanically damaged surfaces were observed. The concentration of phosphorus impurity in the AVO28 crystal was determined to be 3.2(5) × 10 12 cm -3 under illumination, which was consistent with that estimated using FTIR spectroscopy, 10(10) × 10 12 cm -3 .\n- 3) As a result, the mass deficit correction due to the nine types of vacancy defects was estimated to be 0.0(2) θ g for 1-kg AVO28 spheres.\n- 2) Anisotropic EPR signals of vacancy defects in the AVO28 crystal were not observed. This demonstrates that the concentrations of the nine types of vacancy defects in the silicon crystal with unpaired electrons, which were identified in previous EPR studies, were less than the detection limit of our EPR measurement, around 1 × 10 12 cm -3 .\n\nQuantifies the mass deficit due to vacancy defects in silicon crystals for the kilogram definition, highlighting the detection limit of EPR measurements and the absence of anisotropic signals, indicating low concentrations of identified vacancy types.",
    "original_text": "For the future realization of the kilogram using the XRCD method, it is essential to ensure the reliability of the mass deficit correction for vacancy defects. Therefore, we conducted quantitative EPR measurements on 28 Si single crystal AVO28 at 25 K both in a dark environment and under illumination. The following conclusions were obtained.\n- 1) Isotropic EPR spectra from phosphorus impurity and defects near the mechanically damaged surfaces were observed. The concentration of phosphorus impurity in the AVO28 crystal was determined to be 3.2(5) × 10 12 cm -3 under illumination, which was consistent with that estimated using FTIR spectroscopy, 10(10) × 10 12 cm -3 .\n- 3) As a result, the mass deficit correction due to the nine types of vacancy defects was estimated to be 0.0(2) θ g for 1-kg AVO28 spheres.\n- 2) Anisotropic EPR signals of vacancy defects in the AVO28 crystal were not observed. This demonstrates that the concentrations of the nine types of vacancy defects in the silicon crystal with unpaired electrons, which were identified in previous EPR studies, were less than the detection limit of our EPR measurement, around 1 × 10 12 cm -3 .",
    "context": "Quantifies the mass deficit due to vacancy defects in silicon crystals for the kilogram definition, highlighting the detection limit of EPR measurements and the absence of anisotropic signals, indicating low concentrations of identified vacancy types.",
    "document": "Electron_Paramagnetic_Resonance_Study_on_28Si_Single_Crystal_for_the_Future_Realization_of_the_Kilogram.pdf",
    "pages": [
      7
    ],
    "id": "998431a844c076cf215365434a903cefbd13beeb0ca90117653b63d407231fbb"
  },
  {
    "text": "Several parts of the sample preparation process were conducted in the clean room for analog-digital superconductivity (CRAVITY) at the National Institute of Advanced Industrial Science and Technology and MicroNano Open Innovation Center of Micromachine Center.\n\nProvides context about the sample preparation environment and its affiliation with research institutions.",
    "original_text": "Several parts of the sample preparation process were conducted in the clean room for analog-digital superconductivity (CRAVITY) at the National Institute of Advanced Industrial Science and Technology and MicroNano Open Innovation Center of Micromachine Center.",
    "context": "Provides context about the sample preparation environment and its affiliation with research institutions.",
    "document": "Electron_Paramagnetic_Resonance_Study_on_28Si_Single_Crystal_for_the_Future_Realization_of_the_Kilogram.pdf",
    "pages": [
      7
    ],
    "id": "3f68bc061a2f296b13cf8af22417f0ad9df5807bdfbedcc2af186b0c58e8f4a7"
  },
  {
    "text": "- [1] Consultative Committee for Mass and Related Quantities, Mise en Pratique of the Definition of the Kilogram in the SI, Ver. 11.3 . Accessed: Nov. 9, 2018. [Online]. Available: http://www.bipm.org/\n- [2] R. D. Deslattes and E. G. Kessler, 'The molar volume of silicon: Discrepancies and limitations,' IEEE Trans. Instrum. Meas. , vol. 48, no. 2, pp. 238-241, Apr. 1999.\n- [3] J. Gebauer, F. Rudolf, A. Polity, R. Krause-Rehberg, J. Martin, and P. Becker, 'On the sensitivity limit of positron annihilation: Detection of vacancies in as-grown silicon,' Appl. Phys. A, Solids Surf. , vol. 68, no. 4, pp. 411-416, Apr. 1999.\n- [4] G. D'Agostino, M. Di Luzio, G. Mana, L. Martino, M. Oddone, and C. P. Sasso, 'Quantification of the void volume in single-crystal silicon,' Anal. Chem. , vol. 88, no. 23, pp. 11678-11683, Oct. 2016.\n- [5] B. Andreas et al. , 'Counting the atoms in a 28 Si crystal for a new kilogram definition,' Metrologia , vol. 48, no. 2, pp. S1-S13, Mar. 2011.\n- [6] G. D. Watkins, 'A review of EPR studies in irradiated silicon,' in Proc. 7th Int. Conf. Phys. Semiconductors , Paris, France, 1964, pp. 97-113.\n- [7] S. Mizushima, K. Fujii, and T. Umeda, 'Electron paramagnetic resonance study on a 28 Si single crystal for the future realization of the kilogram,' in Proc. CPEM , Paris, France, Jul. 2018, pp. 1-2.\n- [8] P. J. Mohr, D. B. Newell, and B. N. Taylor, 'CODATA recommended values of the fundamental physical constants: 2014,' Rev. Mod. Phys. , vol. 88, no. 3, p. 035009, Sep. 2016.\n- [9] H. A. Jahn and E. Teller, 'Stability of polyatomic molecules in degenerate electronic states. I-Orbital degeneracy,' Proc. Roy. Soc. London A , vol. 161, no. 905, pp. 220-235, Jul. 1937.\n[10] Y.-H. Lee and J. W. Corbett, 'EPR study of defects in neutronirradiated silicon: Quenched-in alignment under < 110 > -uniaxial stress,' Phys. Rev. B, Condens. Matter , vol. 9, no. 10, pp. 4351-4361, May 1974.\n[11] G. D. Watkins, 'An EPR study of the lattice vacancy in silicon,' J. Phys. Soc. Jpn. , vol. 18, pp. 22-27, Mar. 1963.\n[12] T. Umeda, S. Hagiwara, M. Katagiri, N. Mizuochi, and J. Isoya, 'A Webbased database for EPR centers in semiconductors,' Phys. B, Condens. Matter , vols. 376-377, pp. 249-252, Apr. 2006.\n[13] C. A. J. Ammerlaan et al. , 'Paramagnetic centers in silicon,' in LandoltBörnstein Group III Condensed Matter , vol. 41A2 α . Berlin, Germany: Springer-Verlag, 2002, pp. 244-308.\n[14] Y.-H. Lee and J. W. Corbett, 'EPR studies in neutron-irradiated silicon: A negative charge state of a nonplanar five-vacancy cluster ( V 5 -),' Phys. Rev. B, Condens. Matter , vol. 8, no. 6, pp. 2810-2826, Sep. 1973.\n[15] A. Kawasuso, M. Hasegawa, M. Suezawa, S. Yamaguchi, and K. Sumino, 'Annealing processes of vacancies in silicon induced by electron irradiation: Analysis using positron lifetime measurement,' Mater. Sci. Forum , vols. 175-178, pp. 423-426, Jan. 1995.\n[16] Y. Azuma et al. , 'Improved measurement results for the Avogadro constant using a 28 Si-enriched crystal,' Metrologia , vol. 52, no. 2, pp. 360-375, Mar. 2015.\n[17] G. Feher, 'Sensitivity considerations in microwave paramagnetic resonance absorption techniques,' Bell Syst. Tech. J. , vol. 36, no. 2, pp. 449-484, Mar. 1957.\n[18] J. R. Harbridge, G. A. Rinard, R. W. Quine, S. S. Eaton, and G. R. Eaton, 'Enhanced signal intensities obtained by out-of-phase rapid-passage EPR for samples with long electron spin relaxation times,' J. Magn. Reson. , vol. 156, no. 1, pp. 41-51, May 2002.\n[19] S. Mizushima, M. Ueki, and K. Fujii, 'Mass measurement of 1 kg silicon spheres to establish a density standard,' Metrologia , vol. 41, no. 2, pp. S68-S74, Mar. 2004.\n[20] T. F. Ciszek, T. Wang, T. Schuyler, and A. Rohatgi, 'Some effects of crystal growth parameters on minority carrier lifetime in floatzoned silicon,' J. Electrochem. Soc. , vol. 136, no. 1, pp. 230-234, Jan. 1989.\n[21] R. Brunetti, C. Jacoboni, F. Nava, L. Reggiani, G. Bosman, and R. J. J. Zijlstra, 'Diffusion coefficient of electrons in silicon,' J. Appl. Phys. , vol. 52, no. 11, pp. 6713-6722, Nov. 1981.\n[22] R. C. Fletcher, W. A. Yager, G. L. Pearson, and F. R. Merritt, 'Hyperfine splitting in spin resonance of group V donors in silicon,' Phys. Rev. , vol. 95, no. 3, pp. 844-845, Aug. 1954.\n[23] G. Feher, 'Electron spin resonance experiments on donors in silicon. I. Electronic structure of donors by the electron nuclear double resonance technique,' Phys. Rev. J. Arch. , vol. 114, no. 5, pp. 1219-1244, Jun. 1959.\n[24] G. K. Walters and T. L. Estle, 'Paramagnetic resonance of defects introduced near the surface of solids by mechanical damage,' J. Appl. Phys. , vol. 32, no. 10, pp. 1854-1859, Oct. 1961.\n[25] R. A. Weeks, 'The many varieties of E' centers: A review,' J. NonCrystalline Solids , vol. 179, pp. 1-9, Nov. 1994.\n[26] G. Bartl et al. , 'A new 28 Si single crystal: Counting the atoms for the new kilogram definition,' Metrologia , vol. 54, no. 5, pp. 693-715, Oct. 2017.\n[27] W. B. Jackson, D. K. Biegelsen, R. J. Nemanich, and J. C. Knights, 'Optical absorption spectra of surface or interface states in hydrogenated amorphous silicon,' Appl. Phys. Lett. , vol. 42, no. 1, pp. 105-107, Jan. 1983.\n[28] K. Murakami, K. Masuda, Y. Aoyagi, and S. Namba, 'Experimental tests of non-thermal effect for pulsed-laser annealing by time-resolved reflectivity and EPR measurements,' Phys. B + C , vol. 116, nos. 1-3, pp. 564-569, Feb. 1983.\n[29] M. Sprenger, E. G. Sieverts, S. H. Müller, and C. A. J. Ammerlaan, 'Electron paramagnetic resonance of a nitrogen-related centre in electron irradiated silicon,' Solid State Commun. , vol. 51, no. 12, pp. 951-955, Sep. 1984.\n\nProvides supporting evidence for the argument on the new kilogram definition, specifically detailing research on silicon crystal defects and mass measurements.",
    "original_text": "- [1] Consultative Committee for Mass and Related Quantities, Mise en Pratique of the Definition of the Kilogram in the SI, Ver. 11.3 . Accessed: Nov. 9, 2018. [Online]. Available: http://www.bipm.org/\n- [2] R. D. Deslattes and E. G. Kessler, 'The molar volume of silicon: Discrepancies and limitations,' IEEE Trans. Instrum. Meas. , vol. 48, no. 2, pp. 238-241, Apr. 1999.\n- [3] J. Gebauer, F. Rudolf, A. Polity, R. Krause-Rehberg, J. Martin, and P. Becker, 'On the sensitivity limit of positron annihilation: Detection of vacancies in as-grown silicon,' Appl. Phys. A, Solids Surf. , vol. 68, no. 4, pp. 411-416, Apr. 1999.\n- [4] G. D'Agostino, M. Di Luzio, G. Mana, L. Martino, M. Oddone, and C. P. Sasso, 'Quantification of the void volume in single-crystal silicon,' Anal. Chem. , vol. 88, no. 23, pp. 11678-11683, Oct. 2016.\n- [5] B. Andreas et al. , 'Counting the atoms in a 28 Si crystal for a new kilogram definition,' Metrologia , vol. 48, no. 2, pp. S1-S13, Mar. 2011.\n- [6] G. D. Watkins, 'A review of EPR studies in irradiated silicon,' in Proc. 7th Int. Conf. Phys. Semiconductors , Paris, France, 1964, pp. 97-113.\n- [7] S. Mizushima, K. Fujii, and T. Umeda, 'Electron paramagnetic resonance study on a 28 Si single crystal for the future realization of the kilogram,' in Proc. CPEM , Paris, France, Jul. 2018, pp. 1-2.\n- [8] P. J. Mohr, D. B. Newell, and B. N. Taylor, 'CODATA recommended values of the fundamental physical constants: 2014,' Rev. Mod. Phys. , vol. 88, no. 3, p. 035009, Sep. 2016.\n- [9] H. A. Jahn and E. Teller, 'Stability of polyatomic molecules in degenerate electronic states. I-Orbital degeneracy,' Proc. Roy. Soc. London A , vol. 161, no. 905, pp. 220-235, Jul. 1937.\n[10] Y.-H. Lee and J. W. Corbett, 'EPR study of defects in neutronirradiated silicon: Quenched-in alignment under < 110 > -uniaxial stress,' Phys. Rev. B, Condens. Matter , vol. 9, no. 10, pp. 4351-4361, May 1974.\n[11] G. D. Watkins, 'An EPR study of the lattice vacancy in silicon,' J. Phys. Soc. Jpn. , vol. 18, pp. 22-27, Mar. 1963.\n[12] T. Umeda, S. Hagiwara, M. Katagiri, N. Mizuochi, and J. Isoya, 'A Webbased database for EPR centers in semiconductors,' Phys. B, Condens. Matter , vols. 376-377, pp. 249-252, Apr. 2006.\n[13] C. A. J. Ammerlaan et al. , 'Paramagnetic centers in silicon,' in LandoltBörnstein Group III Condensed Matter , vol. 41A2 α . Berlin, Germany: Springer-Verlag, 2002, pp. 244-308.\n[14] Y.-H. Lee and J. W. Corbett, 'EPR studies in neutron-irradiated silicon: A negative charge state of a nonplanar five-vacancy cluster ( V 5 -),' Phys. Rev. B, Condens. Matter , vol. 8, no. 6, pp. 2810-2826, Sep. 1973.\n[15] A. Kawasuso, M. Hasegawa, M. Suezawa, S. Yamaguchi, and K. Sumino, 'Annealing processes of vacancies in silicon induced by electron irradiation: Analysis using positron lifetime measurement,' Mater. Sci. Forum , vols. 175-178, pp. 423-426, Jan. 1995.\n[16] Y. Azuma et al. , 'Improved measurement results for the Avogadro constant using a 28 Si-enriched crystal,' Metrologia , vol. 52, no. 2, pp. 360-375, Mar. 2015.\n[17] G. Feher, 'Sensitivity considerations in microwave paramagnetic resonance absorption techniques,' Bell Syst. Tech. J. , vol. 36, no. 2, pp. 449-484, Mar. 1957.\n[18] J. R. Harbridge, G. A. Rinard, R. W. Quine, S. S. Eaton, and G. R. Eaton, 'Enhanced signal intensities obtained by out-of-phase rapid-passage EPR for samples with long electron spin relaxation times,' J. Magn. Reson. , vol. 156, no. 1, pp. 41-51, May 2002.\n[19] S. Mizushima, M. Ueki, and K. Fujii, 'Mass measurement of 1 kg silicon spheres to establish a density standard,' Metrologia , vol. 41, no. 2, pp. S68-S74, Mar. 2004.\n[20] T. F. Ciszek, T. Wang, T. Schuyler, and A. Rohatgi, 'Some effects of crystal growth parameters on minority carrier lifetime in floatzoned silicon,' J. Electrochem. Soc. , vol. 136, no. 1, pp. 230-234, Jan. 1989.\n[21] R. Brunetti, C. Jacoboni, F. Nava, L. Reggiani, G. Bosman, and R. J. J. Zijlstra, 'Diffusion coefficient of electrons in silicon,' J. Appl. Phys. , vol. 52, no. 11, pp. 6713-6722, Nov. 1981.\n[22] R. C. Fletcher, W. A. Yager, G. L. Pearson, and F. R. Merritt, 'Hyperfine splitting in spin resonance of group V donors in silicon,' Phys. Rev. , vol. 95, no. 3, pp. 844-845, Aug. 1954.\n[23] G. Feher, 'Electron spin resonance experiments on donors in silicon. I. Electronic structure of donors by the electron nuclear double resonance technique,' Phys. Rev. J. Arch. , vol. 114, no. 5, pp. 1219-1244, Jun. 1959.\n[24] G. K. Walters and T. L. Estle, 'Paramagnetic resonance of defects introduced near the surface of solids by mechanical damage,' J. Appl. Phys. , vol. 32, no. 10, pp. 1854-1859, Oct. 1961.\n[25] R. A. Weeks, 'The many varieties of E' centers: A review,' J. NonCrystalline Solids , vol. 179, pp. 1-9, Nov. 1994.\n[26] G. Bartl et al. , 'A new 28 Si single crystal: Counting the atoms for the new kilogram definition,' Metrologia , vol. 54, no. 5, pp. 693-715, Oct. 2017.\n[27] W. B. Jackson, D. K. Biegelsen, R. J. Nemanich, and J. C. Knights, 'Optical absorption spectra of surface or interface states in hydrogenated amorphous silicon,' Appl. Phys. Lett. , vol. 42, no. 1, pp. 105-107, Jan. 1983.\n[28] K. Murakami, K. Masuda, Y. Aoyagi, and S. Namba, 'Experimental tests of non-thermal effect for pulsed-laser annealing by time-resolved reflectivity and EPR measurements,' Phys. B + C , vol. 116, nos. 1-3, pp. 564-569, Feb. 1983.\n[29] M. Sprenger, E. G. Sieverts, S. H. Müller, and C. A. J. Ammerlaan, 'Electron paramagnetic resonance of a nitrogen-related centre in electron irradiated silicon,' Solid State Commun. , vol. 51, no. 12, pp. 951-955, Sep. 1984.",
    "context": "Provides supporting evidence for the argument on the new kilogram definition, specifically detailing research on silicon crystal defects and mass measurements.",
    "document": "Electron_Paramagnetic_Resonance_Study_on_28Si_Single_Crystal_for_the_Future_Realization_of_the_Kilogram.pdf",
    "pages": [
      8,
      7
    ],
    "id": "5ca5c88c16491fce8aa4ec6340d6dfd55eb350892cb68288350661e92e0a7fa8"
  },
  {
    "text": "[30] R. Jones, S. Öberg, F. B. Rasmussen, and B. B. Nielsen, 'Identification of the dominant nitrogen defect in silicon,' Phys. Rev. Lett. , vol. 72, no. 12, pp. 1882-1885, Mar. 1994.\n[31] T. Abe, H. Harada, and J. Chikawa, 'Swirl defects in float-zoned silicon crystals,' Phys. B + C , vol. 116, nos. 1-3, pp. 139-147, Feb. 1983.\nShigeki Mizushima was born in Toyama, Japan, in 1972. He received the B.S. and M.S. degrees in physics from The University of Tokyo, Tokyo, Japan, in 1995 and 1997, respectively.\nHe is currently with the National Metrology Institute of Japan, National Institute of Advanced Industrial Science and Technology, Tsukuba, Japan. His current research interests include electron paramagnetic resonance study on silicon crystals, mass measurement using a mass comparator, and gravity measurement using an absolute gravimeter.\nNaoki Kuramoto was born in Sagamihara, Japan, in 1971. He received the B.S., M.S., and Ph.D. degrees in chemistry from Saga University, Saga, Japan, in 1993, 1995, and 1998, respectively.\nFrom 1995 to 1998, he was a Research Fellow with the Japan Society for the Promotion of Science, Tokyo, Japan, (DC1, for doctoral course student). From 1998 to 1999, he was with the Tokyo University of Agriculture and Technology, Tokyo, in the field of physical acoustics. In 1999, he joined the National Metrology Institute of Japan/the\nNational Institute of Advanced Industrial Science and Technology (formerly, the National Research Laboratory of Metrology), Tsukuba, Japan, where he is currently the Group Leader of the Mass Standards Group. He has developed an optical interferometer to measure the volume of 28 Si-enriched spheres by optical frequency tuning to lead the new definition of the kilogram based on the Planck constant. His current research interests include laser interferometry and mass standards.\nDr. Kuramoto is a member of the Japan Society of Applied Physics, the Chemical Society of Japan, and the Society of Instrument and Control Engineers.\nKenichi Fujii received the B.E., M.E., and Ph.D. degrees from Keio University, Yokohama, Japan, in 1982, 1984, and 1997, respectively. His doctoral dissertation focused on the absolute measurement of the density of silicon crystals.\nIn 1984, he joined the National Metrology Institute of Japan (formerly, the National Research Laboratory of Metrology), Tsukuba, Japan. In 1988, he started an absolute measurement of the density of silicon crystals for the determination of the Avogadro constant. He developed a scanning-type optical inter- ferometer for measuring the diameter of the silicon sphere. From 1994 to 1996, he was a Guest Researcher with the Electricity Division, National Institute of Standards and Technology, Gaithersburg, MD, USA, where he was involved in the watt balance experiment. He is currently the Prime Senior Researcher with the Research Institute for Engineering Measurement, Tsukuba.\nDr. Fujii is the member of the CODATA Task Group on Fundamental Constants and the Consultative Committee for Units of the International Committee on Weights and Measures. He serves as the Coordinator for the International Avogadro Coordination Project and the Chairperson for the Working Group on Density and Viscosity of the Consultative Committee for Mass and Related Quantities.\nTakahide Umeda received the B.E., M.E., and Ph.D. degrees from the University of Tsukuba, Tsukuba, Japan, in 1994, 1996, and 1999, respectively. His Ph.D. dissertation focused on localized electronic states in solar-cell materials studied by electron spin resonance (ESR) spectroscopy.\nFrom 1998 to 1999, he was a Doctoral and a PostDoctoral Fellow with the Japan Society of Promotion of Science, where he investigated ESR spectroscopy on crystalline silicon surfaces. From 1999 to 2003, he was a Researcher with the Fundamental Research\nLaboratory and Silicon Systems Research Laboratory, NEC Corporation, Tsukuba, Japan. Since 2003, he has been an Associate Professor with the University of Tsukuba, Tsukuba, Japan, where he has been a Faculty of Pure and Applied Sciences since 2010. His current research interests include magnetic resonance spectroscopy on defects and impurities in semiconductors and semiconductor devices, e.g., silicon and its large-scale integrated circuits, silicon carbide and its power transistors, gallium nitride, and diamond.\n\nProvides background on researchers involved in mass standards and silicon measurement, specifically highlighting their affiliations and research interests related to silicon and related materials.",
    "original_text": "[30] R. Jones, S. Öberg, F. B. Rasmussen, and B. B. Nielsen, 'Identification of the dominant nitrogen defect in silicon,' Phys. Rev. Lett. , vol. 72, no. 12, pp. 1882-1885, Mar. 1994.\n[31] T. Abe, H. Harada, and J. Chikawa, 'Swirl defects in float-zoned silicon crystals,' Phys. B + C , vol. 116, nos. 1-3, pp. 139-147, Feb. 1983.\nShigeki Mizushima was born in Toyama, Japan, in 1972. He received the B.S. and M.S. degrees in physics from The University of Tokyo, Tokyo, Japan, in 1995 and 1997, respectively.\nHe is currently with the National Metrology Institute of Japan, National Institute of Advanced Industrial Science and Technology, Tsukuba, Japan. His current research interests include electron paramagnetic resonance study on silicon crystals, mass measurement using a mass comparator, and gravity measurement using an absolute gravimeter.\nNaoki Kuramoto was born in Sagamihara, Japan, in 1971. He received the B.S., M.S., and Ph.D. degrees in chemistry from Saga University, Saga, Japan, in 1993, 1995, and 1998, respectively.\nFrom 1995 to 1998, he was a Research Fellow with the Japan Society for the Promotion of Science, Tokyo, Japan, (DC1, for doctoral course student). From 1998 to 1999, he was with the Tokyo University of Agriculture and Technology, Tokyo, in the field of physical acoustics. In 1999, he joined the National Metrology Institute of Japan/the\nNational Institute of Advanced Industrial Science and Technology (formerly, the National Research Laboratory of Metrology), Tsukuba, Japan, where he is currently the Group Leader of the Mass Standards Group. He has developed an optical interferometer to measure the volume of 28 Si-enriched spheres by optical frequency tuning to lead the new definition of the kilogram based on the Planck constant. His current research interests include laser interferometry and mass standards.\nDr. Kuramoto is a member of the Japan Society of Applied Physics, the Chemical Society of Japan, and the Society of Instrument and Control Engineers.\nKenichi Fujii received the B.E., M.E., and Ph.D. degrees from Keio University, Yokohama, Japan, in 1982, 1984, and 1997, respectively. His doctoral dissertation focused on the absolute measurement of the density of silicon crystals.\nIn 1984, he joined the National Metrology Institute of Japan (formerly, the National Research Laboratory of Metrology), Tsukuba, Japan. In 1988, he started an absolute measurement of the density of silicon crystals for the determination of the Avogadro constant. He developed a scanning-type optical inter- ferometer for measuring the diameter of the silicon sphere. From 1994 to 1996, he was a Guest Researcher with the Electricity Division, National Institute of Standards and Technology, Gaithersburg, MD, USA, where he was involved in the watt balance experiment. He is currently the Prime Senior Researcher with the Research Institute for Engineering Measurement, Tsukuba.\nDr. Fujii is the member of the CODATA Task Group on Fundamental Constants and the Consultative Committee for Units of the International Committee on Weights and Measures. He serves as the Coordinator for the International Avogadro Coordination Project and the Chairperson for the Working Group on Density and Viscosity of the Consultative Committee for Mass and Related Quantities.\nTakahide Umeda received the B.E., M.E., and Ph.D. degrees from the University of Tsukuba, Tsukuba, Japan, in 1994, 1996, and 1999, respectively. His Ph.D. dissertation focused on localized electronic states in solar-cell materials studied by electron spin resonance (ESR) spectroscopy.\nFrom 1998 to 1999, he was a Doctoral and a PostDoctoral Fellow with the Japan Society of Promotion of Science, where he investigated ESR spectroscopy on crystalline silicon surfaces. From 1999 to 2003, he was a Researcher with the Fundamental Research\nLaboratory and Silicon Systems Research Laboratory, NEC Corporation, Tsukuba, Japan. Since 2003, he has been an Associate Professor with the University of Tsukuba, Tsukuba, Japan, where he has been a Faculty of Pure and Applied Sciences since 2010. His current research interests include magnetic resonance spectroscopy on defects and impurities in semiconductors and semiconductor devices, e.g., silicon and its large-scale integrated circuits, silicon carbide and its power transistors, gallium nitride, and diamond.",
    "context": "Provides background on researchers involved in mass standards and silicon measurement, specifically highlighting their affiliations and research interests related to silicon and related materials.",
    "document": "Electron_Paramagnetic_Resonance_Study_on_28Si_Single_Crystal_for_the_Future_Realization_of_the_Kilogram.pdf",
    "pages": [
      8
    ],
    "id": "d3e6ab78221b285349b2efdd97cc9198775c061479987625a21c9080a33d846a"
  },
  {
    "text": "Raúl Castilla-Arquillo , Graduate Student Member, IEEE , Anthony Mandow , Member, IEEE , Carlos J. Pérez-del-Pulgar , Member, IEEE , César Álvarez-Llamas , José M. Vadillo , and Javier Laserna\nAbstract -Planetary rover missions on Mars have suffered entrapments and serious mobility incidents due to soil assessment limitations of stereo RGB cameras, which cannot characterize relevant physical phenomena such as thermal behavior that depend on granularity and cohesion. In particular, thermal inertia estimations are already being used to assess geophysical properties from 1-D low-resolution measurements by onboard thermopiles. However, no high-resolution measurements are currently available to characterize Martian soils for safer navigation in future missions, so new experimental methods are required to capture and analyze thermal images with planetary conditions in Earth-based experiments. In this work, we propose a novel measurement system configuration and experimental methodology to capture thermal images using isolated multipurpose environmental chambers (MECs) to replicate the temperature and pressure conditions of Mars. Furthermore, the system has allowed to measure diurnal cycles for four soil types of known physical characteristics under Martian and Earth pressures to perform a unique quantitative analysis and comparison of thermal behavior and thermal inertia for soil assessment. Even if no actual Martian infrared (IR) images are available for comparison, results indicate a correlation between granularity and thermal inertia that is consistent with available thermopile measurements recorded by rover's onsite. Furthermore, the set of measurements acquired in the experiments has been made available to the scientific community.\nof slippage, skidding, and entrapment on granular terrains, which caused delays and considerable mobility issues during the Curiosity and Spirit rover missions [2], [3], [4]. For instance, the Spirit rover became entrapped while trying to traverse undetected loose sand hidden under a thin surface layer of duricrust and subsequently stopped responding to commands [5], highlighting the importance of accurate soil assessment for future missions.\nIndex Terms -Measurement equipment, multipurpose environmental chamber (MEC), planetary robotics, thermal imagery, thermal inertia.\n\nThe research proposes a novel experimental system using multipurpose environmental chambers to replicate Martian conditions and measure thermal behavior and inertia of soils, aiming to overcome limitations of existing soil assessment methods for future rover missions.",
    "original_text": "Raúl Castilla-Arquillo , Graduate Student Member, IEEE , Anthony Mandow , Member, IEEE , Carlos J. Pérez-del-Pulgar , Member, IEEE , César Álvarez-Llamas , José M. Vadillo , and Javier Laserna\nAbstract -Planetary rover missions on Mars have suffered entrapments and serious mobility incidents due to soil assessment limitations of stereo RGB cameras, which cannot characterize relevant physical phenomena such as thermal behavior that depend on granularity and cohesion. In particular, thermal inertia estimations are already being used to assess geophysical properties from 1-D low-resolution measurements by onboard thermopiles. However, no high-resolution measurements are currently available to characterize Martian soils for safer navigation in future missions, so new experimental methods are required to capture and analyze thermal images with planetary conditions in Earth-based experiments. In this work, we propose a novel measurement system configuration and experimental methodology to capture thermal images using isolated multipurpose environmental chambers (MECs) to replicate the temperature and pressure conditions of Mars. Furthermore, the system has allowed to measure diurnal cycles for four soil types of known physical characteristics under Martian and Earth pressures to perform a unique quantitative analysis and comparison of thermal behavior and thermal inertia for soil assessment. Even if no actual Martian infrared (IR) images are available for comparison, results indicate a correlation between granularity and thermal inertia that is consistent with available thermopile measurements recorded by rover's onsite. Furthermore, the set of measurements acquired in the experiments has been made available to the scientific community.\nof slippage, skidding, and entrapment on granular terrains, which caused delays and considerable mobility issues during the Curiosity and Spirit rover missions [2], [3], [4]. For instance, the Spirit rover became entrapped while trying to traverse undetected loose sand hidden under a thin surface layer of duricrust and subsequently stopped responding to commands [5], highlighting the importance of accurate soil assessment for future missions.\nIndex Terms -Measurement equipment, multipurpose environmental chamber (MEC), planetary robotics, thermal imagery, thermal inertia.",
    "context": "The research proposes a novel experimental system using multipurpose environmental chambers to replicate Martian conditions and measure thermal behavior and inertia of soils, aiming to overcome limitations of existing soil assessment methods for future rover missions.",
    "document": "Thermal_Imagery_for_Rover_Soil_Assessment_Using_a_Multipurpose_Environmental_Chamber_Under_Simulated_Mars_Conditions.pdf",
    "pages": [
      1
    ],
    "id": "44e2bf3e9d2f6c5174625e17d33897e20b49672f8233e33a7886c60e81462c43"
  },
  {
    "text": "R EMOTE assessment of soil characteristics can be crucial for the safety and efficiency of a broad range of tasks related to the navigation of planetary mobile robots such as odometry, environment mapping, or energetic consumption [1]. Soil assessment is essential to mitigate the risks\nManuscript received 24 July 2023; revised 15 November 2023; accepted 30 November 2023. Date of publication 25 December 2023; date of current version 15 January 2024. This work was supported in part by the Andalusian Regional Government through the Project Intelligent Multimodal Sensor for Identification of Terramechanic Characteristics in Off-Road Vehicles (IMSITER) under Grant P18-RT-991 and in part by the Open Access funding provided by Universidad de Málaga/CBUA. The Associate Editor coordinating the review process was Dr. Libing Bai. (Corresponding author: Carlos J. Pérez-del-Pulgar.)\nRaúl Castilla-Arquillo, Anthony Mandow, and Carlos J. Pérez-del-Pulgar are with the Institute for Mechatronics Engineering and Cyber-Physical Systems (IMECH.UMA), University of Málaga, Andalucía Tech, 29070 Málaga, Spain. César Álvarez-Llamas, José M. Vadillo, and Javier Laserna are with the UMALASERLAB, Department of Analytical Chemistry, University of Málaga, 29010 Málaga, Spain (e-mail: carlosperez@uma.es).\nDigital Object Identifier 10.1109/TIM.2023.3346528\nRemote sensors onboard mobile robots, such as RGB stereo cameras or 3-D laser scanners [6], can be used to infer soil characteristics like roughness and slope [7], [8] without soil contact, but do not allow assessment of relevant subsurface properties for traversability (e.g., granularity and soil cohesion). In this sense, infrared (IR) remote sensors such as thermopiles and thermal cameras have provided relevant data to infer subsurface properties, as the thermal behavior of the surface is influenced by the composition of deeper soil layers [9]. Thus, soil composition can be remotely inferred by estimating the soil thermal inertia, which is the key material property that affects the surface thermal behavior, over daily cycles of temperature variations [10]. Thermopiles are being actively used in the Curiosity and Perseverance rovers to perform measurements of Martian surface thermal behavior to assess the geophysical properties of areas of interest. Remote temperature measurements taken during their traverses have been employed to infer the thermal inertia of several types of soils not only for scientific purposes [11], [12] but also for rover slip prediction [13]. However, thermopiles measure 1-D data from surfaces in the order of several square meters, which poses limitations for characterizing heterogeneous soils.\nFurthermore, quantitative analysis of soil characteristics inferred from thermal inertia is very challenging in a planetary context because the actual terrain is not accessible for experimental validation [13]. Capturing high-resolution thermal images of soils of known physical characteristics under simulated planetary conditions is important for experiments aimed at estimating thermal inertia, which is notably influenced by atmospheric pressure. To the best of our knowledge, no comparative analysis of diurnal cycle surface thermal behavior has been performed for different soils under Martian and Earth pressures.\nIn this context, multipurpose environmental chambers (MECs) are experimental measurement equipment that can function under conditions that closely resemble the tempera-\nFig. 1. UMALASERLAB Mars Environment Chamber used in the experiments.\ntures and pressures found on other planets, such as Mars [14], [15], [16], but reported experiments are limited to constant conditions. These chambers are constructed with isolated walls so that the effects of external conditions such as temperature, pressure, or humidity are negligible, but novel instrument setup methods are required to perform remote thermal measurements through an appropriate IR viewport to withstand the pressure differential and temperatures reached by the MEC and to simulate diurnal temperature cycles.\nThis article proposes a measurement system configuration and experimental methodology to capture thermal images using MECs to replicate the temperature and pressure conditions of Martian diurnal cycles. The instrument setup and experimental methodology have been applied to the UMALASERLAB Multipurpose Thermal Vacuum Chamber [17] (see Fig. 1), which can also replicate the atmospheric CO2 composition of Mars. Moreover, we apply the proposed system to offer a novel quantitative analysis of thermal behavior and thermal inertia of four types of soil with known physical characteristics for diurnal cycles with Martian and Earth pressures. Furthermore, the dataset resulting from the experiments has been made available to the scientific community.\nThis article is organized as follows. Section II discusses related work. Section III reviews thermal inertia formulation. Section IV presents the proposed MEC-based remote thermal measurement system. Section V describes the experimental setup. Section VI presents an experimental analysis. Finally, Section VII offers conclusions and provides insight into future works.\n\nHighlights the importance of soil assessment for planetary rover navigation, emphasizing the limitations of current remote sensing methods and the need for new experimental approaches to characterize thermal behavior and inertia.",
    "original_text": "R EMOTE assessment of soil characteristics can be crucial for the safety and efficiency of a broad range of tasks related to the navigation of planetary mobile robots such as odometry, environment mapping, or energetic consumption [1]. Soil assessment is essential to mitigate the risks\nManuscript received 24 July 2023; revised 15 November 2023; accepted 30 November 2023. Date of publication 25 December 2023; date of current version 15 January 2024. This work was supported in part by the Andalusian Regional Government through the Project Intelligent Multimodal Sensor for Identification of Terramechanic Characteristics in Off-Road Vehicles (IMSITER) under Grant P18-RT-991 and in part by the Open Access funding provided by Universidad de Málaga/CBUA. The Associate Editor coordinating the review process was Dr. Libing Bai. (Corresponding author: Carlos J. Pérez-del-Pulgar.)\nRaúl Castilla-Arquillo, Anthony Mandow, and Carlos J. Pérez-del-Pulgar are with the Institute for Mechatronics Engineering and Cyber-Physical Systems (IMECH.UMA), University of Málaga, Andalucía Tech, 29070 Málaga, Spain. César Álvarez-Llamas, José M. Vadillo, and Javier Laserna are with the UMALASERLAB, Department of Analytical Chemistry, University of Málaga, 29010 Málaga, Spain (e-mail: carlosperez@uma.es).\nDigital Object Identifier 10.1109/TIM.2023.3346528\nRemote sensors onboard mobile robots, such as RGB stereo cameras or 3-D laser scanners [6], can be used to infer soil characteristics like roughness and slope [7], [8] without soil contact, but do not allow assessment of relevant subsurface properties for traversability (e.g., granularity and soil cohesion). In this sense, infrared (IR) remote sensors such as thermopiles and thermal cameras have provided relevant data to infer subsurface properties, as the thermal behavior of the surface is influenced by the composition of deeper soil layers [9]. Thus, soil composition can be remotely inferred by estimating the soil thermal inertia, which is the key material property that affects the surface thermal behavior, over daily cycles of temperature variations [10]. Thermopiles are being actively used in the Curiosity and Perseverance rovers to perform measurements of Martian surface thermal behavior to assess the geophysical properties of areas of interest. Remote temperature measurements taken during their traverses have been employed to infer the thermal inertia of several types of soils not only for scientific purposes [11], [12] but also for rover slip prediction [13]. However, thermopiles measure 1-D data from surfaces in the order of several square meters, which poses limitations for characterizing heterogeneous soils.\nFurthermore, quantitative analysis of soil characteristics inferred from thermal inertia is very challenging in a planetary context because the actual terrain is not accessible for experimental validation [13]. Capturing high-resolution thermal images of soils of known physical characteristics under simulated planetary conditions is important for experiments aimed at estimating thermal inertia, which is notably influenced by atmospheric pressure. To the best of our knowledge, no comparative analysis of diurnal cycle surface thermal behavior has been performed for different soils under Martian and Earth pressures.\nIn this context, multipurpose environmental chambers (MECs) are experimental measurement equipment that can function under conditions that closely resemble the tempera-\nFig. 1. UMALASERLAB Mars Environment Chamber used in the experiments.\ntures and pressures found on other planets, such as Mars [14], [15], [16], but reported experiments are limited to constant conditions. These chambers are constructed with isolated walls so that the effects of external conditions such as temperature, pressure, or humidity are negligible, but novel instrument setup methods are required to perform remote thermal measurements through an appropriate IR viewport to withstand the pressure differential and temperatures reached by the MEC and to simulate diurnal temperature cycles.\nThis article proposes a measurement system configuration and experimental methodology to capture thermal images using MECs to replicate the temperature and pressure conditions of Martian diurnal cycles. The instrument setup and experimental methodology have been applied to the UMALASERLAB Multipurpose Thermal Vacuum Chamber [17] (see Fig. 1), which can also replicate the atmospheric CO2 composition of Mars. Moreover, we apply the proposed system to offer a novel quantitative analysis of thermal behavior and thermal inertia of four types of soil with known physical characteristics for diurnal cycles with Martian and Earth pressures. Furthermore, the dataset resulting from the experiments has been made available to the scientific community.\nThis article is organized as follows. Section II discusses related work. Section III reviews thermal inertia formulation. Section IV presents the proposed MEC-based remote thermal measurement system. Section V describes the experimental setup. Section VI presents an experimental analysis. Finally, Section VII offers conclusions and provides insight into future works.",
    "context": "Highlights the importance of soil assessment for planetary rover navigation, emphasizing the limitations of current remote sensing methods and the need for new experimental approaches to characterize thermal behavior and inertia.",
    "document": "Thermal_Imagery_for_Rover_Soil_Assessment_Using_a_Multipurpose_Environmental_Chamber_Under_Simulated_Mars_Conditions.pdf",
    "pages": [
      1,
      2
    ],
    "id": "c766beb259406ed8b66e1bfea2488bfbb84371bbf4bd897897b52a6301820050"
  },
  {
    "text": "Planetary exploration rovers are fit with remote sensors such as RGB stereo cameras [18] for mission planning assistance, but these cameras have not been suitable to avoid the risks of slippage, skidding, and entrapment on granular terrains, which have resulted in delays and considerable mobility issues during the Curiosity and Spirit rover missions [2], [3], [4]. As pointed out in [13], the Spirit rover could have avoided getting trapped in the sand if a thermal camera had been used to assess the duricrust-covered sandy area by contrasting thermal inertia because, unlike visual surface appearance, this physical phenomenon is directly correlated to properties such as particle size, soil cohesion, porosity, and bedrock abundance, which are essential for vehicle slippage prevention and safe mobility.\nThermal inertia represents a combination of physical properties that are not directly measurable in practice, so simplified estimations based on surface temperature observations (either from satellite measurements or on-site data) are required. Thermal inertia can be estimated by fitting the measured thermal daily cycles with the temperature obtained from a surface thermal model, which entails solving numerical models by iterating the values of several unknown parameters until the model's curve fits the measurements [19]. Alternatively, the apparent thermal inertia (ATI) [20] allows a simpler computation, but it does not consider the surface net heat flux. In this work, we use a method based on the daily amplitude of surface net heat flux and temperature to calculate the thermal inertia of a surface subjected to the Sun's heating [21].\nSatellite-based measurements of thermal inertia have provided a means to determine the physical properties of the Earth's surface on a global scale, which would be impractical to obtain through on-site measurements [20], [22]. Similarly, satellite measurements of the Martian surface have been helpful to identify potential areas of scientific interest for future exploration [23], [24]. Satellite measurements, although valuable for large-scale assessments, often lack the required resolution for robotic navigation and can be influenced by clouds and other atmospheric conditions.\nAs for on-site measurements, the deployment of thermopiles on Mars rovers has been used to identify soil characteristics of traversed paths at higher resolution than satellites by acquiring diurnal cycle measurements [25], [26]. The Curiosity rover uses the ground temperature sensor (GTS), positioned at a height of 1.6 m, with a downward-looking field of view (FOV) covering 100 m 2 of terrain [27]. Furthermore, the Perseverance rover features the thermal IR sensor (TIRS), a combination of five thermopiles, one of which measures ground temperature from a height of 1.5 m with an FOV of approximately 3 m 2 [28]. Nonetheless, 1-D IR sensors such as thermopiles offer insufficient resolution to distinguish between adjacent heterogeneous soil regions [13].\nCurrent rover missions already incorporate multispectral 2-D cameras for narrowband visible/near-IR (VNIR) for mineralogical measurements [18], [29], but long-wave IR (LWIR) imagery is needed for thermal behavior assessment. On Earth, on-site thermal cameras have been employed to estimate diurnal thermal inertia of different soils, even if results were limited by the effect of Earth pressure [30], [31].\nFurther development on planetary soil assessment with LWIR cameras is hindered by the lack of actual thermal images from Mars rovers [32]. This is especially critical in view of the rapid development of machine learning methods, which demand large amounts of training measurements [33], [34], [35]. This problem has been addressed by creating synthetic Martian data using generative neural networks [36].\nParticularly, synthetic IR Martian images for thermal inertia estimation were generated from a multimodal combination of Earth data, including RGB imagery [32]. Alternatively, more accurate planetary conditions such as pressure and atmospheric composition can be physically simulated in MECs. For instance, JPL developed a large facility for the simulation of intense solar radiation in interplanetary vacuum conditions [14]. More recently, a small-scale MEC was developed to simulate planetary pressure and temperature conditions for testing instrumentation, procedures, and materials, called SpaceQ [15]. Moreover, the Mars MECs in [16] and [17] are used for VNIR and laser-induced breakdown spectroscopy (LIBS) measurements. Since this equipment was not conceived neither for LWIR cameras nor to simulate diurnal cycles, novel system configurations, and experimental methodologies are needed to adapt MECs for thermal behavior measurements under planetary conditions.\nThe major novelties and advances of this work in the instrumentation and measurement context are the following.\n- 1) We offer an experimental methodology for MECs to simulate Martian diurnal cycles with an appropriate IR viewport setup for remote thermal measurements.\n- 2) We provide technical details on the implementation of the experimental methodology in the UMALASERLAB MEC [17].\n- 3) We provide a diurnal cycle quantitative soil analysis from high-resolution thermal images captured under Martian and Earth conditions (i.e., pressure and atmospheric composition).\n- 4) We offer a public dataset of the radiometric images and data resulting from the MEC experiments.\nTo the best knowledge of the authors, no previous works have addressed these issues.\n\nProvides a methodology for simulating Martian diurnal cycles using multipurpose environmental chambers for thermal behavior measurements.",
    "original_text": "Planetary exploration rovers are fit with remote sensors such as RGB stereo cameras [18] for mission planning assistance, but these cameras have not been suitable to avoid the risks of slippage, skidding, and entrapment on granular terrains, which have resulted in delays and considerable mobility issues during the Curiosity and Spirit rover missions [2], [3], [4]. As pointed out in [13], the Spirit rover could have avoided getting trapped in the sand if a thermal camera had been used to assess the duricrust-covered sandy area by contrasting thermal inertia because, unlike visual surface appearance, this physical phenomenon is directly correlated to properties such as particle size, soil cohesion, porosity, and bedrock abundance, which are essential for vehicle slippage prevention and safe mobility.\nThermal inertia represents a combination of physical properties that are not directly measurable in practice, so simplified estimations based on surface temperature observations (either from satellite measurements or on-site data) are required. Thermal inertia can be estimated by fitting the measured thermal daily cycles with the temperature obtained from a surface thermal model, which entails solving numerical models by iterating the values of several unknown parameters until the model's curve fits the measurements [19]. Alternatively, the apparent thermal inertia (ATI) [20] allows a simpler computation, but it does not consider the surface net heat flux. In this work, we use a method based on the daily amplitude of surface net heat flux and temperature to calculate the thermal inertia of a surface subjected to the Sun's heating [21].\nSatellite-based measurements of thermal inertia have provided a means to determine the physical properties of the Earth's surface on a global scale, which would be impractical to obtain through on-site measurements [20], [22]. Similarly, satellite measurements of the Martian surface have been helpful to identify potential areas of scientific interest for future exploration [23], [24]. Satellite measurements, although valuable for large-scale assessments, often lack the required resolution for robotic navigation and can be influenced by clouds and other atmospheric conditions.\nAs for on-site measurements, the deployment of thermopiles on Mars rovers has been used to identify soil characteristics of traversed paths at higher resolution than satellites by acquiring diurnal cycle measurements [25], [26]. The Curiosity rover uses the ground temperature sensor (GTS), positioned at a height of 1.6 m, with a downward-looking field of view (FOV) covering 100 m 2 of terrain [27]. Furthermore, the Perseverance rover features the thermal IR sensor (TIRS), a combination of five thermopiles, one of which measures ground temperature from a height of 1.5 m with an FOV of approximately 3 m 2 [28]. Nonetheless, 1-D IR sensors such as thermopiles offer insufficient resolution to distinguish between adjacent heterogeneous soil regions [13].\nCurrent rover missions already incorporate multispectral 2-D cameras for narrowband visible/near-IR (VNIR) for mineralogical measurements [18], [29], but long-wave IR (LWIR) imagery is needed for thermal behavior assessment. On Earth, on-site thermal cameras have been employed to estimate diurnal thermal inertia of different soils, even if results were limited by the effect of Earth pressure [30], [31].\nFurther development on planetary soil assessment with LWIR cameras is hindered by the lack of actual thermal images from Mars rovers [32]. This is especially critical in view of the rapid development of machine learning methods, which demand large amounts of training measurements [33], [34], [35]. This problem has been addressed by creating synthetic Martian data using generative neural networks [36].\nParticularly, synthetic IR Martian images for thermal inertia estimation were generated from a multimodal combination of Earth data, including RGB imagery [32]. Alternatively, more accurate planetary conditions such as pressure and atmospheric composition can be physically simulated in MECs. For instance, JPL developed a large facility for the simulation of intense solar radiation in interplanetary vacuum conditions [14]. More recently, a small-scale MEC was developed to simulate planetary pressure and temperature conditions for testing instrumentation, procedures, and materials, called SpaceQ [15]. Moreover, the Mars MECs in [16] and [17] are used for VNIR and laser-induced breakdown spectroscopy (LIBS) measurements. Since this equipment was not conceived neither for LWIR cameras nor to simulate diurnal cycles, novel system configurations, and experimental methodologies are needed to adapt MECs for thermal behavior measurements under planetary conditions.\nThe major novelties and advances of this work in the instrumentation and measurement context are the following.\n- 1) We offer an experimental methodology for MECs to simulate Martian diurnal cycles with an appropriate IR viewport setup for remote thermal measurements.\n- 2) We provide technical details on the implementation of the experimental methodology in the UMALASERLAB MEC [17].\n- 3) We provide a diurnal cycle quantitative soil analysis from high-resolution thermal images captured under Martian and Earth conditions (i.e., pressure and atmospheric composition).\n- 4) We offer a public dataset of the radiometric images and data resulting from the MEC experiments.\nTo the best knowledge of the authors, no previous works have addressed these issues.",
    "context": "Provides a methodology for simulating Martian diurnal cycles using multipurpose environmental chambers for thermal behavior measurements.",
    "document": "Thermal_Imagery_for_Rover_Soil_Assessment_Using_a_Multipurpose_Environmental_Chamber_Under_Simulated_Mars_Conditions.pdf",
    "pages": [
      2,
      3
    ],
    "id": "fc2bc85b93f8899f2b30c383c10ed16e810a09b2cc714d1e25351a36588d0f6e"
  },
  {
    "text": "This section reviews thermal inertia concepts, the use of the thermal diffusion equation to model the Martian surface thermal behavior, and a method to estimate thermal inertia based on surface temperature gradients.\n\nReviews thermal inertia, the thermal diffusion equation, and a method for estimating thermal inertia using surface temperature gradients – providing a foundational understanding for the subsequent experimental methodology.",
    "original_text": "This section reviews thermal inertia concepts, the use of the thermal diffusion equation to model the Martian surface thermal behavior, and a method to estimate thermal inertia based on surface temperature gradients.",
    "context": "Reviews thermal inertia, the thermal diffusion equation, and a method for estimating thermal inertia using surface temperature gradients – providing a foundational understanding for the subsequent experimental methodology.",
    "document": "Thermal_Imagery_for_Rover_Soil_Assessment_Using_a_Multipurpose_Environmental_Chamber_Under_Simulated_Mars_Conditions.pdf",
    "pages": [
      3
    ],
    "id": "7debdbc079b0feaf1da2361f47cc3f5bc034d32e9b65709e11bc7db1c0451ca2"
  },
  {
    "text": "Thermal inertia, I , is defined as follows :\n<!-- formula-not-decoded -->\nwhere k is the bulk thermal conductivity, p is the bulk density, and c is the soil specific heat capacity. Thermal inertia is the property of a material that affects the resistance of a soil to change its temperature. A higher thermal inertia value means a slower heating of the soil. Thermal conductivity is the parameter that mainly influences thermal inertia, which is affected by three different heat transfer mechanisms [10]\n<!-- formula-not-decoded -->\nwhere kr is the transfer across pore spaces, kc is the conduction between grains contact areas, and kg is the conduction of the gas which fills the pores between grains. Pressure greatly determines which term acquires the most relevance. Gas conduction ( kg ) dominates at pressures between 0.1 and 1000 mbar, where there is a near-linear relationship between particle size and thermal conductivity for granular soils [37], [38]. In this case, loose granular soils have lower thermal inertia than compacted rocky soils [39]. However, the relationship is not so strong at pressures higher than 1000 mbar. Thus, it is much easier to estimate the soil characteristics based on thermal inertia at Martian pressure than at Earth pressure. On the other hand, it would be significantly harder on the Moon, where the conduction mechanism is not presented due to vacuum.\n\nDefines thermal inertia as a material property affecting soil temperature change resistance; highlights the influence of pressure on thermal conductivity and the relative ease of estimating soil characteristics at Martian pressure compared to Earth.",
    "original_text": "Thermal inertia, I , is defined as follows :\n<!-- formula-not-decoded -->\nwhere k is the bulk thermal conductivity, p is the bulk density, and c is the soil specific heat capacity. Thermal inertia is the property of a material that affects the resistance of a soil to change its temperature. A higher thermal inertia value means a slower heating of the soil. Thermal conductivity is the parameter that mainly influences thermal inertia, which is affected by three different heat transfer mechanisms [10]\n<!-- formula-not-decoded -->\nwhere kr is the transfer across pore spaces, kc is the conduction between grains contact areas, and kg is the conduction of the gas which fills the pores between grains. Pressure greatly determines which term acquires the most relevance. Gas conduction ( kg ) dominates at pressures between 0.1 and 1000 mbar, where there is a near-linear relationship between particle size and thermal conductivity for granular soils [37], [38]. In this case, loose granular soils have lower thermal inertia than compacted rocky soils [39]. However, the relationship is not so strong at pressures higher than 1000 mbar. Thus, it is much easier to estimate the soil characteristics based on thermal inertia at Martian pressure than at Earth pressure. On the other hand, it would be significantly harder on the Moon, where the conduction mechanism is not presented due to vacuum.",
    "context": "Defines thermal inertia as a material property affecting soil temperature change resistance; highlights the influence of pressure on thermal conductivity and the relative ease of estimating soil characteristics at Martian pressure compared to Earth.",
    "document": "Thermal_Imagery_for_Rover_Soil_Assessment_Using_a_Multipurpose_Environmental_Chamber_Under_Simulated_Mars_Conditions.pdf",
    "pages": [
      3
    ],
    "id": "b9b3f42e6ba24ca61b2c64b86c2b1289ba70259065326d66bc0cfe75c018a98f"
  },
  {
    "text": "Martian surface thermal behavior can be expressed as a boundary condition on the thermal diffusion equation derived from its surface energy budget\n<!-- formula-not-decoded -->\nwhere G is the net heat flux expressed in W/m 2 , A is the albedo, σ B is the Stefan-Boltzmann's constant, R sw is the downwelling shortwave (SW) radiation absorbed from the Sun, R lw is the downwelling longwave (LW) radiation emitted by the atmosphere and the Sun, ϵ is the thermal emissivity, F CO2 is the seasonal CO2 condensation, P is the period of a diurnal cycle, Ts is the surface temperature, and the term (∂ T /∂ Z ′ ) | Z ′ = 0 is the temperature gradient evaluated at the surface of the terrain, being Z ′ the distance into the terrain normalized to the thermal skin depth. The sign convention is to use a positive sign when modeling the heating of the terrain and a negative sign when modeling its cooling.\nThe F CO2 term of (3) is negligible for Martian surfaces between equatorial and mid-latitudes that present no frost. Moreover, the downwelling LW radiation is not considered, as previous on-site measurements have shown it to be an order of magnitude smaller than the rest of the terms [11]. Thus, the simplified equation for the soil thermal behavior is\n<!-- formula-not-decoded -->\nwhere the soil thermal behavior depends on the SW incident Sun's radiation, thermal inertia, and the surface radiative emission.\n\nDescribes the simplified equation used to model Martian surface thermal behavior, incorporating solar radiation, thermal inertia, and radiative emission.",
    "original_text": "Martian surface thermal behavior can be expressed as a boundary condition on the thermal diffusion equation derived from its surface energy budget\n<!-- formula-not-decoded -->\nwhere G is the net heat flux expressed in W/m 2 , A is the albedo, σ B is the Stefan-Boltzmann's constant, R sw is the downwelling shortwave (SW) radiation absorbed from the Sun, R lw is the downwelling longwave (LW) radiation emitted by the atmosphere and the Sun, ϵ is the thermal emissivity, F CO2 is the seasonal CO2 condensation, P is the period of a diurnal cycle, Ts is the surface temperature, and the term (∂ T /∂ Z ′ ) | Z ′ = 0 is the temperature gradient evaluated at the surface of the terrain, being Z ′ the distance into the terrain normalized to the thermal skin depth. The sign convention is to use a positive sign when modeling the heating of the terrain and a negative sign when modeling its cooling.\nThe F CO2 term of (3) is negligible for Martian surfaces between equatorial and mid-latitudes that present no frost. Moreover, the downwelling LW radiation is not considered, as previous on-site measurements have shown it to be an order of magnitude smaller than the rest of the terms [11]. Thus, the simplified equation for the soil thermal behavior is\n<!-- formula-not-decoded -->\nwhere the soil thermal behavior depends on the SW incident Sun's radiation, thermal inertia, and the surface radiative emission.",
    "context": "Describes the simplified equation used to model Martian surface thermal behavior, incorporating solar radiation, thermal inertia, and radiative emission.",
    "document": "Thermal_Imagery_for_Rover_Soil_Assessment_Using_a_Multipurpose_Environmental_Chamber_Under_Simulated_Mars_Conditions.pdf",
    "pages": [
      3
    ],
    "id": "10116baa1d009be2c51c42caf57c033c070c8e5fa376d2a2f1a18e1d28e55b82"
  },
  {
    "text": "In this work, we estimate thermal inertia based on the daily amplitude of surface net heat flux and temperature of a surface subjected to the Sun's heating [21]. This method considers a sinusoidal approximation of the Earth's net heat flux and surface temperatures for a diurnal period P , which also applies to Mars' heat fluxes and temperatures [11]. The thermal inertia of a given soil is estimated as\n<!-- formula-not-decoded -->\nwhere 1 Ts = T max -T min and the net heat flux is expressed as 1 Gs = G max -G min, being G max and G min the maximum and\nFig. 2. Schematic of the proposed physical MEC-based configuration.\nminimum values of the net heat flux, respectively. The result can be multiplied by a 4186 coefficient to express the inertia in thermal inertia units, tiu ≡ ( Ws 1 / 2 / m 2 K ) . Throughout this work, I sin will always be expressed in tiu.\n\nEstablishes a method for estimating thermal inertia using daily net heat flux and surface temperature variations, incorporating a sinusoidal approximation and expressing results in thermal inertia units (tiu).",
    "original_text": "In this work, we estimate thermal inertia based on the daily amplitude of surface net heat flux and temperature of a surface subjected to the Sun's heating [21]. This method considers a sinusoidal approximation of the Earth's net heat flux and surface temperatures for a diurnal period P , which also applies to Mars' heat fluxes and temperatures [11]. The thermal inertia of a given soil is estimated as\n<!-- formula-not-decoded -->\nwhere 1 Ts = T max -T min and the net heat flux is expressed as 1 Gs = G max -G min, being G max and G min the maximum and\nFig. 2. Schematic of the proposed physical MEC-based configuration.\nminimum values of the net heat flux, respectively. The result can be multiplied by a 4186 coefficient to express the inertia in thermal inertia units, tiu ≡ ( Ws 1 / 2 / m 2 K ) . Throughout this work, I sin will always be expressed in tiu.",
    "context": "Establishes a method for estimating thermal inertia using daily net heat flux and surface temperature variations, incorporating a sinusoidal approximation and expressing results in thermal inertia units (tiu).",
    "document": "Thermal_Imagery_for_Rover_Soil_Assessment_Using_a_Multipurpose_Environmental_Chamber_Under_Simulated_Mars_Conditions.pdf",
    "pages": [
      3,
      4
    ],
    "id": "769cfcdcd17e38893288d737e26b6643d9019bdc9c97a210b72176e998942fda"
  },
  {
    "text": "In this section, we propose an MEC-based high-resolution remote thermal measurement system to physically simulate soil thermal behavior over diurnal cycles under planetary conditions of pressure and atmospheric composition. The measurement system consists of a physical MEC-based configuration (see Fig. 2) and an experimental methodology. The algorithmic representation of the proposed methodology is shown in Fig. 3.\n\nIntroduces a proposed system for simulating soil thermal behavior under planetary conditions.",
    "original_text": "In this section, we propose an MEC-based high-resolution remote thermal measurement system to physically simulate soil thermal behavior over diurnal cycles under planetary conditions of pressure and atmospheric composition. The measurement system consists of a physical MEC-based configuration (see Fig. 2) and an experimental methodology. The algorithmic representation of the proposed methodology is shown in Fig. 3.",
    "context": "Introduces a proposed system for simulating soil thermal behavior under planetary conditions.",
    "document": "Thermal_Imagery_for_Rover_Soil_Assessment_Using_a_Multipurpose_Environmental_Chamber_Under_Simulated_Mars_Conditions.pdf",
    "pages": [
      4
    ],
    "id": "90e28c32e7aff851be4f579eadf7af0e393782b0c4366a51f24c8ddc26acc1ae"
  },
  {
    "text": "The physical MEC-based configuration (see Fig. 2) allows to perform high-resolution remote temperature measurements under the extreme conditions produced by the MEC. This configuration consists of an inner plate where the sample bins are placed, an external thermal camera connected to a Mini PC for data collection, and an IR viewport. The viewport must allow the IR range from 8 to 14 µ m to pass through with minimal losses. Furthermore, it must withstand the pressure differential and temperatures reached by the MEC.\n\nDetails the physical setup of the experimental system, specifically outlining the components and requirements of the IR viewport for high-resolution remote temperature measurements within the MEC.",
    "original_text": "The physical MEC-based configuration (see Fig. 2) allows to perform high-resolution remote temperature measurements under the extreme conditions produced by the MEC. This configuration consists of an inner plate where the sample bins are placed, an external thermal camera connected to a Mini PC for data collection, and an IR viewport. The viewport must allow the IR range from 8 to 14 µ m to pass through with minimal losses. Furthermore, it must withstand the pressure differential and temperatures reached by the MEC.",
    "context": "Details the physical setup of the experimental system, specifically outlining the components and requirements of the IR viewport for high-resolution remote temperature measurements within the MEC.",
    "document": "Thermal_Imagery_for_Rover_Soil_Assessment_Using_a_Multipurpose_Environmental_Chamber_Under_Simulated_Mars_Conditions.pdf",
    "pages": [
      4
    ],
    "id": "59efb7c6cf84a320b535497b070eba8186c595bf66b1e5bbb441b6a28084ddd2"
  },
  {
    "text": "The experimental methodology (see Fig. 3) consists of three major sequential tasks: the study of the physical characteristics of the soils and preparation of the MEC setup, the adjustment of the inner pressure according to the environment to be replicated, and the physical simulation within the MEC, followed by the estimation of the thermal inertias based on the resulting measurements.\nDuring the MEC preparation task, soils with previously studied physical properties (e.g., granularity) are selected and deposited into the sample bins. Subsequently, these sample bins are positioned on the plate. At this point, thermal insulation between the sample bins and the plate must be guaranteed to prevent potential IR reflections that could distort the measurements. The thermal camera is mounted on the viewport, and its housing is grounded to prevent electrostatic charges generated by the MEC pumps. The thermal camera is then calibrated to ensure accurate measurements of the sample bin surfaces. Finally, the MEC is sealed.\nIn the pressure adjustment task, procedures vary depending on the pressure and temperature range for the experiments. In the case of Earth pressure experiments with temperatures exceeding 0 ◦ C, the physical simulation can start. However, if different conditions are required, such as Mars pressure or a specific air composition (i.e., 95% of CO2 for Mars), the air must be evacuated to create a vacuum environment, followed by the introduction of moisture-free air with the desired composition. This method safeguards the MEC internal systems from potential issues related to the freezing of air moisture.\nIn the physical simulation task, temperatures are set for the MEC heaters to achieve sinusoidal soil sample temperatures resembling those encountered in a real diurnal cycle. The surface energy budget for each sample bin within the MEC can be described as a function of the radiative flux generated by the MEC heaters, as shown in the following equation :\n<!-- formula-not-decoded -->\nwhere Tsi represents the mean surface temperature of each soil sample and T heater corresponds to the temperature of the MEC heaters. Given that the MEC is a closed space with no air movement, natural convection is considered negligible. Within the MEC, the radiation term ϵσ B T 4 heater simulates the active Sun heating, which corresponds to the term ( 1 -A ) R sw of (4).\nOnce the simulated diurnal cycle is complete, the thermal inertia of each soil within the sample bins can be estimated by calculating corresponding the surface temperature difference, denoted as 1 Ts , and the difference in the net heat flux, 1 Gs , using (6). Next, these terms can be applied to (5) to determine the thermal inertia, I sin , in tiu.\n\nDetails the experimental methodology, outlining the three sequential tasks: soil preparation, pressure adjustment, and physical simulation, emphasizing the steps taken to ensure accurate thermal measurements and the subsequent calculation of thermal inertia.",
    "original_text": "The experimental methodology (see Fig. 3) consists of three major sequential tasks: the study of the physical characteristics of the soils and preparation of the MEC setup, the adjustment of the inner pressure according to the environment to be replicated, and the physical simulation within the MEC, followed by the estimation of the thermal inertias based on the resulting measurements.\nDuring the MEC preparation task, soils with previously studied physical properties (e.g., granularity) are selected and deposited into the sample bins. Subsequently, these sample bins are positioned on the plate. At this point, thermal insulation between the sample bins and the plate must be guaranteed to prevent potential IR reflections that could distort the measurements. The thermal camera is mounted on the viewport, and its housing is grounded to prevent electrostatic charges generated by the MEC pumps. The thermal camera is then calibrated to ensure accurate measurements of the sample bin surfaces. Finally, the MEC is sealed.\nIn the pressure adjustment task, procedures vary depending on the pressure and temperature range for the experiments. In the case of Earth pressure experiments with temperatures exceeding 0 ◦ C, the physical simulation can start. However, if different conditions are required, such as Mars pressure or a specific air composition (i.e., 95% of CO2 for Mars), the air must be evacuated to create a vacuum environment, followed by the introduction of moisture-free air with the desired composition. This method safeguards the MEC internal systems from potential issues related to the freezing of air moisture.\nIn the physical simulation task, temperatures are set for the MEC heaters to achieve sinusoidal soil sample temperatures resembling those encountered in a real diurnal cycle. The surface energy budget for each sample bin within the MEC can be described as a function of the radiative flux generated by the MEC heaters, as shown in the following equation :\n<!-- formula-not-decoded -->\nwhere Tsi represents the mean surface temperature of each soil sample and T heater corresponds to the temperature of the MEC heaters. Given that the MEC is a closed space with no air movement, natural convection is considered negligible. Within the MEC, the radiation term ϵσ B T 4 heater simulates the active Sun heating, which corresponds to the term ( 1 -A ) R sw of (4).\nOnce the simulated diurnal cycle is complete, the thermal inertia of each soil within the sample bins can be estimated by calculating corresponding the surface temperature difference, denoted as 1 Ts , and the difference in the net heat flux, 1 Gs , using (6). Next, these terms can be applied to (5) to determine the thermal inertia, I sin , in tiu.",
    "context": "Details the experimental methodology, outlining the three sequential tasks: soil preparation, pressure adjustment, and physical simulation, emphasizing the steps taken to ensure accurate thermal measurements and the subsequent calculation of thermal inertia.",
    "document": "Thermal_Imagery_for_Rover_Soil_Assessment_Using_a_Multipurpose_Environmental_Chamber_Under_Simulated_Mars_Conditions.pdf",
    "pages": [
      4
    ],
    "id": "772b5887affd6cdcce9878e5f8e352b7056064dd7306e8140d18df5220137629"
  },
  {
    "text": "The measurement system proposed in Section IV was employed to replicate the thermal behavior of four soil samples with known physical properties in a diurnal cycle, under conditions simulating both Earth and Martian environments. This section presents the integration of hardware components to evaluate the proposed system as well as the selection of soil samples.\n\nThis section details the hardware and soil selection for a system designed to simulate soil thermal behavior under Earth and Martian conditions, focusing on replicating a diurnal cycle and estimating soil thermal inertia. It outlines the integration of components, including a thermal camera, Mini PC, and customized viewport adapter, alongside the selection of four soil samples with varying granularities for comparative validation against Perseverance rover data.",
    "original_text": "The measurement system proposed in Section IV was employed to replicate the thermal behavior of four soil samples with known physical properties in a diurnal cycle, under conditions simulating both Earth and Martian environments. This section presents the integration of hardware components to evaluate the proposed system as well as the selection of soil samples.",
    "context": "This section details the hardware and soil selection for a system designed to simulate soil thermal behavior under Earth and Martian conditions, focusing on replicating a diurnal cycle and estimating soil thermal inertia. It outlines the integration of components, including a thermal camera, Mini PC, and customized viewport adapter, alongside the selection of four soil samples with varying granularities for comparative validation against Perseverance rover data.",
    "document": "Thermal_Imagery_for_Rover_Soil_Assessment_Using_a_Multipurpose_Environmental_Chamber_Under_Simulated_Mars_Conditions.pdf",
    "pages": [
      4
    ],
    "id": "faaa0fa79ac294640f6608b17992c0c3ba424492d214406e5aa253a1d8d1f747"
  },
  {
    "text": "The UMALASERLAB MEC (see Fig. 1) is a stainless-steel cylinder of 12 m of length and 1 . 6 m of diameter and viewports on the top and sides [17]. It is equipped with an inner spot-gridded thermal jacket or heater which contains a cooling fluid that can reach a temperature in the range of -72 ◦ C to 127 ◦ C, at a rate of 1 ◦ C / min. The air inside can be pumped\nFig. 3. Flowchart of the proposed experimental methodology.\nout of the chamber until a pressure of 10 -4 mbar is reached and can be replaced by CO2 to simulate the composition of the Mars atmosphere. The outer wall of the chamber has a sealed vacuum compartment to isolate the interior from external environmental conditions. Therefore, the effect of other conditions such as external temperature, pressure, or humidity is negligible. The MEC has ISO160 K-compliant glass viewports for VNIR measurements. Moreover, vacuumcompliant thermocouple gauges are set in the center of its core to measure the air temperature. Additionally, the MEC has a stainless steel plate on rails that allows a payload of up to 70 kg.\nThe thermal vision camera is a PI-640i by Optris based on uncooled microbolometer technology. It is a 320-g LWIR camera that works in the spectral range of 8-14 µ m, has a resolution of 640 × 480 pixels, and a germanium optic with an FOV of 60 ◦ × 45 ◦ . It can measure temperatures from -20 ◦ C to 900 ◦ C with a thermal sensitivity of 0 . 04 ◦ C. We selected this camera due to its high resolution and lightweight, making it suitable for mobile and aerial robots. However, this uncooled camera does not provide temperature measurements below -20 ◦ C, which limited the absolute minimum temperature to which soil samples could be subjected.\nThe thermal camera was connected to a Mini PC Intel NUC with an Intel Core i5 processor of 1.8 GHz and 16 GB of RAM running the software Optris PIX connect. We adjusted the focal length of its optic by using a warm body (i.e., a hand) placed on the plate as a reference. The thermal camera geometric and radiometric calibrations were provided by the manufacturer. The sample bins were placed on the plate perpendicular to the thermal camera at a distance of around 1 . 3 m to have an undistorted view of their surfaces. The plate surface was covered with insulating cardboard and a thick black fabric to avoid IR reflections from the steel.\nWe designed and developed an LWIR viewport adapter to replace one of the ISO160 K viewports (see Fig. 4) to make measurements from outside the MEC using the\nFig. 4. Customized viewport adapter. (a) 3-D model design (dimensions in millimeters): toroid frame (dark gray), germanium window (red), and clamping ring (light gray). (b) Image of the clamped adapter.\nthermal camera. This IR window adapter keeps the inside of the MEC sealed while letting the LWIR range from 8- to 14µ m radiation pass through. We chose an anti-reflection coated germanium circular optic model GEW16AR.20 by MKS Instruments due to its high mechanical resistance and its ability to withstand abrupt thermal changes. We selected a diameter of 74 . 9 and 5 . 0 mm of thickness in order to comply with the minimum thickness required to avoid reaching the germanium's fracture strength caused by the pressure differential between the environment and Martian pressure inside the MEC [40]. Furthermore, an aluminum toroid frame with a screwed clamping ring was crafted to fix the germanium window into the MEC upper ISO160 K-compliant viewports.\n\nThis section details the construction and components of the MEC, including its physical dimensions, internal heating/cooling system, and specialized viewport adapter for external thermal camera measurements.",
    "original_text": "The UMALASERLAB MEC (see Fig. 1) is a stainless-steel cylinder of 12 m of length and 1 . 6 m of diameter and viewports on the top and sides [17]. It is equipped with an inner spot-gridded thermal jacket or heater which contains a cooling fluid that can reach a temperature in the range of -72 ◦ C to 127 ◦ C, at a rate of 1 ◦ C / min. The air inside can be pumped\nFig. 3. Flowchart of the proposed experimental methodology.\nout of the chamber until a pressure of 10 -4 mbar is reached and can be replaced by CO2 to simulate the composition of the Mars atmosphere. The outer wall of the chamber has a sealed vacuum compartment to isolate the interior from external environmental conditions. Therefore, the effect of other conditions such as external temperature, pressure, or humidity is negligible. The MEC has ISO160 K-compliant glass viewports for VNIR measurements. Moreover, vacuumcompliant thermocouple gauges are set in the center of its core to measure the air temperature. Additionally, the MEC has a stainless steel plate on rails that allows a payload of up to 70 kg.\nThe thermal vision camera is a PI-640i by Optris based on uncooled microbolometer technology. It is a 320-g LWIR camera that works in the spectral range of 8-14 µ m, has a resolution of 640 × 480 pixels, and a germanium optic with an FOV of 60 ◦ × 45 ◦ . It can measure temperatures from -20 ◦ C to 900 ◦ C with a thermal sensitivity of 0 . 04 ◦ C. We selected this camera due to its high resolution and lightweight, making it suitable for mobile and aerial robots. However, this uncooled camera does not provide temperature measurements below -20 ◦ C, which limited the absolute minimum temperature to which soil samples could be subjected.\nThe thermal camera was connected to a Mini PC Intel NUC with an Intel Core i5 processor of 1.8 GHz and 16 GB of RAM running the software Optris PIX connect. We adjusted the focal length of its optic by using a warm body (i.e., a hand) placed on the plate as a reference. The thermal camera geometric and radiometric calibrations were provided by the manufacturer. The sample bins were placed on the plate perpendicular to the thermal camera at a distance of around 1 . 3 m to have an undistorted view of their surfaces. The plate surface was covered with insulating cardboard and a thick black fabric to avoid IR reflections from the steel.\nWe designed and developed an LWIR viewport adapter to replace one of the ISO160 K viewports (see Fig. 4) to make measurements from outside the MEC using the\nFig. 4. Customized viewport adapter. (a) 3-D model design (dimensions in millimeters): toroid frame (dark gray), germanium window (red), and clamping ring (light gray). (b) Image of the clamped adapter.\nthermal camera. This IR window adapter keeps the inside of the MEC sealed while letting the LWIR range from 8- to 14µ m radiation pass through. We chose an anti-reflection coated germanium circular optic model GEW16AR.20 by MKS Instruments due to its high mechanical resistance and its ability to withstand abrupt thermal changes. We selected a diameter of 74 . 9 and 5 . 0 mm of thickness in order to comply with the minimum thickness required to avoid reaching the germanium's fracture strength caused by the pressure differential between the environment and Martian pressure inside the MEC [40]. Furthermore, an aluminum toroid frame with a screwed clamping ring was crafted to fix the germanium window into the MEC upper ISO160 K-compliant viewports.",
    "context": "This section details the construction and components of the MEC, including its physical dimensions, internal heating/cooling system, and specialized viewport adapter for external thermal camera measurements.",
    "document": "Thermal_Imagery_for_Rover_Soil_Assessment_Using_a_Multipurpose_Environmental_Chamber_Under_Simulated_Mars_Conditions.pdf",
    "pages": [
      4,
      5
    ],
    "id": "46321ea342e929a88a81567d958c1adf87a3d74c071536ba83cf7a1db44071db"
  },
  {
    "text": "Four sample bins with soils of different characteristics were selected for the experiments (see Fig. 5). Table I summarizes characteristics, sorted by granularity. Three of the bins contained granular soils and one contained an example of bedrock. We plotted a granularity chart of the granular soils (see Fig. 6) by passing them through sieves with grids of different sizes. In terms of homogeneity, soil C is the most homogeneous,\nFig. 5. Sample bins of soils of different granularities introduced into the MEC.\nFig. 6. Granularity chart of the soils.\nTABLE I SAMPLE BINS CHARACTERISTICS\nas more than 90% of its grains have a diameter of 0 . 7-1 mm. It is followed by soil B, whose grains are mostly concentrated on the size of less than 2 . 0 mm. Finally, soil A is classified as the most heterogeneous, as it consists of a mixture of several grain sizes.\n\nDescribes the selection and characterization of four soil samples – three granular and one bedrock – for the MEC experiments, highlighting the varying granularity and homogeneity of the soils, with soil C being the most homogeneous and soil A the most heterogeneous.",
    "original_text": "Four sample bins with soils of different characteristics were selected for the experiments (see Fig. 5). Table I summarizes characteristics, sorted by granularity. Three of the bins contained granular soils and one contained an example of bedrock. We plotted a granularity chart of the granular soils (see Fig. 6) by passing them through sieves with grids of different sizes. In terms of homogeneity, soil C is the most homogeneous,\nFig. 5. Sample bins of soils of different granularities introduced into the MEC.\nFig. 6. Granularity chart of the soils.\nTABLE I SAMPLE BINS CHARACTERISTICS\nas more than 90% of its grains have a diameter of 0 . 7-1 mm. It is followed by soil B, whose grains are mostly concentrated on the size of less than 2 . 0 mm. Finally, soil A is classified as the most heterogeneous, as it consists of a mixture of several grain sizes.",
    "context": "Describes the selection and characterization of four soil samples – three granular and one bedrock – for the MEC experiments, highlighting the varying granularity and homogeneity of the soils, with soil C being the most homogeneous and soil A the most heterogeneous.",
    "document": "Thermal_Imagery_for_Rover_Soil_Assessment_Using_a_Multipurpose_Environmental_Chamber_Under_Simulated_Mars_Conditions.pdf",
    "pages": [
      5,
      6
    ],
    "id": "dad45fc9bb263378fd2195fcd3a7b6ec8e29d8254c2211d50049183318032414"
  },
  {
    "text": "In this section, we apply the proposed system configuration and methodology to provide a diurnal cycle quantitative soil analysis from high-resolution thermal images captured under Martian and Earth conditions (i.e., pressure and atmospheric composition). In particular, the captured thermal images are processed to analyze the soils thermal behavior and to estimate their thermal inertia. For validation purposes, the thermal\nTABLE II DESCRIPTION OF THE MEC EXPERIMENTS\ninertia values acquired in the experiments conducted under Martian conditions are compared with the estimations of thermal inertia derived from the on-site surface temperature measurements taken by the Perseverance rover.\n\nThis section details the application of the proposed thermal analysis system to quantify soil thermal behavior under both Martian and Earth conditions. The core objective is to validate the system’s thermal inertia estimations against data collected by the Perseverance rover, providing a crucial step in assessing the system’s reliability and accuracy for future planetary exploration.",
    "original_text": "In this section, we apply the proposed system configuration and methodology to provide a diurnal cycle quantitative soil analysis from high-resolution thermal images captured under Martian and Earth conditions (i.e., pressure and atmospheric composition). In particular, the captured thermal images are processed to analyze the soils thermal behavior and to estimate their thermal inertia. For validation purposes, the thermal\nTABLE II DESCRIPTION OF THE MEC EXPERIMENTS\ninertia values acquired in the experiments conducted under Martian conditions are compared with the estimations of thermal inertia derived from the on-site surface temperature measurements taken by the Perseverance rover.",
    "context": "This section details the application of the proposed thermal analysis system to quantify soil thermal behavior under both Martian and Earth conditions. The core objective is to validate the system’s thermal inertia estimations against data collected by the Perseverance rover, providing a crucial step in assessing the system’s reliability and accuracy for future planetary exploration.",
    "document": "Thermal_Imagery_for_Rover_Soil_Assessment_Using_a_Multipurpose_Environmental_Chamber_Under_Simulated_Mars_Conditions.pdf",
    "pages": [
      6
    ],
    "id": "00ea0a1a0712d6b0933195d0bcc2ef523471cea60d23d75869c666186fe38a4f"
  },
  {
    "text": "Two sets of experiments, namely, pair-1 and pair-2, were conducted within the MEC using the soils specified in Table I to ensure redundancy in measurements. These experiment pairs encompassed conditions representing Earth-standard pressure (1000 mbar) (1 and 3) as well as Mars-like pressure (8 mbar) (2 and 4). Table II provides an overview of the key characteristics of each experiment. Due to limitations in MEC connectivity, subsurface temperature data could only be equatorial for one of the soils on each experiment. For pair-1 and pair-2, the subsurface thermocouple gauge was positioned at a depth of 3 cm in soils A and C, respectively.\nDuring the Mars-like experiments, the MEC was filled with air matching Mars' carbon dioxide (CO2) atmospheric composition (i.e., 95%). Environmental measurements taken near the equatorial region on Mars by the Mars Science Laboratory (MSL) indicated mean daily air temperatures around -50 ◦ C with approximate amplitudes of 60 ◦ C [41]. Thus, sinusoidal temperatures of similar amplitude were simulated in diurnal cycles of experimental actuation period Pe by manual input of constant heating and cooling setpoints for MEC actuation.\nSoil surface temperatures were measured by means of the Optris PI-640i thermal camera. The measurements were made assuming no prior knowledge of the soils, as would be the case in actual rover soil assessment implementations. Thus, emissivity ( ϵ ) was considered to be unitary and the albedo ( A ) to be zero for all the soils, according to Kirchhoff's law: 1 = A + ϵ [42]. Polygonal areas delimiting each soil were defined in the acquired thermal images, with the pixels containing the thermocouple gauge being excluded to avoid affecting temperature measurements.\nFigs. 7-10 show the diurnal cycle temperatures for all soil types from Experiments 1-4, respectively. Besides, Figs. 11 and 12 present surface and subsurface temperature readings for soil A (Experiments 1 and 2) and soil C (Experiments 3 and 4), respectively. All the figures display the heater, setpoint, and air temperatures. The transient phase is considered to conclude once an inflection point is reached in the rising heater temperature response.\nTable III provides the mean soil surface temperatures for the pixels within the corresponding polygonal area, along with the standard deviations for the four experiments. In this table, 1 Ts = T max -T init , where T max represents the maximum\nFig. 7. Diurnal cycle temperatures for Experiment 1 at Earth's pressure ( p = 1000 mbar). (a) Mean temperature. (b) Standard deviation.\nFig. 8. Diurnal cycle temperatures for Experiment 2 at Martian pressure ( p = 8 mbar). (a) Mean temperature. (b) Standard deviation.\nFig. 9. Diurnal cycle temperatures for Experiment 3 at Earth's pressure ( p = 1000 mbar). (a) Mean temperature. (b) Standard deviation.\nmean temperature and T init is the mean temperature at the beginning of the actuation. T tran denotes the standard deviation temperature for each soil when the actuation transient phase concludes. The net heat fluxes were calculated by applying the surface energy budget equation for each sample bin within the MEC, as described in (6). This computation used the mean temperatures of the soils and the MEC heater temperatures recorded during the experiments. Additionally, the increase in net heat flux during the experiments was determined as 1 Gs = G max -G init, where G max represents the maximum net heat flux and G init is the net heat flux at the beginning of the actuation. The thermal inertia estimates in (5), denoted as I sin , were calculated based on the values of 1 Ts , 1 Gs , and Pe obtained during the experiments.\n\nDetails of two sets of experiments were conducted within the MEC, comparing Earth’s standard pressure (1000 mbar) with Mars-like pressure (8 mbar). Subsurface temperature data was limited to equatorial soils for each experiment pair. The experiments involved simulating Martian atmospheric composition (95% CO2) and sinusoidal temperature cycles. Surface temperatures were measured using a thermal camera, assuming emissivity and albedo of zero for the soils. Data analysis included calculating thermal inertia estimates based on temperature and heat flux measurements.",
    "original_text": "Two sets of experiments, namely, pair-1 and pair-2, were conducted within the MEC using the soils specified in Table I to ensure redundancy in measurements. These experiment pairs encompassed conditions representing Earth-standard pressure (1000 mbar) (1 and 3) as well as Mars-like pressure (8 mbar) (2 and 4). Table II provides an overview of the key characteristics of each experiment. Due to limitations in MEC connectivity, subsurface temperature data could only be equatorial for one of the soils on each experiment. For pair-1 and pair-2, the subsurface thermocouple gauge was positioned at a depth of 3 cm in soils A and C, respectively.\nDuring the Mars-like experiments, the MEC was filled with air matching Mars' carbon dioxide (CO2) atmospheric composition (i.e., 95%). Environmental measurements taken near the equatorial region on Mars by the Mars Science Laboratory (MSL) indicated mean daily air temperatures around -50 ◦ C with approximate amplitudes of 60 ◦ C [41]. Thus, sinusoidal temperatures of similar amplitude were simulated in diurnal cycles of experimental actuation period Pe by manual input of constant heating and cooling setpoints for MEC actuation.\nSoil surface temperatures were measured by means of the Optris PI-640i thermal camera. The measurements were made assuming no prior knowledge of the soils, as would be the case in actual rover soil assessment implementations. Thus, emissivity ( ϵ ) was considered to be unitary and the albedo ( A ) to be zero for all the soils, according to Kirchhoff's law: 1 = A + ϵ [42]. Polygonal areas delimiting each soil were defined in the acquired thermal images, with the pixels containing the thermocouple gauge being excluded to avoid affecting temperature measurements.\nFigs. 7-10 show the diurnal cycle temperatures for all soil types from Experiments 1-4, respectively. Besides, Figs. 11 and 12 present surface and subsurface temperature readings for soil A (Experiments 1 and 2) and soil C (Experiments 3 and 4), respectively. All the figures display the heater, setpoint, and air temperatures. The transient phase is considered to conclude once an inflection point is reached in the rising heater temperature response.\nTable III provides the mean soil surface temperatures for the pixels within the corresponding polygonal area, along with the standard deviations for the four experiments. In this table, 1 Ts = T max -T init , where T max represents the maximum\nFig. 7. Diurnal cycle temperatures for Experiment 1 at Earth's pressure ( p = 1000 mbar). (a) Mean temperature. (b) Standard deviation.\nFig. 8. Diurnal cycle temperatures for Experiment 2 at Martian pressure ( p = 8 mbar). (a) Mean temperature. (b) Standard deviation.\nFig. 9. Diurnal cycle temperatures for Experiment 3 at Earth's pressure ( p = 1000 mbar). (a) Mean temperature. (b) Standard deviation.\nmean temperature and T init is the mean temperature at the beginning of the actuation. T tran denotes the standard deviation temperature for each soil when the actuation transient phase concludes. The net heat fluxes were calculated by applying the surface energy budget equation for each sample bin within the MEC, as described in (6). This computation used the mean temperatures of the soils and the MEC heater temperatures recorded during the experiments. Additionally, the increase in net heat flux during the experiments was determined as 1 Gs = G max -G init, where G max represents the maximum net heat flux and G init is the net heat flux at the beginning of the actuation. The thermal inertia estimates in (5), denoted as I sin , were calculated based on the values of 1 Ts , 1 Gs , and Pe obtained during the experiments.",
    "context": "Details of two sets of experiments were conducted within the MEC, comparing Earth’s standard pressure (1000 mbar) with Mars-like pressure (8 mbar). Subsurface temperature data was limited to equatorial soils for each experiment pair. The experiments involved simulating Martian atmospheric composition (95% CO2) and sinusoidal temperature cycles. Surface temperatures were measured using a thermal camera, assuming emissivity and albedo of zero for the soils. Data analysis included calculating thermal inertia estimates based on temperature and heat flux measurements.",
    "document": "Thermal_Imagery_for_Rover_Soil_Assessment_Using_a_Multipurpose_Environmental_Chamber_Under_Simulated_Mars_Conditions.pdf",
    "pages": [
      6,
      7
    ],
    "id": "34e25ba9a9053bd96db25667aa554303aa9904fc89a34000cc620921f322b56b"
  },
  {
    "text": "This section analyzes thermal behavior measurements in terms of the surface mean temperatures, the soil standard deviation curves, the subsurface temperatures, and thermal inertia.\nFirst, we compare the increment in the surface mean temperature, denoted as 1 Ts , of pair-1 [Figs. 7(a) and 8(a)]. These graphs show that only the bedrock is distinguishable at Earth's pressure, as all the granular soils exhibit similar values. In contrast, at Martian pressure, three prominent groups can be observed; from highest to lowest: soil C; soils A and B; and bedrock. Furthermore, both graphs reveal a slight temporal delay in the bedrock temperature curve compared to the other soils. The same analysis can be applied to the graphs of pair-2 [Figs. 9(a) and 10(a)].\nFig. 10. Diurnal cycle temperatures for Experiment 4 at Martian pressure ( p = 8 mbar). (a) Mean temperature. (b) Standard deviation.\nFig. 11. Diurnal cycle surface and subsurface temperatures of soil A. (a) Earth's pressure (Experiment 1). (b) Martian pressure (Experiment 2).\nFig. 12. Diurnal cycle surface and subsurface temperatures of soil C. (a) Earth's pressure (Experiment 3). (b) Martian pressure (Experiment 4).\nSecond, we compare the values of standard deviation at the end of the transient phase ( T tran ). Figs. 7(b) and 8(b) offer results for pair-1. At both Earth's and Mars' pressures, T tran renders distinct values for each soil type, which shows that this parameter could be indicative of homogeneity: heterogeneous soils (e.g., soil A) exhibit higher T tran values than homogeneous soils (e.g., soil C). This behavior becomes more pronounced under Martian conditions. A similar analysis can be applied to the graphs of pair-2 [Figs. 9(b) and 10(b)].\nconstitutes an increase of 26.72%. Regarding soil C in pair-2 [Fig. 12(a) and (b)], under Earth conditions, the maximum difference is 14 . 4 ◦ C, whereas under Martian-like conditions, it is 24 . 4 ◦ C, constituting a 69.44% increase. In general, the analysis indicates that this difference increases when the pressure decreases, as it gets more difficult for the heat to be transmitted vertically.\nNext, we compare the mean surface temperatures with the thermocouples subsurface temperatures of soil A in pair-1 [Fig. 11(a) and (b)]. In this case, the maximum difference between the surface and subsurface temperature is 11 . 6 ◦ C and 14 . 7 ◦ C at Earth's and Mars' pressure, respectively; which\nRegarding the sinusoidal estimations, I sin in Table III, it is observed that thermal inertia increases when pressure decreases. Soils composed of larger particles, e.g., bedrock, exhibit higher thermal inertia; whereas soils with smaller particles, e.g., soil C, show lower thermal inertia. According to the MEC experimental data, at Earth's pressure, the relative difference between the soil with the highest thermal inertia and\nFig. 13. Examples of Perseverance's Navcam dataset images taken at Sols (Martian days) 193, 237, and 118, respectively. (a) Bedrock. (b) Intermediate soil. (c) Sandy soil.\nTABLE III SURFACE TEMPERATURES AND HEAT FLUXES OF THE SAMPLED SOILS AT EARTH'S AND MARS' PRESSURES AND ESTIMATED THERMAL INERTIA\nthe one with the lowest thermal inertia is 4.20%. However, at Martian pressure, this difference increases significantly to 42.84%. This observation indicates that soils can be better assessed at Martian pressure compared to Earth's pressure.\n\nAnalysis of surface temperature trends and standard deviation curves reveals distinct groupings of soils under Martian pressure, with bedrock exhibiting a noticeable temperature delay compared to granular soils. Furthermore, standard deviation values demonstrate a correlation with soil homogeneity, with heterogeneous soils showing higher deviations than homogeneous ones, particularly under Martian conditions.",
    "original_text": "This section analyzes thermal behavior measurements in terms of the surface mean temperatures, the soil standard deviation curves, the subsurface temperatures, and thermal inertia.\nFirst, we compare the increment in the surface mean temperature, denoted as 1 Ts , of pair-1 [Figs. 7(a) and 8(a)]. These graphs show that only the bedrock is distinguishable at Earth's pressure, as all the granular soils exhibit similar values. In contrast, at Martian pressure, three prominent groups can be observed; from highest to lowest: soil C; soils A and B; and bedrock. Furthermore, both graphs reveal a slight temporal delay in the bedrock temperature curve compared to the other soils. The same analysis can be applied to the graphs of pair-2 [Figs. 9(a) and 10(a)].\nFig. 10. Diurnal cycle temperatures for Experiment 4 at Martian pressure ( p = 8 mbar). (a) Mean temperature. (b) Standard deviation.\nFig. 11. Diurnal cycle surface and subsurface temperatures of soil A. (a) Earth's pressure (Experiment 1). (b) Martian pressure (Experiment 2).\nFig. 12. Diurnal cycle surface and subsurface temperatures of soil C. (a) Earth's pressure (Experiment 3). (b) Martian pressure (Experiment 4).\nSecond, we compare the values of standard deviation at the end of the transient phase ( T tran ). Figs. 7(b) and 8(b) offer results for pair-1. At both Earth's and Mars' pressures, T tran renders distinct values for each soil type, which shows that this parameter could be indicative of homogeneity: heterogeneous soils (e.g., soil A) exhibit higher T tran values than homogeneous soils (e.g., soil C). This behavior becomes more pronounced under Martian conditions. A similar analysis can be applied to the graphs of pair-2 [Figs. 9(b) and 10(b)].\nconstitutes an increase of 26.72%. Regarding soil C in pair-2 [Fig. 12(a) and (b)], under Earth conditions, the maximum difference is 14 . 4 ◦ C, whereas under Martian-like conditions, it is 24 . 4 ◦ C, constituting a 69.44% increase. In general, the analysis indicates that this difference increases when the pressure decreases, as it gets more difficult for the heat to be transmitted vertically.\nNext, we compare the mean surface temperatures with the thermocouples subsurface temperatures of soil A in pair-1 [Fig. 11(a) and (b)]. In this case, the maximum difference between the surface and subsurface temperature is 11 . 6 ◦ C and 14 . 7 ◦ C at Earth's and Mars' pressure, respectively; which\nRegarding the sinusoidal estimations, I sin in Table III, it is observed that thermal inertia increases when pressure decreases. Soils composed of larger particles, e.g., bedrock, exhibit higher thermal inertia; whereas soils with smaller particles, e.g., soil C, show lower thermal inertia. According to the MEC experimental data, at Earth's pressure, the relative difference between the soil with the highest thermal inertia and\nFig. 13. Examples of Perseverance's Navcam dataset images taken at Sols (Martian days) 193, 237, and 118, respectively. (a) Bedrock. (b) Intermediate soil. (c) Sandy soil.\nTABLE III SURFACE TEMPERATURES AND HEAT FLUXES OF THE SAMPLED SOILS AT EARTH'S AND MARS' PRESSURES AND ESTIMATED THERMAL INERTIA\nthe one with the lowest thermal inertia is 4.20%. However, at Martian pressure, this difference increases significantly to 42.84%. This observation indicates that soils can be better assessed at Martian pressure compared to Earth's pressure.",
    "context": "Analysis of surface temperature trends and standard deviation curves reveals distinct groupings of soils under Martian pressure, with bedrock exhibiting a noticeable temperature delay compared to granular soils. Furthermore, standard deviation values demonstrate a correlation with soil homogeneity, with heterogeneous soils showing higher deviations than homogeneous ones, particularly under Martian conditions.",
    "document": "Thermal_Imagery_for_Rover_Soil_Assessment_Using_a_Multipurpose_Environmental_Chamber_Under_Simulated_Mars_Conditions.pdf",
    "pages": [
      8,
      9,
      7
    ],
    "id": "489d3d380b8b559abbf63281ba1cf35ab2706a67ec9a2ceb6f3c443f4805d044"
  },
  {
    "text": "Even if no actual Martian IR images are available for validation, the consistency of experimental thermal inertia estimations is checked with respect to published analyses of actual rover missions as well as thermopile temperature data from available datasets.\nThe estimated thermal inertia values under Martian pressure of the bedrock sample bin are consistent with the on-site thermal inertia obtained by Curiosity for bedrock-dominated surfaces ( ∼ 350-550 tiu) [26]. Regarding soil C, its estimated thermal inertia closely aligns with surfaces characterized by an average particle size of approximately 1 mm, as indicated by data from Curiosity's observations ( ∼ 265-375 tiu) [25]. On the other hand, soils A and B exhibit similar thermal inertia values, despite having different particle sizes. This is consistent with findings indicating that soils with particle sizes\nFig. 14. Diurnal Martian surface temperature recorded by Perseverance's TIRS instrument.\nranging from 1 mm to a few centimeters maintain constant thermal inertia of about 420 tiu [39].\nTemperature data from three distinct types of soils, as recorded by the Perseverance rover's TIRS instrument, were extracted from a publicly available dataset 1 provided by NASA. The selection of these soils was based on their visual resemblance to the ones used in our experiments (refer to Fig. 13): Mars' bedrock corresponds to the bedrock sample bin, Mars' intermediate soil is akin to soils A and B, and Mars' sandy soil corresponds to soil C. For the sake of comparison, we selected Perseverance data where the TIRS was pointing at the terrains in the figure for a whole diurnal cycle. The surface temperatures of the three soils during a Martian diurnal cycle are shown in Fig. 14.\nIn Table IV, we calculated the surface net heat flux values, 1 G mars, as the difference between the maximum and minimum heat flux during that Martian day, using the simplified Martian surface model equation (4) and Mars downwelling SW radiation, R sw, extracted from [12]. Similarly, the increment in surface temperature, 1 T mars, is the difference between the maximum and minimum temperature during that Martian day.\nTABLE IV RELEVANT SURFACE VALUES OF MARTIAN SOILS AND COMPARISON OF ESTIMATED THERMAL INERTIAS\n\n, 1 = . , 2 = 0/ .2. , 3 = . , 4 = 490. , 5 = 421. Sandy, 1 = 460. Sandy, 2 = 76.8. Sandy, 3 = 224. Sandy, 4 = 348. Sandy, 5 = 375\n1\nFig. 15. Example of a thermal image of the sample bins during the experiments.\nThe thermal inertia estimations based on Perseverance's data, I mars , were calculated using (5). On the other hand, the MEC-based thermal inertia estimations, I mec, were computed as the mean values of the thermal inertia estimations under Mars' pressure of the equivalent soils in Table III.\nSimilar to the MEC experiments on Martian pressure, three prominent groups can be observed in Table IV based on the surface temperature; from highest to lowest: sandy soil, intermediate soil, and bedrock. Additionally, when comparing the thermal inertia estimations based on Perseverance's data with the MEC-based thermal inertia estimations, a relative error of 6.79%, 7.68%, and 7.76% is observed for the sandy, intermediate, and bedrock soils, respectively.\n\nValidates experimental thermal inertia estimations against published rover data and thermopile temperature measurements, demonstrating consistency with on-site estimations for bedrock (350-550 tiu), soil C (265-375 tiu), and soils A/B (420 tiu).",
    "original_text": "Even if no actual Martian IR images are available for validation, the consistency of experimental thermal inertia estimations is checked with respect to published analyses of actual rover missions as well as thermopile temperature data from available datasets.\nThe estimated thermal inertia values under Martian pressure of the bedrock sample bin are consistent with the on-site thermal inertia obtained by Curiosity for bedrock-dominated surfaces ( ∼ 350-550 tiu) [26]. Regarding soil C, its estimated thermal inertia closely aligns with surfaces characterized by an average particle size of approximately 1 mm, as indicated by data from Curiosity's observations ( ∼ 265-375 tiu) [25]. On the other hand, soils A and B exhibit similar thermal inertia values, despite having different particle sizes. This is consistent with findings indicating that soils with particle sizes\nFig. 14. Diurnal Martian surface temperature recorded by Perseverance's TIRS instrument.\nranging from 1 mm to a few centimeters maintain constant thermal inertia of about 420 tiu [39].\nTemperature data from three distinct types of soils, as recorded by the Perseverance rover's TIRS instrument, were extracted from a publicly available dataset 1 provided by NASA. The selection of these soils was based on their visual resemblance to the ones used in our experiments (refer to Fig. 13): Mars' bedrock corresponds to the bedrock sample bin, Mars' intermediate soil is akin to soils A and B, and Mars' sandy soil corresponds to soil C. For the sake of comparison, we selected Perseverance data where the TIRS was pointing at the terrains in the figure for a whole diurnal cycle. The surface temperatures of the three soils during a Martian diurnal cycle are shown in Fig. 14.\nIn Table IV, we calculated the surface net heat flux values, 1 G mars, as the difference between the maximum and minimum heat flux during that Martian day, using the simplified Martian surface model equation (4) and Mars downwelling SW radiation, R sw, extracted from [12]. Similarly, the increment in surface temperature, 1 T mars, is the difference between the maximum and minimum temperature during that Martian day.\nTABLE IV RELEVANT SURFACE VALUES OF MARTIAN SOILS AND COMPARISON OF ESTIMATED THERMAL INERTIAS\n\n, 1 = . , 2 = 0/ .2. , 3 = . , 4 = 490. , 5 = 421. Sandy, 1 = 460. Sandy, 2 = 76.8. Sandy, 3 = 224. Sandy, 4 = 348. Sandy, 5 = 375\n1\nFig. 15. Example of a thermal image of the sample bins during the experiments.\nThe thermal inertia estimations based on Perseverance's data, I mars , were calculated using (5). On the other hand, the MEC-based thermal inertia estimations, I mec, were computed as the mean values of the thermal inertia estimations under Mars' pressure of the equivalent soils in Table III.\nSimilar to the MEC experiments on Martian pressure, three prominent groups can be observed in Table IV based on the surface temperature; from highest to lowest: sandy soil, intermediate soil, and bedrock. Additionally, when comparing the thermal inertia estimations based on Perseverance's data with the MEC-based thermal inertia estimations, a relative error of 6.79%, 7.68%, and 7.76% is observed for the sandy, intermediate, and bedrock soils, respectively.",
    "context": "Validates experimental thermal inertia estimations against published rover data and thermopile temperature measurements, demonstrating consistency with on-site estimations for bedrock (350-550 tiu), soil C (265-375 tiu), and soils A/B (420 tiu).",
    "document": "Thermal_Imagery_for_Rover_Soil_Assessment_Using_a_Multipurpose_Environmental_Chamber_Under_Simulated_Mars_Conditions.pdf",
    "pages": [
      9,
      10
    ],
    "id": "ac40a6887750b81a3f25711dc8a00c0b72237a82aeea4683d2d029b9299f3951"
  },
  {
    "text": "During the experiments, we collected a total of 9225 radiometric images, which were saved as plain text 640 × 480 matrices with each cell containing the temperature in degree Celsius. Snapshots of the thermal images were processed to facilitate direct viewing. An example of one of these snapshots is shown in Fig. 15. Finally, spreadsheets were generated with the heaters, air, and subsurface temperatures recorded by the thermocouples. To the authors' knowledge, no similar dataset exists in the literature. A public dataset with the recorded data can be found at Zenodo. 2\n\nCollection and preservation of 9225 radiometric images, saved as 640x480 text matrices with temperature data, alongside spreadsheets of heater, air, and subsurface temperatures – this data is publicly available on Zenodo.",
    "original_text": "During the experiments, we collected a total of 9225 radiometric images, which were saved as plain text 640 × 480 matrices with each cell containing the temperature in degree Celsius. Snapshots of the thermal images were processed to facilitate direct viewing. An example of one of these snapshots is shown in Fig. 15. Finally, spreadsheets were generated with the heaters, air, and subsurface temperatures recorded by the thermocouples. To the authors' knowledge, no similar dataset exists in the literature. A public dataset with the recorded data can be found at Zenodo. 2",
    "context": "Collection and preservation of 9225 radiometric images, saved as 640x480 text matrices with temperature data, alongside spreadsheets of heater, air, and subsurface temperatures – this data is publicly available on Zenodo.",
    "document": "Thermal_Imagery_for_Rover_Soil_Assessment_Using_a_Multipurpose_Environmental_Chamber_Under_Simulated_Mars_Conditions.pdf",
    "pages": [
      10
    ],
    "id": "ccdba912954638af6bdb4de197ce9f123a3b187713e0363ea03dbb69e024a070"
  },
  {
    "text": "This article has proposed a measurement system configuration and experimental methodology to capture thermal images using MECs to replicate the temperature, pressure, and atmospheric conditions of Martian diurnal cycles. Furthermore, the proposed system has been applied to the UMALASERLAB Mars Environment Chamber to collect a total of 9225 radiometric images and environmental data of four soil types of known physical characteristics for diurnal cycles with Martian and Earth pressures. This unique set of measurements, which has been made publicly available, has allowed us to perform a novel qualitative analysis of soil thermal inertia.\nThe proposed approach is an innovative solution to address the lack of actual IR images from current Mars rovers, especially in the context of the recent groundbreaking advances in machine learning that demand large amounts of measurements for safe planetary rover navigation based on soil assessment, classification, segmentation, and image understanding. Furthermore, the ability of the system to perform experiments under controlled Earth's and Mars' conditions can contribute to the characterization of future robotic experiments on Earth that aim to replicate planetary conditions accurately.\nThe possibility to compare the same soil types under controlled Earth and Martian conditions has clearly revealed that the relative difference between thermal inertia extremes (i.e., the distinctiveness of the estimation) of analyzed soils is up to ten times larger in Mars, which corroborates that thermal inertia could offer better soil assessment performance under lower pressures. Furthermore, even if no actual Martian IR images are available for validation, the thermal inertia values obtained in the experiments are consistent with real on-site estimations performed by rovers on Mars.\nIn summary, the major advantages of the proposed approach with respect to the state-of-the-art methods are as follows.\n- 1) High-resolution 2-D thermal imaging allows studying soils with heterogeneous characteristics, which is not possible with current thermopile data from Mars rovers.\n- 2) MECs can provide high-fidelity data in contrast to synthetic IR images produced from RGB imagery.\n- 3) Experiments can be conducted for soils of known physical characteristics under different controlled conditions (e.g., Earth versus Mars, and diurnal cycles), which is not possible with available thermopile data from Mars rovers, where ground-truth soil characteristics are not available.\nFuture work may be focused on enhancing the proposed measurement system by using cooled thermal cameras to decrease the minimum temperature that can be remotely measured. Furthermore, the development of control systems capable of producing adaptable sinusoidal temperature profiles in the MEC could help to increase the realism of the physical simulations.\n\nSummarizes the experimental methodology and its key advantages, highlighting the system's ability to replicate Martian conditions and provide high-fidelity thermal inertia data for soil analysis, ultimately addressing the limitations of existing Martian rover data.",
    "original_text": "This article has proposed a measurement system configuration and experimental methodology to capture thermal images using MECs to replicate the temperature, pressure, and atmospheric conditions of Martian diurnal cycles. Furthermore, the proposed system has been applied to the UMALASERLAB Mars Environment Chamber to collect a total of 9225 radiometric images and environmental data of four soil types of known physical characteristics for diurnal cycles with Martian and Earth pressures. This unique set of measurements, which has been made publicly available, has allowed us to perform a novel qualitative analysis of soil thermal inertia.\nThe proposed approach is an innovative solution to address the lack of actual IR images from current Mars rovers, especially in the context of the recent groundbreaking advances in machine learning that demand large amounts of measurements for safe planetary rover navigation based on soil assessment, classification, segmentation, and image understanding. Furthermore, the ability of the system to perform experiments under controlled Earth's and Mars' conditions can contribute to the characterization of future robotic experiments on Earth that aim to replicate planetary conditions accurately.\nThe possibility to compare the same soil types under controlled Earth and Martian conditions has clearly revealed that the relative difference between thermal inertia extremes (i.e., the distinctiveness of the estimation) of analyzed soils is up to ten times larger in Mars, which corroborates that thermal inertia could offer better soil assessment performance under lower pressures. Furthermore, even if no actual Martian IR images are available for validation, the thermal inertia values obtained in the experiments are consistent with real on-site estimations performed by rovers on Mars.\nIn summary, the major advantages of the proposed approach with respect to the state-of-the-art methods are as follows.\n- 1) High-resolution 2-D thermal imaging allows studying soils with heterogeneous characteristics, which is not possible with current thermopile data from Mars rovers.\n- 2) MECs can provide high-fidelity data in contrast to synthetic IR images produced from RGB imagery.\n- 3) Experiments can be conducted for soils of known physical characteristics under different controlled conditions (e.g., Earth versus Mars, and diurnal cycles), which is not possible with available thermopile data from Mars rovers, where ground-truth soil characteristics are not available.\nFuture work may be focused on enhancing the proposed measurement system by using cooled thermal cameras to decrease the minimum temperature that can be remotely measured. Furthermore, the development of control systems capable of producing adaptable sinusoidal temperature profiles in the MEC could help to increase the realism of the physical simulations.",
    "context": "Summarizes the experimental methodology and its key advantages, highlighting the system's ability to replicate Martian conditions and provide high-fidelity thermal inertia data for soil analysis, ultimately addressing the limitations of existing Martian rover data.",
    "document": "Thermal_Imagery_for_Rover_Soil_Assessment_Using_a_Multipurpose_Environmental_Chamber_Under_Simulated_Mars_Conditions.pdf",
    "pages": [
      10
    ],
    "id": "37c744196dba9ef6291037693e96d45e955619fbd279a3a30c7131f1cdf0c02f"
  },
  {
    "text": "- [1] J. Y. Wong, Theory of Ground Vehicles . Hoboken, NJ, USA: Wiley, 2022.\n- [2] R. E. Arvidson et al., 'Mars science laboratory curiosity rover megaripple crossings up to sol 710 in gale Crater,' J. Field Robot. , vol. 34, no. 3, pp. 495-518, May 2017.\n- [3] R. Gonzalez and K. Iagnemma, 'Slippage estimation and compensation for planetary exploration rovers. State of the art and future challenges,' J. Field Robot. , vol. 35, no. 4, pp. 564-577, Jun. 2018.\n- [4] L. Feng et al., 'An instrumented wheel to measure the Wheel-Terrain interactions of planetary robotic Wheel-on-Limb system on sandy terrains,' IEEE Trans. Instrum. Meas. , vol. 71, pp. 1-13, 2022.\n- [5] R. E. Arvidson et al., 'Spirit Mars rover mission: Overview and selected results from the northern home plate winter haven to the side of scamander Crater,' J. Geophys. Res., Planets , vol. 115, no. E7, pp. 1-19, Jul. 2010.\n- [6] D. C. Guastella and G. Muscato, 'Learning-based methods of perception and navigation for ground vehicles in unstructured environments: A review,' Sensors , vol. 21, no. 1, p. 73, Dec. 2020.\n- [7] M. G. H. Nampoothiri, B. Vinayakumar, Y. Sunny, and R. Antony, 'Recent developments in terrain identification, classification, parameter estimation for the navigation of autonomous robots,' Social Netw. Appl. Sci. , vol. 3, no. 4, pp. 1-14, Apr. 2021.\n- [8] P. Borges et al., 'A survey on terrain traversability analysis for autonomous ground vehicles: Methods, sensors, and challenges,' Field Robot. , vol. 2, no. 1, pp. 1567-1627, Mar. 2022.\n- [9] S. Chhaniyara et al., 'Terrain trafficability analysis and soil mechanical property identification for planetary rovers: A survey,' J. Terramechanics , vol. 49, no. 2, pp. 115-128, Apr. 2012.\n- [10] N. E. Putzig, Thermal Inertia and Surface Heterogeneity on Mars . Boulder, CO, USA: Univ. Colorado at Boulder, 2006.\n- [11] G. M. Martínez et al., 'Surface energy budget and thermal inertia at gale crater: Calculations from ground-based measurements,' J. Geophys. Res., Planets , vol. 119, no. 8, pp. 1822-1838, Aug. 2014.\n- [12] G. M. Martínez et al., 'Surface energy budget, albedo, and thermal inertia at Jezero crater, Mars, as observed from the Mars 2020 MEDA instrument,' J. Geophys. Res., Planets , vol. 128, no. 2, pp. 1-23, Feb. 2023.\n- [13] C. Cunningham, I. A. Nesnas, and W. L. Whittaker, 'Improving slip prediction on Mars using thermal inertia measurements,' Auto. Robots , vol. 43, no. 2, pp. 503-521, Feb. 2019.\n- [14] J. Harrell and M. Largould, 'The 25-Ft space simulator at the jet propulsion laboratory,' Jet Propuls. Lab., Pasadena, CA, USA, Tech. Rep. 32-1415, 1969.\n- [15] A. V. Ramachandran, M. I. Nazarious, T. Mathanlal, M.-P. Zorzano, and J. Martín-Torres, 'Space environmental chamber for planetary studies,' Sensors , vol. 20, no. 14, p. 3996, Jul. 2020.\n- [16] Z. Wu et al., 'A Mars environment chamber coupled with multiple in situ spectral sensors for Mars exploration,' Sensors , vol. 21, no. 7, p. 2519, Apr. 2021.\n- [17] C. Alvarez-Llamas, P. Purohit, J. Moros, P. Lucena, J. Laserna, and J. Vadillo, 'A multipurpose thermal vacuum chamber for planetary research compatible with stand-off laser spectroscopies,' in Proc. Lunar Planet. Sci. Conf. , 2021, p. 2330.\n- [18] J. N. Maki et al., 'The Mars 2020 engineering cameras and microphone on the perseverance rover: A next-generation imaging system for Mars exploration,' Space Sci. Rev. , vol. 216, no. 8, pp. 1-48, Dec. 2020.\n- [19] H. H. Kieffer, 'Thermal model for analysis of Mars infrared mapping,' J. Geophys. Res., Planets , vol. 118, no. 3, pp. 451-470, Mar. 2013.\n- [20] J. C. Price, 'Thermal inertia mapping: A new view of the Earth,' J. Geophys. Res. , vol. 82, no. 18, pp. 2582-2590, Jun. 1977.\n- [21] J. Wang, R. L. Bras, G. Sivandran, and R. G. Knox, 'A simple method for the estimation of thermal inertia,' Geophys. Res. Lett. , vol. 37, no. 5, pp. 1-15, Mar. 2010.\n- [22] T. N. Carlson, J. K. Dodd, S. G. Benjamin, and J. N. Cooper, 'Satellite estimation of the surface energy balance, moisture availability and thermal inertia,' J. Appl. Meteorol. , vol. 20, no. 1, pp. 67-87, Jan. 1981.\n- [23] M. Mellon, 'High-resolution thermal inertia mapping from the Mars global surveyor thermal emission spectrometer,' Icarus , vol. 148, no. 2, pp. 437-455, Dec. 2000.\n- [24] J. R. Christian, R. E. Arvidson, J. A. O'Sullivan, A. R. Vasavada, and C. M. Weitz, 'CRISM-based high spatial resolution thermal inertia mapping along Curiosity's traverses in gale Crater,' J. Geophys. Res., Planets , vol. 127, no. 5, pp. 1-33, May 2022.\n- [25] V. E. Hamilton et al., 'Observations and preliminary science results from the first 100 sols of MSL rover environmental monitoring station ground temperature sensor measurements at Gale Crater,' J. Geophys. Res., Planets , vol. 119, no. 4, pp. 745-770, Apr. 2014.\n- [26] A. R. Vasavada, S. Piqueux, K. W. Lewis, M. T. Lemmon, and M. D. Smith, 'Thermophysical properties along Curiosity's traverse in Gale crater, Mars, derived from the REMS ground temperature sensor,' Icarus , vol. 284, pp. 372-386, Mar. 2017.\n- [27] J. Gómez-Elvira et al., 'REMS: The environmental sensor suite for the Mars science laboratory rover,' Space Sci. Rev. , vol. 170, nos. 1-4, pp. 583-640, Sep. 2012.\n- [28] J. Pérez-Izquierdo, E. Sebastián, G. M. Martínez, A. Bravo, M. Ramos, and J. A. R. Manfredi, 'The thermal infrared sensor (TIRS) of the Mars environmental dynamics analyzer (MEDA) instrument onboard Mars 2020, a general description and performance analysis,' Measurement , vol. 122, pp. 432-442, Jul. 2018.\n- [29] J. F. Bell et al., 'The Mars 2020 perseverance rover mast camera zoom (Mastcam-Z) multispectral, stereoscopic imaging investigation,' Space Sci. Rev. , vol. 217, no. 1, pp. 1-40, Feb. 2021.\n- [30] C. Cunningham, I. Nesnas, and W. L. Whittaker, 'Terrain traversability prediction by imaging thermal transients,' in Proc. IEEE Int. Conf. Robot. Autom. (ICRA) , May 2015, pp. 3947-3952.\n- [31] R. González, A. López, and K. Iagnemma, 'Thermal vision, moisture content, and vegetation in the context of off-road mobile robots,' J. Terramechanics , vol. 70, pp. 35-48, Apr. 2017.\n\nProvides supporting evidence for the argument on thermal inertia and its potential for improved soil assessment under lower pressures, highlighting the system’s ability to capture Martian diurnal cycles and compare soil properties under controlled Earth and Martian conditions.",
    "original_text": "- [1] J. Y. Wong, Theory of Ground Vehicles . Hoboken, NJ, USA: Wiley, 2022.\n- [2] R. E. Arvidson et al., 'Mars science laboratory curiosity rover megaripple crossings up to sol 710 in gale Crater,' J. Field Robot. , vol. 34, no. 3, pp. 495-518, May 2017.\n- [3] R. Gonzalez and K. Iagnemma, 'Slippage estimation and compensation for planetary exploration rovers. State of the art and future challenges,' J. Field Robot. , vol. 35, no. 4, pp. 564-577, Jun. 2018.\n- [4] L. Feng et al., 'An instrumented wheel to measure the Wheel-Terrain interactions of planetary robotic Wheel-on-Limb system on sandy terrains,' IEEE Trans. Instrum. Meas. , vol. 71, pp. 1-13, 2022.\n- [5] R. E. Arvidson et al., 'Spirit Mars rover mission: Overview and selected results from the northern home plate winter haven to the side of scamander Crater,' J. Geophys. Res., Planets , vol. 115, no. E7, pp. 1-19, Jul. 2010.\n- [6] D. C. Guastella and G. Muscato, 'Learning-based methods of perception and navigation for ground vehicles in unstructured environments: A review,' Sensors , vol. 21, no. 1, p. 73, Dec. 2020.\n- [7] M. G. H. Nampoothiri, B. Vinayakumar, Y. Sunny, and R. Antony, 'Recent developments in terrain identification, classification, parameter estimation for the navigation of autonomous robots,' Social Netw. Appl. Sci. , vol. 3, no. 4, pp. 1-14, Apr. 2021.\n- [8] P. Borges et al., 'A survey on terrain traversability analysis for autonomous ground vehicles: Methods, sensors, and challenges,' Field Robot. , vol. 2, no. 1, pp. 1567-1627, Mar. 2022.\n- [9] S. Chhaniyara et al., 'Terrain trafficability analysis and soil mechanical property identification for planetary rovers: A survey,' J. Terramechanics , vol. 49, no. 2, pp. 115-128, Apr. 2012.\n- [10] N. E. Putzig, Thermal Inertia and Surface Heterogeneity on Mars . Boulder, CO, USA: Univ. Colorado at Boulder, 2006.\n- [11] G. M. Martínez et al., 'Surface energy budget and thermal inertia at gale crater: Calculations from ground-based measurements,' J. Geophys. Res., Planets , vol. 119, no. 8, pp. 1822-1838, Aug. 2014.\n- [12] G. M. Martínez et al., 'Surface energy budget, albedo, and thermal inertia at Jezero crater, Mars, as observed from the Mars 2020 MEDA instrument,' J. Geophys. Res., Planets , vol. 128, no. 2, pp. 1-23, Feb. 2023.\n- [13] C. Cunningham, I. A. Nesnas, and W. L. Whittaker, 'Improving slip prediction on Mars using thermal inertia measurements,' Auto. Robots , vol. 43, no. 2, pp. 503-521, Feb. 2019.\n- [14] J. Harrell and M. Largould, 'The 25-Ft space simulator at the jet propulsion laboratory,' Jet Propuls. Lab., Pasadena, CA, USA, Tech. Rep. 32-1415, 1969.\n- [15] A. V. Ramachandran, M. I. Nazarious, T. Mathanlal, M.-P. Zorzano, and J. Martín-Torres, 'Space environmental chamber for planetary studies,' Sensors , vol. 20, no. 14, p. 3996, Jul. 2020.\n- [16] Z. Wu et al., 'A Mars environment chamber coupled with multiple in situ spectral sensors for Mars exploration,' Sensors , vol. 21, no. 7, p. 2519, Apr. 2021.\n- [17] C. Alvarez-Llamas, P. Purohit, J. Moros, P. Lucena, J. Laserna, and J. Vadillo, 'A multipurpose thermal vacuum chamber for planetary research compatible with stand-off laser spectroscopies,' in Proc. Lunar Planet. Sci. Conf. , 2021, p. 2330.\n- [18] J. N. Maki et al., 'The Mars 2020 engineering cameras and microphone on the perseverance rover: A next-generation imaging system for Mars exploration,' Space Sci. Rev. , vol. 216, no. 8, pp. 1-48, Dec. 2020.\n- [19] H. H. Kieffer, 'Thermal model for analysis of Mars infrared mapping,' J. Geophys. Res., Planets , vol. 118, no. 3, pp. 451-470, Mar. 2013.\n- [20] J. C. Price, 'Thermal inertia mapping: A new view of the Earth,' J. Geophys. Res. , vol. 82, no. 18, pp. 2582-2590, Jun. 1977.\n- [21] J. Wang, R. L. Bras, G. Sivandran, and R. G. Knox, 'A simple method for the estimation of thermal inertia,' Geophys. Res. Lett. , vol. 37, no. 5, pp. 1-15, Mar. 2010.\n- [22] T. N. Carlson, J. K. Dodd, S. G. Benjamin, and J. N. Cooper, 'Satellite estimation of the surface energy balance, moisture availability and thermal inertia,' J. Appl. Meteorol. , vol. 20, no. 1, pp. 67-87, Jan. 1981.\n- [23] M. Mellon, 'High-resolution thermal inertia mapping from the Mars global surveyor thermal emission spectrometer,' Icarus , vol. 148, no. 2, pp. 437-455, Dec. 2000.\n- [24] J. R. Christian, R. E. Arvidson, J. A. O'Sullivan, A. R. Vasavada, and C. M. Weitz, 'CRISM-based high spatial resolution thermal inertia mapping along Curiosity's traverses in gale Crater,' J. Geophys. Res., Planets , vol. 127, no. 5, pp. 1-33, May 2022.\n- [25] V. E. Hamilton et al., 'Observations and preliminary science results from the first 100 sols of MSL rover environmental monitoring station ground temperature sensor measurements at Gale Crater,' J. Geophys. Res., Planets , vol. 119, no. 4, pp. 745-770, Apr. 2014.\n- [26] A. R. Vasavada, S. Piqueux, K. W. Lewis, M. T. Lemmon, and M. D. Smith, 'Thermophysical properties along Curiosity's traverse in Gale crater, Mars, derived from the REMS ground temperature sensor,' Icarus , vol. 284, pp. 372-386, Mar. 2017.\n- [27] J. Gómez-Elvira et al., 'REMS: The environmental sensor suite for the Mars science laboratory rover,' Space Sci. Rev. , vol. 170, nos. 1-4, pp. 583-640, Sep. 2012.\n- [28] J. Pérez-Izquierdo, E. Sebastián, G. M. Martínez, A. Bravo, M. Ramos, and J. A. R. Manfredi, 'The thermal infrared sensor (TIRS) of the Mars environmental dynamics analyzer (MEDA) instrument onboard Mars 2020, a general description and performance analysis,' Measurement , vol. 122, pp. 432-442, Jul. 2018.\n- [29] J. F. Bell et al., 'The Mars 2020 perseverance rover mast camera zoom (Mastcam-Z) multispectral, stereoscopic imaging investigation,' Space Sci. Rev. , vol. 217, no. 1, pp. 1-40, Feb. 2021.\n- [30] C. Cunningham, I. Nesnas, and W. L. Whittaker, 'Terrain traversability prediction by imaging thermal transients,' in Proc. IEEE Int. Conf. Robot. Autom. (ICRA) , May 2015, pp. 3947-3952.\n- [31] R. González, A. López, and K. Iagnemma, 'Thermal vision, moisture content, and vegetation in the context of off-road mobile robots,' J. Terramechanics , vol. 70, pp. 35-48, Apr. 2017.",
    "context": "Provides supporting evidence for the argument on thermal inertia and its potential for improved soil assessment under lower pressures, highlighting the system’s ability to capture Martian diurnal cycles and compare soil properties under controlled Earth and Martian conditions.",
    "document": "Thermal_Imagery_for_Rover_Soil_Assessment_Using_a_Multipurpose_Environmental_Chamber_Under_Simulated_Mars_Conditions.pdf",
    "pages": [
      11
    ],
    "id": "a4f6d753f0c787dfc10e3d91f161cee4700a1d6b47ad0e67db9dbbf001925db9"
  },
  {
    "text": "- [32] Y. Iwashita et al., 'Virtual IR sensing for planetary rovers: Improved terrain classification and thermal inertia estimation,' IEEE Robot. Autom. Lett. , vol. 5, no. 4, pp. 6302-6309, Oct. 2020.\n- [33] D. Atha, R. M. Swan, A. Didier, Z. Hasnain, and M. Ono, 'Multimission terrain classifier for safe rover navigation and automated science,' in Proc. IEEE Aerosp. Conf. (AERO) , Mar. 2022, pp. 1-13.\n- [34] L. Mandrake et al., 'Space applications of a trusted AI framework: Experiences and lessons learned,' in Proc. IEEE Aerosp. Conf. (AERO) , Mar. 2022, pp. 1-20.\n- [35] H. Song, J. Zhang, S. Du, D. Ni, Y. Liu, and Y. Sun, 'Constraining the thermal inertia of Mars utilizing machine learning techniques,' Monthly Notices Roy. Astronomical Soc. , vol. 522, no. 2, pp. 1697-1705, Apr. 2023.\n- [36] T. P. Nagle-McNaughton, L. A. Scuderi, and N. Erickson, 'Squeezing data from a rock: Machine learning for Martian science,' Geosciences , vol. 12, no. 6, p. 248, Jun. 2022.\n- [37] M. A. Presley and P. R. Christensen, 'The effect of bulk density and particle size sorting on the thermal conductivity of particulate materials under Martian atmospheric pressures,' J. Geophys. Res., Planets , vol. 102, no. E4, pp. 9221-9229, Apr. 1997.\n- [38] S. Masamune and J. M. Smith, 'Thermal conductivity of beds of spherical particles,' Ind. Eng. Chem. Fundam. , vol. 2, no. 2, pp. 136-143, May 1963.\n- [39] B. M. Jakosky, 'On the thermal properties of Martian fines,' Icarus , vol. 66, no. 1, pp. 117-124, Apr. 1986.\n- [40] P. R. Yoder Jr., Opto-Mechanical Systems Design . Boca Raton, FL, USA: CRC Press, 2005.\n- [41] G. Martínez et al., 'The modern near-surface Martian climate: A review of in-situ meteorological data from Viking to Curiosity,' Space Sci. Rev. , vol. 212, pp. 295-338, Oct. 2017.\n- [42] M. Vollmer, Infrared Thermal Imaging . Cham, Switzerland: Springer, 2021.\nRaúl Castilla-Arquillo (Graduate Student Member, IEEE) received the B.Sc. degree in electronics, robotics, and mechatronics engineering from the University of Seville, Seville, Spain, in 2019, and the M.Sc. degree in mechatronics engineering from the University of Málaga (UMA), Málaga, Spain, in 2020, where he is currently pursuing the Ph.D. degree in mechatronics engineering.\nIn 2019, he worked for over a year as a firmware engineer in the industry. Afterward, he became a member of the Space Robotics Laboratory, UMA, where he started developing artificial vision algorithms as part of a collaboration agreement between the European Space Agency (ESA) and UMA. His research interests are focused on developing machine learning techniques to improve the perception of space robotic systems using advanced sensors, such as stereo and thermal cameras.\nAnthony Mandow (Member, IEEE) received the M.Eng. and Ph.D. degrees in computer systems from the Universidad de Málaga (UMA), Málaga, Spain, in 1992 and 1998, respectively.\nHe is currently an Associate Professor with the Department of Systems Engineering and Automation, UMA, and a member of the UMA Group for Robotics and Mechatronics. Since 2016, he has been an Academic Coordinator with the UMA Doctoral Program of mechatronics engineering. Since 2021, he has been serving as the Director of the Insti- tute for Mechatronics Engineering and Cyber-Physical Systems, UMA (IMECH.UMA). He has coauthored more than 100 conference and journal papers and has been engaged as a principal investigator in research projects related to field and disaster robotics, multirobot systems, terrain modeling for navigation, artificial intelligence for cyberphysical systems, and innovative practices in higher education.\nCarlos J. Pérez-del-Pulgar (Member, IEEE) received the M.S. and Ph.D. degrees in computer science from the University of Málaga (UMA), Málaga, Spain, in 2004 and 2016, respectively.\nIn 2004, he was given a permanent position as a Research Support Staff with UMA in 2017. Since 2010, he has been a part-time Assistant Lecturer with the Electrical Engineering Faculty, UMA, where he has been responsible for different subjects related to automation and robotics and is currently an Associate Professor and responsible for the Space\nRobotics Laboratory. He has more than 60 publications on these topics and has been involved in more than 15 different Spanish and international projects. His research interests include machine learning, surgical robotics, space robotics, and autonomous astronomy.\nCésar Álvarez-Llamas received the B.Sc. degree in physics and the M.Sc. degree in analytical chemistry from the University of Oviedo, Oviedo, Spain, in 2011 and 2012, respectively, and the Ph.D. degree (Hons.) in physics from the University of Oviedo, working on the laser-induced breakdown spectroscopy (LIBS) technique in 2017.\nAs a Post-Doctoral Researcher, he studied portable LIBS equipment with the Carnot Interdisciplinary Laboratory Burgundy (ICB), Dijon, France; the analysis of collimated nanoparticles with Commissariat\nà l'énergie atomique et aux énergies alternatives (CEA), Saclay, France; and the use of LIBS together with acoustic signals for Mars geo-exploration applications under the NASA Mars 2020 mission in UMALASERLAB, University of Málaga, Málaga, Spain. He is currently working on the applications of kHz LIBS with the Institute Light and Matter (ILM), Lyon, France.\nJosé M. Vadillo received the B.S. degree in biology from the University of Navarra, Pamplona, Spain, in 1989, and the Ph.D. degree in chemistry from the University of Málaga, Málaga, Spain, in 1999, with a dissertation based on the use of pulsed laser combined with optical emission spectroscopy for the analysis of samples in condensed phase.\nHe has completed scientific stays in different research centers (the Los Alamos National Laboratory, University of Michigan, Ann Arbor, MI, USA; the Swiss Politechnic Institute, Zürich, Switzerland;\nand Stanford University, Stanford, CA, USA). He was a Royal Society of Chemistry Scholar in 2000, a Fulbright Scholar from 2000 to 2002, and recruited under the 'Ramón y Cajal' Program with the University of Málaga from 2002 to 2007. He was an Assistant Professor with the University of Málaga from 2008 to 2010, where he has been a Permanent Professor since 2010. His scientific interests include instrumental developments and applied spectroscopy, with a spectral emphasis on laser-based methods and mass spectrometry.\nJavier Laserna received the degree in chemistry from the University of Granada, Granada, Spain, in 1978, and the Ph.D. degree in analytical chemistry from the University of Málaga, Málaga, Spain, in 1980.\nHe then joined the faculty of the University of Málaga as an Assistant Professor and became a Full Professor in 1999. He is currently a Co-Investigator for the SuperCam instrument on NASA's Mars 2020 Perseverance rover. He has participated in several research projects funded by the European Commission's Framework Program both as a member of the consortium and as a coordinator. He has been also a principal investigator of more than 25 research projects funded by the Spanish Central Administration and the Regional Administration of Andalucia. He has published more than 300 scientific articles, six books, and book chapters. He is a Co-Inventor of seven patents held by the University of Málaga. He has supervised 35 Ph.D. students. He has delivered numerous invited plenary and keynote conferences at international meetings. His current research interests include the investigation of new measurement principles based on atomic emission, molecular absorption and dispersion, and the understanding of the fundamental phenomena that govern analytical measurements. He is also interested in the development of analytical instrumentation for laser-induced breakdown spectroscopy, remote laser chemical analysis, and online and fieldable analytical measurements. The application areas include the analysis of energetic materials, development of sensors for CBNRE threats, lasers for cultural heritage, and LIBS and Raman spectroscopy for space exploration.\nProf. Laserna was awarded the National Prize for research in analytical chemistry of the Royal Spanish Society of Chemistry in 2009, the National Prize of the Spanish Society of Applied Spectroscopy for his research career in applied spectroscopy in 2010, the 2018 Lester W. Strock Award from the Society of Applied Spectroscopy (USA), and the Ioannes Marcus Marci Medal of the Czech Spectroscopic Society (Czech Republic).\n\nProvides supporting evidence for the argument on thermal inertia of Mars, utilizing machine learning techniques and incorporating data from various spectroscopic methods.",
    "original_text": "- [32] Y. Iwashita et al., 'Virtual IR sensing for planetary rovers: Improved terrain classification and thermal inertia estimation,' IEEE Robot. Autom. Lett. , vol. 5, no. 4, pp. 6302-6309, Oct. 2020.\n- [33] D. Atha, R. M. Swan, A. Didier, Z. Hasnain, and M. Ono, 'Multimission terrain classifier for safe rover navigation and automated science,' in Proc. IEEE Aerosp. Conf. (AERO) , Mar. 2022, pp. 1-13.\n- [34] L. Mandrake et al., 'Space applications of a trusted AI framework: Experiences and lessons learned,' in Proc. IEEE Aerosp. Conf. (AERO) , Mar. 2022, pp. 1-20.\n- [35] H. Song, J. Zhang, S. Du, D. Ni, Y. Liu, and Y. Sun, 'Constraining the thermal inertia of Mars utilizing machine learning techniques,' Monthly Notices Roy. Astronomical Soc. , vol. 522, no. 2, pp. 1697-1705, Apr. 2023.\n- [36] T. P. Nagle-McNaughton, L. A. Scuderi, and N. Erickson, 'Squeezing data from a rock: Machine learning for Martian science,' Geosciences , vol. 12, no. 6, p. 248, Jun. 2022.\n- [37] M. A. Presley and P. R. Christensen, 'The effect of bulk density and particle size sorting on the thermal conductivity of particulate materials under Martian atmospheric pressures,' J. Geophys. Res., Planets , vol. 102, no. E4, pp. 9221-9229, Apr. 1997.\n- [38] S. Masamune and J. M. Smith, 'Thermal conductivity of beds of spherical particles,' Ind. Eng. Chem. Fundam. , vol. 2, no. 2, pp. 136-143, May 1963.\n- [39] B. M. Jakosky, 'On the thermal properties of Martian fines,' Icarus , vol. 66, no. 1, pp. 117-124, Apr. 1986.\n- [40] P. R. Yoder Jr., Opto-Mechanical Systems Design . Boca Raton, FL, USA: CRC Press, 2005.\n- [41] G. Martínez et al., 'The modern near-surface Martian climate: A review of in-situ meteorological data from Viking to Curiosity,' Space Sci. Rev. , vol. 212, pp. 295-338, Oct. 2017.\n- [42] M. Vollmer, Infrared Thermal Imaging . Cham, Switzerland: Springer, 2021.\nRaúl Castilla-Arquillo (Graduate Student Member, IEEE) received the B.Sc. degree in electronics, robotics, and mechatronics engineering from the University of Seville, Seville, Spain, in 2019, and the M.Sc. degree in mechatronics engineering from the University of Málaga (UMA), Málaga, Spain, in 2020, where he is currently pursuing the Ph.D. degree in mechatronics engineering.\nIn 2019, he worked for over a year as a firmware engineer in the industry. Afterward, he became a member of the Space Robotics Laboratory, UMA, where he started developing artificial vision algorithms as part of a collaboration agreement between the European Space Agency (ESA) and UMA. His research interests are focused on developing machine learning techniques to improve the perception of space robotic systems using advanced sensors, such as stereo and thermal cameras.\nAnthony Mandow (Member, IEEE) received the M.Eng. and Ph.D. degrees in computer systems from the Universidad de Málaga (UMA), Málaga, Spain, in 1992 and 1998, respectively.\nHe is currently an Associate Professor with the Department of Systems Engineering and Automation, UMA, and a member of the UMA Group for Robotics and Mechatronics. Since 2016, he has been an Academic Coordinator with the UMA Doctoral Program of mechatronics engineering. Since 2021, he has been serving as the Director of the Insti- tute for Mechatronics Engineering and Cyber-Physical Systems, UMA (IMECH.UMA). He has coauthored more than 100 conference and journal papers and has been engaged as a principal investigator in research projects related to field and disaster robotics, multirobot systems, terrain modeling for navigation, artificial intelligence for cyberphysical systems, and innovative practices in higher education.\nCarlos J. Pérez-del-Pulgar (Member, IEEE) received the M.S. and Ph.D. degrees in computer science from the University of Málaga (UMA), Málaga, Spain, in 2004 and 2016, respectively.\nIn 2004, he was given a permanent position as a Research Support Staff with UMA in 2017. Since 2010, he has been a part-time Assistant Lecturer with the Electrical Engineering Faculty, UMA, where he has been responsible for different subjects related to automation and robotics and is currently an Associate Professor and responsible for the Space\nRobotics Laboratory. He has more than 60 publications on these topics and has been involved in more than 15 different Spanish and international projects. His research interests include machine learning, surgical robotics, space robotics, and autonomous astronomy.\nCésar Álvarez-Llamas received the B.Sc. degree in physics and the M.Sc. degree in analytical chemistry from the University of Oviedo, Oviedo, Spain, in 2011 and 2012, respectively, and the Ph.D. degree (Hons.) in physics from the University of Oviedo, working on the laser-induced breakdown spectroscopy (LIBS) technique in 2017.\nAs a Post-Doctoral Researcher, he studied portable LIBS equipment with the Carnot Interdisciplinary Laboratory Burgundy (ICB), Dijon, France; the analysis of collimated nanoparticles with Commissariat\nà l'énergie atomique et aux énergies alternatives (CEA), Saclay, France; and the use of LIBS together with acoustic signals for Mars geo-exploration applications under the NASA Mars 2020 mission in UMALASERLAB, University of Málaga, Málaga, Spain. He is currently working on the applications of kHz LIBS with the Institute Light and Matter (ILM), Lyon, France.\nJosé M. Vadillo received the B.S. degree in biology from the University of Navarra, Pamplona, Spain, in 1989, and the Ph.D. degree in chemistry from the University of Málaga, Málaga, Spain, in 1999, with a dissertation based on the use of pulsed laser combined with optical emission spectroscopy for the analysis of samples in condensed phase.\nHe has completed scientific stays in different research centers (the Los Alamos National Laboratory, University of Michigan, Ann Arbor, MI, USA; the Swiss Politechnic Institute, Zürich, Switzerland;\nand Stanford University, Stanford, CA, USA). He was a Royal Society of Chemistry Scholar in 2000, a Fulbright Scholar from 2000 to 2002, and recruited under the 'Ramón y Cajal' Program with the University of Málaga from 2002 to 2007. He was an Assistant Professor with the University of Málaga from 2008 to 2010, where he has been a Permanent Professor since 2010. His scientific interests include instrumental developments and applied spectroscopy, with a spectral emphasis on laser-based methods and mass spectrometry.\nJavier Laserna received the degree in chemistry from the University of Granada, Granada, Spain, in 1978, and the Ph.D. degree in analytical chemistry from the University of Málaga, Málaga, Spain, in 1980.\nHe then joined the faculty of the University of Málaga as an Assistant Professor and became a Full Professor in 1999. He is currently a Co-Investigator for the SuperCam instrument on NASA's Mars 2020 Perseverance rover. He has participated in several research projects funded by the European Commission's Framework Program both as a member of the consortium and as a coordinator. He has been also a principal investigator of more than 25 research projects funded by the Spanish Central Administration and the Regional Administration of Andalucia. He has published more than 300 scientific articles, six books, and book chapters. He is a Co-Inventor of seven patents held by the University of Málaga. He has supervised 35 Ph.D. students. He has delivered numerous invited plenary and keynote conferences at international meetings. His current research interests include the investigation of new measurement principles based on atomic emission, molecular absorption and dispersion, and the understanding of the fundamental phenomena that govern analytical measurements. He is also interested in the development of analytical instrumentation for laser-induced breakdown spectroscopy, remote laser chemical analysis, and online and fieldable analytical measurements. The application areas include the analysis of energetic materials, development of sensors for CBNRE threats, lasers for cultural heritage, and LIBS and Raman spectroscopy for space exploration.\nProf. Laserna was awarded the National Prize for research in analytical chemistry of the Royal Spanish Society of Chemistry in 2009, the National Prize of the Spanish Society of Applied Spectroscopy for his research career in applied spectroscopy in 2010, the 2018 Lester W. Strock Award from the Society of Applied Spectroscopy (USA), and the Ioannes Marcus Marci Medal of the Czech Spectroscopic Society (Czech Republic).",
    "context": "Provides supporting evidence for the argument on thermal inertia of Mars, utilizing machine learning techniques and incorporating data from various spectroscopic methods.",
    "document": "Thermal_Imagery_for_Rover_Soil_Assessment_Using_a_Multipurpose_Environmental_Chamber_Under_Simulated_Mars_Conditions.pdf",
    "pages": [
      11,
      12
    ],
    "id": "2d1c3b06081d7920c520bb4b7ff3aade62a0e226a4d3d706c38d8b57cb52c1cb"
  },
  {
    "text": "George F. Hurlburt, STEMCorp\nGeorge K. Thiruvathukal, Loyola University Chicago\nMaria R. Lee, Shih Chien University, Taiwan\nT he notion of graph reasoning is not new. The  heretofore  curious  and  obscure branch  of  mathematics,  graph  theory, extends to Leonhard Euler in the 18th century. 1 The application of graph theory to all manner of networks is quite new, however. Graph theory has exploded, largely due to the existence of the Internet.\nAn  unintended  consequence  of  the  Internet was to reveal how interconnected the world has always been. While the term 'network' used to refer  almost  exclusively  to  electronic  transmission  systems,  suddenly,  networks  are  everywhere. They are social. They are supply chains. They  are  part  and  parcel  of  ecosystems.  They are  food  chains.  They  transport  people,  signals, and energy. They exist throughout biology, physics, chemistry, and economics. They are in our DNA, our software, and our malware. Even dark matter now appears to be interconnected, almost like gigantic intergalactic neurons. Albert-László Barabási's Linked: How Everything Is Connected to Everything Else (Plume, 2003) defines the value proposition behind the rapidly emerging field of network science . 2\nUnlike  the  Industrial  Age-which  comprised lots of immutable physical laws governing highly predictable, linear, and deterministic behaviortoday's networked world is nonlinear and seemingly messy by comparison. Cause and effect in the networked world are often decoupled, making interpretation difficult without  the  right mathematical tools. In a networked world, reductionism fails, given that the whole often exceeds the sum of the parts. Nonetheless, applied graph theory leads to quantitative sense-making in an otherwise  seemingly  senseless  world-a  world increasingly dominated by data that are instantaneous,  enormous,  and  interconnected  in  innumerable ways. In such a volatile world, where\n\nThis chunk introduces the rise of graph theory and its application to diverse networks, largely due to the Internet's impact, contrasting it with the more rigid structures of the Industrial Age and highlighting the challenges of interpreting complex, interconnected systems.",
    "original_text": "George F. Hurlburt, STEMCorp\nGeorge K. Thiruvathukal, Loyola University Chicago\nMaria R. Lee, Shih Chien University, Taiwan\nT he notion of graph reasoning is not new. The  heretofore  curious  and  obscure branch  of  mathematics,  graph  theory, extends to Leonhard Euler in the 18th century. 1 The application of graph theory to all manner of networks is quite new, however. Graph theory has exploded, largely due to the existence of the Internet.\nAn  unintended  consequence  of  the  Internet was to reveal how interconnected the world has always been. While the term 'network' used to refer  almost  exclusively  to  electronic  transmission  systems,  suddenly,  networks  are  everywhere. They are social. They are supply chains. They  are  part  and  parcel  of  ecosystems.  They are  food  chains.  They  transport  people,  signals, and energy. They exist throughout biology, physics, chemistry, and economics. They are in our DNA, our software, and our malware. Even dark matter now appears to be interconnected, almost like gigantic intergalactic neurons. Albert-László Barabási's Linked: How Everything Is Connected to Everything Else (Plume, 2003) defines the value proposition behind the rapidly emerging field of network science . 2\nUnlike  the  Industrial  Age-which  comprised lots of immutable physical laws governing highly predictable, linear, and deterministic behaviortoday's networked world is nonlinear and seemingly messy by comparison. Cause and effect in the networked world are often decoupled, making interpretation difficult without  the  right mathematical tools. In a networked world, reductionism fails, given that the whole often exceeds the sum of the parts. Nonetheless, applied graph theory leads to quantitative sense-making in an otherwise  seemingly  senseless  world-a  world increasingly dominated by data that are instantaneous,  enormous,  and  interconnected  in  innumerable ways. In such a volatile world, where",
    "context": "This chunk introduces the rise of graph theory and its application to diverse networks, largely due to the Internet's impact, contrasting it with the more rigid structures of the Industrial Age and highlighting the challenges of interpreting complex, interconnected systems.",
    "document": "The_Graph_Database_Jack_of_All_Trades_or_Just_Not_SQL.pdf",
    "pages": [
      1
    ],
    "id": "c34dae13ace6bd2ec2aafbadfa7bf88edb425be12169d009774d20c1d362cf38"
  },
  {
    "text": "even things share massive data, the graph database is a logical fallout and a suitable alternative to the venerable relational database management system (RDBMS).\n\nIntroduces the graph database as a viable alternative to the traditional RDBMS, highlighting its relevance in a data-saturated world dominated by interconnected networks.",
    "original_text": "even things share massive data, the graph database is a logical fallout and a suitable alternative to the venerable relational database management system (RDBMS).",
    "context": "Introduces the graph database as a viable alternative to the traditional RDBMS, highlighting its relevance in a data-saturated world dominated by interconnected networks.",
    "document": "The_Graph_Database_Jack_of_All_Trades_or_Just_Not_SQL.pdf",
    "pages": [
      2
    ],
    "id": "11de78e06a4540cbe7f0c9645238c08eef2047bb2feed7448d89702cea09203e"
  },
  {
    "text": "The  long-favored  RDBMS  is  an  artifact  of  the foregone linear age. Relational algebra, the mathematics  underlying  the  RDBMS,  grew  out  of  a need to efficiently compress data during the 1960s, when storage was both limited and very expensive. 3 The RDBMS still serves well in transactionrich, process-stable environments in which relationships  can  be  expressed  as  one-to-one, one-to-many, or many-to-one. For example, huge credit-card  processing  systems  operate  reliably from  large-scale  24/7/365  RDBMS  installations. For this reason, and because RDBMS technology is good for data aggregation, this technology will not be going away. Much like television did not replace radio, the RDBMS has a definite niche. When predefined transactions become less prevalent  or  process  dynamics  vary  frequently,  the RDBMS  becomes  exceedingly  costly  to  document  and  manipulate.  When  the  preponderance  of  relationships  becomes  many-to-many, RDBMS performance takes  a  nosedive.  Moreover, the RDBMS schema, as a formal means of defining entity relationships, is typically inflexible,  requiring  high  maintenance  to  effect  the most minute change.\nAs an interesting aside, although RDBMS became  the  de  facto  standard  for  databases,  the 1960s  also  produced  early  database  technology similar to graph databases. The hierarchical model  was  created  at  IBM  to  represent  treestructured relationships, in which individual records are arranged in a treelike fashion (an idea that  is  mimicked  with  foreign  keys  in  RDBMS today and enforced with triggers). Similarly, the network model of the late 1960s was an early attempt to model objects and their relationshipsan idea that would re-emerge in the 1980s with object-oriented  databases.  Thus,  the  notion  of graph  databases  can  be  thought  of  as  a  more modern  rendition  of  these  nascent  attempts  to build  more  tree-  or  graph-like  databases  combined with the advances of the web era, wherein unstructured (or weakly structured) data (for example, in JSON) can be used to represent node and edge data (and metadata).\n\nProvides historical context for the rise of graph databases, contrasting them with the established RDBMS and outlining their origins in earlier database models.",
    "original_text": "The  long-favored  RDBMS  is  an  artifact  of  the foregone linear age. Relational algebra, the mathematics  underlying  the  RDBMS,  grew  out  of  a need to efficiently compress data during the 1960s, when storage was both limited and very expensive. 3 The RDBMS still serves well in transactionrich, process-stable environments in which relationships  can  be  expressed  as  one-to-one, one-to-many, or many-to-one. For example, huge credit-card  processing  systems  operate  reliably from  large-scale  24/7/365  RDBMS  installations. For this reason, and because RDBMS technology is good for data aggregation, this technology will not be going away. Much like television did not replace radio, the RDBMS has a definite niche. When predefined transactions become less prevalent  or  process  dynamics  vary  frequently,  the RDBMS  becomes  exceedingly  costly  to  document  and  manipulate.  When  the  preponderance  of  relationships  becomes  many-to-many, RDBMS performance takes  a  nosedive.  Moreover, the RDBMS schema, as a formal means of defining entity relationships, is typically inflexible,  requiring  high  maintenance  to  effect  the most minute change.\nAs an interesting aside, although RDBMS became  the  de  facto  standard  for  databases,  the 1960s  also  produced  early  database  technology similar to graph databases. The hierarchical model  was  created  at  IBM  to  represent  treestructured relationships, in which individual records are arranged in a treelike fashion (an idea that  is  mimicked  with  foreign  keys  in  RDBMS today and enforced with triggers). Similarly, the network model of the late 1960s was an early attempt to model objects and their relationshipsan idea that would re-emerge in the 1980s with object-oriented  databases.  Thus,  the  notion  of graph  databases  can  be  thought  of  as  a  more modern  rendition  of  these  nascent  attempts  to build  more  tree-  or  graph-like  databases  combined with the advances of the web era, wherein unstructured (or weakly structured) data (for example, in JSON) can be used to represent node and edge data (and metadata).",
    "context": "Provides historical context for the rise of graph databases, contrasting them with the established RDBMS and outlining their origins in earlier database models.",
    "document": "The_Graph_Database_Jack_of_All_Trades_or_Just_Not_SQL.pdf",
    "pages": [
      2
    ],
    "id": "8350d3047ff83713b2c899f655b8a11f80de2f4adc588dc1c4de6ab75d092b91"
  },
  {
    "text": "By contrast, the graph database is built around the notion  of  efficiently  managing  many-to-many, property-laden relationships that coexist in highly dynamic environments. Sporting impressive performance numbers for the many-to-many relationships  common  to  networks,  the  graph database becomes an ideal way to represent and analyze complex nonlinear networks. It has the drawback, however, of being inefficient with endto-end transaction processing and rapid associative summarization as its relational predecessor.\nGraphs are expressed in node-arc-node (subject-predicate-object) triples. This notion of a graph is fundamentally straightforward. Nodes generally  represent  physical  or  conceptual  objects,  typically  associated with objects as represented  in  a  programming  language.  Nodes  can typically have one or many descriptive properties ascribed to them. Arcs, or edges, represent metaphysical constructs that connect or create relationships between nodes or properties. 4  In some graph representations, properties can also be assigned to arcs.\nThus, the assertion that 'Jack knows Jill' is a simple  triple  expressing  a  relationship  between two nodes-in this case, human beings. Other triple  assertions  might  also  apply:  'Jack  uses  a pail,' 'Jill has thirst,' 'Jack carries the pail,' 'a hill  leads  to  water,'  'Jack  climbs  the  hill,'  'Jill climbs the hill,' and so on. Many graph databases go  a  step  further  and  allow  the  attachment  of properties to both nodes and relationships. Thus, Jack can take the properties such as 'male,' 'age 19,'  'loving,'  or  'physically  fit,'  while  the  pail can be ascribed the properties 'used' or perhaps 'leaky.' The 'Jack uses a pail' relationship might also have an assigned property to better describe what the use of a pail can be.\nThe graph database, a popular variant of the NoSQL ('not only SQL') database, has grown as an  effective  tool  for  representing  dynamic  network-related relationships. Unlike the simple Jack and Jill nursery rhyme relationships, which only entail a few triples, significant graphs can easily grow to many millions, or even billions, of triples. To accommodate databases of this size, specialized supportive and performance-enhancing hardware  and  algorithms  have  arrived  on  the market. The parallel-processer-based graphics processing  unit  (GPU)  also  offers  hardware\nalternatives to accelerate large-scale graph processing. 5 Some  firms,  such  as  Cray, 6 have developed  specially  configured  supercomputers to digest and return rapid results from massively scaled graphs.\nThe graph database, while still in its relative infancy, shows great promise for traversing complex paths to establish the linkages and influences that momentarily connect cause to effect via a chain of events. Because time is a factor, and nodes come and go over time, the resulting cause and effect relationships themselves are often fleeting. As such, the graph database offers the tantalizing ability to understand a growing myriad of network behaviors both qualitatively and quantitatively.\n\nThis section details the core structure and functionality of graph databases, emphasizing their efficiency in managing complex, many-to-many relationships—a key distinction from relational databases. It highlights their use in representing networks and their potential for analyzing dynamic connections, alongside the hardware and algorithmic advancements supporting their scalability.",
    "original_text": "By contrast, the graph database is built around the notion  of  efficiently  managing  many-to-many, property-laden relationships that coexist in highly dynamic environments. Sporting impressive performance numbers for the many-to-many relationships  common  to  networks,  the  graph database becomes an ideal way to represent and analyze complex nonlinear networks. It has the drawback, however, of being inefficient with endto-end transaction processing and rapid associative summarization as its relational predecessor.\nGraphs are expressed in node-arc-node (subject-predicate-object) triples. This notion of a graph is fundamentally straightforward. Nodes generally  represent  physical  or  conceptual  objects,  typically  associated with objects as represented  in  a  programming  language.  Nodes  can typically have one or many descriptive properties ascribed to them. Arcs, or edges, represent metaphysical constructs that connect or create relationships between nodes or properties. 4  In some graph representations, properties can also be assigned to arcs.\nThus, the assertion that 'Jack knows Jill' is a simple  triple  expressing  a  relationship  between two nodes-in this case, human beings. Other triple  assertions  might  also  apply:  'Jack  uses  a pail,' 'Jill has thirst,' 'Jack carries the pail,' 'a hill  leads  to  water,'  'Jack  climbs  the  hill,'  'Jill climbs the hill,' and so on. Many graph databases go  a  step  further  and  allow  the  attachment  of properties to both nodes and relationships. Thus, Jack can take the properties such as 'male,' 'age 19,'  'loving,'  or  'physically  fit,'  while  the  pail can be ascribed the properties 'used' or perhaps 'leaky.' The 'Jack uses a pail' relationship might also have an assigned property to better describe what the use of a pail can be.\nThe graph database, a popular variant of the NoSQL ('not only SQL') database, has grown as an  effective  tool  for  representing  dynamic  network-related relationships. Unlike the simple Jack and Jill nursery rhyme relationships, which only entail a few triples, significant graphs can easily grow to many millions, or even billions, of triples. To accommodate databases of this size, specialized supportive and performance-enhancing hardware  and  algorithms  have  arrived  on  the market. The parallel-processer-based graphics processing  unit  (GPU)  also  offers  hardware\nalternatives to accelerate large-scale graph processing. 5 Some  firms,  such  as  Cray, 6 have developed  specially  configured  supercomputers to digest and return rapid results from massively scaled graphs.\nThe graph database, while still in its relative infancy, shows great promise for traversing complex paths to establish the linkages and influences that momentarily connect cause to effect via a chain of events. Because time is a factor, and nodes come and go over time, the resulting cause and effect relationships themselves are often fleeting. As such, the graph database offers the tantalizing ability to understand a growing myriad of network behaviors both qualitatively and quantitatively.",
    "context": "This section details the core structure and functionality of graph databases, emphasizing their efficiency in managing complex, many-to-many relationships—a key distinction from relational databases. It highlights their use in representing networks and their potential for analyzing dynamic connections, alongside the hardware and algorithmic advancements supporting their scalability.",
    "document": "The_Graph_Database_Jack_of_All_Trades_or_Just_Not_SQL.pdf",
    "pages": [
      2,
      3
    ],
    "id": "942daab34f508783c63428bbaaa0318ebd9a935a860297d2c6b4211257f758c5"
  },
  {
    "text": "The qualitative aspect of a graph database comes into  play  when  data  are  queried,  particularly when based on like properties common to numerous  nodes  or  arcs.  This,  however,  necessitates some salient precautions. The quality of the data in any graph depends on the quality of the relationships.  Thus,  while  not  initially  requiring the demanding rigor of the RDBMS-related entity-relationship  diagrams  (ERD),  the  graph database nonetheless requires some degree of effective front-end modeling.\nThe good news is that,  unlike  ERDs,  simple graph  models  are  visual,  flexible,  and  accommodate changes on the fly.  If  the  relationships are straightforward, and their number is limited, the data can be self-describing. A simple graph model shows the family of overarching relationships between nodes in a given graph environment. As the data grows to large collections of instances,  such  as  the  number  of  sensors  in  a burgeoning Internet of Things (IoT), the graph model becomes necessary to sort out the many varied  nodal  properties  and  meta-relationship types these nodes and properties might possess.\nThe bad news is that as mission criticality and scale  grow,  the  requisite  modeling  can  grow  to the  proportion  of  full-blown  semantic  ontology.  This  sophisticated  level  of  modeling  often requires  at  least  as  much  or  more  design  forethought than an ERD.\nIn  either  case,  poorly  thought-through  relationships contribute to poorly defined graph environments. For example, defining a node by a foreign key from a former RDBMS does not yield a great deal of meaningful information in a graph data environment. Rather, the declarations of all nodes, properties,  and  relationships  need  to  be explicit and should conform to a generalized pattern defined via the specific graph model. To alleviate storage consistency concerns, many graph databases  do  support  the  Atomic,  Consistent, Isolated, and Durable (ACID) consistency model, which is a spin-off storage-locking scheme from RDBMS technology (bit.ly/2gfNjQu).\nScale  introduces  yet  another  qualitative  concern. It is easy for graph data to grow rapidly as more  and  more  instances  are  brought  to  bear. This relates to the classic 'how much is enough' dilemma when building any model. The problem is heightened in the graph database environment, because each graph instantiation is typically isolated on a single graph server. As the graph grows in  scale,  the  related  query  complexity  grows  as well.  Fortunately,  graphs  can  be  intelligently reduced  to  more  salient  subgraphs  that  can  be better managed, queried, and understood. This reinforces the growing practice of persisting data in a relational or appropriate nongraph NoSQL environment. The choice of an appropriate data persistence tool often depends on tolerance for the  amount  of  structure  or  lack  thereof  in  the data. From this larger corpus, subgraphs (database 'views') can be intelligently isolated for further analysis of dynamics as a specific subgraph drawn  from  a  larger  graph.  As  scale  increases, modern  algorithms  can  assist  in  the  subgraph mining process. 7\nIronically,  SQL  might  prove  to  be  a  useful means  to  permit  building  pattern-based  relationships  from  simple  taxonomy-based  arrays of descriptive data stored as relational data. For example,  we  might  wish  to  view  an  IoT  graph phenomenon  from  its  supply  chain  perspective to examine the efficiency of material flow to meet an operational IoT requirement in a given timeframe.  At  another  time,  however,  isolating actual IoT operations as governed by interaction among the nodes and their properties might be important to fully understand mission effectiveness. Although the RDBMS environment cannot support the requisite number of many-to-many relationships entailed in the graph, it can nonetheless serve as a storehouse for the data necessary  to  efficiently  generate  the  requisite  model (both role and rule)-based relationships. In such\n\nHighlights the need for careful front-end modeling in graph databases as data scales, emphasizing that complex relationships require more design foresight than traditional ERDs and addressing concerns about data quality and storage consistency.",
    "original_text": "The qualitative aspect of a graph database comes into  play  when  data  are  queried,  particularly when based on like properties common to numerous  nodes  or  arcs.  This,  however,  necessitates some salient precautions. The quality of the data in any graph depends on the quality of the relationships.  Thus,  while  not  initially  requiring the demanding rigor of the RDBMS-related entity-relationship  diagrams  (ERD),  the  graph database nonetheless requires some degree of effective front-end modeling.\nThe good news is that,  unlike  ERDs,  simple graph  models  are  visual,  flexible,  and  accommodate changes on the fly.  If  the  relationships are straightforward, and their number is limited, the data can be self-describing. A simple graph model shows the family of overarching relationships between nodes in a given graph environment. As the data grows to large collections of instances,  such  as  the  number  of  sensors  in  a burgeoning Internet of Things (IoT), the graph model becomes necessary to sort out the many varied  nodal  properties  and  meta-relationship types these nodes and properties might possess.\nThe bad news is that as mission criticality and scale  grow,  the  requisite  modeling  can  grow  to the  proportion  of  full-blown  semantic  ontology.  This  sophisticated  level  of  modeling  often requires  at  least  as  much  or  more  design  forethought than an ERD.\nIn  either  case,  poorly  thought-through  relationships contribute to poorly defined graph environments. For example, defining a node by a foreign key from a former RDBMS does not yield a great deal of meaningful information in a graph data environment. Rather, the declarations of all nodes, properties,  and  relationships  need  to  be explicit and should conform to a generalized pattern defined via the specific graph model. To alleviate storage consistency concerns, many graph databases  do  support  the  Atomic,  Consistent, Isolated, and Durable (ACID) consistency model, which is a spin-off storage-locking scheme from RDBMS technology (bit.ly/2gfNjQu).\nScale  introduces  yet  another  qualitative  concern. It is easy for graph data to grow rapidly as more  and  more  instances  are  brought  to  bear. This relates to the classic 'how much is enough' dilemma when building any model. The problem is heightened in the graph database environment, because each graph instantiation is typically isolated on a single graph server. As the graph grows in  scale,  the  related  query  complexity  grows  as well.  Fortunately,  graphs  can  be  intelligently reduced  to  more  salient  subgraphs  that  can  be better managed, queried, and understood. This reinforces the growing practice of persisting data in a relational or appropriate nongraph NoSQL environment. The choice of an appropriate data persistence tool often depends on tolerance for the  amount  of  structure  or  lack  thereof  in  the data. From this larger corpus, subgraphs (database 'views') can be intelligently isolated for further analysis of dynamics as a specific subgraph drawn  from  a  larger  graph.  As  scale  increases, modern  algorithms  can  assist  in  the  subgraph mining process. 7\nIronically,  SQL  might  prove  to  be  a  useful means  to  permit  building  pattern-based  relationships  from  simple  taxonomy-based  arrays of descriptive data stored as relational data. For example,  we  might  wish  to  view  an  IoT  graph phenomenon  from  its  supply  chain  perspective to examine the efficiency of material flow to meet an operational IoT requirement in a given timeframe.  At  another  time,  however,  isolating actual IoT operations as governed by interaction among the nodes and their properties might be important to fully understand mission effectiveness. Although the RDBMS environment cannot support the requisite number of many-to-many relationships entailed in the graph, it can nonetheless serve as a storehouse for the data necessary  to  efficiently  generate  the  requisite  model (both role and rule)-based relationships. In such",
    "context": "Highlights the need for careful front-end modeling in graph databases as data scales, emphasizing that complex relationships require more design foresight than traditional ERDs and addressing concerns about data quality and storage consistency.",
    "document": "The_Graph_Database_Jack_of_All_Trades_or_Just_Not_SQL.pdf",
    "pages": [
      3
    ],
    "id": "8c8b78b02c543fd0cd735c0f89d48b5ca052174db6c15e4e39ebf78220628782"
  },
  {
    "text": "fashion, if accompanied by a well-designed user interface,  data  entry  becomes  more  straightforward and does not require graph database language expertise on top of subject matter expertise.\nThis  notion  of  hybrid  systems  also  has  the distinct  advantage  of  permitting  some  level  of seamless coupling between various graph databases. Although some query languages, such as Neo4j's Cypher 8  language, are becoming popular, there is little semantic commonality among the  various  graph  languages  in  use  and  their rules  of  syntax.  For  example,  higher  volume graph  databases  rely  on  stylized  variations  of RDF 9  to enumerate their triples. Thus, the notion of sharing data between various graph databases is a function of the user's tolerance for expressing the same triples in differing syntactical frameworks. Needless to say, if cross graph database sharing is required, this concern could burden a beleaguered subject matter expert who must now enter the data as well as master new methods of its expression.\n\nHighlights the importance of user-friendly interfaces and the challenges of seamless data sharing between different graph databases, emphasizing the need for subject matter expertise to navigate varying query languages.",
    "original_text": "fashion, if accompanied by a well-designed user interface,  data  entry  becomes  more  straightforward and does not require graph database language expertise on top of subject matter expertise.\nThis  notion  of  hybrid  systems  also  has  the distinct  advantage  of  permitting  some  level  of seamless coupling between various graph databases. Although some query languages, such as Neo4j's Cypher 8  language, are becoming popular, there is little semantic commonality among the  various  graph  languages  in  use  and  their rules  of  syntax.  For  example,  higher  volume graph  databases  rely  on  stylized  variations  of RDF 9  to enumerate their triples. Thus, the notion of sharing data between various graph databases is a function of the user's tolerance for expressing the same triples in differing syntactical frameworks. Needless to say, if cross graph database sharing is required, this concern could burden a beleaguered subject matter expert who must now enter the data as well as master new methods of its expression.",
    "context": "Highlights the importance of user-friendly interfaces and the challenges of seamless data sharing between different graph databases, emphasizing the need for subject matter expertise to navigate varying query languages.",
    "document": "The_Graph_Database_Jack_of_All_Trades_or_Just_Not_SQL.pdf",
    "pages": [
      4
    ],
    "id": "27cb740f8b66969d38676e66860409629ea5a67c1ae5c090749197613306248a"
  },
  {
    "text": "Although  not  fully  incorporated  in  most  commercial  graph  databases,  the  qualitative  promise will come as metric algorithms, derived from graph  theory,  are  applied  as  analytical  tools. Search  algorithms,  based  on  path  traversalalthough  already  an  important  family  of  graph database  algorithms-are  just  the  beginning  of a  wide  array  of  metrics,  now  extending  beyond mere  networks  to  far  more  daunting  networks of networks. 10 Although research is still increasing  our  understanding  of  the  complex  characteristics of networks of networks, it is becoming evident  that  these  relationships  can  be  demonstrated  mathematically  using  graph  theory.  In time,  built-in  mathematical  functions,  residing in  graph  databases,  could  add  a  level  of  depth to  truly  understanding  and  quantifying  graph relationships. If it is advantageous to eventually design  networks  of  all  types  to  perform  useful purposes, the quantitative aspect of this design cannot be overlooked. For example, as large-scale systems embracing massively embedded software are already known to form complex adaptive relationships, it is doubtful whether large software systems  of  systems  can  be  effectively  evaluated without  some  quantitative  expression  of  their operations.\n\nThe text suggests that graph databases will evolve to incorporate mathematical analysis and metrics derived from graph theory, potentially enabling more sophisticated evaluation of complex network systems.",
    "original_text": "Although  not  fully  incorporated  in  most  commercial  graph  databases,  the  qualitative  promise will come as metric algorithms, derived from graph  theory,  are  applied  as  analytical  tools. Search  algorithms,  based  on  path  traversalalthough  already  an  important  family  of  graph database  algorithms-are  just  the  beginning  of a  wide  array  of  metrics,  now  extending  beyond mere  networks  to  far  more  daunting  networks of networks. 10 Although research is still increasing  our  understanding  of  the  complex  characteristics of networks of networks, it is becoming evident  that  these  relationships  can  be  demonstrated  mathematically  using  graph  theory.  In time,  built-in  mathematical  functions,  residing in  graph  databases,  could  add  a  level  of  depth to  truly  understanding  and  quantifying  graph relationships. If it is advantageous to eventually design  networks  of  all  types  to  perform  useful purposes, the quantitative aspect of this design cannot be overlooked. For example, as large-scale systems embracing massively embedded software are already known to form complex adaptive relationships, it is doubtful whether large software systems  of  systems  can  be  effectively  evaluated without  some  quantitative  expression  of  their operations.",
    "context": "The text suggests that graph databases will evolve to incorporate mathematical analysis and metrics derived from graph theory, potentially enabling more sophisticated evaluation of complex network systems.",
    "document": "The_Graph_Database_Jack_of_All_Trades_or_Just_Not_SQL.pdf",
    "pages": [
      4
    ],
    "id": "020ba830f8e40b809122785610dd2d36e698d1a0e6e71b93b82156aed734a5d9"
  },
  {
    "text": "This  special  issue  explores  the  emergent  world of graph databases in increasing depth. It starts with four articles that establish the dimensions of graph data modeling. We begin with Zuopeng Zhang's 'Graph Databases for Knowledge Management,' which differentiates  between  an RDBMS ERD and a graph data model (GDM). In  'Modeling  Graph  Database  Schema,'  Noa Roy-Hubara, Lior Rokach, Bracha Shapira, and Peretz  Shoval  then  demonstrate  a  technique  to map the ERD to the GDM. As graph databases grow  to  support  full-blown enterprise  knowledge graphs , appropriate graph modeling presents elevated  sophistication  and  rigor.  Jans  Aasman explores  these  challenges  in  'Transmuting  Information to  Knowledge  with  an  Enterprise Knowledge Graph.' Finally, in a specialized use case, 'Modeling XACML Security Policies Using Graph Databases,' Fidel Paniagua Diez, Amrutha Chikkanayakanahalli Vasu, Diego Suárez Touceda,  and  José  María  Sierra  Cámara  demonstrate a  method  to  efficiently  store  eXtensible  Access Control  Markup  Language  (XACML)  security policies using an optimized graph model.\nOur final  article  deals  with  enhancing  graph database performance. Here, we turn our focus to hardware specifically focused on optimal graph database performance. 'High-Performance with an  In-GPU  Graph  Database  Cache,'  by  Shin Morishima  and  Hiroki  Matsutani,  examines GPU efficiency when distributed and optimized for performance.\nW e  commend these excellent articles to you  for  the  awareness  they  build  regarding the state of the art in graph databases. We look forward to a growing trend in such insightful articles as the still young graph database industry continues to mature.\n\nEstablishes the foundational concepts of graph data modeling, contrasting them with relational database models and outlining a technique for mapping ERDs to graph data models.",
    "original_text": "This  special  issue  explores  the  emergent  world of graph databases in increasing depth. It starts with four articles that establish the dimensions of graph data modeling. We begin with Zuopeng Zhang's 'Graph Databases for Knowledge Management,' which differentiates  between  an RDBMS ERD and a graph data model (GDM). In  'Modeling  Graph  Database  Schema,'  Noa Roy-Hubara, Lior Rokach, Bracha Shapira, and Peretz  Shoval  then  demonstrate  a  technique  to map the ERD to the GDM. As graph databases grow  to  support  full-blown enterprise  knowledge graphs , appropriate graph modeling presents elevated  sophistication  and  rigor.  Jans  Aasman explores  these  challenges  in  'Transmuting  Information to  Knowledge  with  an  Enterprise Knowledge Graph.' Finally, in a specialized use case, 'Modeling XACML Security Policies Using Graph Databases,' Fidel Paniagua Diez, Amrutha Chikkanayakanahalli Vasu, Diego Suárez Touceda,  and  José  María  Sierra  Cámara  demonstrate a  method  to  efficiently  store  eXtensible  Access Control  Markup  Language  (XACML)  security policies using an optimized graph model.\nOur final  article  deals  with  enhancing  graph database performance. Here, we turn our focus to hardware specifically focused on optimal graph database performance. 'High-Performance with an  In-GPU  Graph  Database  Cache,'  by  Shin Morishima  and  Hiroki  Matsutani,  examines GPU efficiency when distributed and optimized for performance.\nW e  commend these excellent articles to you  for  the  awareness  they  build  regarding the state of the art in graph databases. We look forward to a growing trend in such insightful articles as the still young graph database industry continues to mature.",
    "context": "Establishes the foundational concepts of graph data modeling, contrasting them with relational database models and outlining a technique for mapping ERDs to graph data models.",
    "document": "The_Graph_Database_Jack_of_All_Trades_or_Just_Not_SQL.pdf",
    "pages": [
      4
    ],
    "id": "64eccf5ba55841d9dc108a3ecd0335706a3a9254b4d69b2deafb3a066cc935a5"
  },
  {
    "text": "1. B. Bollobas, Modern Graph Theory , Springer, 1998.\n2. A.L. Barabási, Linked: The New Science of Networks , 1st ed., Perseus Books Group, 2002.\n3.  D. Kronke and K. Dolan, Database Processing , 3rd ed., Scientific Research Associates, 1998, pp. 19-20.\n4. N. Jatana  et  al.,  'A  Survey  and  Comparison  of  Relational  and  Non-Relational  Database,' Int'l  J.  Eng., Research & Technology , vol. 1, no. 6, 2012, pp. 1-5.\n5.  J.D.  Owens  et  al.,  'GPU  Computing,' Proc.  IEEE , vol. 96, no. 5, 2008, pp. 879-899.\n6.  S.R. Sukumar and N. Bond, 'Mining Large Heterogeneous  Graphs  Using  Cray's  Urika,' Proc.  ORNL Computational Data Analytics Workshop , 2013.\n7.  J.  Huan  et  al.,  'Spin:  Mining  Maximal  Frequent Subgraphs from Graph Databases,' Proc.  10th  ACM SIGKDD Int'l Conf. Knowledge Discovery and Data Mining , 2004, pp. 581-586.\n8. I.  Robinson,  J.  Webber,  and  E.  Eifrenm, Graph  Databases, New Opportunities for Connected Data , 2nd ed., O'Riley Media, 2015.\n9.  R.  Angles  and  G.  Gutierrez,  'Querying  RDF  Data from a Graph Database Perspective,' Proc.  European Semantic Web Conf. , 2005, pp. 346-360.\n10.  S. Boccaletti et al., 'The Structure and Dynamics of Multilayer Networks,' Physics Reports, vol. 544, no. 1, 2014, pp. 1-122.\nGeorge F. Hurlburt is the chief scientist at STEMCorp, a nonprofit corporation that works in the public sector to further economic development via adoption of network science to advance autonomous technologies as useful tools for human use. Contact him at ghurlburt@change-index.com.\nGeorge K. Thiruvathukal is a full professor of computer science at Loyola University Chicago and visiting computer scientist at Argonne National Laboratory. His research interests include parallel and distributed computing, software engineering,  history  of  computing,  and  interdisciplinary computing applications. Contact him at gkt@cs.luc.edu.\nMaria R. Lee is  a  full  professor  of  information  technology and management at Shih Chien University, Taiwan, and  a  visiting  professor  at  the  Advanced  Data  Analytics (ADA) Lab at SooChow University, China. Her research interests include big data analytics, e-commerce, and artificial  intelligence.  Lee  received  a  PhD  in  computer  science and engineering from the University of New South Wales, Australia. Contact her at maria.lee@g2.usc.edu.tw.\n\nLists of relevant academic publications and research papers related to database technologies and graph databases.",
    "original_text": "1. B. Bollobas, Modern Graph Theory , Springer, 1998.\n2. A.L. Barabási, Linked: The New Science of Networks , 1st ed., Perseus Books Group, 2002.\n3.  D. Kronke and K. Dolan, Database Processing , 3rd ed., Scientific Research Associates, 1998, pp. 19-20.\n4. N. Jatana  et  al.,  'A  Survey  and  Comparison  of  Relational  and  Non-Relational  Database,' Int'l  J.  Eng., Research & Technology , vol. 1, no. 6, 2012, pp. 1-5.\n5.  J.D.  Owens  et  al.,  'GPU  Computing,' Proc.  IEEE , vol. 96, no. 5, 2008, pp. 879-899.\n6.  S.R. Sukumar and N. Bond, 'Mining Large Heterogeneous  Graphs  Using  Cray's  Urika,' Proc.  ORNL Computational Data Analytics Workshop , 2013.\n7.  J.  Huan  et  al.,  'Spin:  Mining  Maximal  Frequent Subgraphs from Graph Databases,' Proc.  10th  ACM SIGKDD Int'l Conf. Knowledge Discovery and Data Mining , 2004, pp. 581-586.\n8. I.  Robinson,  J.  Webber,  and  E.  Eifrenm, Graph  Databases, New Opportunities for Connected Data , 2nd ed., O'Riley Media, 2015.\n9.  R.  Angles  and  G.  Gutierrez,  'Querying  RDF  Data from a Graph Database Perspective,' Proc.  European Semantic Web Conf. , 2005, pp. 346-360.\n10.  S. Boccaletti et al., 'The Structure and Dynamics of Multilayer Networks,' Physics Reports, vol. 544, no. 1, 2014, pp. 1-122.\nGeorge F. Hurlburt is the chief scientist at STEMCorp, a nonprofit corporation that works in the public sector to further economic development via adoption of network science to advance autonomous technologies as useful tools for human use. Contact him at ghurlburt@change-index.com.\nGeorge K. Thiruvathukal is a full professor of computer science at Loyola University Chicago and visiting computer scientist at Argonne National Laboratory. His research interests include parallel and distributed computing, software engineering,  history  of  computing,  and  interdisciplinary computing applications. Contact him at gkt@cs.luc.edu.\nMaria R. Lee is  a  full  professor  of  information  technology and management at Shih Chien University, Taiwan, and  a  visiting  professor  at  the  Advanced  Data  Analytics (ADA) Lab at SooChow University, China. Her research interests include big data analytics, e-commerce, and artificial  intelligence.  Lee  received  a  PhD  in  computer  science and engineering from the University of New South Wales, Australia. Contact her at maria.lee@g2.usc.edu.tw.",
    "context": "Lists of relevant academic publications and research papers related to database technologies and graph databases.",
    "document": "The_Graph_Database_Jack_of_All_Trades_or_Just_Not_SQL.pdf",
    "pages": [
      4,
      5
    ],
    "id": "d14743b24b8026362cdd00e691f6844f44361352eae839216c5bfabe5118c572"
  },
  {
    "text": "IT Professional seeks original submissions on technology solutions for the enterprise. Topics include\n- emerging technologies,\n- social software,\n- cloud computing,\n- Web 2.0 and services,\n- cybersecurity,\n- mobile computing,\n- green IT,\n- data management and mining,\n- systems integration,\n- communication networks,\n- datacenter operations,\n- IT asset management, and\n- RFID,\n- health information technology.\nWe welcome articles accompanied by web-based demos. For more information, see our author guidelines at www.computer.org/itpro/author.htm.\n\nRestates the document's call for original submissions on enterprise technology solutions.",
    "original_text": "IT Professional seeks original submissions on technology solutions for the enterprise. Topics include\n- emerging technologies,\n- social software,\n- cloud computing,\n- Web 2.0 and services,\n- cybersecurity,\n- mobile computing,\n- green IT,\n- data management and mining,\n- systems integration,\n- communication networks,\n- datacenter operations,\n- IT asset management, and\n- RFID,\n- health information technology.\nWe welcome articles accompanied by web-based demos. For more information, see our author guidelines at www.computer.org/itpro/author.htm.",
    "context": "Restates the document's call for original submissions on enterprise technology solutions.",
    "document": "The_Graph_Database_Jack_of_All_Trades_or_Just_Not_SQL.pdf",
    "pages": [
      5
    ],
    "id": "13a0edf0a592a25dd0d9968eb0cba7f21cb6ab6b54bf9d673d3818e565954e7e"
  },
  {
    "text": "Received 29 September 2023, accepted 20 October 2023, date of publication 23 October 2023, date of current version 1 November 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3327131\n\nProvides publication details and funding acknowledgements for a study on land consolidation in rural revitalization.",
    "original_text": "Received 29 September 2023, accepted 20 October 2023, date of publication 23 October 2023, date of current version 1 November 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3327131",
    "context": "Provides publication details and funding acknowledgements for a study on land consolidation in rural revitalization.",
    "document": "The_Application_of_the_SOFM_Neural_Network_and_Internet_of_Things_in_Rural_Revitalization.pdf",
    "pages": [
      1
    ],
    "id": "2337b45df9c57b706586a7e0e4a53c2b136a1f7c3d327ac14ddb58bdde91e028"
  },
  {
    "text": "1 College of Ecology and Environment, Nanjing Forestry University, Nanjing 210037, China\n- 2 School of Economics and Management, Nanchang Institute of Science and Technology, Nanchang 330108, China\n- 3 School of Marxism, Hefei Vocational College of Finance and Economics, Hefei 230601, China\n4 College of Humanities and Social Sciences, Nanjing Forestry University, Nanjing 210037, China\n5 School of Digital Economy and Trade, Wenzhou Polytechnic, Wenzhou 325035, China\nCorresponding author: Luote Dai (luote26@wzpt.edu.cn)\nThis study was supported by the key project of Humanities and Social Sciences (SK2021A0776); the Anhui Philosophy and Social Science planning Project (AHSKY2020D53); the Social Sciences of Jiangxi Province 14th Five-Year Plan (2023) Fund Project (23YJ15); the key project of the first Huang Yanpei Vocational Education Thought Research Planning project of China Vocational Education Association ''Huang Yanpei's View of Professional Quality and its Contemporary Value'' (ZJS2022Zd50); Anhui Vocational and Adult Education Association Education and teaching research planning project ''Vocational education role positioning in rural revitalization strategy'' (Azcj2021187) as well as Hefei Vocational College of Finance and Economics Humanities and social science key project (XZK202103R). This work was also supported by Wenzhou Philosophy and Social Science Planning Project (23QN14YB).\nABSTRACT To promote rural revitalization under the premise of fully considering the ecological risk factors of land consolidation, this study takes A County in Shaanxi Province as a case study and introduces Self-Organizing Feature Map (SOFM) neural network and Internet of Things (IoT) technology to partition the land. In this study, a comprehensive index system is constructed based on IoT technology, and the relevant factors are quantitatively analyzed from the perspective of land consolidation ecological risk. Then, the land consolidation project area's attribute and geographic space domains are used as the input of the SOFM neural network to reveal the distribution and influence degree of each factor in the area and determine the zoning pattern of land consolidation in A County. The results show that the rural revitalization land zoning pattern of land consolidation in A County, Shaanxi Province, is divided into four land consolidation areas. Firstly, the priority remediation area soon covers 27 administrative villages with a total area of 28090 hm 2 , accounting for 32.75%. Secondly, the moderate renovation area is soon classified into 37 administrative villages, with a total area of 15986 hm 2 , equivalent to 18.55%. In the medium term, the land-saving renovation area covers 39 administrative villages with a total area of 19686 hm 2 , accounting for 22.75% of the total area. Finally, in the long-term restricted remediation area, 37 administrative villages are divided, with a total area of 22081 hm 2 , accounting for 25.67% of the total area. These data results provide a quantitative basis for land consolidation planning in this area to achieve the goal of rural land revitalization in different time ranges. This study is significant for implementing land consolidation projects in rural revitalization and provides a valuable reference for developing other similar areas.\nINDEX TERMS SOFM neural network, Internet of Things technology, digital finance, rural revitalization, land allocation.\n\nDetails of the study's funding sources and institutional affiliations are provided, outlining the research's support and the researchers involved.",
    "original_text": "1 College of Ecology and Environment, Nanjing Forestry University, Nanjing 210037, China\n- 2 School of Economics and Management, Nanchang Institute of Science and Technology, Nanchang 330108, China\n- 3 School of Marxism, Hefei Vocational College of Finance and Economics, Hefei 230601, China\n4 College of Humanities and Social Sciences, Nanjing Forestry University, Nanjing 210037, China\n5 School of Digital Economy and Trade, Wenzhou Polytechnic, Wenzhou 325035, China\nCorresponding author: Luote Dai (luote26@wzpt.edu.cn)\nThis study was supported by the key project of Humanities and Social Sciences (SK2021A0776); the Anhui Philosophy and Social Science planning Project (AHSKY2020D53); the Social Sciences of Jiangxi Province 14th Five-Year Plan (2023) Fund Project (23YJ15); the key project of the first Huang Yanpei Vocational Education Thought Research Planning project of China Vocational Education Association ''Huang Yanpei's View of Professional Quality and its Contemporary Value'' (ZJS2022Zd50); Anhui Vocational and Adult Education Association Education and teaching research planning project ''Vocational education role positioning in rural revitalization strategy'' (Azcj2021187) as well as Hefei Vocational College of Finance and Economics Humanities and social science key project (XZK202103R). This work was also supported by Wenzhou Philosophy and Social Science Planning Project (23QN14YB).\nABSTRACT To promote rural revitalization under the premise of fully considering the ecological risk factors of land consolidation, this study takes A County in Shaanxi Province as a case study and introduces Self-Organizing Feature Map (SOFM) neural network and Internet of Things (IoT) technology to partition the land. In this study, a comprehensive index system is constructed based on IoT technology, and the relevant factors are quantitatively analyzed from the perspective of land consolidation ecological risk. Then, the land consolidation project area's attribute and geographic space domains are used as the input of the SOFM neural network to reveal the distribution and influence degree of each factor in the area and determine the zoning pattern of land consolidation in A County. The results show that the rural revitalization land zoning pattern of land consolidation in A County, Shaanxi Province, is divided into four land consolidation areas. Firstly, the priority remediation area soon covers 27 administrative villages with a total area of 28090 hm 2 , accounting for 32.75%. Secondly, the moderate renovation area is soon classified into 37 administrative villages, with a total area of 15986 hm 2 , equivalent to 18.55%. In the medium term, the land-saving renovation area covers 39 administrative villages with a total area of 19686 hm 2 , accounting for 22.75% of the total area. Finally, in the long-term restricted remediation area, 37 administrative villages are divided, with a total area of 22081 hm 2 , accounting for 25.67% of the total area. These data results provide a quantitative basis for land consolidation planning in this area to achieve the goal of rural land revitalization in different time ranges. This study is significant for implementing land consolidation projects in rural revitalization and provides a valuable reference for developing other similar areas.\nINDEX TERMS SOFM neural network, Internet of Things technology, digital finance, rural revitalization, land allocation.",
    "context": "Details of the study's funding sources and institutional affiliations are provided, outlining the research's support and the researchers involved.",
    "document": "The_Application_of_the_SOFM_Neural_Network_and_Internet_of_Things_in_Rural_Revitalization.pdf",
    "pages": [
      1
    ],
    "id": "f8fc15acc241b7287153b461ea5e85f6c032a55a5b0d65be46bfe06873b6bb53"
  },
  {
    "text": "Rural revitalization holds a crucial position in China's national development strategy. However, the process of rural revitalization is accompanied by a series of complex issues and challenges, including imbalanced regional development and uneven resource allocation [1]. Moreover, the tradi-\nThe associate editor coordinating the review of this manuscript and approving it for publication was Abdel-Hamid Soliman .\nVOLUME 11, 2023\ntional financial system and land consolidation methods have limitations in meeting the demands of rural revitalization. Therefore, the motivation of this study is to explore how digital finance and Internet of Things (IoT) technology can better support rural revitalization to address the following issues. Firstly, rural revitalization faces serious challenges regarding sustainable development and ecological conservation. Traditional land consolidation methods often struggle to consider ecological risk factors rationally, which can easily lead to environmental degradation. Hence, there is a need to find a\nmethod that can more accurately quantify and control ecological risks. Secondly, rural revitalization requires more refined land allocation and resource management. Digital finance and IoT technology have significant potential to provide real-time data and intelligent decision support, aiding in precise land resource allocation and efficient utilization. Applying digital finance and IoT technology can improve the coverage and convenience of rural financial services and promote rural economic development, thereby enhancing the sense of achievement and happiness among rural residents [2].\nThe Self-Organizing Feature Map (SOFM) neural network is an artificial neural network model used for unsupervised learning, and its application context spans multiple domains and problems. In the era of big data, high-dimensional datasets have become increasingly common. The SOFM neural network is widely employed to map high-dimensional data into lower-dimensional spaces for visualization and analysis, making them highly useful in data mining, exploration, and visualization.In fields such as image recognition, computer vision, and medical image processing, SOFM can assist in understanding and processing complex image data. In pattern recognition, the SOFM network can identify and classify patterns, encompassing applications in speech recognition, handwritten character recognition, bioinformatics, and more. Additionally, the SOFM network finds utility in financial market data analysis, such as stock price prediction, portfolio optimization, and market trend analysis. It can aid in discovering underlying correlations and patterns within data [3]. In summary, the application scope of the SOFM neural network covers a wide range of fields. Its capabilities in handling complex data, pattern recognition, data dimensionality reduction, and clustering tasks have gained widespread recognition, providing powerful tools for addressing problems across various domains.\nIoT is a technology that connects various physical devices, sensors, and objects to the internet for data collection, analysis, and remote control. In rural revitalization, IoT technology plays a crucial role and can help address various issues in rural development, such as resource management, agricultural production, and environmental monitoring. IoT technology can monitor agricultural data such as soil humidity, weather conditions, and crop growth status. This aids in precise agricultural management, involving field irrigation, fertilization, pest monitoring, and more, ultimately improving crop quality and yields.\nTherefore, based on the SOFM neural network and IoT technology, this study aims to realize the intelligent application of digital finance in rural revitalization by considering the ecological risk factors of land consolidation. This study seeks to provide new solutions and insights for rural development in rural revitalization.\n\nAddresses challenges in rural revitalization by exploring how digital finance and IoT technology can better support land consolidation, considering ecological risk factors. Highlights the need for more refined land allocation and resource management, and introduces the SOFM neural network and IoT technology as potential solutions.",
    "original_text": "Rural revitalization holds a crucial position in China's national development strategy. However, the process of rural revitalization is accompanied by a series of complex issues and challenges, including imbalanced regional development and uneven resource allocation [1]. Moreover, the tradi-\nThe associate editor coordinating the review of this manuscript and approving it for publication was Abdel-Hamid Soliman .\nVOLUME 11, 2023\ntional financial system and land consolidation methods have limitations in meeting the demands of rural revitalization. Therefore, the motivation of this study is to explore how digital finance and Internet of Things (IoT) technology can better support rural revitalization to address the following issues. Firstly, rural revitalization faces serious challenges regarding sustainable development and ecological conservation. Traditional land consolidation methods often struggle to consider ecological risk factors rationally, which can easily lead to environmental degradation. Hence, there is a need to find a\nmethod that can more accurately quantify and control ecological risks. Secondly, rural revitalization requires more refined land allocation and resource management. Digital finance and IoT technology have significant potential to provide real-time data and intelligent decision support, aiding in precise land resource allocation and efficient utilization. Applying digital finance and IoT technology can improve the coverage and convenience of rural financial services and promote rural economic development, thereby enhancing the sense of achievement and happiness among rural residents [2].\nThe Self-Organizing Feature Map (SOFM) neural network is an artificial neural network model used for unsupervised learning, and its application context spans multiple domains and problems. In the era of big data, high-dimensional datasets have become increasingly common. The SOFM neural network is widely employed to map high-dimensional data into lower-dimensional spaces for visualization and analysis, making them highly useful in data mining, exploration, and visualization.In fields such as image recognition, computer vision, and medical image processing, SOFM can assist in understanding and processing complex image data. In pattern recognition, the SOFM network can identify and classify patterns, encompassing applications in speech recognition, handwritten character recognition, bioinformatics, and more. Additionally, the SOFM network finds utility in financial market data analysis, such as stock price prediction, portfolio optimization, and market trend analysis. It can aid in discovering underlying correlations and patterns within data [3]. In summary, the application scope of the SOFM neural network covers a wide range of fields. Its capabilities in handling complex data, pattern recognition, data dimensionality reduction, and clustering tasks have gained widespread recognition, providing powerful tools for addressing problems across various domains.\nIoT is a technology that connects various physical devices, sensors, and objects to the internet for data collection, analysis, and remote control. In rural revitalization, IoT technology plays a crucial role and can help address various issues in rural development, such as resource management, agricultural production, and environmental monitoring. IoT technology can monitor agricultural data such as soil humidity, weather conditions, and crop growth status. This aids in precise agricultural management, involving field irrigation, fertilization, pest monitoring, and more, ultimately improving crop quality and yields.\nTherefore, based on the SOFM neural network and IoT technology, this study aims to realize the intelligent application of digital finance in rural revitalization by considering the ecological risk factors of land consolidation. This study seeks to provide new solutions and insights for rural development in rural revitalization.",
    "context": "Addresses challenges in rural revitalization by exploring how digital finance and IoT technology can better support land consolidation, considering ecological risk factors. Highlights the need for more refined land allocation and resource management, and introduces the SOFM neural network and IoT technology as potential solutions.",
    "document": "The_Application_of_the_SOFM_Neural_Network_and_Internet_of_Things_in_Rural_Revitalization.pdf",
    "pages": [
      1,
      2
    ],
    "id": "531bcc198b1c7e747ede272da846fff9e2daae703d080466354b5bb690b35853"
  },
  {
    "text": "This study explores how sustainable rural revitalization can be achieved through the guidance of digital finance and IoT\ntechnology, utilizing the SOFM neural network. Specifically, it focuses on A County in Shaanxi Province, using land consolidation as an entry point. It constructs a comprehensive indicator system from the perspective of ecological risks to quantitatively analyze key factors in the land consolidation project. The research objectives of this study are as follows. 1. Throughquantitative analysis, the extent of ecological risk impact on the land consolidation project in A County, Shaanxi Province, is revealed. 2. Using the SOFM neural network method and combining the attribute and geographical space domains of the land consolidation project area, the land use for rural revitalization in A County, Shaanxi Province, is delineated.\n\nThis chunk outlines the study’s methodology and objectives, specifically detailing the use of a SOFM neural network to analyze ecological risks and delineate land use patterns for rural revitalization in A County, Shaanxi Province. It highlights the goal of quantifying ecological impact and creating a land use plan based on this analysis.",
    "original_text": "This study explores how sustainable rural revitalization can be achieved through the guidance of digital finance and IoT\ntechnology, utilizing the SOFM neural network. Specifically, it focuses on A County in Shaanxi Province, using land consolidation as an entry point. It constructs a comprehensive indicator system from the perspective of ecological risks to quantitatively analyze key factors in the land consolidation project. The research objectives of this study are as follows. 1. Throughquantitative analysis, the extent of ecological risk impact on the land consolidation project in A County, Shaanxi Province, is revealed. 2. Using the SOFM neural network method and combining the attribute and geographical space domains of the land consolidation project area, the land use for rural revitalization in A County, Shaanxi Province, is delineated.",
    "context": "This chunk outlines the study’s methodology and objectives, specifically detailing the use of a SOFM neural network to analyze ecological risks and delineate land use patterns for rural revitalization in A County, Shaanxi Province. It highlights the goal of quantifying ecological impact and creating a land use plan based on this analysis.",
    "document": "The_Application_of_the_SOFM_Neural_Network_and_Internet_of_Things_in_Rural_Revitalization.pdf",
    "pages": [
      2
    ],
    "id": "9a91c5c96b489330f0892124d5de0a2a5b8515d0f6c08fcf684612316abad513"
  },
  {
    "text": "Many researchers and scholars have discussed this. Kusadokoro and Chitose used county panel data to evaluate the impact of road infrastructure development in Inner Mongolia in China on regional economic growth and urban-rural income inequality from 1999 to 2018. The results showed that the number of road infrastructure in Inner Mongolia had a strong positive impact on economic growth, but a strong negative impact on urban-rural income inequality at the county level [4]. Qu et al. employed large-scale remote sensing data, a topographic fluctuation range model, and a spatial econometric model to analyze the multifunctional change and its dynamic mechanism of gully agriculture in the Loess Plateau to understand the internal meaning of evolution and differentiation at the basin level. The results indicated that many key policies in the Loess Plateau would directly affect the evolution path of functions, thus providing policy ideas for the high-quality development of agriculture in the Loess Plateau [5]. Luo et al., taking the Shenzhen-Shantou Special Cooperation Zone (SSSCZ) as an example, constructed a runoff model based on SSSCZ and a rainstorm and flood model based on Geographic Information System (GIS). They discussed the spatial distribution characteristics of rainstorms and flood risks lasting for 100 years. It was found that applyingthe neural network method to urban planning and construction can comprehensively consider waterlogging, ecology, population, and so on [6].Jia et al. proposed a more powerful multi-swarm artificial bee colony algorithm (ABCA). The classic ABCA was improved in the algorithm, and multi-group and exclusive operation strategies were adopted to make it suitable for the optimal parameter setting tracking of the SOFM network so that this network could be applied to a dynamic environment. The results denoted that the algorithm was superior to the classical SOFM algorithm in clustering purity and effectiveness. It was a promising classification method for dynamic environment data streams [7]. To improve autonomous vehicles' handling performance and stability, a coordinated control strategy based on stability judgment was proposed by Yao et al. [8]. Firstly, the stability judgment scheme based on the SOFM neural network and K-Means algorithm was adopted to evaluate the real-time stability level of vehicles. The results suggested that the\nstability judgment scheme and coordinated control strategy of the algorithm could meet the requirements of path-tracking accuracy and enhance its processing and stability [8].\nThese studies deeply analyze different aspects of rural revitalization and provide an essential theoretical basis for future development and policy formulation. Machine learning algorithms such as SOFM neural network and multi-swarm ABCA, as well as the application of IoT technology, provide a new way for intelligent and accurate decision-making in rural revitalization and construction. These technologies not only optimize the allocation of resources and improve the efficiency of decision-making but also better meet the diverse needs of rural residents and promote the development of rural revitalization in a more sustainable and dynamic direction. It is necessary to tap further the potential of machine learning algorithms and IoT technology, and customize intelligent rural revitalization strategies according to actual needs to inject new impetus into the development of rural revitalization.\n\nProvides a review of relevant studies on rural revitalization strategies, including the impact of infrastructure development, remote sensing analysis, and urban planning techniques.",
    "original_text": "Many researchers and scholars have discussed this. Kusadokoro and Chitose used county panel data to evaluate the impact of road infrastructure development in Inner Mongolia in China on regional economic growth and urban-rural income inequality from 1999 to 2018. The results showed that the number of road infrastructure in Inner Mongolia had a strong positive impact on economic growth, but a strong negative impact on urban-rural income inequality at the county level [4]. Qu et al. employed large-scale remote sensing data, a topographic fluctuation range model, and a spatial econometric model to analyze the multifunctional change and its dynamic mechanism of gully agriculture in the Loess Plateau to understand the internal meaning of evolution and differentiation at the basin level. The results indicated that many key policies in the Loess Plateau would directly affect the evolution path of functions, thus providing policy ideas for the high-quality development of agriculture in the Loess Plateau [5]. Luo et al., taking the Shenzhen-Shantou Special Cooperation Zone (SSSCZ) as an example, constructed a runoff model based on SSSCZ and a rainstorm and flood model based on Geographic Information System (GIS). They discussed the spatial distribution characteristics of rainstorms and flood risks lasting for 100 years. It was found that applyingthe neural network method to urban planning and construction can comprehensively consider waterlogging, ecology, population, and so on [6].Jia et al. proposed a more powerful multi-swarm artificial bee colony algorithm (ABCA). The classic ABCA was improved in the algorithm, and multi-group and exclusive operation strategies were adopted to make it suitable for the optimal parameter setting tracking of the SOFM network so that this network could be applied to a dynamic environment. The results denoted that the algorithm was superior to the classical SOFM algorithm in clustering purity and effectiveness. It was a promising classification method for dynamic environment data streams [7]. To improve autonomous vehicles' handling performance and stability, a coordinated control strategy based on stability judgment was proposed by Yao et al. [8]. Firstly, the stability judgment scheme based on the SOFM neural network and K-Means algorithm was adopted to evaluate the real-time stability level of vehicles. The results suggested that the\nstability judgment scheme and coordinated control strategy of the algorithm could meet the requirements of path-tracking accuracy and enhance its processing and stability [8].\nThese studies deeply analyze different aspects of rural revitalization and provide an essential theoretical basis for future development and policy formulation. Machine learning algorithms such as SOFM neural network and multi-swarm ABCA, as well as the application of IoT technology, provide a new way for intelligent and accurate decision-making in rural revitalization and construction. These technologies not only optimize the allocation of resources and improve the efficiency of decision-making but also better meet the diverse needs of rural residents and promote the development of rural revitalization in a more sustainable and dynamic direction. It is necessary to tap further the potential of machine learning algorithms and IoT technology, and customize intelligent rural revitalization strategies according to actual needs to inject new impetus into the development of rural revitalization.",
    "context": "Provides a review of relevant studies on rural revitalization strategies, including the impact of infrastructure development, remote sensing analysis, and urban planning techniques.",
    "document": "The_Application_of_the_SOFM_Neural_Network_and_Internet_of_Things_in_Rural_Revitalization.pdf",
    "pages": [
      2,
      3
    ],
    "id": "9f17f357e053729ce8f979ff41af5a1a1983650dedd007277db4fde3509b7524"
  },
  {
    "text": "To achieve the goal of rural revitalization and development based on digital finance and IoT technology, the SOFM neural network is adopted as the core method [9].The SOFM neural network, with unsupervised learning characteristics, offers unique advantages for analyzing complex spatial relationships in rural areas. This network can preserve the topological structure of data, meaning it retains the spatial relationships between data points, which is crucial for land consolidation and resource management. For instance, when planning land use in rural areas, SOFM can help identify key information such as land types, land quality, and land-use history and integrate them to formulate more effective land consolidation strategies. Additionally, SOFM can achieve data dimensionality reduction, compressing large datasets into more manageable forms, thus reducing computational and time costs. Furthermore, IoT technology provides rich sources of data for digital finance research. IoT devices can gather information about rural areas through real-time data collection and transmission, such as meteorological data, soil moisture, crop growth conditions, etc. These data can be used for intelligent decision-making, such as precise irrigation and fertilization, and support monitoring and early warning systems, helping farmers better manage agricultural production.\nThe SOFM neural network is an unsupervised learning algorithm capable of transforming high-dimensional data into a two-dimensional grid structure through self-organizing mapping of input data, thus realizing data clustering and visual analysis [10], [11], [12]. The SOFM neural network operates on the foundational principle of competitive learning, structured around input and competitive layers. The competitive layer is usually a two-dimensional grid, and each node represents the weight of a feature vector [13]. During network training, the input data is compared with the weight vector in the competition layer, and the best-matching neuron is selected as the winning neuron. Subsequently, the topolog-\nical mapping of the input data is realized by updating the weights of the winning neuron and its surrounding neurons [14], [15], [16].The research method roadmap is illustrated in Figure 1.\nFirstly, the SOFM neural network model starts competitive learning by accepting input samples. In this process, samples with similar functional attributes exhibit a similar trend in spatial distribution, while samples with large differences in functional attributes keep a certain interval in space. This principle makes it possible to cluster irregular input samples automatically [17]. When the number of input samples is sufficient, its probability density can be approximately regarded as the weight distribution, which can be expressed in the neurons of the output layer of the neural network. This results in that all samples with large probability density will be gathered in a specific area in the output space [18], [19], [20], [21].\nRural revitalization projects can benefit by combining the SOFM neural network with IoT technology. The richness of data allows decision-makers to gain better insights into the conditions of rural areas, enabling them to formulate policies and plans that are more targeted. The automation and analytical capabilities streamline the decision-making process, reducing the workload associated with manual data processing. Most importantly, real-time capabilities enable decisions to respond more rapidly to changing situations, such as sudden weather variations or disaster events, thereby reducing potential risks.\nFIGURE 1. Roadmap of research methods.\nThe topology of the SOFM neural network is presented in Figure 2.\nFIGURE 2. Topological structure of SOFM neural network.\n\nHighlights the SOFM neural network’s role in analyzing spatial relationships for land consolidation and resource management, emphasizing its data dimensionality reduction capabilities and the value of IoT data for digital finance research.",
    "original_text": "To achieve the goal of rural revitalization and development based on digital finance and IoT technology, the SOFM neural network is adopted as the core method [9].The SOFM neural network, with unsupervised learning characteristics, offers unique advantages for analyzing complex spatial relationships in rural areas. This network can preserve the topological structure of data, meaning it retains the spatial relationships between data points, which is crucial for land consolidation and resource management. For instance, when planning land use in rural areas, SOFM can help identify key information such as land types, land quality, and land-use history and integrate them to formulate more effective land consolidation strategies. Additionally, SOFM can achieve data dimensionality reduction, compressing large datasets into more manageable forms, thus reducing computational and time costs. Furthermore, IoT technology provides rich sources of data for digital finance research. IoT devices can gather information about rural areas through real-time data collection and transmission, such as meteorological data, soil moisture, crop growth conditions, etc. These data can be used for intelligent decision-making, such as precise irrigation and fertilization, and support monitoring and early warning systems, helping farmers better manage agricultural production.\nThe SOFM neural network is an unsupervised learning algorithm capable of transforming high-dimensional data into a two-dimensional grid structure through self-organizing mapping of input data, thus realizing data clustering and visual analysis [10], [11], [12]. The SOFM neural network operates on the foundational principle of competitive learning, structured around input and competitive layers. The competitive layer is usually a two-dimensional grid, and each node represents the weight of a feature vector [13]. During network training, the input data is compared with the weight vector in the competition layer, and the best-matching neuron is selected as the winning neuron. Subsequently, the topolog-\nical mapping of the input data is realized by updating the weights of the winning neuron and its surrounding neurons [14], [15], [16].The research method roadmap is illustrated in Figure 1.\nFirstly, the SOFM neural network model starts competitive learning by accepting input samples. In this process, samples with similar functional attributes exhibit a similar trend in spatial distribution, while samples with large differences in functional attributes keep a certain interval in space. This principle makes it possible to cluster irregular input samples automatically [17]. When the number of input samples is sufficient, its probability density can be approximately regarded as the weight distribution, which can be expressed in the neurons of the output layer of the neural network. This results in that all samples with large probability density will be gathered in a specific area in the output space [18], [19], [20], [21].\nRural revitalization projects can benefit by combining the SOFM neural network with IoT technology. The richness of data allows decision-makers to gain better insights into the conditions of rural areas, enabling them to formulate policies and plans that are more targeted. The automation and analytical capabilities streamline the decision-making process, reducing the workload associated with manual data processing. Most importantly, real-time capabilities enable decisions to respond more rapidly to changing situations, such as sudden weather variations or disaster events, thereby reducing potential risks.\nFIGURE 1. Roadmap of research methods.\nThe topology of the SOFM neural network is presented in Figure 2.\nFIGURE 2. Topological structure of SOFM neural network.",
    "context": "Highlights the SOFM neural network’s role in analyzing spatial relationships for land consolidation and resource management, emphasizing its data dimensionality reduction capabilities and the value of IoT data for digital finance research.",
    "document": "The_Application_of_the_SOFM_Neural_Network_and_Internet_of_Things_in_Rural_Revitalization.pdf",
    "pages": [
      3,
      4
    ],
    "id": "7640bbd52b851ac25f23e20eb5ea81e12b950b63451f5a93648d99319089c8e4"
  },
  {
    "text": "Firstly, the SOFM neural network model is established, the parameter values are set, and then the coordinates of the input data set and the index values of various attribute spaces are dimensionless [22], [23], [24]. Subsequently, this study uses the method of mixed distance to describe the similarity between sampling points [25], [26], [27]. A point set has dual attributes of time and space, and its definition is as follows :\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n{ g 1 N , g 2 N , . . . , g G N } indicates geographical space, and G = 1 , 2 , 3. { a 1 N , a 2 N , . . . , a D N } signifies the attribute space; D denotes the number of attributes; D s ij refers to the size of the geographical space between two points. wd stands for the weight of attribute d . ∑ wd = 1. a d i and a d j represent the value of attribute d in point i and the value of attribute d in point j . ws means the weight of geographical space. wa indicates the weight of attribute space, ws + wa = 1.\nIn the SOFM neural network, the updating rule of neuron weight can be expressed as :\n<!-- formula-not-decoded -->\n1 wji means the weight update, α ( t ) is the learning rate, hij indicates the proximity function, xi refers to the input data; wji represents the neuron weight [28], [29], [30], [31].\nFor attribute space index normalization, the following equation can be used :\n<!-- formula-not-decoded -->\nThe calculation of mixing distance can be written as :\n<!-- formula-not-decoded -->\nDij demonstrates mixed distance; D s ij shows geospatial distance; a d j and a d j indicate attribute values in attribute space; ws and wa are geospatial and attribute space weights, respectively [32], [33], [34], [35]. In self-organizing partition, the spatial distance metric can be represented as :\n<!-- formula-not-decoded -->\nD s ij expresses the geographical distance between two points;( xi , yi ) and ( xj , yj ) represent the coordinates of two points. The proximity function hij can use the Gaussian function to measure the proximity among neurons :\n<!-- formula-not-decoded -->\nDij represents the distance between neurons; σ refers to the diffusion parameter of the Gaussian function [36], [37]. The adjusted partition boundary can be calculated according to the detection results of outliers, and the examples are as follows :\nAdjusted boundary value = Original boundary value\n+ Outlier intensity × Adjustment coefficient (8)\n\nEstablishes the foundational parameters and methodology for the SOFM neural network model, including data normalization and spatial distance calculations – crucial steps in the network’s self-organizing process.",
    "original_text": "Firstly, the SOFM neural network model is established, the parameter values are set, and then the coordinates of the input data set and the index values of various attribute spaces are dimensionless [22], [23], [24]. Subsequently, this study uses the method of mixed distance to describe the similarity between sampling points [25], [26], [27]. A point set has dual attributes of time and space, and its definition is as follows :\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\n{ g 1 N , g 2 N , . . . , g G N } indicates geographical space, and G = 1 , 2 , 3. { a 1 N , a 2 N , . . . , a D N } signifies the attribute space; D denotes the number of attributes; D s ij refers to the size of the geographical space between two points. wd stands for the weight of attribute d . ∑ wd = 1. a d i and a d j represent the value of attribute d in point i and the value of attribute d in point j . ws means the weight of geographical space. wa indicates the weight of attribute space, ws + wa = 1.\nIn the SOFM neural network, the updating rule of neuron weight can be expressed as :\n<!-- formula-not-decoded -->\n1 wji means the weight update, α ( t ) is the learning rate, hij indicates the proximity function, xi refers to the input data; wji represents the neuron weight [28], [29], [30], [31].\nFor attribute space index normalization, the following equation can be used :\n<!-- formula-not-decoded -->\nThe calculation of mixing distance can be written as :\n<!-- formula-not-decoded -->\nDij demonstrates mixed distance; D s ij shows geospatial distance; a d j and a d j indicate attribute values in attribute space; ws and wa are geospatial and attribute space weights, respectively [32], [33], [34], [35]. In self-organizing partition, the spatial distance metric can be represented as :\n<!-- formula-not-decoded -->\nD s ij expresses the geographical distance between two points;( xi , yi ) and ( xj , yj ) represent the coordinates of two points. The proximity function hij can use the Gaussian function to measure the proximity among neurons :\n<!-- formula-not-decoded -->\nDij represents the distance between neurons; σ refers to the diffusion parameter of the Gaussian function [36], [37]. The adjusted partition boundary can be calculated according to the detection results of outliers, and the examples are as follows :\nAdjusted boundary value = Original boundary value\n+ Outlier intensity × Adjustment coefficient (8)",
    "context": "Establishes the foundational parameters and methodology for the SOFM neural network model, including data normalization and spatial distance calculations – crucial steps in the network’s self-organizing process.",
    "document": "The_Application_of_the_SOFM_Neural_Network_and_Internet_of_Things_in_Rural_Revitalization.pdf",
    "pages": [
      4
    ],
    "id": "4319f5214e1f5aa7845f1a0923f0e04537a1758d732c23deb28e412d123d8c29"
  },
  {
    "text": "The existing land consolidation projects in A County of Shaanxi Province are distributed in various towns and villages in this area, involving many factors, and there are complex relationships among them [38]. By constructing a relative risk model, the results of exposure-hazard analysis are characterized, and the relative risk value of each risk community is calculated. The calculation process reads :\n<!-- formula-not-decoded -->\nRSi represents the relative risk value of the i th risk community. j is the source of risk. k means the habitat type. m refers to the ecological receptor type. Sij signifies the density of risk sources. Hik indicates the habitat abundance. Xjk is the exposure coefficient. Ekm expresses the response coefficient [39], [40]. The density of risk sources is the ratio of the area of a risk source in the risk community to the maximum area of this risk source in the risk community. Habitat abundance is the ratio of a habitat area in the risk community to the maximum value of this habitat area in the risk community. The exposure coefficient is the ratio of the area of a risk source in the habitat to the total area of the habitat [41], [42], [43].\nThe risk area contains various potential sources and the corresponding habitat area, as indicated in Figure 3.\nA, B, C, D, E, and F represent six areas in A County of Shaanxi Province, and the habitat area of the risk community in A County of Shaanxi Province is shown in Figure 4.\nThe density of risk sources in each risk community in A County, Shaanxi Province, is displayed in Figure 5.\nThe habitat abundance of each risk community in A County, Shaanxi Province is suggested in Figure 6.\nThis study collected data for the land consolidation project from local government authorities and rural cooperatives in A County, Shaanxi Province. These data encompassed\nFIGURE 3. The risk area includes all kinds of potential risk sources and the corresponding habitat area.\nFIGURE 4. Habitat area of risk community in A County of Shaanxi province.\nFIGURE 5. Density of risk sources in various risk communities in A County, Shaanxi Province.\nhistorical records of land consolidation, project progress, and resource utilization, among other aspects. IoT sensors were deployed within the land consolidation project areas to monitor soil quality, meteorological conditions, water quality, and farmland conditions. These sensors collected real-time\nFIGURE 6. Habitat abundance of various risk communities in A County, Shaanxi Province.\ndata and transmitted it to a data center through networks. Digital financial data was sourced from rural financial institutions, encompassing information about loan disbursements, the usage of financial products, and the financial behaviors of rural residents. These data helped analyze the effectiveness of digital finance in rural revitalization.Land quality indicators comprised soil pH, organic matter content, nutrient levels, etc., which were used to assess soil fertility and suitability. Meteorological data included temperature, humidity, rainfall, etc., for analyzing the impact of climate on farmland yields. Farmland production information involved variables like crop types, growing seasons, yields, etc., to evaluate the agricultural production outcomes of the land consolidation projects. Digital financial indicators covered metrics like financial product adoption rates, the penetration of rural financial services, loan default rates, etc., to assess the contribution of digital finance to rural revitalization. Quantitative analysis employed spatial data analysis methods, involving GIS analysis of geographic data in the land consolidation project areas to study land use patterns and resource allocation.\n\nThis section details a risk assessment model applied to land consolidation projects in A County, Shaanxi Province, utilizing data on risk sources, habitat types, and ecological receptors to quantify the relative risk within different communities.",
    "original_text": "The existing land consolidation projects in A County of Shaanxi Province are distributed in various towns and villages in this area, involving many factors, and there are complex relationships among them [38]. By constructing a relative risk model, the results of exposure-hazard analysis are characterized, and the relative risk value of each risk community is calculated. The calculation process reads :\n<!-- formula-not-decoded -->\nRSi represents the relative risk value of the i th risk community. j is the source of risk. k means the habitat type. m refers to the ecological receptor type. Sij signifies the density of risk sources. Hik indicates the habitat abundance. Xjk is the exposure coefficient. Ekm expresses the response coefficient [39], [40]. The density of risk sources is the ratio of the area of a risk source in the risk community to the maximum area of this risk source in the risk community. Habitat abundance is the ratio of a habitat area in the risk community to the maximum value of this habitat area in the risk community. The exposure coefficient is the ratio of the area of a risk source in the habitat to the total area of the habitat [41], [42], [43].\nThe risk area contains various potential sources and the corresponding habitat area, as indicated in Figure 3.\nA, B, C, D, E, and F represent six areas in A County of Shaanxi Province, and the habitat area of the risk community in A County of Shaanxi Province is shown in Figure 4.\nThe density of risk sources in each risk community in A County, Shaanxi Province, is displayed in Figure 5.\nThe habitat abundance of each risk community in A County, Shaanxi Province is suggested in Figure 6.\nThis study collected data for the land consolidation project from local government authorities and rural cooperatives in A County, Shaanxi Province. These data encompassed\nFIGURE 3. The risk area includes all kinds of potential risk sources and the corresponding habitat area.\nFIGURE 4. Habitat area of risk community in A County of Shaanxi province.\nFIGURE 5. Density of risk sources in various risk communities in A County, Shaanxi Province.\nhistorical records of land consolidation, project progress, and resource utilization, among other aspects. IoT sensors were deployed within the land consolidation project areas to monitor soil quality, meteorological conditions, water quality, and farmland conditions. These sensors collected real-time\nFIGURE 6. Habitat abundance of various risk communities in A County, Shaanxi Province.\ndata and transmitted it to a data center through networks. Digital financial data was sourced from rural financial institutions, encompassing information about loan disbursements, the usage of financial products, and the financial behaviors of rural residents. These data helped analyze the effectiveness of digital finance in rural revitalization.Land quality indicators comprised soil pH, organic matter content, nutrient levels, etc., which were used to assess soil fertility and suitability. Meteorological data included temperature, humidity, rainfall, etc., for analyzing the impact of climate on farmland yields. Farmland production information involved variables like crop types, growing seasons, yields, etc., to evaluate the agricultural production outcomes of the land consolidation projects. Digital financial indicators covered metrics like financial product adoption rates, the penetration of rural financial services, loan default rates, etc., to assess the contribution of digital finance to rural revitalization. Quantitative analysis employed spatial data analysis methods, involving GIS analysis of geographic data in the land consolidation project areas to study land use patterns and resource allocation.",
    "context": "This section details a risk assessment model applied to land consolidation projects in A County, Shaanxi Province, utilizing data on risk sources, habitat types, and ecological receptors to quantify the relative risk within different communities.",
    "document": "The_Application_of_the_SOFM_Neural_Network_and_Internet_of_Things_in_Rural_Revitalization.pdf",
    "pages": [
      4,
      5
    ],
    "id": "ff9bb92e86794a455023f7fdc531a3afc57ca795753f705566bb46f5124329dd"
  },
  {
    "text": "The acquisition of the required basic data mainly covers the following aspects: ① It covers fundamental land use data, comprising surveys of land use status, data concerning land use changes, agricultural land zoning data, and comprehensive land use planning. ② It includes data on natural resources and environmental conditions, including terrain slope, elevation, geological conditions, soil characteristics, hydrological information, etc. ③ It spans socio-economic data, including administrative divisions, population statistics, regional gross output value, and road and traffic distribution.\nAmongtheabove data, the basic land use data mainly come from the database of land use change (2022 edition) and agricultural land quality classification (2021 edition) in A County of Shaanxi Province. The data on natural resources and environmental conditions are mainly obtained from the image map of A County in Shaanxi Province, the 1: 10000 digital\nelevation mode (DEM) data of A County in Shaanxi Province, and the statistical yearbook of A County in Shaanxi Province in 2022. Socio-economic data mainly come from the statistical yearbook of A County in Shaanxi Province in 2022, and the accounts of relevant government units. The data on land consolidation status principally comes from the database of land consolidation planning in A County, Shaanxi Province (covering the data from 2016 to 2020), the investigation results of cultivated land reserve resources, and the field survey and research data carried out during the compilation of land consolidation planning.\n\nProvides a detailed breakdown of the data sources used for the land consolidation project analysis.",
    "original_text": "The acquisition of the required basic data mainly covers the following aspects: ① It covers fundamental land use data, comprising surveys of land use status, data concerning land use changes, agricultural land zoning data, and comprehensive land use planning. ② It includes data on natural resources and environmental conditions, including terrain slope, elevation, geological conditions, soil characteristics, hydrological information, etc. ③ It spans socio-economic data, including administrative divisions, population statistics, regional gross output value, and road and traffic distribution.\nAmongtheabove data, the basic land use data mainly come from the database of land use change (2022 edition) and agricultural land quality classification (2021 edition) in A County of Shaanxi Province. The data on natural resources and environmental conditions are mainly obtained from the image map of A County in Shaanxi Province, the 1: 10000 digital\nelevation mode (DEM) data of A County in Shaanxi Province, and the statistical yearbook of A County in Shaanxi Province in 2022. Socio-economic data mainly come from the statistical yearbook of A County in Shaanxi Province in 2022, and the accounts of relevant government units. The data on land consolidation status principally comes from the database of land consolidation planning in A County, Shaanxi Province (covering the data from 2016 to 2020), the investigation results of cultivated land reserve resources, and the field survey and research data carried out during the compilation of land consolidation planning.",
    "context": "Provides a detailed breakdown of the data sources used for the land consolidation project analysis.",
    "document": "The_Application_of_the_SOFM_Neural_Network_and_Internet_of_Things_in_Rural_Revitalization.pdf",
    "pages": [
      5,
      6
    ],
    "id": "290d8a317688943f9ca108be00f6beea4afff274ad6b3b2dc0df78282446283b"
  },
  {
    "text": "Toverify the application effect of digital finance and IoT technology in rural revitalization, A County in Shaanxi Province is selected as the research case. In ecology, a risk source is one or more physical, chemical, or biological factors that may negativelyimpact the environment. According to the relevant requirements, the secondary risk sources with little influence and low possibility are excluded.Finally, agricultural land consolidation, rural construction land consolidation, land reclamation, and land development are determined as the four risk sources in this study. After considering the influence degree and object of different risk sources, this study divided cultivated land, woodland, residential area, and mining land into four types of habitats. Ecological receptors refer to biological or abiotic entities adversely affected by the development of land remediation activities. This study selects soil, water environment, biodiversity, and landscape pattern as four ecological receptors to conform to the situation.\n\nDefines the research context and methodology for assessing ecological risks related to land consolidation projects in A County, Shaanxi Province, including the selection of risk sources and ecological receptors.",
    "original_text": "Toverify the application effect of digital finance and IoT technology in rural revitalization, A County in Shaanxi Province is selected as the research case. In ecology, a risk source is one or more physical, chemical, or biological factors that may negativelyimpact the environment. According to the relevant requirements, the secondary risk sources with little influence and low possibility are excluded.Finally, agricultural land consolidation, rural construction land consolidation, land reclamation, and land development are determined as the four risk sources in this study. After considering the influence degree and object of different risk sources, this study divided cultivated land, woodland, residential area, and mining land into four types of habitats. Ecological receptors refer to biological or abiotic entities adversely affected by the development of land remediation activities. This study selects soil, water environment, biodiversity, and landscape pattern as four ecological receptors to conform to the situation.",
    "context": "Defines the research context and methodology for assessing ecological risks related to land consolidation projects in A County, Shaanxi Province, including the selection of risk sources and ecological receptors.",
    "document": "The_Application_of_the_SOFM_Neural_Network_and_Internet_of_Things_in_Rural_Revitalization.pdf",
    "pages": [
      6
    ],
    "id": "160f75f0e8f4e5bb89f3facc1fea4033c7949f89aa48a592aab853dfb5912d27"
  },
  {
    "text": "SOFM neural network parameters: the number of iteration times, input, and output nodes of the SOFM neural network are set. The SOFM neural network can effectively divide A County in Shaanxi Province into land consolidation project areas with similar attributes and adjacent spaces through the input data of the attribute and geographic space domains. Wherein the number of input and output nodes is 10 and 30. The number of iterations is 1000.\nWeight of comprehensive index system: According to the research objectives and case characteristics, the weights of different comprehensive indexes are set to analyze the related factors of land consolidation projects quantitatively. The setting of these weights will affect the final rural revitalization land zoning pattern. Among them, the ecological risk weight of land consolidation is 0.4. The time urgency weight is 0.3. The spatial suitability weight is 0.3.\n\nDetails are provided on the parameters and weighting system used for a SOFM neural network and a comprehensive index system to delineate land consolidation zones in A County, Shaanxi Province, with specific emphasis on the ecological risk weight of 0.4.",
    "original_text": "SOFM neural network parameters: the number of iteration times, input, and output nodes of the SOFM neural network are set. The SOFM neural network can effectively divide A County in Shaanxi Province into land consolidation project areas with similar attributes and adjacent spaces through the input data of the attribute and geographic space domains. Wherein the number of input and output nodes is 10 and 30. The number of iterations is 1000.\nWeight of comprehensive index system: According to the research objectives and case characteristics, the weights of different comprehensive indexes are set to analyze the related factors of land consolidation projects quantitatively. The setting of these weights will affect the final rural revitalization land zoning pattern. Among them, the ecological risk weight of land consolidation is 0.4. The time urgency weight is 0.3. The spatial suitability weight is 0.3.",
    "context": "Details are provided on the parameters and weighting system used for a SOFM neural network and a comprehensive index system to delineate land consolidation zones in A County, Shaanxi Province, with specific emphasis on the ecological risk weight of 0.4.",
    "document": "The_Application_of_the_SOFM_Neural_Network_and_Internet_of_Things_in_Rural_Revitalization.pdf",
    "pages": [
      6
    ],
    "id": "f74489f9d24f2f6c5c0bd43a198fe60f95b6a06b19ef73ef49463ae4afac7bb0"
  },
  {
    "text": "The ecological risk analysis results of different ecological receptors are exhibited in Table 1.\nThe above data results demonstrate differences among different areas regardingsoil ecological risk, water environment ecological risk, biodiversity ecological risk, and landscape pattern ecological risk. Soil ecological risk values reflect\nTABLE 1. The ecological risk analysis results of ecological receptors.\nsoil fertility, pollution levels, and sustainability for land use. Lower soil ecological risk values indicate better suitability for agricultural production and resource utilization, which is crucial for rural economic development in the context of rural revitalization. The extremely low soil ecological risk values in Area C suggest that this area holds potential advantages for agricultural development. Water environment ecological risk values reflect the quality of water resources and the degree of protection in the area. Lower water environment ecological risk values illustrate cleaner and more sustainable water resources, which are essential for rural residents' drinking water safety and agricultural irrigation. Area C's low water environment ecological risk values indicate superior water resources. Biodiversity ecological risk values reflect the health of ecosystems and species diversity within the area. Lower biodiversity ecological risk values signify relatively intact ecosystems, which contribute to maintaining ecological balance and ecosystem services. Landscape pattern ecological risk values reflect the rationality and sustainability of land use patterns within the area. Rational landscape patterns help improve the efficient utilization of land resources and the sustainability of agricultural production. The higher landscape pattern ecological risk values in Areas A and D highlight the necessity of land consolidation projects to optimize land use patterns.\nIn short, the data results highlight the unique characteristics of different areas regarding ecological risks, which are closely related to the goals of rural revitalization. Analyzing and addressing these ecological risks can help improve the ecological environment in rural areas, enhance the sustainability of rural economies, and facilitate the implementation of rural revitalization strategies.\nThe specific results of rural revitalization land consolidation zoning in A County, Shaanxi Province are plotted in Figure 7.\nThe above data presents the rural revitalization land zoning pattern of land consolidation in A County, Shaanxi Province, divided into four land consolidation areas. Firstly, the priority remediation area encompasses 27 administrative villages,spanning a total area of 28090 hm 2 . This area constitutes 32.75% of the overall region. Secondly, there are 37 administrative villages in the moderate renovation area\nFIGURE 7. Specific results of land consolidation zoning for rural revitalization in A County, Shaanxi Province.\nsoon, covering 15,986 hm 2 , equivalent to 18.55% of the entire area. Thirdly, in the medium term, the land-saving renovation area covers 39 administrative villages with a total area of 19686 hm 2 , accounting for 22.75%. Finally, in the longterm restricted remediation area, 37 administrative villages, with a total area spanning 22,081 hm 2 , representing 25.67% of the total area. These data findings furnish a quantitative foundation for land consolidation planning within this area, enabling the realization of consolidation objectives across different timeframes.\n\nSummarizes the ecological risk analysis results, highlighting differences in risk levels across various receptors and the need for targeted strategies to improve rural revitalization.",
    "original_text": "The ecological risk analysis results of different ecological receptors are exhibited in Table 1.\nThe above data results demonstrate differences among different areas regardingsoil ecological risk, water environment ecological risk, biodiversity ecological risk, and landscape pattern ecological risk. Soil ecological risk values reflect\nTABLE 1. The ecological risk analysis results of ecological receptors.\nsoil fertility, pollution levels, and sustainability for land use. Lower soil ecological risk values indicate better suitability for agricultural production and resource utilization, which is crucial for rural economic development in the context of rural revitalization. The extremely low soil ecological risk values in Area C suggest that this area holds potential advantages for agricultural development. Water environment ecological risk values reflect the quality of water resources and the degree of protection in the area. Lower water environment ecological risk values illustrate cleaner and more sustainable water resources, which are essential for rural residents' drinking water safety and agricultural irrigation. Area C's low water environment ecological risk values indicate superior water resources. Biodiversity ecological risk values reflect the health of ecosystems and species diversity within the area. Lower biodiversity ecological risk values signify relatively intact ecosystems, which contribute to maintaining ecological balance and ecosystem services. Landscape pattern ecological risk values reflect the rationality and sustainability of land use patterns within the area. Rational landscape patterns help improve the efficient utilization of land resources and the sustainability of agricultural production. The higher landscape pattern ecological risk values in Areas A and D highlight the necessity of land consolidation projects to optimize land use patterns.\nIn short, the data results highlight the unique characteristics of different areas regarding ecological risks, which are closely related to the goals of rural revitalization. Analyzing and addressing these ecological risks can help improve the ecological environment in rural areas, enhance the sustainability of rural economies, and facilitate the implementation of rural revitalization strategies.\nThe specific results of rural revitalization land consolidation zoning in A County, Shaanxi Province are plotted in Figure 7.\nThe above data presents the rural revitalization land zoning pattern of land consolidation in A County, Shaanxi Province, divided into four land consolidation areas. Firstly, the priority remediation area encompasses 27 administrative villages,spanning a total area of 28090 hm 2 . This area constitutes 32.75% of the overall region. Secondly, there are 37 administrative villages in the moderate renovation area\nFIGURE 7. Specific results of land consolidation zoning for rural revitalization in A County, Shaanxi Province.\nsoon, covering 15,986 hm 2 , equivalent to 18.55% of the entire area. Thirdly, in the medium term, the land-saving renovation area covers 39 administrative villages with a total area of 19686 hm 2 , accounting for 22.75%. Finally, in the longterm restricted remediation area, 37 administrative villages, with a total area spanning 22,081 hm 2 , representing 25.67% of the total area. These data findings furnish a quantitative foundation for land consolidation planning within this area, enabling the realization of consolidation objectives across different timeframes.",
    "context": "Summarizes the ecological risk analysis results, highlighting differences in risk levels across various receptors and the need for targeted strategies to improve rural revitalization.",
    "document": "The_Application_of_the_SOFM_Neural_Network_and_Internet_of_Things_in_Rural_Revitalization.pdf",
    "pages": [
      6,
      7
    ],
    "id": "bc512021a804c206efe81b0f99fadb0c1fbd768b67fde23138523cad7c58decd"
  },
  {
    "text": "The ecological risks of landscape pattern (the ecological risk value is between 0.01 and 1.62) and soil (the ecological risk value is between 0.01 and 1.46) in A County of Shaanxi Province are markedly higher than those of the other three types of ecological receptors. This that improper measures during the land consolidation project's execution may lead to landscape fragmentation and soil degradation in the area.In contrast, the ecological risks of the water environment (ranging from 0.01 to 1.08) and biodiversity (ranging from 0.01 to 1.04) exhibit slightly lower values. This is consistent with that A County in Shaanxi Province is situated in the southeast paddy field agricultural area, with developed irrigation conditions and rich biological species characteristics in the subtropical monsoon climate zone. The results of rural revitalization land consolidation zoning show four different land consolidation areas: the priority consolidation area, moderate consolidation area, medium-term land-saving consolidation area, and restricted consolidation area. In addition, IoT technology is introduced to strengthen the monitoring and management of land remediation. Through IoT technology, land use, soil quality, and water resource utilization can be monitored in real-time, thus providing more accurate data support for remediation planning. Moreover, digital financial technology provides strong financial support and financial services for rural revitalization. Through digital finance, peo-\nple can realize the accurate investment of funds and promote the development of rural industries and financial inclusion.\nThe limitation of the study data is that the quality of the data used here may be limited. For instance, the accuracy and coverage of soil, water quality, and meteorological data may affect the precision of ecological risk assessment. Therefore, data collection and processing errors during the process could introduce biases. To enhance the credibility of the results, multiple data sources and various analytical methods can be utilized to validate the findings.\n\nHighlights ecological risks of landscape pattern and soil as significantly higher than other receptors, indicating potential for fragmentation and degradation due to land consolidation; emphasizes the role of IoT and digital finance in supporting remediation and rural development.",
    "original_text": "The ecological risks of landscape pattern (the ecological risk value is between 0.01 and 1.62) and soil (the ecological risk value is between 0.01 and 1.46) in A County of Shaanxi Province are markedly higher than those of the other three types of ecological receptors. This that improper measures during the land consolidation project's execution may lead to landscape fragmentation and soil degradation in the area.In contrast, the ecological risks of the water environment (ranging from 0.01 to 1.08) and biodiversity (ranging from 0.01 to 1.04) exhibit slightly lower values. This is consistent with that A County in Shaanxi Province is situated in the southeast paddy field agricultural area, with developed irrigation conditions and rich biological species characteristics in the subtropical monsoon climate zone. The results of rural revitalization land consolidation zoning show four different land consolidation areas: the priority consolidation area, moderate consolidation area, medium-term land-saving consolidation area, and restricted consolidation area. In addition, IoT technology is introduced to strengthen the monitoring and management of land remediation. Through IoT technology, land use, soil quality, and water resource utilization can be monitored in real-time, thus providing more accurate data support for remediation planning. Moreover, digital financial technology provides strong financial support and financial services for rural revitalization. Through digital finance, peo-\nple can realize the accurate investment of funds and promote the development of rural industries and financial inclusion.\nThe limitation of the study data is that the quality of the data used here may be limited. For instance, the accuracy and coverage of soil, water quality, and meteorological data may affect the precision of ecological risk assessment. Therefore, data collection and processing errors during the process could introduce biases. To enhance the credibility of the results, multiple data sources and various analytical methods can be utilized to validate the findings.",
    "context": "Highlights ecological risks of landscape pattern and soil as significantly higher than other receptors, indicating potential for fragmentation and degradation due to land consolidation; emphasizes the role of IoT and digital finance in supporting remediation and rural development.",
    "document": "The_Application_of_the_SOFM_Neural_Network_and_Internet_of_Things_in_Rural_Revitalization.pdf",
    "pages": [
      7
    ],
    "id": "43b6bdbc2c4ab9c2ed436a86bd11ad5cb973d2603f78c5eec40ec9d828694930"
  },
  {
    "text": "This study discusses the application of digital finance based on the SOFM neural network and IoT technology in rural revitalization. Taking A County in Shaanxi Province as a case, a comprehensive index system is constructed, and the related factors of land consolidation projects are quantitatively analyzed. Through the results of the SOFM neural network, A county in Shaanxi Province is successfully divided into land consolidation project areas with similar attributes and adjacent space, and the zoning pattern of rural revitalization land is formulated.\nThekeyfindings of this study are as follows. (1) Significant differences exist in soil ecological risk, water environment ecological risk, biodiversity ecological risk, and landscape pattern ecological risk among different regions. Area C exhibits lower risk levels across all ecological risk aspects, while Area B shows higher risk values. (2) The study results underscore the importance of land consolidation. Areas A and D have relatively higher landscape pattern ecological risk, indicating the necessity of land consolidation projects to optimize land use patterns and enhance land ecological quality. These key findings have significant implications for rural revitalization and development. Firstly, the differences in ecological risk highlight the need for targeted policies and plans to accommodate the ecological environment needs of different areas. Rural revitalization strategies must consider the ecological variations among areas to ensure sustainable development. Secondly, land consolidation is crucial in improving the land ecological quality and landscape patterns. For areas with higher landscape pattern ecological risk, the planning and implementing land consolidation projects will contribute to more efficient land resource utilization, providing a better land foundation for rural revitalization. In summary, these key findings emphasize the importance of ecological environment and land use in rural revitalization and provide policymakers with insights on effectively promoting sustainable rural development.\nThis study has made several specific contributions to rural revitalization, advancing existing knowledge. In comparison to other research, the primary contributions are as follows. Firstly, there is an improvement in the ecological risk assessment method. This study has developed a comprehensive ecological risk assessment method by considering\nmultiple ecological factors such as soil, water environment, biodiversity, and landscape patterns. This method is conducive to gaining a more comprehensive understanding of the ecological health status in various areas, providing more specific guidance for rural revitalization planning. Secondly, the importance of land consolidation projects is emphasized. The study results highlight the crucial role of land consolidation in improving land ecological quality and landscape patterns. This offers strong evidence for decision-makers and planners, demonstrating the essential nature of land consolidation projects for the sustainability of rural revitalization.\n\nHighlights the application of digital finance and SOFM neural networks for land consolidation zoning, identifying distinct ecological risk profiles across A County’s land consolidation areas and emphasizing the necessity of land consolidation to improve ecological quality and resource utilization.",
    "original_text": "This study discusses the application of digital finance based on the SOFM neural network and IoT technology in rural revitalization. Taking A County in Shaanxi Province as a case, a comprehensive index system is constructed, and the related factors of land consolidation projects are quantitatively analyzed. Through the results of the SOFM neural network, A county in Shaanxi Province is successfully divided into land consolidation project areas with similar attributes and adjacent space, and the zoning pattern of rural revitalization land is formulated.\nThekeyfindings of this study are as follows. (1) Significant differences exist in soil ecological risk, water environment ecological risk, biodiversity ecological risk, and landscape pattern ecological risk among different regions. Area C exhibits lower risk levels across all ecological risk aspects, while Area B shows higher risk values. (2) The study results underscore the importance of land consolidation. Areas A and D have relatively higher landscape pattern ecological risk, indicating the necessity of land consolidation projects to optimize land use patterns and enhance land ecological quality. These key findings have significant implications for rural revitalization and development. Firstly, the differences in ecological risk highlight the need for targeted policies and plans to accommodate the ecological environment needs of different areas. Rural revitalization strategies must consider the ecological variations among areas to ensure sustainable development. Secondly, land consolidation is crucial in improving the land ecological quality and landscape patterns. For areas with higher landscape pattern ecological risk, the planning and implementing land consolidation projects will contribute to more efficient land resource utilization, providing a better land foundation for rural revitalization. In summary, these key findings emphasize the importance of ecological environment and land use in rural revitalization and provide policymakers with insights on effectively promoting sustainable rural development.\nThis study has made several specific contributions to rural revitalization, advancing existing knowledge. In comparison to other research, the primary contributions are as follows. Firstly, there is an improvement in the ecological risk assessment method. This study has developed a comprehensive ecological risk assessment method by considering\nmultiple ecological factors such as soil, water environment, biodiversity, and landscape patterns. This method is conducive to gaining a more comprehensive understanding of the ecological health status in various areas, providing more specific guidance for rural revitalization planning. Secondly, the importance of land consolidation projects is emphasized. The study results highlight the crucial role of land consolidation in improving land ecological quality and landscape patterns. This offers strong evidence for decision-makers and planners, demonstrating the essential nature of land consolidation projects for the sustainability of rural revitalization.",
    "context": "Highlights the application of digital finance and SOFM neural networks for land consolidation zoning, identifying distinct ecological risk profiles across A County’s land consolidation areas and emphasizing the necessity of land consolidation to improve ecological quality and resource utilization.",
    "document": "The_Application_of_the_SOFM_Neural_Network_and_Internet_of_Things_in_Rural_Revitalization.pdf",
    "pages": [
      8,
      7
    ],
    "id": "945e62f7dc8eaa06cb18cadf36997409840d82d972e96f017b10891ba148744b"
  },
  {
    "text": "In practical application, the quality and availability of data are very important for the model effect. Future research can further explore how to obtain more accurate and comprehensive data and enhance the model's prediction accuracy. Although the SOFM neural network performs well in dividing regions, it can still further improve the model's algorithm and structure to improve the partition results' stability and reliability. Meanwhile, verifying the partition results with the actual situation is necessary to ensure the model's practical application effect. This study focuses on the rural revitalization of land consolidation projects, but rural revitalization involves many factors. Future research can consider more factors, such as industrial layout and infrastructure construction, and realize multi-factor comprehensive planning and decision-making.\nIn conclusion, with the support of digital finance and IoT technology, this study has successfully utilized the SOFM neural network method to delineate rural revitalization land use in A County, Shaanxi Province. This study provides scientific support for the development of rural revitalization. Future research directions should focus on further investigating the role and impact of digital finance in rural revitalization. This involves examining the contributions of various digital finance tools to rural financial services, agricultural production, and rural economic development to formulate more specific policies and strategies.\n\nHighlights the importance of data quality and model refinement for practical application of the SOFM neural network in rural revitalization planning, suggesting future research should incorporate broader factors beyond land consolidation.",
    "original_text": "In practical application, the quality and availability of data are very important for the model effect. Future research can further explore how to obtain more accurate and comprehensive data and enhance the model's prediction accuracy. Although the SOFM neural network performs well in dividing regions, it can still further improve the model's algorithm and structure to improve the partition results' stability and reliability. Meanwhile, verifying the partition results with the actual situation is necessary to ensure the model's practical application effect. This study focuses on the rural revitalization of land consolidation projects, but rural revitalization involves many factors. Future research can consider more factors, such as industrial layout and infrastructure construction, and realize multi-factor comprehensive planning and decision-making.\nIn conclusion, with the support of digital finance and IoT technology, this study has successfully utilized the SOFM neural network method to delineate rural revitalization land use in A County, Shaanxi Province. This study provides scientific support for the development of rural revitalization. Future research directions should focus on further investigating the role and impact of digital finance in rural revitalization. This involves examining the contributions of various digital finance tools to rural financial services, agricultural production, and rural economic development to formulate more specific policies and strategies.",
    "context": "Highlights the importance of data quality and model refinement for practical application of the SOFM neural network in rural revitalization planning, suggesting future research should incorporate broader factors beyond land consolidation.",
    "document": "The_Application_of_the_SOFM_Neural_Network_and_Internet_of_Things_in_Rural_Revitalization.pdf",
    "pages": [
      8
    ],
    "id": "4d8bfefb76544266b8745f9785fbf7f52dba3cf234283bac0e0b88d7030ba50b"
  },
  {
    "text": "- [1] L. Pang, L. Wang, P. Yuan, L. Yan, Q. Yang, and J. Xiao, ''Feasibility study on identifying seed viability of Sophora japonica with optimized deep neural network and hyperspectral imaging,'' Comput. Electron. Agricult. , vol. 190, Nov. 2021, Art. no. 106426, doi: 10.1016/j.compag.2021.106426.\n- [2] S. Saghafian, B. Tomlin, and S. Biller, ''The Internet of Things and information fusion: Who talks to who?'' Manuf. Services Oper. Manag. , vol. 1, no. 1, p. 24, Jan. 2022, doi: 10.1287/msom.2020.0958.\n- [3] H. N. Bhor and M. Kalla, ''TRUST-based features for detecting the intruders in the Internet of Things network using deep learning,'' Comput. Intell. , vol. 38, no. 2, pp. 438-462, Apr. 2022, doi: 10.1111/ coin.12473.\n- [4] M. Kusadokoro and A. Chitose, ''The impact of road infrastructure development on economic growth and urban-rural income inequality in inner Mongolia, China,'' Jpn. J. Agricult. Econ. , vol. 24, pp. 29-34, Mar. 2022, doi: 10.18480/jjae.24.0_29.\n- [5] L. Qu, Y. Li, Y. Wang, S. Dong, and Q. Wen, ''Dynamic evolution and the mechanism of modern gully agriculture regional function in the Loess Plateau,'' J. Geograph. Sci. , vol. 32, no. 11, pp. 2229-2250, Nov. 2022, doi: 10.1007/s11442-022-2045-y.\n- [6] K. Luo, Z. Wang, W. Sha, J. Wu, H. Wang, and Q. Zhu, ''Integrating sponge city concept and neural network into land suitability assessment: Evidence from a satellite town of Shenzhen metropolitan area,'' Land , vol. 10, no. 8, p. 872, Aug. 2021, doi: 10.3390/land10080872.\n- [7] D. Jia, F. Li, and J. Tu, ''A multi-swarm ABC algorithm for parameters optimization of SOFM neural network in dynamic environment,'' Int. J. Comput. Intell. Appl. , vol. 20, no. 2, Jun. 2021, Art. no. 2150014, doi: 10.1142/S1469026821500140.\n- [8] X. Yao, X. Gu, and P. Jiang, ''Coordination control of active front steering and direct yaw moment control based on stability judgment for AVs stability enhancement,'' Proc. Inst. Mech. Eng. D, J. Automobile Eng. , vol. 236, no. 1, pp. 59-74, Jan. 2022, doi: 10.1177/09544070211018104.\n- [9] J. Mabrouki, M. Azrour, G. Fattah, D. Dhiba, and S. E. Hajjaji, ''Intelligent monitoring system for biogas detection based on the Internet of Things: Mohammedia, Morocco city landfill case,'' Big Data Mining Anal. , vol. 4, no. 1, pp. 10-17, Mar. 2021, doi: 10.26599/BDMA.2020.9020017.\n- [10] H. E. Rostam, H. Motameni, and R. Enayatifar, ''Privacy-preserving in the Internet of Things based on steganography and chaotic functions,'' Optik , vol. 258, May 2022, Art. no. 168864, doi: 10.1016/j.ijleo.2022.168864.\n- [11] K. G. Eze and C. M. Akujuobi, ''Design and evaluation of a distributed security framework for the Internet of Things,'' J. Signal Inf. Process. , vol. 1, no. 1, p. 13, Mar. 2022, doi: 10.4236/jsip.2022.131001.\n- [12] A. Rejeb, Z. Suhaiza, K. Rejeb, S. Seuring, and H. Treiblmaier, ''The Internet of Things and the circular economy: A systematic literature review and research agenda,'' J. Cleaner Prod. , vol. 350, May 2022, Art. no. 131439, doi: 10.1016/j.jclepro.2022.131439.\n- [13] V. Dinu, S. P. Lazr, and I. A. Pop, ''Factors that influence the adoption of the Internet of Things in tourism by Romanian consumers,'' Amfiteatru Econ. , vol. 23, no. 57, pp. 360-375, Jan. 2021, doi: 10.24818/EA/2021/57/360.\n- [14] T. Setiaji, C. Budiyanto, and R. A. Yuana, ''The contribution of the Internet of Things and smart systems to agricultural practices: A survey,'' IOPConf. Ser., Mater. Sci. Eng. , vol. 1098, no. 5, Mar. 2021, Art. no. 052100, doi: 10.1088/1757-899X/1098/5/052100.\n- [15] Y. R. Leong, F. P. Tajudeen, and W. C. Yeong, ''Bibliometric and content analysis of the Internet of Things research: A social science perspective,'' Online Inf. Rev. , vol. 45, no. 6, pp. 1148-1166, Oct. 2021, doi: 10.1108/OIR-08-2020-0358.\n- [16] N. Kn and R. E. Adhytyas, ''Garbage bin monitoring system based on the Internet of Things at University Dirgantara Marsekal Suryadarma,'' Int. J. Educ. Manage. Eng. , vol. 11, no. 2, pp. 1-12, Apr. 2021, doi: 10.5815/ijeme.2021.02.01.\n- [17] I.-F. Liliana, I. Bogdan-Stefan, and B. Mariana, ''Exploring the Romanian students' intention to use the Internet of Things for sustainable education,'' Econ. Comput. Econ. Cybern. Stud. Res. , vol. 55, no. 2, pp. 109-123, Jun. 2021, doi: 10.24818/18423264/55.2.21.07.\n- [18] N. N. Nikolayev, ''The Internet of Things in transport technology improvement and project learning,'' IOP Conf. Ser., Mater. Sci. Eng. , vol. 1083, no. 1, Feb. 2021, Art. no. 012068, doi: 10.1088/1757899X/1083/1/012068.\n- [19] A. Yazdinejad, R. M. Parizi, A. Dehghantanha, H. Karimipour, G. Srivastava, and M. Aledhari, ''Enabling drones in the Internet of Things with decentralized blockchain-based security,'' IEEE Internet Things J. , vol. 8, no. 8, pp. 6406-6415, Apr. 2021, doi: 10.1109/JIOT.2020.3015382.\n- [20] S. Alhomdy, M. Q. Al-Shamiry, and G. Al-Gafary, ''An energy-based wireless sensor network model to improve the performance of the Internet of Things,'' Int. J. Comput. Appl. , vol. 174, no. 18, pp. 1-9, Feb. 2021, doi: 10.5120/ijca2021921063.\n- [21] Y. Song, ''Reflections on digital transformation in Chinese regional financial holding companies,'' Mod. Econ. Manag. Forum , vol. 4, no. 1, pp. 11-15, Aug. 2023, doi: 10.32629/memf.v4i1.1241.\n\nProvides a feasibility study on identifying seed viability of Sophora japonica using deep neural networks and hyperspectral imaging, alongside research on IoT applications in various sectors (smart manufacturing, environmental monitoring, transportation, and security). It also explores the impact of digital finance and infrastructure development on rural revitalization.",
    "original_text": "- [1] L. Pang, L. Wang, P. Yuan, L. Yan, Q. Yang, and J. Xiao, ''Feasibility study on identifying seed viability of Sophora japonica with optimized deep neural network and hyperspectral imaging,'' Comput. Electron. Agricult. , vol. 190, Nov. 2021, Art. no. 106426, doi: 10.1016/j.compag.2021.106426.\n- [2] S. Saghafian, B. Tomlin, and S. Biller, ''The Internet of Things and information fusion: Who talks to who?'' Manuf. Services Oper. Manag. , vol. 1, no. 1, p. 24, Jan. 2022, doi: 10.1287/msom.2020.0958.\n- [3] H. N. Bhor and M. Kalla, ''TRUST-based features for detecting the intruders in the Internet of Things network using deep learning,'' Comput. Intell. , vol. 38, no. 2, pp. 438-462, Apr. 2022, doi: 10.1111/ coin.12473.\n- [4] M. Kusadokoro and A. Chitose, ''The impact of road infrastructure development on economic growth and urban-rural income inequality in inner Mongolia, China,'' Jpn. J. Agricult. Econ. , vol. 24, pp. 29-34, Mar. 2022, doi: 10.18480/jjae.24.0_29.\n- [5] L. Qu, Y. Li, Y. Wang, S. Dong, and Q. Wen, ''Dynamic evolution and the mechanism of modern gully agriculture regional function in the Loess Plateau,'' J. Geograph. Sci. , vol. 32, no. 11, pp. 2229-2250, Nov. 2022, doi: 10.1007/s11442-022-2045-y.\n- [6] K. Luo, Z. Wang, W. Sha, J. Wu, H. Wang, and Q. Zhu, ''Integrating sponge city concept and neural network into land suitability assessment: Evidence from a satellite town of Shenzhen metropolitan area,'' Land , vol. 10, no. 8, p. 872, Aug. 2021, doi: 10.3390/land10080872.\n- [7] D. Jia, F. Li, and J. Tu, ''A multi-swarm ABC algorithm for parameters optimization of SOFM neural network in dynamic environment,'' Int. J. Comput. Intell. Appl. , vol. 20, no. 2, Jun. 2021, Art. no. 2150014, doi: 10.1142/S1469026821500140.\n- [8] X. Yao, X. Gu, and P. Jiang, ''Coordination control of active front steering and direct yaw moment control based on stability judgment for AVs stability enhancement,'' Proc. Inst. Mech. Eng. D, J. Automobile Eng. , vol. 236, no. 1, pp. 59-74, Jan. 2022, doi: 10.1177/09544070211018104.\n- [9] J. Mabrouki, M. Azrour, G. Fattah, D. Dhiba, and S. E. Hajjaji, ''Intelligent monitoring system for biogas detection based on the Internet of Things: Mohammedia, Morocco city landfill case,'' Big Data Mining Anal. , vol. 4, no. 1, pp. 10-17, Mar. 2021, doi: 10.26599/BDMA.2020.9020017.\n- [10] H. E. Rostam, H. Motameni, and R. Enayatifar, ''Privacy-preserving in the Internet of Things based on steganography and chaotic functions,'' Optik , vol. 258, May 2022, Art. no. 168864, doi: 10.1016/j.ijleo.2022.168864.\n- [11] K. G. Eze and C. M. Akujuobi, ''Design and evaluation of a distributed security framework for the Internet of Things,'' J. Signal Inf. Process. , vol. 1, no. 1, p. 13, Mar. 2022, doi: 10.4236/jsip.2022.131001.\n- [12] A. Rejeb, Z. Suhaiza, K. Rejeb, S. Seuring, and H. Treiblmaier, ''The Internet of Things and the circular economy: A systematic literature review and research agenda,'' J. Cleaner Prod. , vol. 350, May 2022, Art. no. 131439, doi: 10.1016/j.jclepro.2022.131439.\n- [13] V. Dinu, S. P. Lazr, and I. A. Pop, ''Factors that influence the adoption of the Internet of Things in tourism by Romanian consumers,'' Amfiteatru Econ. , vol. 23, no. 57, pp. 360-375, Jan. 2021, doi: 10.24818/EA/2021/57/360.\n- [14] T. Setiaji, C. Budiyanto, and R. A. Yuana, ''The contribution of the Internet of Things and smart systems to agricultural practices: A survey,'' IOPConf. Ser., Mater. Sci. Eng. , vol. 1098, no. 5, Mar. 2021, Art. no. 052100, doi: 10.1088/1757-899X/1098/5/052100.\n- [15] Y. R. Leong, F. P. Tajudeen, and W. C. Yeong, ''Bibliometric and content analysis of the Internet of Things research: A social science perspective,'' Online Inf. Rev. , vol. 45, no. 6, pp. 1148-1166, Oct. 2021, doi: 10.1108/OIR-08-2020-0358.\n- [16] N. Kn and R. E. Adhytyas, ''Garbage bin monitoring system based on the Internet of Things at University Dirgantara Marsekal Suryadarma,'' Int. J. Educ. Manage. Eng. , vol. 11, no. 2, pp. 1-12, Apr. 2021, doi: 10.5815/ijeme.2021.02.01.\n- [17] I.-F. Liliana, I. Bogdan-Stefan, and B. Mariana, ''Exploring the Romanian students' intention to use the Internet of Things for sustainable education,'' Econ. Comput. Econ. Cybern. Stud. Res. , vol. 55, no. 2, pp. 109-123, Jun. 2021, doi: 10.24818/18423264/55.2.21.07.\n- [18] N. N. Nikolayev, ''The Internet of Things in transport technology improvement and project learning,'' IOP Conf. Ser., Mater. Sci. Eng. , vol. 1083, no. 1, Feb. 2021, Art. no. 012068, doi: 10.1088/1757899X/1083/1/012068.\n- [19] A. Yazdinejad, R. M. Parizi, A. Dehghantanha, H. Karimipour, G. Srivastava, and M. Aledhari, ''Enabling drones in the Internet of Things with decentralized blockchain-based security,'' IEEE Internet Things J. , vol. 8, no. 8, pp. 6406-6415, Apr. 2021, doi: 10.1109/JIOT.2020.3015382.\n- [20] S. Alhomdy, M. Q. Al-Shamiry, and G. Al-Gafary, ''An energy-based wireless sensor network model to improve the performance of the Internet of Things,'' Int. J. Comput. Appl. , vol. 174, no. 18, pp. 1-9, Feb. 2021, doi: 10.5120/ijca2021921063.\n- [21] Y. Song, ''Reflections on digital transformation in Chinese regional financial holding companies,'' Mod. Econ. Manag. Forum , vol. 4, no. 1, pp. 11-15, Aug. 2023, doi: 10.32629/memf.v4i1.1241.",
    "context": "Provides a feasibility study on identifying seed viability of Sophora japonica using deep neural networks and hyperspectral imaging, alongside research on IoT applications in various sectors (smart manufacturing, environmental monitoring, transportation, and security). It also explores the impact of digital finance and infrastructure development on rural revitalization.",
    "document": "The_Application_of_the_SOFM_Neural_Network_and_Internet_of_Things_in_Rural_Revitalization.pdf",
    "pages": [
      8
    ],
    "id": "d0160e91a197c5745c90b63b84b7b40b3f4f1348660d98f5645868be6f706682"
  },
  {
    "text": "- [22] J. Y. Qi and Y. Ren, ''Digital economy development, institutional quality and upstreamness of global value chains,'' Front. China Econ. , vol. 17, no. 1, pp. 24-57, Jun. 2022, doi: 10.3868/s060-015-022-0002-8.\n- [23] A. Risman, B. Mulyana, B. A. Silvatika, and A. S. Sulaeman, ''The effect of digital finance on financial stability,'' Manage. Sci. Lett. , vol. 2, no. 7, pp. 1979-1984, 2021, doi: 10.5267/j.msl.2021.3.012.\n- [24] S. Chen and H. Zhang, ''Does digital finance promote manufacturing servitization: Micro evidence from China,'' Int. Rev. Econ. Finance , vol. 76, pp. 856-869, Nov. 2021, doi: 10.1016/j.iref.2021.07.018.\n- [25] Y. Hao, ''Digital inclusive finance risk prevention based on machine learning and neural network algorithms,'' J. Intell. Fuzzy Syst. , vol. 2, no. 2, pp. 1-11, Jun. 2021, doi: 10.3233/JIFS-219061.\n- [26] S. M. Sapian, N. Abdulkadir, and N. Ibrahim, ''Trade finance in digital era: Can FinTech harness the current risks and challenges?'' J. Muamalat Islamic Finance Res. , vol. 1, no. 2, pp. 78-89, Jun. 2021, doi: 10.33102/jmifr.v18i1.331.\n- [27] Y. Wang, ''Erratum to: Towards the abstract system theory of system science for cognitive and intelligent systems,'' Complex Intell. Syst. , vol. 1, nos. 1-4, p. 23, Dec. 2015, doi: 10.1007/s40747-016-0007-7.\n- [28] Z. Sultana, M. A. R. Khan, and N. Jahan, ''Early breast cancer detection utilizing artificial neural network,'' WSEAS Trans. Biol. Biomed. , vol. 18, pp. 32-42, Mar. 2021, doi: 10.37394/23208.2021.18.4.\n- [29] X. Zeng, Y. Zhao, and Z. Cheng, ''Development and research of rural renewable energy management and ecological management information system under the background of beautiful rural revitalization strategy,'' Sustain. Comput., Informat. Syst. , vol. 30, Jun. 2021, Art. no. 100553, doi: 10.1016/j.suscom.2021.100553.\n- [30] B. David, C. Ritam, F. Ram, and S. Malik, ''Land rezoning and structural transformation in rural India: Evidence from the industrial areas program,'' World Bank Econ. Rev. , vol. 2, no. 2, pp. 13-25, Aug. 2021, doi: 10.1093/wber/lhab004.\n- [31] Y. Wei and S. Chen, ''Spatial correlation and carbon balance zoning of land use carbon emissions in Fujian Province,'' Acta Ecol. Sinica , vol. 41, no. 14, pp. 5814-5824, 2021, doi: 10.5846/stxb202006301705.\n- [32] S. Zhang, H. Wang, and Y. Zheng, ''Study on the influencing mechanism and path of rural talent revitalization under the rural revitalization strategyBased on the in-depth investigation of Zhejiang villages,'' Open J. Social Sci. , vol. 10, no. 5, pp. 539-544, 2022, doi: 10.4236/jss.2022.105036.\n- [33] Y. Peng, ''Study on the path of building a sports and leisure town with 10,000 mu ecological tea garden in western Hunan under the background of rural revitalization,'' Mod. Econ. Manag. Forum. , vol. 3, no. 1, pp. 1-5, Aug. 2022, doi: 10.32629/memf.v3i1.610.\n- [34] L. I. Yan, G. Zhang, T. Lin, H. Ye, Y. Liu, T. Chen, and W. Liu, ''The spatiotemporal changes of remote sensing ecological index in towns and the influencing factors: A case study of Jizhou District, Tianjin,'' Acta Ecol. Sin. , vol. 42, no. 2, pp. 474-486, Jan. 2022, doi: 10.5846/stxb202101270292.\n- [35] J. Yang, R. Yang, M.-H. Chen, C.-H. Su, Y. Zhi, and J. Xi, ''Effects of rural revitalization on rural tourism,'' J. Hospitality Tourism Manage. , vol. 47, pp. 35-45, Jun. 2021, doi: 10.1016/j.jhtm. 2021.02.008.\n- [36] J. Liang, ''Value analysis and realization of artistic intervention in rural revitalization based on the fuzzy clustering algorithm,'' Sci. Program. , vol. 2022, pp. 1-9, Jan. 2022, doi: 10.1155/2022/3107440.\n- [37] M. Mbah and A. Franz, ''Revitalization and branding of rural communities in Cameroon using a circular approach for sustainable developmentA proposal for the Batibo municipality,'' Sustainability , vol. 13, no. 12, p. 6908, Jun. 2021, doi: 10.3390/su13126908.\n- [38] K. Tan, X. Zhao, J. Pu, S. Li, Y. Li, P. Miao, and Q. Wang, ''Zoning regulation and development model for water and land resources in the Karst mountainous region of Southwest China,'' Land Use Policy , vol. 109, Oct. 2021, Art. no. 105683, doi: 10.1016/j.landusepol.2021.105683.\n- [39] J. Chen, G. Bolt, Y. Wang, X. Feng, and X. Li, ''An empirical diagnosis of the school-to-work process for rural and agricultural development in China,'' Sustainability , vol. 13, no. 2, p. 778, Jan. 2021, doi: 10.3390/su13020778.\n- [40] J. Li, Z. Han, and M. Xian, ''Exploration and application of agriculturetourism technologies based on rape flowers in rural revitalization of China,'' Oil Crop Sci. , vol. 7, no. 3, pp. 122-126, Jul. 2022, doi: 10.1016/j.ocsci.2022.08.002.\n- [41] W. Wu, Y. Li, and Y. Liu, ''What constrains impoverished rural regions: A case study of Henan province in central China,'' Habitat Int. , vol. 119, Jan. 2022, Art. no. 102477, doi: 10.1016/j.habitatint.2021.102477.\n- [42] Q. Zhou, G. Yang, H. Song, J. Guo, Y. Zhang, S. Wei, L. Qu, L. A. Gutierrez, and S. Qiao, ''A BiLSTM cardinality estimator in complex database systems based on attention mechanism,'' J. Intell. Technol. , vol. 7, no. 3, pp. 537-546, Sep. 2022, doi: 10.1049/cit2.12069.\n- [43] L. Qu, X. Fang, T. Xie, H. Xu, G. Yang, and W. Liu, ''Nanozyme-catalyzed cascade reactions for high-sensitive glucose sensing and efficient bacterial killing,'' Sens. Actuators B, Chem. , vol. 353, Feb. 2022, Art. no. 131156, doi: 10.1016/j.snb.2021.131156.\n\nProvides a collection of research examining the impact of digital finance and technology on various aspects of rural development, including economic growth, financial stability, and revitalization strategies.",
    "original_text": "- [22] J. Y. Qi and Y. Ren, ''Digital economy development, institutional quality and upstreamness of global value chains,'' Front. China Econ. , vol. 17, no. 1, pp. 24-57, Jun. 2022, doi: 10.3868/s060-015-022-0002-8.\n- [23] A. Risman, B. Mulyana, B. A. Silvatika, and A. S. Sulaeman, ''The effect of digital finance on financial stability,'' Manage. Sci. Lett. , vol. 2, no. 7, pp. 1979-1984, 2021, doi: 10.5267/j.msl.2021.3.012.\n- [24] S. Chen and H. Zhang, ''Does digital finance promote manufacturing servitization: Micro evidence from China,'' Int. Rev. Econ. Finance , vol. 76, pp. 856-869, Nov. 2021, doi: 10.1016/j.iref.2021.07.018.\n- [25] Y. Hao, ''Digital inclusive finance risk prevention based on machine learning and neural network algorithms,'' J. Intell. Fuzzy Syst. , vol. 2, no. 2, pp. 1-11, Jun. 2021, doi: 10.3233/JIFS-219061.\n- [26] S. M. Sapian, N. Abdulkadir, and N. Ibrahim, ''Trade finance in digital era: Can FinTech harness the current risks and challenges?'' J. Muamalat Islamic Finance Res. , vol. 1, no. 2, pp. 78-89, Jun. 2021, doi: 10.33102/jmifr.v18i1.331.\n- [27] Y. Wang, ''Erratum to: Towards the abstract system theory of system science for cognitive and intelligent systems,'' Complex Intell. Syst. , vol. 1, nos. 1-4, p. 23, Dec. 2015, doi: 10.1007/s40747-016-0007-7.\n- [28] Z. Sultana, M. A. R. Khan, and N. Jahan, ''Early breast cancer detection utilizing artificial neural network,'' WSEAS Trans. Biol. Biomed. , vol. 18, pp. 32-42, Mar. 2021, doi: 10.37394/23208.2021.18.4.\n- [29] X. Zeng, Y. Zhao, and Z. Cheng, ''Development and research of rural renewable energy management and ecological management information system under the background of beautiful rural revitalization strategy,'' Sustain. Comput., Informat. Syst. , vol. 30, Jun. 2021, Art. no. 100553, doi: 10.1016/j.suscom.2021.100553.\n- [30] B. David, C. Ritam, F. Ram, and S. Malik, ''Land rezoning and structural transformation in rural India: Evidence from the industrial areas program,'' World Bank Econ. Rev. , vol. 2, no. 2, pp. 13-25, Aug. 2021, doi: 10.1093/wber/lhab004.\n- [31] Y. Wei and S. Chen, ''Spatial correlation and carbon balance zoning of land use carbon emissions in Fujian Province,'' Acta Ecol. Sinica , vol. 41, no. 14, pp. 5814-5824, 2021, doi: 10.5846/stxb202006301705.\n- [32] S. Zhang, H. Wang, and Y. Zheng, ''Study on the influencing mechanism and path of rural talent revitalization under the rural revitalization strategyBased on the in-depth investigation of Zhejiang villages,'' Open J. Social Sci. , vol. 10, no. 5, pp. 539-544, 2022, doi: 10.4236/jss.2022.105036.\n- [33] Y. Peng, ''Study on the path of building a sports and leisure town with 10,000 mu ecological tea garden in western Hunan under the background of rural revitalization,'' Mod. Econ. Manag. Forum. , vol. 3, no. 1, pp. 1-5, Aug. 2022, doi: 10.32629/memf.v3i1.610.\n- [34] L. I. Yan, G. Zhang, T. Lin, H. Ye, Y. Liu, T. Chen, and W. Liu, ''The spatiotemporal changes of remote sensing ecological index in towns and the influencing factors: A case study of Jizhou District, Tianjin,'' Acta Ecol. Sin. , vol. 42, no. 2, pp. 474-486, Jan. 2022, doi: 10.5846/stxb202101270292.\n- [35] J. Yang, R. Yang, M.-H. Chen, C.-H. Su, Y. Zhi, and J. Xi, ''Effects of rural revitalization on rural tourism,'' J. Hospitality Tourism Manage. , vol. 47, pp. 35-45, Jun. 2021, doi: 10.1016/j.jhtm. 2021.02.008.\n- [36] J. Liang, ''Value analysis and realization of artistic intervention in rural revitalization based on the fuzzy clustering algorithm,'' Sci. Program. , vol. 2022, pp. 1-9, Jan. 2022, doi: 10.1155/2022/3107440.\n- [37] M. Mbah and A. Franz, ''Revitalization and branding of rural communities in Cameroon using a circular approach for sustainable developmentA proposal for the Batibo municipality,'' Sustainability , vol. 13, no. 12, p. 6908, Jun. 2021, doi: 10.3390/su13126908.\n- [38] K. Tan, X. Zhao, J. Pu, S. Li, Y. Li, P. Miao, and Q. Wang, ''Zoning regulation and development model for water and land resources in the Karst mountainous region of Southwest China,'' Land Use Policy , vol. 109, Oct. 2021, Art. no. 105683, doi: 10.1016/j.landusepol.2021.105683.\n- [39] J. Chen, G. Bolt, Y. Wang, X. Feng, and X. Li, ''An empirical diagnosis of the school-to-work process for rural and agricultural development in China,'' Sustainability , vol. 13, no. 2, p. 778, Jan. 2021, doi: 10.3390/su13020778.\n- [40] J. Li, Z. Han, and M. Xian, ''Exploration and application of agriculturetourism technologies based on rape flowers in rural revitalization of China,'' Oil Crop Sci. , vol. 7, no. 3, pp. 122-126, Jul. 2022, doi: 10.1016/j.ocsci.2022.08.002.\n- [41] W. Wu, Y. Li, and Y. Liu, ''What constrains impoverished rural regions: A case study of Henan province in central China,'' Habitat Int. , vol. 119, Jan. 2022, Art. no. 102477, doi: 10.1016/j.habitatint.2021.102477.\n- [42] Q. Zhou, G. Yang, H. Song, J. Guo, Y. Zhang, S. Wei, L. Qu, L. A. Gutierrez, and S. Qiao, ''A BiLSTM cardinality estimator in complex database systems based on attention mechanism,'' J. Intell. Technol. , vol. 7, no. 3, pp. 537-546, Sep. 2022, doi: 10.1049/cit2.12069.\n- [43] L. Qu, X. Fang, T. Xie, H. Xu, G. Yang, and W. Liu, ''Nanozyme-catalyzed cascade reactions for high-sensitive glucose sensing and efficient bacterial killing,'' Sens. Actuators B, Chem. , vol. 353, Feb. 2022, Art. no. 131156, doi: 10.1016/j.snb.2021.131156.",
    "context": "Provides a collection of research examining the impact of digital finance and technology on various aspects of rural development, including economic growth, financial stability, and revitalization strategies.",
    "document": "The_Application_of_the_SOFM_Neural_Network_and_Internet_of_Things_in_Rural_Revitalization.pdf",
    "pages": [
      8,
      9
    ],
    "id": "f331d347959c144a76f778231beeac138a3d2d0ae7dad028f7c6460e81a798cc"
  },
  {
    "text": "Jiaqi Cui , Member, IEEE , Gang Ming , Fang Wang , Junyao Li , Pengfei Wang , Songbai Kang , Feng Zhao , Da Zhong , and Ganghua Mei\nAbstract -Lamp-pumped rubidium atomic frequency standard (RAFS) is one of the most commonly utilized atomic frequency standards. Over the past few decades, the RAFS's frequency stability performance has improved rapidly, and the best one has been in the 10 -13 τ -1 / 2 level. In this article, we demonstrate an RAFS with stability in the 10 -14 τ -1 / 2 level for the first time. In design of the physics package (PP), a rubidium spectral lamp with Xe as the starter gas was used as the pumping light source. The light was filtered by using optical and isotope double-filtering technique. A large slotted tube microwave cavity and a rubidium absorption cell with a diameter of 40 mm were utilized to enhance the atomic discrimination signal. A sealed box was designed for the PP to isolate it from the barometric environment. A low phase noise 6.835-GHz microwave was employed to interrogate the rubidium clock transition. Based on the quantitative analysis of the signal-to-noise ratio (SNR) of the atomic discrimination signal and the phase noise of the interrogation microwave, the stability of the RAFS was predicted to be 7.6 × 10 -14 τ -1 / 2 . The short-term stability of the RAFS was measured by using a hydrogen maser and an optical microwave generator (OMG) as references, and the results are 9.0 × 10 -14 τ -1 / 2 (1-100 s) and 9.1 × 10 -14 τ -1 / 2 (1-100 s), respectively. The measured results are in agreement with the predicted one.\nits limit in frequency stability performance, the short-term stability may be not better than 1 × 10 -11 τ -1 / 2 , and the longterm stability, not better than 1 × 10 -13 [2]. The situation changed dramatically in the 1990s when the RAFS for GPS block IIR satellites was produced, which is of short-term stability 3 × 10 -12 τ -1 / 2 and long-term stability in 10 -14 level [3]. Furthermore, in the later 2000s, the RAFS for GPS IIM and IIF satellites was reported, its short-term stability reaches 1 × 10 -12 τ -1 / 2 , and long-term stability is in 10 -15 level [4].\nIndex Terms -Environmental effect, interrogation microwave, phase noise, physics package (PP), rubidium atomic frequency standard (RAFS), signal-to-noise ratio (SNR).\n\nDemonstrates a new RAFS with a stability of 10^-14 τ^-1/2, building upon previous RAFS performance limits and showcasing agreement with predicted stability values based on SNR and phase noise analysis.",
    "original_text": "Jiaqi Cui , Member, IEEE , Gang Ming , Fang Wang , Junyao Li , Pengfei Wang , Songbai Kang , Feng Zhao , Da Zhong , and Ganghua Mei\nAbstract -Lamp-pumped rubidium atomic frequency standard (RAFS) is one of the most commonly utilized atomic frequency standards. Over the past few decades, the RAFS's frequency stability performance has improved rapidly, and the best one has been in the 10 -13 τ -1 / 2 level. In this article, we demonstrate an RAFS with stability in the 10 -14 τ -1 / 2 level for the first time. In design of the physics package (PP), a rubidium spectral lamp with Xe as the starter gas was used as the pumping light source. The light was filtered by using optical and isotope double-filtering technique. A large slotted tube microwave cavity and a rubidium absorption cell with a diameter of 40 mm were utilized to enhance the atomic discrimination signal. A sealed box was designed for the PP to isolate it from the barometric environment. A low phase noise 6.835-GHz microwave was employed to interrogate the rubidium clock transition. Based on the quantitative analysis of the signal-to-noise ratio (SNR) of the atomic discrimination signal and the phase noise of the interrogation microwave, the stability of the RAFS was predicted to be 7.6 × 10 -14 τ -1 / 2 . The short-term stability of the RAFS was measured by using a hydrogen maser and an optical microwave generator (OMG) as references, and the results are 9.0 × 10 -14 τ -1 / 2 (1-100 s) and 9.1 × 10 -14 τ -1 / 2 (1-100 s), respectively. The measured results are in agreement with the predicted one.\nits limit in frequency stability performance, the short-term stability may be not better than 1 × 10 -11 τ -1 / 2 , and the longterm stability, not better than 1 × 10 -13 [2]. The situation changed dramatically in the 1990s when the RAFS for GPS block IIR satellites was produced, which is of short-term stability 3 × 10 -12 τ -1 / 2 and long-term stability in 10 -14 level [3]. Furthermore, in the later 2000s, the RAFS for GPS IIM and IIF satellites was reported, its short-term stability reaches 1 × 10 -12 τ -1 / 2 , and long-term stability is in 10 -15 level [4].\nIndex Terms -Environmental effect, interrogation microwave, phase noise, physics package (PP), rubidium atomic frequency standard (RAFS), signal-to-noise ratio (SNR).",
    "context": "Demonstrates a new RAFS with a stability of 10^-14 τ^-1/2, building upon previous RAFS performance limits and showcasing agreement with predicted stability values based on SNR and phase noise analysis.",
    "document": "Realization_of_a_Rubidium_Atomic_Frequency_Standard_With_Short-Term_Stability_in_10141_2_Level.pdf",
    "pages": [
      1
    ],
    "id": "63777b37e617586ef9d52d500cfdeb31e9848c49e2d05c71b7575d88c9cfa1f6"
  },
  {
    "text": "A S ONE of the most widely used atomic frequency standards, the lamp-pumped vapor cell rubidium atomic frequency standard (RAFS) was first developed in the 1960s [1]. In the early 1980s, nearly two decades later, it was commonly recognized that the RAFS had reached\nManuscript received 30 September 2023; revised 24 November 2023; accepted 6 December 2023. Date of publication 1 January 2024; date of current version 15 January 2024. The Associate Editor coordinating the review process was Dr. Rosenda Valdes Arencibia. (Corresponding authors: Da Zhong; Ganghua Mei.)\nJiaqi Cui and Junyao Li are with the Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan 430071, China, and also with the University of Chinese Academy of Sciences, Beijing 100049, China (e-mail: baozoudeshuimolin@gmail.com; 542468041@qq.com).\nGang Ming, Fang Wang, Pengfei Wang, Songbai Kang, Feng Zhao, Da Zhong, and Ganghua Mei are with the Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan 430071, China (e-mail: ming@apm.ac.cn; fang_wang@apm.ac.cn; wpengfei@apm.ac.cn; kangsongbai@apm.ac.cn; zf_lucky@apm.ac.cn; zhongda@apm.ac.cn; mei@apm.ac.cn).\nDigital Object Identifier 10.1109/TIM.2023.3348883\nOur laboratory has been engaged in the development of RAFS for BeiDou navigation satellite system for more than two decades. Since 2007, three-generation RAFSs have been developed. The average performance of the first-generation product is 3 × 10 -12 τ -1 / 2 for short-term stability and 4 × 10 -14 for one-day stability [5]. The second-generation product has an average performance of 1.5 × 10 -12 τ -1 / 2 for short-term stability and 9.4 × 10 -15 for one-day stability. The average performance of the third-generation product is 6.1 × 10 -13 τ -1 / 2 for short-term stability and 3.9 × 10 -15 for one-day stability [6]. In recent years, we have been developing RAFS with even higher stability. Hao et al. [7] designed a desk RAFS system with a stability of 2.4 × 10 -13 τ -1 / 2 . Nie et al. [8] integrated the system into a whole RAFS unit, and a stability of 2 × 10 -13 τ -1 / 2 was achieved. In 2022, we reported an RAFS with a stability of 1.5 × 10 -13 τ -1 / 2 [9].\nRecently, a new prototype of RAFS was designed in our laboratory. Design work focuses on enhancing the signal-tonoise ratio (SNR) of the atomic discrimination signal, reducing the phase noise of the interrogation microwave, and depressing the environmental effect of the atomic transition. A large size slotted tube microwave cavity and a rubidium absorption cell with 40 mm diameter were designed and utilized to enhance the atomic discrimination signal. Optical and isotope double-filtering technique was used for pumping light to reduce the shot noise of the signal. A low phase noise frequency synthesizer was developed to generate the 6.835-GHz interrogate microwave. To depress the environmental effect, a sealed box was designed to isolate the physics package (PP) from barometric environment. Tests showed that the RAFS is of a short-term stability in 10 -14 τ -1 / 2 level. In this article, design details and main characteristics of the RAFS are presented and analyzed, and the possible applications of the RAFS are discussed.\nFig. 1. Block diagram of the RAFS.\n\nIntroduces the design and characteristics of a new RAFS prototype with a short-term stability of 10^-14 τ^-1/2, focusing on enhancing SNR, reducing phase noise, and mitigating environmental effects.",
    "original_text": "A S ONE of the most widely used atomic frequency standards, the lamp-pumped vapor cell rubidium atomic frequency standard (RAFS) was first developed in the 1960s [1]. In the early 1980s, nearly two decades later, it was commonly recognized that the RAFS had reached\nManuscript received 30 September 2023; revised 24 November 2023; accepted 6 December 2023. Date of publication 1 January 2024; date of current version 15 January 2024. The Associate Editor coordinating the review process was Dr. Rosenda Valdes Arencibia. (Corresponding authors: Da Zhong; Ganghua Mei.)\nJiaqi Cui and Junyao Li are with the Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan 430071, China, and also with the University of Chinese Academy of Sciences, Beijing 100049, China (e-mail: baozoudeshuimolin@gmail.com; 542468041@qq.com).\nGang Ming, Fang Wang, Pengfei Wang, Songbai Kang, Feng Zhao, Da Zhong, and Ganghua Mei are with the Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan 430071, China (e-mail: ming@apm.ac.cn; fang_wang@apm.ac.cn; wpengfei@apm.ac.cn; kangsongbai@apm.ac.cn; zf_lucky@apm.ac.cn; zhongda@apm.ac.cn; mei@apm.ac.cn).\nDigital Object Identifier 10.1109/TIM.2023.3348883\nOur laboratory has been engaged in the development of RAFS for BeiDou navigation satellite system for more than two decades. Since 2007, three-generation RAFSs have been developed. The average performance of the first-generation product is 3 × 10 -12 τ -1 / 2 for short-term stability and 4 × 10 -14 for one-day stability [5]. The second-generation product has an average performance of 1.5 × 10 -12 τ -1 / 2 for short-term stability and 9.4 × 10 -15 for one-day stability. The average performance of the third-generation product is 6.1 × 10 -13 τ -1 / 2 for short-term stability and 3.9 × 10 -15 for one-day stability [6]. In recent years, we have been developing RAFS with even higher stability. Hao et al. [7] designed a desk RAFS system with a stability of 2.4 × 10 -13 τ -1 / 2 . Nie et al. [8] integrated the system into a whole RAFS unit, and a stability of 2 × 10 -13 τ -1 / 2 was achieved. In 2022, we reported an RAFS with a stability of 1.5 × 10 -13 τ -1 / 2 [9].\nRecently, a new prototype of RAFS was designed in our laboratory. Design work focuses on enhancing the signal-tonoise ratio (SNR) of the atomic discrimination signal, reducing the phase noise of the interrogation microwave, and depressing the environmental effect of the atomic transition. A large size slotted tube microwave cavity and a rubidium absorption cell with 40 mm diameter were designed and utilized to enhance the atomic discrimination signal. Optical and isotope double-filtering technique was used for pumping light to reduce the shot noise of the signal. A low phase noise frequency synthesizer was developed to generate the 6.835-GHz interrogate microwave. To depress the environmental effect, a sealed box was designed to isolate the physics package (PP) from barometric environment. Tests showed that the RAFS is of a short-term stability in 10 -14 τ -1 / 2 level. In this article, design details and main characteristics of the RAFS are presented and analyzed, and the possible applications of the RAFS are discussed.\nFig. 1. Block diagram of the RAFS.",
    "context": "Introduces the design and characteristics of a new RAFS prototype with a short-term stability of 10^-14 τ^-1/2, focusing on enhancing SNR, reducing phase noise, and mitigating environmental effects.",
    "document": "Realization_of_a_Rubidium_Atomic_Frequency_Standard_With_Short-Term_Stability_in_10141_2_Level.pdf",
    "pages": [
      1,
      2
    ],
    "id": "cb0875da1ec9fd377394121816575ba16440c9112dcf4230bd93bdd6843d3c82"
  },
  {
    "text": "The electronic structure of the RAFS is shown in Fig. 1. It operates as a frequency-locked loop, utilizing the PP as an atomic frequency discriminator. The reference frequency of the PP is 6.835 GHz, corresponding to the frequency of the transition between hyperfine levels F = 2 and mF = 0 and F = 1 and mF = 0 of the ground state 5S1 / 2 of the 87 Rb atom. The transition is also called clock transition. Normally, a 10-MHz voltage-controlled crystal oscillator (VCXO) is used as a local oscillator (LO). The synthesizer converts the 10-MHz signal of the VCXO to a frequency-modulated 6.835-GHz microwave with modulation frequency fM . The microwave is applied to the PP to interrogate the rubidium clock transition, creating the discrimination signal. After being amplified by the preamplifier (PreAMP), the discrimination signal goes to the phase sensitive detector (PSD) for demodulation, then enters the integrator for filtering and dc amplifying, and finally is transformed into the correction voltage signal to control the frequency of the VCXO, realizing locking of the whole loop.\nThe SNR of the discrimination signal strongly affects frequency stability which is the key performance of the RAFS. Since the discrimination signal is stimulated by the interrogation microwave provided by the synthesizer, the frequency stability is also influenced by the noise of the microwave. Research showed that the phase noise of the interrogation microwave at 2 nfM will deteriorate the frequency stability through the intermodulation effect of the locking loop [10]. In addition, the frequency stability is affected by environmental effects of rubidium atoms. The effects originate from the sensitivity of the atomic transition frequency to fluctuations of various environmental parameters, such as temperature, magnetic field, microwave power, light intensity, barometric pressure, and so on. Taking all the three factors into account, Allan deviation frequency stability of an RAFS can be written as [6]\n<!-- formula-not-decoded -->\nwhere τ is the averaging time and\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nFig. 2. Structure of the PP.\n<!-- formula-not-decoded -->\nAs implied by (1)-(4), three fractional stabilities contribute to the frequency stability of the RAFS. The first one σ SNR (τ) is SNR limited stability [11], determined by the noise power spectral density SN and the discrimination slope KD that measures the signal intensity, where SN = 2 eI 0 with e being the electron charge and I 0, the shot noise current [11]. The second one σ PN (τ) is the phase noise limited stability, determined by S ϕ( 2 nfM ) , the single sideband power spectral density of the phase noise of the interrogation microwave at 2 nfM ( C 2 n is a constant related to the modulation frequency fM and the carrier frequency been modulated). In the calculation of σ PN (τ) by using (3), one usually considers only the contribution of the n = 1 term, since the contributions of high-order terms are very small [10]. The third one σ EE (τ) is environmental effect limited stability. In (4), 1υ/1 Pi and σ Pi (τ) represent the frequency shift coefficient and the stability of the i th environmental parameter Pi , respectively.\nIt should be noticed that σ SNR (τ) and σ PN (τ) determine the short-term stability and the limit of long-term stability of the RAFS, and σ EE (τ) determines principally the long-term stability. Formulas (1)-(4) also imply that, for the purpose of improving the stability performance of the RAFS, one should focus on enhancing the SNR of the discrimination signal, reducing the phase noise of interrogation microwave, and depressing the environmental effect.\n\nDescribes the electronic structure and operation of the RAFS, outlining the frequency-locked loop, the role of the PP as an atomic frequency discriminator, and the influence of noise and environmental effects on frequency stability.",
    "original_text": "The electronic structure of the RAFS is shown in Fig. 1. It operates as a frequency-locked loop, utilizing the PP as an atomic frequency discriminator. The reference frequency of the PP is 6.835 GHz, corresponding to the frequency of the transition between hyperfine levels F = 2 and mF = 0 and F = 1 and mF = 0 of the ground state 5S1 / 2 of the 87 Rb atom. The transition is also called clock transition. Normally, a 10-MHz voltage-controlled crystal oscillator (VCXO) is used as a local oscillator (LO). The synthesizer converts the 10-MHz signal of the VCXO to a frequency-modulated 6.835-GHz microwave with modulation frequency fM . The microwave is applied to the PP to interrogate the rubidium clock transition, creating the discrimination signal. After being amplified by the preamplifier (PreAMP), the discrimination signal goes to the phase sensitive detector (PSD) for demodulation, then enters the integrator for filtering and dc amplifying, and finally is transformed into the correction voltage signal to control the frequency of the VCXO, realizing locking of the whole loop.\nThe SNR of the discrimination signal strongly affects frequency stability which is the key performance of the RAFS. Since the discrimination signal is stimulated by the interrogation microwave provided by the synthesizer, the frequency stability is also influenced by the noise of the microwave. Research showed that the phase noise of the interrogation microwave at 2 nfM will deteriorate the frequency stability through the intermodulation effect of the locking loop [10]. In addition, the frequency stability is affected by environmental effects of rubidium atoms. The effects originate from the sensitivity of the atomic transition frequency to fluctuations of various environmental parameters, such as temperature, magnetic field, microwave power, light intensity, barometric pressure, and so on. Taking all the three factors into account, Allan deviation frequency stability of an RAFS can be written as [6]\n<!-- formula-not-decoded -->\nwhere τ is the averaging time and\n<!-- formula-not-decoded -->\n<!-- formula-not-decoded -->\nFig. 2. Structure of the PP.\n<!-- formula-not-decoded -->\nAs implied by (1)-(4), three fractional stabilities contribute to the frequency stability of the RAFS. The first one σ SNR (τ) is SNR limited stability [11], determined by the noise power spectral density SN and the discrimination slope KD that measures the signal intensity, where SN = 2 eI 0 with e being the electron charge and I 0, the shot noise current [11]. The second one σ PN (τ) is the phase noise limited stability, determined by S ϕ( 2 nfM ) , the single sideband power spectral density of the phase noise of the interrogation microwave at 2 nfM ( C 2 n is a constant related to the modulation frequency fM and the carrier frequency been modulated). In the calculation of σ PN (τ) by using (3), one usually considers only the contribution of the n = 1 term, since the contributions of high-order terms are very small [10]. The third one σ EE (τ) is environmental effect limited stability. In (4), 1υ/1 Pi and σ Pi (τ) represent the frequency shift coefficient and the stability of the i th environmental parameter Pi , respectively.\nIt should be noticed that σ SNR (τ) and σ PN (τ) determine the short-term stability and the limit of long-term stability of the RAFS, and σ EE (τ) determines principally the long-term stability. Formulas (1)-(4) also imply that, for the purpose of improving the stability performance of the RAFS, one should focus on enhancing the SNR of the discrimination signal, reducing the phase noise of interrogation microwave, and depressing the environmental effect.",
    "context": "Describes the electronic structure and operation of the RAFS, outlining the frequency-locked loop, the role of the PP as an atomic frequency discriminator, and the influence of noise and environmental effects on frequency stability.",
    "document": "Realization_of_a_Rubidium_Atomic_Frequency_Standard_With_Short-Term_Stability_in_10141_2_Level.pdf",
    "pages": [
      2
    ],
    "id": "8d2845d1d7990fed54e5b0150bf1eab7dea5483190d51871bbdda50d327828ab"
  },
  {
    "text": "The atomic discrimination signal originates from a process of optical-microwave double resonance of rubidium atoms that takes place in the PP. Rubidium atoms are first optically pumped by using a rubidium spectral lamp to create population reversion between two clock transition energy levels. Interrogation microwave is then applied to rubidium atoms to stimulate the clock transition. The SNR of the discrimination signal is determined by the characteristics of the PP.\nThe PP used is schematically shown in Fig. 2. A 87 Rb spectral lamp with Xe as starter gas was utilized as the source of pumping light. As shown by Hao et al. [12], this\nFig. 3. (a) Slotted tube and (b) entrance cap of the slotted tube microwave cavity.\nkind of spectral lump could output intense rubidium D1 and D2 line lights with narrow linewidth. After being collimated by the lens, the light is filtered by the 85 Rb filter cell to eliminate the a components in D1 and D2 lines and then filtered by a bandpass optical filter with a center wavelength of 786 nm and bandwidth of 22 nm to eliminate the light emitted by starter gas. This choice of light source and the double-filtering scheme enables to enhance pumping efficiency and meanwhile to reduce the noise of atomic signal. The filtered light then goes to the absorption cell inside a slotted tube microwave cavity composed of a slotted tube, a cylinder, an entrance cap, and an end cap. The absorption cell, with a diameter of 40 mm, contains 87 Rb vapor and mixed buffer gases of Ar and N2. Both the filter cell and the absorption cell were heated and temperature-controlled at appropriate temperatures to create stable and useful rubidium vapors. A modulated 6.835-GHz microwave is electrically coupled into the cavity to interrogate the clock transition of rubidium vapor atoms in the absorption cell. The photocell was used to detect the intensity of the transmitted light through the absorption cell, which varies when the atomic clock transition takes place. A Helmholtz coil produces a uniform C field to define quantization axis for atomic transition. Three layers of magnetic shield were employed to reduce the disturbance of environmental magnetic field, two of which were installed on the PP.\nThe slotted tube cavity is a nonstandard microwave cavity we developed nearly two decades ago [13]. The crucial component of the cavity is a slotted tube, whose structural parameters play main role in determining the resonance frequency and mode of the cavity [14]. An obvious advantage of the cavity over the commonly used standard TE111 and TE011 cavities is that its size could be flexibly designed. This property enables to use large size absorption cell to enhance the atomic signal SNR. Actually, a series of cavities with different sizes have been developed in our laboratory [5], [6], [7], [8], [9]. In this work, we used the cavity with an inner diameter of 40 mm, the largest one we have designed so far. The structure of the slotted tube is shown in Fig. 3. Fig. 3 also shows the structure of the entrance cap of the cavity, which is needed for a large size slotted tube cavity. The central grid structure of the cap is used to eliminate RF disturbance from outside to resonance mode of the cavity and let pumping light pass through. The entrance cap also acts as an optical aperture to collimate the pumping light into a light beam with a diameter of 32 mm, slightly smaller than that of the absorption cell.\nFig. 4. Zeeman transition spectrum of hyperfine sublevels in ground state of 87 Rb atoms. Totally, nine Zeeman transitions were observed. Peak 4 at the center is the clock transition. Peaks 3 and 5 compose of two transitions each.\nFor high-performance RAFS design, the microwave distribution inside the cavity is extremely important since the clock transition can be excited only by the magnetic microwave field parallel to the quantization axis. The parallelism property is described quantitatively by ξ , the field orientation factor defined as [15]\n<!-- formula-not-decoded -->\nwhere V defines the volume of interaction region of atoms and microwave, Hz is the magnetic microwave field parallel to quantization axis, and H is the total field. ξ could be obtained by measuring the Zeeman transition spectrum of 87 Rb vapor atoms. The experimentally measured spectrum for the used PP is shown in Fig. 4. Based on the spectrum, ξ could be simply calculated through the relation [16]\n<!-- formula-not-decoded -->\nwhere I 3 (υ) , I 4 (υ) , and I 5 (υ) are the transition intensity distributions of the central three transitions. The finally obtained ξ for the used cavity is 0.82. We believe that it is much higher than those of the traditional standard cavities.\nThe SNR limited stability σ SNR ( τ) of the PP was estimated. First, the light path and the working temperatures of the absorption cell, the filter cell, and the lamp bulb were optimized for maximum discrimination slope Kd . The optimal temperatures were found to be 68 ◦ C, 93 ◦ C, and 109 ◦ C, respectively. In this condition, the shot noise current I 0, i.e., background current of the photocell, was then measured to be 211 µ A, and corresponding noise power spectral density SN was calculated to be 8.2 pA/Hz 1 / 2 . Fig. 5 shows the measured frequency discrimination curve of the RAFS. Kd , the slope of the central part of the curve, was calculated to be 18.0 nA/Hz. By using (2), σ SNR ( τ) was estimated to be 4.7 × 10 -14 τ -1 / 2 , implying that the PP enables to realize a stability in medium 10 -14 τ -1 / 2 level.\nFig. 5. Frequency discrimination curve of the PP.\nFig. 6. Schematic of the frequency synthesizer.\n\nDescribes the optical-microwave double resonance process within the physics package, outlining the light source, filtering, and absorption cell design to enhance the atomic signal SNR.",
    "original_text": "The atomic discrimination signal originates from a process of optical-microwave double resonance of rubidium atoms that takes place in the PP. Rubidium atoms are first optically pumped by using a rubidium spectral lamp to create population reversion between two clock transition energy levels. Interrogation microwave is then applied to rubidium atoms to stimulate the clock transition. The SNR of the discrimination signal is determined by the characteristics of the PP.\nThe PP used is schematically shown in Fig. 2. A 87 Rb spectral lamp with Xe as starter gas was utilized as the source of pumping light. As shown by Hao et al. [12], this\nFig. 3. (a) Slotted tube and (b) entrance cap of the slotted tube microwave cavity.\nkind of spectral lump could output intense rubidium D1 and D2 line lights with narrow linewidth. After being collimated by the lens, the light is filtered by the 85 Rb filter cell to eliminate the a components in D1 and D2 lines and then filtered by a bandpass optical filter with a center wavelength of 786 nm and bandwidth of 22 nm to eliminate the light emitted by starter gas. This choice of light source and the double-filtering scheme enables to enhance pumping efficiency and meanwhile to reduce the noise of atomic signal. The filtered light then goes to the absorption cell inside a slotted tube microwave cavity composed of a slotted tube, a cylinder, an entrance cap, and an end cap. The absorption cell, with a diameter of 40 mm, contains 87 Rb vapor and mixed buffer gases of Ar and N2. Both the filter cell and the absorption cell were heated and temperature-controlled at appropriate temperatures to create stable and useful rubidium vapors. A modulated 6.835-GHz microwave is electrically coupled into the cavity to interrogate the clock transition of rubidium vapor atoms in the absorption cell. The photocell was used to detect the intensity of the transmitted light through the absorption cell, which varies when the atomic clock transition takes place. A Helmholtz coil produces a uniform C field to define quantization axis for atomic transition. Three layers of magnetic shield were employed to reduce the disturbance of environmental magnetic field, two of which were installed on the PP.\nThe slotted tube cavity is a nonstandard microwave cavity we developed nearly two decades ago [13]. The crucial component of the cavity is a slotted tube, whose structural parameters play main role in determining the resonance frequency and mode of the cavity [14]. An obvious advantage of the cavity over the commonly used standard TE111 and TE011 cavities is that its size could be flexibly designed. This property enables to use large size absorption cell to enhance the atomic signal SNR. Actually, a series of cavities with different sizes have been developed in our laboratory [5], [6], [7], [8], [9]. In this work, we used the cavity with an inner diameter of 40 mm, the largest one we have designed so far. The structure of the slotted tube is shown in Fig. 3. Fig. 3 also shows the structure of the entrance cap of the cavity, which is needed for a large size slotted tube cavity. The central grid structure of the cap is used to eliminate RF disturbance from outside to resonance mode of the cavity and let pumping light pass through. The entrance cap also acts as an optical aperture to collimate the pumping light into a light beam with a diameter of 32 mm, slightly smaller than that of the absorption cell.\nFig. 4. Zeeman transition spectrum of hyperfine sublevels in ground state of 87 Rb atoms. Totally, nine Zeeman transitions were observed. Peak 4 at the center is the clock transition. Peaks 3 and 5 compose of two transitions each.\nFor high-performance RAFS design, the microwave distribution inside the cavity is extremely important since the clock transition can be excited only by the magnetic microwave field parallel to the quantization axis. The parallelism property is described quantitatively by ξ , the field orientation factor defined as [15]\n<!-- formula-not-decoded -->\nwhere V defines the volume of interaction region of atoms and microwave, Hz is the magnetic microwave field parallel to quantization axis, and H is the total field. ξ could be obtained by measuring the Zeeman transition spectrum of 87 Rb vapor atoms. The experimentally measured spectrum for the used PP is shown in Fig. 4. Based on the spectrum, ξ could be simply calculated through the relation [16]\n<!-- formula-not-decoded -->\nwhere I 3 (υ) , I 4 (υ) , and I 5 (υ) are the transition intensity distributions of the central three transitions. The finally obtained ξ for the used cavity is 0.82. We believe that it is much higher than those of the traditional standard cavities.\nThe SNR limited stability σ SNR ( τ) of the PP was estimated. First, the light path and the working temperatures of the absorption cell, the filter cell, and the lamp bulb were optimized for maximum discrimination slope Kd . The optimal temperatures were found to be 68 ◦ C, 93 ◦ C, and 109 ◦ C, respectively. In this condition, the shot noise current I 0, i.e., background current of the photocell, was then measured to be 211 µ A, and corresponding noise power spectral density SN was calculated to be 8.2 pA/Hz 1 / 2 . Fig. 5 shows the measured frequency discrimination curve of the RAFS. Kd , the slope of the central part of the curve, was calculated to be 18.0 nA/Hz. By using (2), σ SNR ( τ) was estimated to be 4.7 × 10 -14 τ -1 / 2 , implying that the PP enables to realize a stability in medium 10 -14 τ -1 / 2 level.\nFig. 5. Frequency discrimination curve of the PP.\nFig. 6. Schematic of the frequency synthesizer.",
    "context": "Describes the optical-microwave double resonance process within the physics package, outlining the light source, filtering, and absorption cell design to enhance the atomic signal SNR.",
    "document": "Realization_of_a_Rubidium_Atomic_Frequency_Standard_With_Short-Term_Stability_in_10141_2_Level.pdf",
    "pages": [
      2,
      3,
      4
    ],
    "id": "78eeabfe49055a86c7235641c64560cd9ba3ad62a7c7d4ee0b29acb3ba0e0d70"
  },
  {
    "text": "In design of the electronics of the RAFS, main attention was paid to reduce the phase noise and stabilizing the power of the interrogation microwave produced by the synthesizer.\nThe structure of the synthesizer is schematically shown in Fig. 6. The interrogation microwave originates from a 10-MHz LO with a stability of 1 × 10 -12 at 1 s. The 100-MHz frequency of the oven-controlled crystal oscillator (OCXO) with near-end phase noise of -153 dBc/Hz at 2 fM (here fM = 136 Hz) is phase-locked to that of the LO through phase detector PD1. Due to the 30-Hz bandwidth of the locking loop, the locked 100-MHz signal is of stability of the LO and the near-end phase noise of the OCXO. For ensuring low phase noise and high spectral purity of the 6.835-GHz microwave, two dielectric resonator oscillators (DROs) with low far-end phase noise are employed, of which DRO1 resonates at 6.80 GHz and DRO2 resonates at 6.835 GHz. The 6.80-GHz frequency of the DRO1 is phase-locked directly to the 100 MHz through PD2. In order to lock the 6.835 GHz of the DRO2 to the 100 MHz also, the frequencies of DRO1 and DRO2 are mixed in the double-balanced mixer (DBM), creating a 35-MHz signal. After being filtered by the low-pass filter (LPF), the 35-MHz signal is phase compared in PD3 with the 35-MHz signal produced by the direct digital synthesizer (DDS) from the 100-MHz signal. The phase comparison signal is then used to control the 6.835-GHz frequency of the DRO2. The finally obtained 6.835-GHz microwave has near-end phase noise property determined by the OCXO, far-end phase noise property determined by the DRO2, and short-term stability property determined by the LO.\nThe modulation of the 6.835-GHz microwave is realized by modulating the 35-MHz signal of the DDS. The modulation\nFig. 7. Spectrum of the 6.835-GHz interrogation microwave.\nFig. 8. Phase noise measurement result of the 6.835-GHz microwave.\nsignal is a 136-Hz square wave. The modulated 6.835-GHz microwave is coupled to the PP to interrogate the clock transition of the rubidium atoms.\nThe interrogation microwave obtained this way is of high spectral purity. As shown in Fig. 7, the output microwave contains only the 6.835-GHz component, and no obvious harmonic or spurious components are observed in a 500-MHz range nearby. In this case, automatic-level control (ALC) of the microwave power will be more efficient since there is less influence of harmonic or spurious components. The stabilization of the microwave power helps to reduce the microwave power shift of the clock transition.\nThe phase noise of the 6.835-GHz microwave was measured, and the result is shown in Fig. 8. It can be found from Fig. 8 that the phase noise of the microwave at 2 fM (272 Hz) is about -110dBc/Hz. Based on this result and by using (4), the phase noise limited stability σ PN (τ) of the RAFS is estimated to be 6.0 × 10 -14 τ -1 / 2 .\n\nFocuses on the design and implementation of the synthesizer for the RAFS, detailing the phase-locked loop structure and the use of DROs to achieve low phase noise and a stable interrogation microwave frequency.",
    "original_text": "In design of the electronics of the RAFS, main attention was paid to reduce the phase noise and stabilizing the power of the interrogation microwave produced by the synthesizer.\nThe structure of the synthesizer is schematically shown in Fig. 6. The interrogation microwave originates from a 10-MHz LO with a stability of 1 × 10 -12 at 1 s. The 100-MHz frequency of the oven-controlled crystal oscillator (OCXO) with near-end phase noise of -153 dBc/Hz at 2 fM (here fM = 136 Hz) is phase-locked to that of the LO through phase detector PD1. Due to the 30-Hz bandwidth of the locking loop, the locked 100-MHz signal is of stability of the LO and the near-end phase noise of the OCXO. For ensuring low phase noise and high spectral purity of the 6.835-GHz microwave, two dielectric resonator oscillators (DROs) with low far-end phase noise are employed, of which DRO1 resonates at 6.80 GHz and DRO2 resonates at 6.835 GHz. The 6.80-GHz frequency of the DRO1 is phase-locked directly to the 100 MHz through PD2. In order to lock the 6.835 GHz of the DRO2 to the 100 MHz also, the frequencies of DRO1 and DRO2 are mixed in the double-balanced mixer (DBM), creating a 35-MHz signal. After being filtered by the low-pass filter (LPF), the 35-MHz signal is phase compared in PD3 with the 35-MHz signal produced by the direct digital synthesizer (DDS) from the 100-MHz signal. The phase comparison signal is then used to control the 6.835-GHz frequency of the DRO2. The finally obtained 6.835-GHz microwave has near-end phase noise property determined by the OCXO, far-end phase noise property determined by the DRO2, and short-term stability property determined by the LO.\nThe modulation of the 6.835-GHz microwave is realized by modulating the 35-MHz signal of the DDS. The modulation\nFig. 7. Spectrum of the 6.835-GHz interrogation microwave.\nFig. 8. Phase noise measurement result of the 6.835-GHz microwave.\nsignal is a 136-Hz square wave. The modulated 6.835-GHz microwave is coupled to the PP to interrogate the clock transition of the rubidium atoms.\nThe interrogation microwave obtained this way is of high spectral purity. As shown in Fig. 7, the output microwave contains only the 6.835-GHz component, and no obvious harmonic or spurious components are observed in a 500-MHz range nearby. In this case, automatic-level control (ALC) of the microwave power will be more efficient since there is less influence of harmonic or spurious components. The stabilization of the microwave power helps to reduce the microwave power shift of the clock transition.\nThe phase noise of the 6.835-GHz microwave was measured, and the result is shown in Fig. 8. It can be found from Fig. 8 that the phase noise of the microwave at 2 fM (272 Hz) is about -110dBc/Hz. Based on this result and by using (4), the phase noise limited stability σ PN (τ) of the RAFS is estimated to be 6.0 × 10 -14 τ -1 / 2 .",
    "context": "Focuses on the design and implementation of the synthesizer for the RAFS, detailing the phase-locked loop structure and the use of DROs to achieve low phase noise and a stable interrogation microwave frequency.",
    "document": "Realization_of_a_Rubidium_Atomic_Frequency_Standard_With_Short-Term_Stability_in_10141_2_Level.pdf",
    "pages": [
      4
    ],
    "id": "e244832c8f39c3334ec82db30594f0598aea8b6d2a1384723411a04d47f097ef"
  },
  {
    "text": "As a common sense, for an ordinary RAFS, the influence of environmental effect on short-term stability can be ignored. When an RAFS with 10 -14 τ -1 / 2 level stability is expected to be realized, however, the influence must be considered.\nThe first observed unacceptable influence comes from the barometric pressure effect. In our earlier frequency stability measurements of the designed RAFS, the 100-s stability\nFig. 9. Influence of the barometric effect to frequency stability of the RAFS.\nTABLE I\nFREQUENCY SHIFT COEFFICIENT OF THE RAFS PHYSICAL PARAMETERS\n\nFrequency shift coefficicnt, Barometric pressure = IPa 10-15 /. Frequency shift coefficicnt, Absorption cell temperature = 6x10-W/K. Frequency shift coefficicnt, Light intensity = -1.7x10-12 /%. Frequency shift coefficicnt, Microwave power = 6x10-14/%. Frequency shift coefficicnt, Magnetic field = 3.7x10-9/G\nwas found to be around 2.5 × 10 -14 and cannot reach the 10 -15 level. To make it clear if this is caused by barometric effect, we put the RAFS into a large sealed chamber and measured its frequency shift coefficient of the barometric pressure by changing the air pressure in the chamber and recording the output frequency of the RAFS. The coefficient was experimentally measured to be 7 × 10 -15 /Pa. The RAFS was then taken back to the open air environment and the barometric pressure fluctuation was monitored. The barometric pressure was recorded once every 10 s in a period of 50 000 s. Based on the fluctuation data and the obtained frequency shift coefficient, the influence of the barometric effect to stability of the RAFS was calculated by using (4), and the result is shown as the upper line in Fig. 9. The influence to 100-s stability is 2.2 × 10 -14 , in agreement with the measured result. To reduce the barometric effect, a sealed box was designed and used to envelop the PP to isolate it from barometric environment. The resulted influence of the barometric effect is shown as the lower line in Fig. 9. It can be seen that the influence is reduced by nearly one order of magnitude. The barometric effect originates essentially from the buffer gas pressure effect inside the absorption cell [17]. Barometric pressure variation deforms slightly the cell shape, thus changes the buffer gas pressure inside the cell and finally causes a frequency shift of the rubidium atomic transition.\nThe effects of absorption cell temperature, pumping light intensity, 6.835-GHz microwave power, and magnetic C field were also examined in similar way. The frequency shift coefficients of the five parameters are listed in Table I, and the contributions of their fluctuations to the stability of the RAFS with averaging time 10-400 s are shown in Fig. 10.\nAs can be seen from Fig. 10, all the environmental effects have influences on the stability of the RAFS to some degree. The total influence has been controlled to a 10 -15 level with τ ≤ 100 s, allowing realizing a 10 -14 τ -1 / 2 level short-term stability. When τ > 100 s, however, the influence increases\nFig. 10. Contributions of environmental effects to frequency stability of the RAFS. All the results are obtained when the PP is inside the sealed box.\nFig. 11. Frequency stability test scheme for the RAFS. The test could be conducted by using an H-maser or an ultrastable laser based OMG. The phase noise analyzer (3120A, Symmetricom) is used for frequency comparison of the RAFS with the selected reference.\nwith τ considerably, which means that the environmental effect will influence the medium-term stability of the RAFS. The influence comes mainly from the temperature shift and the barometric shift, as can be seen from Fig. 10.\n\nThe text highlights the significant influence of environmental factors, particularly barometric pressure, on the RAFS’s frequency stability. Specifically, the barometric pressure effect, measured to be 7 × 10 -15 /Pa, contributes to a 2.2 × 10 -14 stability over 100 seconds. To mitigate this, a sealed box was implemented, reducing the influence by nearly an order of magnitude.",
    "original_text": "As a common sense, for an ordinary RAFS, the influence of environmental effect on short-term stability can be ignored. When an RAFS with 10 -14 τ -1 / 2 level stability is expected to be realized, however, the influence must be considered.\nThe first observed unacceptable influence comes from the barometric pressure effect. In our earlier frequency stability measurements of the designed RAFS, the 100-s stability\nFig. 9. Influence of the barometric effect to frequency stability of the RAFS.\nTABLE I\nFREQUENCY SHIFT COEFFICIENT OF THE RAFS PHYSICAL PARAMETERS\n\nFrequency shift coefficicnt, Barometric pressure = IPa 10-15 /. Frequency shift coefficicnt, Absorption cell temperature = 6x10-W/K. Frequency shift coefficicnt, Light intensity = -1.7x10-12 /%. Frequency shift coefficicnt, Microwave power = 6x10-14/%. Frequency shift coefficicnt, Magnetic field = 3.7x10-9/G\nwas found to be around 2.5 × 10 -14 and cannot reach the 10 -15 level. To make it clear if this is caused by barometric effect, we put the RAFS into a large sealed chamber and measured its frequency shift coefficient of the barometric pressure by changing the air pressure in the chamber and recording the output frequency of the RAFS. The coefficient was experimentally measured to be 7 × 10 -15 /Pa. The RAFS was then taken back to the open air environment and the barometric pressure fluctuation was monitored. The barometric pressure was recorded once every 10 s in a period of 50 000 s. Based on the fluctuation data and the obtained frequency shift coefficient, the influence of the barometric effect to stability of the RAFS was calculated by using (4), and the result is shown as the upper line in Fig. 9. The influence to 100-s stability is 2.2 × 10 -14 , in agreement with the measured result. To reduce the barometric effect, a sealed box was designed and used to envelop the PP to isolate it from barometric environment. The resulted influence of the barometric effect is shown as the lower line in Fig. 9. It can be seen that the influence is reduced by nearly one order of magnitude. The barometric effect originates essentially from the buffer gas pressure effect inside the absorption cell [17]. Barometric pressure variation deforms slightly the cell shape, thus changes the buffer gas pressure inside the cell and finally causes a frequency shift of the rubidium atomic transition.\nThe effects of absorption cell temperature, pumping light intensity, 6.835-GHz microwave power, and magnetic C field were also examined in similar way. The frequency shift coefficients of the five parameters are listed in Table I, and the contributions of their fluctuations to the stability of the RAFS with averaging time 10-400 s are shown in Fig. 10.\nAs can be seen from Fig. 10, all the environmental effects have influences on the stability of the RAFS to some degree. The total influence has been controlled to a 10 -15 level with τ ≤ 100 s, allowing realizing a 10 -14 τ -1 / 2 level short-term stability. When τ > 100 s, however, the influence increases\nFig. 10. Contributions of environmental effects to frequency stability of the RAFS. All the results are obtained when the PP is inside the sealed box.\nFig. 11. Frequency stability test scheme for the RAFS. The test could be conducted by using an H-maser or an ultrastable laser based OMG. The phase noise analyzer (3120A, Symmetricom) is used for frequency comparison of the RAFS with the selected reference.\nwith τ considerably, which means that the environmental effect will influence the medium-term stability of the RAFS. The influence comes mainly from the temperature shift and the barometric shift, as can be seen from Fig. 10.",
    "context": "The text highlights the significant influence of environmental factors, particularly barometric pressure, on the RAFS’s frequency stability. Specifically, the barometric pressure effect, measured to be 7 × 10 -15 /Pa, contributes to a 2.2 × 10 -14 stability over 100 seconds. To mitigate this, a sealed box was implemented, reducing the influence by nearly an order of magnitude.",
    "document": "Realization_of_a_Rubidium_Atomic_Frequency_Standard_With_Short-Term_Stability_in_10141_2_Level.pdf",
    "pages": [
      4,
      5
    ],
    "id": "01926c4059eb6fc480fa4bf53529bf3c4fedab137a09fcb92ea7698847ff8e7e"
  },
  {
    "text": "Two references were employed for the frequency stability test of the RAFS. One is a high-performance active hydrogen maser (H-maser) produced by KVARZ (CH1-95), and the other, an ultrastable laser based optical microwave generator (OMG) developed by Qunfeng Chen's group [18]. The test scheme is displayed in Fig. 11.\nIt should be noticed that, if the tested RAFS and the reference have the same magnitude order of short-term frequency stability, the test result does not reflect the real performance of the RAFS. In this case, the contribution of the reference to the test result should be deducted, and the real stability of the RAFS is then\n<!-- formula-not-decoded -->\nwhere σ test (τ) is the test stability of the RAFS and σ R (τ) is the stability of the used reference, which is assumed to be known.\nThe stability result of the RAFS by using the H-maser as the reference is shown in Fig. 12. The result was obtained based on a 5000-s-long frequency data with an averaging time of 1 s. To avoid the influence of frequency drift, the stability performance is given in drift removed Allan deviation. From the circle dot data, the test stability of the RAFS was estimated\nFig. 12. Frequency stability test result of the RAFS with the H-maser as the reference.\nFig. 13. Frequency stability test result of the RAFS with the OMG as the reference.\nto be 1.0 × 10 -13 τ -1 / 2 with averaging time τ up to 100 s. The stability goes worse when τ is larger than 100 s; we believe this is caused by the environmental effect (see Fig. 10). By using the test stability data with τ no larger than 100 s and stability data of the H-maser (square dots in Fig. 12), real stability data with τ up to 100 s was obtained (diamond dots in Fig. 12). By fitting the diamond dot data, the real stability of the RAFS was finally obtained to be 9.0 × 10 -14 τ -1 / 2 with τ up to 100 s (dashed line in Fig. 12).\nStability result by using the OMG as the reference is shown in Fig. 13. Considering excellent short-term stability but poor medium-term stability of the reference, we give here only the short-term stability result with τ ≤ 100 s. As shown in Fig. 13, the test stability of the RAFS is about 9.7 × 10 -14 τ -1 / 2 , and real stability is calculated to be 9.1 × 10 -14 τ -1 / 2 up to 100-s averaging time and strongly supports the result obtained with H-maser.\nBased on the SNR limited stability of 4.7 × 10 -14 τ -1 / 2 and the phase noise limited stability of 6.0 × 10 -14 τ -1 / 2 , the stability of the RAFS was predicted to be 7.6 × 10 -14 τ -1 / 2 . The measured stability result is basically consistent with the predicted one.\n\nProvides a detailed methodology for evaluating the RAFS stability using reference standards (H-maser and OMG) and outlines the process for correcting the test results to determine the true RAFS stability.",
    "original_text": "Two references were employed for the frequency stability test of the RAFS. One is a high-performance active hydrogen maser (H-maser) produced by KVARZ (CH1-95), and the other, an ultrastable laser based optical microwave generator (OMG) developed by Qunfeng Chen's group [18]. The test scheme is displayed in Fig. 11.\nIt should be noticed that, if the tested RAFS and the reference have the same magnitude order of short-term frequency stability, the test result does not reflect the real performance of the RAFS. In this case, the contribution of the reference to the test result should be deducted, and the real stability of the RAFS is then\n<!-- formula-not-decoded -->\nwhere σ test (τ) is the test stability of the RAFS and σ R (τ) is the stability of the used reference, which is assumed to be known.\nThe stability result of the RAFS by using the H-maser as the reference is shown in Fig. 12. The result was obtained based on a 5000-s-long frequency data with an averaging time of 1 s. To avoid the influence of frequency drift, the stability performance is given in drift removed Allan deviation. From the circle dot data, the test stability of the RAFS was estimated\nFig. 12. Frequency stability test result of the RAFS with the H-maser as the reference.\nFig. 13. Frequency stability test result of the RAFS with the OMG as the reference.\nto be 1.0 × 10 -13 τ -1 / 2 with averaging time τ up to 100 s. The stability goes worse when τ is larger than 100 s; we believe this is caused by the environmental effect (see Fig. 10). By using the test stability data with τ no larger than 100 s and stability data of the H-maser (square dots in Fig. 12), real stability data with τ up to 100 s was obtained (diamond dots in Fig. 12). By fitting the diamond dot data, the real stability of the RAFS was finally obtained to be 9.0 × 10 -14 τ -1 / 2 with τ up to 100 s (dashed line in Fig. 12).\nStability result by using the OMG as the reference is shown in Fig. 13. Considering excellent short-term stability but poor medium-term stability of the reference, we give here only the short-term stability result with τ ≤ 100 s. As shown in Fig. 13, the test stability of the RAFS is about 9.7 × 10 -14 τ -1 / 2 , and real stability is calculated to be 9.1 × 10 -14 τ -1 / 2 up to 100-s averaging time and strongly supports the result obtained with H-maser.\nBased on the SNR limited stability of 4.7 × 10 -14 τ -1 / 2 and the phase noise limited stability of 6.0 × 10 -14 τ -1 / 2 , the stability of the RAFS was predicted to be 7.6 × 10 -14 τ -1 / 2 . The measured stability result is basically consistent with the predicted one.",
    "context": "Provides a detailed methodology for evaluating the RAFS stability using reference standards (H-maser and OMG) and outlines the process for correcting the test results to determine the true RAFS stability.",
    "document": "Realization_of_a_Rubidium_Atomic_Frequency_Standard_With_Short-Term_Stability_in_10141_2_Level.pdf",
    "pages": [
      5,
      6
    ],
    "id": "3e31c901c68de7f54cdd93ec867807abd5bede9dfb934e911c18b2c18e502397"
  },
  {
    "text": "A lamp-pumped RAFS with short-term stability in 10 -14 τ -1 / 2 level has been realized. In design of the RAFS, the influences of the SNR of atomic discrimination signal and the phase noise of the interrogation microwave to stability of the RAFS were controlled to 4.7 × 10 -14 τ -1 / 2 and 6.0 × 10 -14 τ -1 / 2 , respectively, and the influence of the environmental effect was controlled to 10 -15 τ -1 / 2 level. Typical short-term stability of the RAFS reached 9.0 × 10 -14 τ -1 / 2 (1-100 s), and symbolizing the vapor cell atomic frequency standard, including the traditional lamp-pumped RAFS and the new generation laser-pumped RAFS, has unprecedentedly achieved a stability in 10 -14 τ -1 / 2 level. To the best of our knowledge, the previous best result is 1.2 × 10 -13 τ -1 / 2 , which is realized recently on a pulsed laserpumped RAFS [19]. The current RAFS device may provide a high-quality oscillator technology, since its 1-s stability has been close to that of the existing best crystal oscillator, while its 10- and 100-s stability and drift performances are much better. If the long-term stability can be improved further, the overall stability of the RAFS will be comparable to that of an active hydrogen maser. Such a compact device can be used as a new generation space clock for satellite navigation system.\n\nSummarizes the achievement of a 10^-14 τ^-1/2 short-term stability level in the RAFS, highlighting its performance compared to previous results and potential as a high-quality oscillator for space clocks.",
    "original_text": "A lamp-pumped RAFS with short-term stability in 10 -14 τ -1 / 2 level has been realized. In design of the RAFS, the influences of the SNR of atomic discrimination signal and the phase noise of the interrogation microwave to stability of the RAFS were controlled to 4.7 × 10 -14 τ -1 / 2 and 6.0 × 10 -14 τ -1 / 2 , respectively, and the influence of the environmental effect was controlled to 10 -15 τ -1 / 2 level. Typical short-term stability of the RAFS reached 9.0 × 10 -14 τ -1 / 2 (1-100 s), and symbolizing the vapor cell atomic frequency standard, including the traditional lamp-pumped RAFS and the new generation laser-pumped RAFS, has unprecedentedly achieved a stability in 10 -14 τ -1 / 2 level. To the best of our knowledge, the previous best result is 1.2 × 10 -13 τ -1 / 2 , which is realized recently on a pulsed laserpumped RAFS [19]. The current RAFS device may provide a high-quality oscillator technology, since its 1-s stability has been close to that of the existing best crystal oscillator, while its 10- and 100-s stability and drift performances are much better. If the long-term stability can be improved further, the overall stability of the RAFS will be comparable to that of an active hydrogen maser. Such a compact device can be used as a new generation space clock for satellite navigation system.",
    "context": "Summarizes the achievement of a 10^-14 τ^-1/2 short-term stability level in the RAFS, highlighting its performance compared to previous results and potential as a high-quality oscillator for space clocks.",
    "document": "Realization_of_a_Rubidium_Atomic_Frequency_Standard_With_Short-Term_Stability_in_10141_2_Level.pdf",
    "pages": [
      6
    ],
    "id": "9cdc80ac2b54afd6e34f9f1da1934e6bbdd2a065001291fc8ba0b4586e2c0d5e"
  },
  {
    "text": "The authors acknowledge Prof. Qunfeng Chen and his team for providing their OMG device for frequency stability test of the RAFS. They also like to thank Dr. Pengcheng Fang for his helps in the testing process.\n\nAcknowledges Prof. Qunfeng Chen and his team for supplying the OMG device and thanks Dr. Pengcheng Fang for assistance with the testing.",
    "original_text": "The authors acknowledge Prof. Qunfeng Chen and his team for providing their OMG device for frequency stability test of the RAFS. They also like to thank Dr. Pengcheng Fang for his helps in the testing process.",
    "context": "Acknowledges Prof. Qunfeng Chen and his team for supplying the OMG device and thanks Dr. Pengcheng Fang for assistance with the testing.",
    "document": "Realization_of_a_Rubidium_Atomic_Frequency_Standard_With_Short-Term_Stability_in_10141_2_Level.pdf",
    "pages": [
      6
    ],
    "id": "e5497479674f002b9f37f85298f4e2936435a80f853aa8ce9cdebe8442803ca0"
  },
  {
    "text": "- [1] A. O. McCoubrey, 'A survey of atomic frequency standards,' Proc. IEEE , vol. 54, no. 2, pp. 116-135, Feb. 1966, doi: 10.1109/PROC.1966.4626.\n- [2] J. Camparo and A. Hudson, 'Mesoscopic physics in vapor-cell atomic clocks,' in Proc. Joint Conf. Eur. Freq. Time Forum IEEE Int. Freq. Control Symp. (EFTF/IFCS) , Besancon, France, Jul. 2017, pp. 47-54, doi: 10.1109/FCS.2017.8088797.\n- [3] T. B. McCaskill, W. G. Reid, O. J. Oaks, R. L. Beard, J. A. Buisson, and H. E. Warren, 'Performance of global positioning system (GPS) on-orbit navstar clocks,' in Proc. IEEE Int. Freq. Control Symp. (49th Annu. Symp.) , Dana Point, CA, USA, May 1999, pp. 75-89.\n- [4] R. T. Dupuis, T. J. Lynch, and J. R. Vaccaro, 'Rubidium frequency standard for the GPS IIF program and modifications for the RAFSMOD program,' in Proc. IEEE IFCS , Honolulu, HI, USA, May 2008, pp. 655-660, doi: 10.1109/FREQ.2008.4623081.\n- [5] G. Mei et al., 'Main features of space Rubidium atomic frequency standard for BeiDou satellites,' in Proc. Eur. Freq. Time Forum (EFTF) , Apr. 2016, pp. 1-4, doi: 10.1109/EFTF.2016.7477803.\n- [6] S. Yan et al., 'Characteristics of the space-borne Rubidium atomic clocks for the BeiDou III navigation satellite system,' SCIENTIA SINICA Phys., Mechanica Astronomica , vol. 51, no. 1, Dec. 2020, Art. no. 019512, doi: 10.1360/SSPMA-2020-0245.\n- [7] Q. Hao, W. Li, S. He, J. Lv, P. Wang, and G. Mei, 'A physics package for Rubidium atomic frequency standard with a short-term stability of 2.4 × 10 -13 τ -1 / 2 ,' Rev. Sci. Instrum. , vol. 87, no. 12, Dec. 2016, Art. no. 123111, doi: 10.1063/1.4972567.\n- [8] S. Nie et al., 'A lamp-pumped Rubidium atomic frequency standard with a short-term stability at the level of 2 × 10 -13 τ -1 / 2 ,' in Proc. CSNC , Beijing, China, 2019, pp. 556-563, doi: 10.1007/978-981-137751-8_53.\n- [9] J. Cui, G. Ming, F. Wang, X. Niu, G. Mei, and D. Zhong, 'Design and studies of an ultra high-performance physics package for vaporcell Rubidium atomic clock,' in Proc. CSNC , Beijing, China, 2022, pp. 403-414, doi: 10.1007/978-981-19-2576-4_36.\n- [10] C. Audoin, V. Candelier, and N. Diamarcq, 'A limit to the frequency stability of passive frequency standards due to an intermodulation effect,' IEEE Trans. Instrum. Meas. , vol. 40, no. 2, pp. 121-125, Apr. 1991, doi: 10.1109/TIM.1990.1032896.\n[11] G. Mileti, J. Q. Deng, F. L. Walls, J. P. Lowe, and R. E. Drullinger, 'Recent progress in laser-pumped Rubidium gas-cell frequency standards,' in Proc. IEEE Int. Freq. Control Symp. , Honolulu, HI, USA, Jun. 1996, pp. 1066-1072, doi: 10.1109/FREQ.1996.560295.\n- [12] Q. Hao, S. He, F. Xu, F. Zhao, and G. Mei, 'Influence of lamp spectral profile on short-term stability and light shift of a Rubidium atomic clock,' in Proc. CSNC , Xi'an, China, 2015, pp. 387-397, doi: 10.1007/978-3-662-46632-2_34.\n- [13] G. Mei, D. Zhong, S. An, J. Liu, and X. Huang, 'Miniaturized microwave cavity for atomic frequency standard,' U.S. Patent US6225870 B1, May 1, 2001.\n- [14] B. Xia, D. Zhong, S. An, and G. Mei, 'Characteristics of a novel kind of miniaturized cavity-cell assembly for Rubidium frequency standards,' IEEE Trans. Instrum. Meas. , vol. 55, no. 3, pp. 1000-1005, Jun. 2006, doi: 10.1109/TIM.2006.873786.\n- [15] C. Stefanucci et al., 'Compact microwave cavity for high performance Rubidium frequency standards,' Rev. Scientific Instrum. , vol. 83, no. 10, Oct. 2012, Art. no. 104706, doi: 10.1063/1.4759023.\n- [16] F. Xu, Q. Hao, P. Wang, G. Ming, and G. Mei, 'A high signal to noise ratio physics package with a slotted-tube cavity for Rubidium atomic clock,' Acta. Metrologica. Sinica. , vol. 37, no. 4, pp. 437-440, May 2016, doi: 10.3969/j.issn.1000-1158.2016.04.23.\n[17] W. Moreno, M. Pellaton, C. Affolderbach, and G. Mileti, 'Barometric effect in vapor-cell atomic clocks,' IEEE Trans. Ultrason., Ferroelectr., Freq. Control , vol. 65, no. 8, pp. 1500-1503, Aug. 2018, doi: 10.1109/TUFFC.2018.2844020.\n- [18] R. Xiao, Y. Xu, Y. Wang, H. Sun, and Q. Chen, 'Transportable 30 cm optical cavity based ultrastable lasers with beating instability of 2 × 10 -16 ,' Appl. Phys. B, Lasers Opt. , vol. 128, Nov. 2022, Art. no. 220, doi: 10.1007/s00340-022-07938-0.\n- [19] M. Gozzelino et al., 'Realization of a pulsed optically pumped RB clock with a frequency stability below 10 -15 ,' Sci. Rep. , vol. 13, no. 1, Aug. 2023, Art. no. 12974, doi: 10.1038/s41598-023-39942-5.\nJiaqi Cui (Member, IEEE) received the B.S. degree in physics from Wuhan University, Wuhan, China, in 2016. He is currently pursuing the Ph.D. degree with the Innovation Academy for Precision Measurement (APM) Science and Technology, CAS, Wuhan.\nHis doctoral research study is the enhancement of SNR of physics package of the RAFS.\nGang Ming received the B.S. degree in electronic information from Wuhan University, Wuhan, China, in 2005, and the Ph.D. degree from the Wuhan Institute of Physics and Mathematics (WIPM, former APM), CAS, Wuhan, in 2012.\nHe is currently a Senior Engineer with APM, CAS. His main research interests include electronics design, parameter optimization, and environmental effect of the RAFS.\nFang Wang received the B.S. degree from Central China Normal University, Wuhan, China, in 2003, and the Ph.D. degree from WIPM, Wuhan, in 2008.\nShe is currently a Research Professor with APM, CAS, Wuhan. Her main work involves rubidium spectral lamp, lifetime, and reliability of the RAFS.\nJunyao Li received the B.S. degree in optoelectronic information engineering from the Huazhong University of Science and Technology, Wuhan, China, in 2016. He is currently pursuing the Ph.D. degree with APM, CAS, Wuhan.\nHis research work involves barometric effect and frequency drift of the RAFS.\n\nProvides a historical overview of Rubidium atomic frequency standards, including their development in space applications (BeiDou satellites) and research into improving their stability and signal-to-noise ratio.",
    "original_text": "- [1] A. O. McCoubrey, 'A survey of atomic frequency standards,' Proc. IEEE , vol. 54, no. 2, pp. 116-135, Feb. 1966, doi: 10.1109/PROC.1966.4626.\n- [2] J. Camparo and A. Hudson, 'Mesoscopic physics in vapor-cell atomic clocks,' in Proc. Joint Conf. Eur. Freq. Time Forum IEEE Int. Freq. Control Symp. (EFTF/IFCS) , Besancon, France, Jul. 2017, pp. 47-54, doi: 10.1109/FCS.2017.8088797.\n- [3] T. B. McCaskill, W. G. Reid, O. J. Oaks, R. L. Beard, J. A. Buisson, and H. E. Warren, 'Performance of global positioning system (GPS) on-orbit navstar clocks,' in Proc. IEEE Int. Freq. Control Symp. (49th Annu. Symp.) , Dana Point, CA, USA, May 1999, pp. 75-89.\n- [4] R. T. Dupuis, T. J. Lynch, and J. R. Vaccaro, 'Rubidium frequency standard for the GPS IIF program and modifications for the RAFSMOD program,' in Proc. IEEE IFCS , Honolulu, HI, USA, May 2008, pp. 655-660, doi: 10.1109/FREQ.2008.4623081.\n- [5] G. Mei et al., 'Main features of space Rubidium atomic frequency standard for BeiDou satellites,' in Proc. Eur. Freq. Time Forum (EFTF) , Apr. 2016, pp. 1-4, doi: 10.1109/EFTF.2016.7477803.\n- [6] S. Yan et al., 'Characteristics of the space-borne Rubidium atomic clocks for the BeiDou III navigation satellite system,' SCIENTIA SINICA Phys., Mechanica Astronomica , vol. 51, no. 1, Dec. 2020, Art. no. 019512, doi: 10.1360/SSPMA-2020-0245.\n- [7] Q. Hao, W. Li, S. He, J. Lv, P. Wang, and G. Mei, 'A physics package for Rubidium atomic frequency standard with a short-term stability of 2.4 × 10 -13 τ -1 / 2 ,' Rev. Sci. Instrum. , vol. 87, no. 12, Dec. 2016, Art. no. 123111, doi: 10.1063/1.4972567.\n- [8] S. Nie et al., 'A lamp-pumped Rubidium atomic frequency standard with a short-term stability at the level of 2 × 10 -13 τ -1 / 2 ,' in Proc. CSNC , Beijing, China, 2019, pp. 556-563, doi: 10.1007/978-981-137751-8_53.\n- [9] J. Cui, G. Ming, F. Wang, X. Niu, G. Mei, and D. Zhong, 'Design and studies of an ultra high-performance physics package for vaporcell Rubidium atomic clock,' in Proc. CSNC , Beijing, China, 2022, pp. 403-414, doi: 10.1007/978-981-19-2576-4_36.\n- [10] C. Audoin, V. Candelier, and N. Diamarcq, 'A limit to the frequency stability of passive frequency standards due to an intermodulation effect,' IEEE Trans. Instrum. Meas. , vol. 40, no. 2, pp. 121-125, Apr. 1991, doi: 10.1109/TIM.1990.1032896.\n[11] G. Mileti, J. Q. Deng, F. L. Walls, J. P. Lowe, and R. E. Drullinger, 'Recent progress in laser-pumped Rubidium gas-cell frequency standards,' in Proc. IEEE Int. Freq. Control Symp. , Honolulu, HI, USA, Jun. 1996, pp. 1066-1072, doi: 10.1109/FREQ.1996.560295.\n- [12] Q. Hao, S. He, F. Xu, F. Zhao, and G. Mei, 'Influence of lamp spectral profile on short-term stability and light shift of a Rubidium atomic clock,' in Proc. CSNC , Xi'an, China, 2015, pp. 387-397, doi: 10.1007/978-3-662-46632-2_34.\n- [13] G. Mei, D. Zhong, S. An, J. Liu, and X. Huang, 'Miniaturized microwave cavity for atomic frequency standard,' U.S. Patent US6225870 B1, May 1, 2001.\n- [14] B. Xia, D. Zhong, S. An, and G. Mei, 'Characteristics of a novel kind of miniaturized cavity-cell assembly for Rubidium frequency standards,' IEEE Trans. Instrum. Meas. , vol. 55, no. 3, pp. 1000-1005, Jun. 2006, doi: 10.1109/TIM.2006.873786.\n- [15] C. Stefanucci et al., 'Compact microwave cavity for high performance Rubidium frequency standards,' Rev. Scientific Instrum. , vol. 83, no. 10, Oct. 2012, Art. no. 104706, doi: 10.1063/1.4759023.\n- [16] F. Xu, Q. Hao, P. Wang, G. Ming, and G. Mei, 'A high signal to noise ratio physics package with a slotted-tube cavity for Rubidium atomic clock,' Acta. Metrologica. Sinica. , vol. 37, no. 4, pp. 437-440, May 2016, doi: 10.3969/j.issn.1000-1158.2016.04.23.\n[17] W. Moreno, M. Pellaton, C. Affolderbach, and G. Mileti, 'Barometric effect in vapor-cell atomic clocks,' IEEE Trans. Ultrason., Ferroelectr., Freq. Control , vol. 65, no. 8, pp. 1500-1503, Aug. 2018, doi: 10.1109/TUFFC.2018.2844020.\n- [18] R. Xiao, Y. Xu, Y. Wang, H. Sun, and Q. Chen, 'Transportable 30 cm optical cavity based ultrastable lasers with beating instability of 2 × 10 -16 ,' Appl. Phys. B, Lasers Opt. , vol. 128, Nov. 2022, Art. no. 220, doi: 10.1007/s00340-022-07938-0.\n- [19] M. Gozzelino et al., 'Realization of a pulsed optically pumped RB clock with a frequency stability below 10 -15 ,' Sci. Rep. , vol. 13, no. 1, Aug. 2023, Art. no. 12974, doi: 10.1038/s41598-023-39942-5.\nJiaqi Cui (Member, IEEE) received the B.S. degree in physics from Wuhan University, Wuhan, China, in 2016. He is currently pursuing the Ph.D. degree with the Innovation Academy for Precision Measurement (APM) Science and Technology, CAS, Wuhan.\nHis doctoral research study is the enhancement of SNR of physics package of the RAFS.\nGang Ming received the B.S. degree in electronic information from Wuhan University, Wuhan, China, in 2005, and the Ph.D. degree from the Wuhan Institute of Physics and Mathematics (WIPM, former APM), CAS, Wuhan, in 2012.\nHe is currently a Senior Engineer with APM, CAS. His main research interests include electronics design, parameter optimization, and environmental effect of the RAFS.\nFang Wang received the B.S. degree from Central China Normal University, Wuhan, China, in 2003, and the Ph.D. degree from WIPM, Wuhan, in 2008.\nShe is currently a Research Professor with APM, CAS, Wuhan. Her main work involves rubidium spectral lamp, lifetime, and reliability of the RAFS.\nJunyao Li received the B.S. degree in optoelectronic information engineering from the Huazhong University of Science and Technology, Wuhan, China, in 2016. He is currently pursuing the Ph.D. degree with APM, CAS, Wuhan.\nHis research work involves barometric effect and frequency drift of the RAFS.",
    "context": "Provides a historical overview of Rubidium atomic frequency standards, including their development in space applications (BeiDou satellites) and research into improving their stability and signal-to-noise ratio.",
    "document": "Realization_of_a_Rubidium_Atomic_Frequency_Standard_With_Short-Term_Stability_in_10141_2_Level.pdf",
    "pages": [
      6,
      7
    ],
    "id": "b880932af7a2a8e075c6ed78f729ced4e434b4ed4ac426e03c39ded24dc1b63f"
  },
  {
    "text": "Pengfei Wang received the B.S. degree from the China University of Geosciences, Wuhan, China, in 2009, and the Ph.D. degree from WIPM, CAS, Wuhan, in 2016.\nHe is currently a Senior Engineer with APM, CAS. His research work involves mainly physics package design for high-performance RAFS and compact RAFS.\nSongbai Kang received the B.S. degree from Wuhan University, Wuhan, China, in 2005, and the Ph.D. degree from WIPM, Wuhan, in 2011.\nAfterward, he completed postdoctoral researches with the University of Neuchâtel, Neuchatel, Switzerland, and the National Institute of Standards and Technology, Boulder, CO, USA. He is currently a Research Professor with APM, CAS, Wuhan. His earlier academic interest is in high SNR physics package of the RAFS. His current research interests include traditional and optical RAFS and atomic sensors.\nFeng Zhao received the B.S. degree from the Huazhong University of Science and Technology, Wuhan, China, in 2000, and the Ph.D. degree from WIPM, CAS, Wuhan, in 2007.\nHe is currently a Research Professor with APM, CAS. His earlier work is the design of long lifetime rubidium spectral lamp. His current research interests include physics package design and parameter optimization of the RAFS.\nDa Zhong received the B.S. and master's degrees in electronics from Beihang University, Beijing, China, in 1983 and 1988, respectively.\nHe is currently a Research Professor with APM, CAS, Wuhan, China. Since 1988, he has been engaged in researches on atomic frequency standards, including the passive hydrogen maser and the RAFS. His current interests include highperformance RAFS.\nGanghua Mei received the B.S. and master's degrees in physics from Wuhan University, Wuhan, China, in 1982 and 1985, respectively, and the Ph.D. degree in radio physics from WIPM, CAS, Wuhan, in 1996.\nIn 1997, he was engaged in researches on the RAFS. As the team head, he led the development of the space RAFS for BeiDou satellites. Since 1998, he has been a Research Professor with WIPM, CAS, and APM, CAS. His earlier academic interests are optical pumping and laser spectrum of atomic beam of alkali and other atoms. His current interests include high-performance RAFS and its applications.\n\nDetails the academic backgrounds and current research interests of researchers at APM, CAS, focusing on RAFS development.",
    "original_text": "Pengfei Wang received the B.S. degree from the China University of Geosciences, Wuhan, China, in 2009, and the Ph.D. degree from WIPM, CAS, Wuhan, in 2016.\nHe is currently a Senior Engineer with APM, CAS. His research work involves mainly physics package design for high-performance RAFS and compact RAFS.\nSongbai Kang received the B.S. degree from Wuhan University, Wuhan, China, in 2005, and the Ph.D. degree from WIPM, Wuhan, in 2011.\nAfterward, he completed postdoctoral researches with the University of Neuchâtel, Neuchatel, Switzerland, and the National Institute of Standards and Technology, Boulder, CO, USA. He is currently a Research Professor with APM, CAS, Wuhan. His earlier academic interest is in high SNR physics package of the RAFS. His current research interests include traditional and optical RAFS and atomic sensors.\nFeng Zhao received the B.S. degree from the Huazhong University of Science and Technology, Wuhan, China, in 2000, and the Ph.D. degree from WIPM, CAS, Wuhan, in 2007.\nHe is currently a Research Professor with APM, CAS. His earlier work is the design of long lifetime rubidium spectral lamp. His current research interests include physics package design and parameter optimization of the RAFS.\nDa Zhong received the B.S. and master's degrees in electronics from Beihang University, Beijing, China, in 1983 and 1988, respectively.\nHe is currently a Research Professor with APM, CAS, Wuhan, China. Since 1988, he has been engaged in researches on atomic frequency standards, including the passive hydrogen maser and the RAFS. His current interests include highperformance RAFS.\nGanghua Mei received the B.S. and master's degrees in physics from Wuhan University, Wuhan, China, in 1982 and 1985, respectively, and the Ph.D. degree in radio physics from WIPM, CAS, Wuhan, in 1996.\nIn 1997, he was engaged in researches on the RAFS. As the team head, he led the development of the space RAFS for BeiDou satellites. Since 1998, he has been a Research Professor with WIPM, CAS, and APM, CAS. His earlier academic interests are optical pumping and laser spectrum of atomic beam of alkali and other atoms. His current interests include high-performance RAFS and its applications.",
    "context": "Details the academic backgrounds and current research interests of researchers at APM, CAS, focusing on RAFS development.",
    "document": "Realization_of_a_Rubidium_Atomic_Frequency_Standard_With_Short-Term_Stability_in_10141_2_Level.pdf",
    "pages": [
      7
    ],
    "id": "1fdcb6df539980f1c52a7118799f26a599dfa3fe51f3766faefd092afec285c0"
  }
]