{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a9da9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package english_wordnet to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package english_wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to /home/martin/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "acf02e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from each page of a PDF file as a list of strings.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        text_blob (str): String containing the text of each page concatenated together.\n",
    "        \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_blob = \"\"\n",
    "    for page in doc:\n",
    "        text_blob += page.get_text(\"text\").replace('-\\n', '').replace('\\n', ' ')\n",
    "    doc.close()\n",
    "    return text_blob\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Removes English stopwords from a given text string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with stopwords removed.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def semantic_chunking(text, max_tokens=300, overlap=100):\n",
    "    # Break into sentences first\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Get sentence embeddings\n",
    "    sentence_embeddings = model.encode(sentences, normalize_embeddings=True)\n",
    "\n",
    "    # Track token count (approximate)\n",
    "    token_counts = [len(sentence.split()) for sentence in sentences]\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_token_count = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # If adding this sentence would exceed our limit, start a new chunk\n",
    "        if current_token_count + token_counts[i] > max_tokens and current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "            # For overlap, find the most semantically similar sentences to include\n",
    "            if overlap > 0 and len(current_chunk) > 0:\n",
    "                start = i-len(current_chunk)\n",
    "                # Get embeddings for current chunk sentences\n",
    "                current_embs = sentence_embeddings[start:i]\n",
    "                # Find sentences with highest similarity to include in overlap\n",
    "                similarities = cosine_similarity([sentence_embeddings[i]], current_embs)[0]\n",
    "                overlap_indices = np.argsort(similarities)[-int(overlap/10):]  # Heuristic for number of sentences\n",
    "                # Add overlapping sentences to new chunk\n",
    "                current_chunk = [sentences[start+idx] for idx in overlap_indices]\n",
    "                current_token_count = sum(token_counts[start+idx] for idx in overlap_indices)\n",
    "            else:\n",
    "                current_chunk = []\n",
    "                current_token_count = 0\n",
    "\n",
    "        current_chunk.append(sentence)\n",
    "        current_token_count += token_counts[i]\n",
    "\n",
    "    # Add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a28c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "55ee4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = extract_text_from_pdf(\"Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf\")\n",
    "extracted_text_chunked = semantic_chunking(extracted_text, max_tokens=300, overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "92e68d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Received July 29, 2018, accepted August 27, 2018, date of publication September 13, 2018, date of current version October 8, 2018. Digital Object Identifier 10.1109/ACCESS.2018.2869735 Stock Market Prediction via Multi-Source Multiple Instance Learning XI ZHANG 1, (Member, IEEE), SIYU QU1, JIEYUN HUANG 1, BINXING FANG1, AND PHILIP YU2, (Fellow, IEEE) 1Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing 100876, China 2Department of Computer Science, The University of Illinois at Chicago, Chicago, IL 60607, USA Corresponding author: Xi Zhang (zhangx@bupt.edu.cn) This work was supported in part by the State Key Development Program of Basic Research of China under Grant 2013CB329605, in part by the Natural Science Foundation of China under Grant 61300014, in part by the NSF under Grant IIS-1526499, Grant IIS-1763325, and Grant CNS-1626432, and in part by the DongGuan Innovative Research Team Program under Grant 201636000100038. ABSTRACT Forecasting the stock market movements is an important and challenging task. As the Web information grows, researchers begin to extract effective indicators (e.g., the events and sentiments) from the Web to facilitate the prediction. However, the indicators obtained in previous studies are usually based on only one data source and thus may not fully cover the factors that can affect the stock market movements. In this paper, to improve the prediction for stock market composite index movements, we exploit the consistencies among different data sources, and develop a multi-source multiple instance model that can effectively combine events, sentiments, as well as the quantitative data into a comprehensive framework. To effectively capture the news events, we successfully apply a novel event extraction and representation method. Evaluations on the data from the year 2015 and 2016 demonstrate the effectiveness of our model.',\n",
       " 'To effectively capture the news events, we successfully apply a novel event extraction and representation method. Received July 29, 2018, accepted August 27, 2018, date of publication September 13, 2018, date of current version October 8, 2018. However, the indicators obtained in previous studies are usually based on only one data source and thus may not fully cover the factors that can affect the stock market movements. Evaluations on the data from the year 2015 and 2016 demonstrate the effectiveness of our model. Digital Object Identifier 10.1109/ACCESS.2018.2869735 Stock Market Prediction via Multi-Source Multiple Instance Learning XI ZHANG 1, (Member, IEEE), SIYU QU1, JIEYUN HUANG 1, BINXING FANG1, AND PHILIP YU2, (Fellow, IEEE) 1Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing 100876, China 2Department of Computer Science, The University of Illinois at Chicago, Chicago, IL 60607, USA Corresponding author: Xi Zhang (zhangx@bupt.edu.cn) This work was supported in part by the State Key Development Program of Basic Research of China under Grant 2013CB329605, in part by the Natural Science Foundation of China under Grant 61300014, in part by the NSF under Grant IIS-1526499, Grant IIS-1763325, and Grant CNS-1626432, and in part by the DongGuan Innovative Research Team Program under Grant 201636000100038. ABSTRACT Forecasting the stock market movements is an important and challenging task. As the Web information grows, researchers begin to extract effective indicators (e.g., the events and sentiments) from the Web to facilitate the prediction. In this paper, to improve the prediction for stock market composite index movements, we exploit the consistencies among different data sources, and develop a multi-source multiple instance model that can effectively combine events, sentiments, as well as the quantitative data into a comprehensive framework. In addition, our approach is able to automatically determine the importance of each data source and identify the crucial input information that is considered to drive the movements, making the predictions interpretable.',\n",
       " 'Received July 29, 2018, accepted August 27, 2018, date of publication September 13, 2018, date of current version October 8, 2018. Evaluations on the data from the year 2015 and 2016 demonstrate the effectiveness of our model. In addition, our approach is able to automatically determine the importance of each data source and identify the crucial input information that is considered to drive the movements, making the predictions interpretable. However, the indicators obtained in previous studies are usually based on only one data source and thus may not fully cover the factors that can affect the stock market movements. To effectively capture the news events, we successfully apply a novel event extraction and representation method. Digital Object Identifier 10.1109/ACCESS.2018.2869735 Stock Market Prediction via Multi-Source Multiple Instance Learning XI ZHANG 1, (Member, IEEE), SIYU QU1, JIEYUN HUANG 1, BINXING FANG1, AND PHILIP YU2, (Fellow, IEEE) 1Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing 100876, China 2Department of Computer Science, The University of Illinois at Chicago, Chicago, IL 60607, USA Corresponding author: Xi Zhang (zhangx@bupt.edu.cn) This work was supported in part by the State Key Development Program of Basic Research of China under Grant 2013CB329605, in part by the Natural Science Foundation of China under Grant 61300014, in part by the NSF under Grant IIS-1526499, Grant IIS-1763325, and Grant CNS-1626432, and in part by the DongGuan Innovative Research Team Program under Grant 201636000100038. ABSTRACT Forecasting the stock market movements is an important and challenging task. As the Web information grows, researchers begin to extract effective indicators (e.g., the events and sentiments) from the Web to facilitate the prediction. In this paper, to improve the prediction for stock market composite index movements, we exploit the consistencies among different data sources, and develop a multi-source multiple instance model that can effectively combine events, sentiments, as well as the quantitative data into a comprehensive framework. INDEX TERMS Stock prediction, multiple instance, event extraction, sentiment analysis.',\n",
       " 'However, the indicators obtained in previous studies are usually based on only one data source and thus may not fully cover the factors that can affect the stock market movements. In addition, our approach is able to automatically determine the importance of each data source and identify the crucial input information that is considered to drive the movements, making the predictions interpretable. ABSTRACT Forecasting the stock market movements is an important and challenging task. To effectively capture the news events, we successfully apply a novel event extraction and representation method. In this paper, to improve the prediction for stock market composite index movements, we exploit the consistencies among different data sources, and develop a multi-source multiple instance model that can effectively combine events, sentiments, as well as the quantitative data into a comprehensive framework. INDEX TERMS Stock prediction, multiple instance, event extraction, sentiment analysis. Evaluations on the data from the year 2015 and 2016 demonstrate the effectiveness of our model. Digital Object Identifier 10.1109/ACCESS.2018.2869735 Stock Market Prediction via Multi-Source Multiple Instance Learning XI ZHANG 1, (Member, IEEE), SIYU QU1, JIEYUN HUANG 1, BINXING FANG1, AND PHILIP YU2, (Fellow, IEEE) 1Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing 100876, China 2Department of Computer Science, The University of Illinois at Chicago, Chicago, IL 60607, USA Corresponding author: Xi Zhang (zhangx@bupt.edu.cn) This work was supported in part by the State Key Development Program of Basic Research of China under Grant 2013CB329605, in part by the Natural Science Foundation of China under Grant 61300014, in part by the NSF under Grant IIS-1526499, Grant IIS-1763325, and Grant CNS-1626432, and in part by the DongGuan Innovative Research Team Program under Grant 201636000100038. Received July 29, 2018, accepted August 27, 2018, date of publication September 13, 2018, date of current version October 8, 2018. As the Web information grows, researchers begin to extract effective indicators (e.g., the events and sentiments) from the Web to facilitate the prediction. I.',\n",
       " 'Received July 29, 2018, accepted August 27, 2018, date of publication September 13, 2018, date of current version October 8, 2018. To effectively capture the news events, we successfully apply a novel event extraction and representation method. In addition, our approach is able to automatically determine the importance of each data source and identify the crucial input information that is considered to drive the movements, making the predictions interpretable. Evaluations on the data from the year 2015 and 2016 demonstrate the effectiveness of our model. As the Web information grows, researchers begin to extract effective indicators (e.g., the events and sentiments) from the Web to facilitate the prediction. Digital Object Identifier 10.1109/ACCESS.2018.2869735 Stock Market Prediction via Multi-Source Multiple Instance Learning XI ZHANG 1, (Member, IEEE), SIYU QU1, JIEYUN HUANG 1, BINXING FANG1, AND PHILIP YU2, (Fellow, IEEE) 1Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing 100876, China 2Department of Computer Science, The University of Illinois at Chicago, Chicago, IL 60607, USA Corresponding author: Xi Zhang (zhangx@bupt.edu.cn) This work was supported in part by the State Key Development Program of Basic Research of China under Grant 2013CB329605, in part by the Natural Science Foundation of China under Grant 61300014, in part by the NSF under Grant IIS-1526499, Grant IIS-1763325, and Grant CNS-1626432, and in part by the DongGuan Innovative Research Team Program under Grant 201636000100038. In this paper, to improve the prediction for stock market composite index movements, we exploit the consistencies among different data sources, and develop a multi-source multiple instance model that can effectively combine events, sentiments, as well as the quantitative data into a comprehensive framework. However, the indicators obtained in previous studies are usually based on only one data source and thus may not fully cover the factors that can affect the stock market movements. INDEX TERMS Stock prediction, multiple instance, event extraction, sentiment analysis. ABSTRACT Forecasting the stock market movements is an important and challenging task. INTRODUCTION Stock markets play important roles in the economic operations of modern society.',\n",
       " 'To effectively capture the news events, we successfully apply a novel event extraction and representation method. Evaluations on the data from the year 2015 and 2016 demonstrate the effectiveness of our model. In addition, our approach is able to automatically determine the importance of each data source and identify the crucial input information that is considered to drive the movements, making the predictions interpretable. Digital Object Identifier 10.1109/ACCESS.2018.2869735 Stock Market Prediction via Multi-Source Multiple Instance Learning XI ZHANG 1, (Member, IEEE), SIYU QU1, JIEYUN HUANG 1, BINXING FANG1, AND PHILIP YU2, (Fellow, IEEE) 1Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing 100876, China 2Department of Computer Science, The University of Illinois at Chicago, Chicago, IL 60607, USA Corresponding author: Xi Zhang (zhangx@bupt.edu.cn) This work was supported in part by the State Key Development Program of Basic Research of China under Grant 2013CB329605, in part by the Natural Science Foundation of China under Grant 61300014, in part by the NSF under Grant IIS-1526499, Grant IIS-1763325, and Grant CNS-1626432, and in part by the DongGuan Innovative Research Team Program under Grant 201636000100038. As the Web information grows, researchers begin to extract effective indicators (e.g., the events and sentiments) from the Web to facilitate the prediction. In this paper, to improve the prediction for stock market composite index movements, we exploit the consistencies among different data sources, and develop a multi-source multiple instance model that can effectively combine events, sentiments, as well as the quantitative data into a comprehensive framework. However, the indicators obtained in previous studies are usually based on only one data source and thus may not fully cover the factors that can affect the stock market movements. ABSTRACT Forecasting the stock market movements is an important and challenging task. INDEX TERMS Stock prediction, multiple instance, event extraction, sentiment analysis. INTRODUCTION Stock markets play important roles in the economic operations of modern society. The estimation of the stock market index is of clear interest to various stakeholders in the market.',\n",
       " 'To effectively capture the news events, we successfully apply a novel event extraction and representation method. Evaluations on the data from the year 2015 and 2016 demonstrate the effectiveness of our model. In addition, our approach is able to automatically determine the importance of each data source and identify the crucial input information that is considered to drive the movements, making the predictions interpretable. As the Web information grows, researchers begin to extract effective indicators (e.g., the events and sentiments) from the Web to facilitate the prediction. However, the indicators obtained in previous studies are usually based on only one data source and thus may not fully cover the factors that can affect the stock market movements. INTRODUCTION Stock markets play important roles in the economic operations of modern society. The estimation of the stock market index is of clear interest to various stakeholders in the market. INDEX TERMS Stock prediction, multiple instance, event extraction, sentiment analysis. In this paper, to improve the prediction for stock market composite index movements, we exploit the consistencies among different data sources, and develop a multi-source multiple instance model that can effectively combine events, sentiments, as well as the quantitative data into a comprehensive framework. ABSTRACT Forecasting the stock market movements is an important and challenging task. According to the Efﬁcient Market Hypothesis (EMH) [1], the stock market prices reﬂect all available information, and thus the prediction naturally relies on information from multiple sources, which can be roughly categorized into (1) quantitative data, e.g., historical prices, turnover rate, and (2) qualitative descriptions, such as the annual reports, announcements, news and social media posts. It is challenging to deal with qualitative data as they are usually unstructured and thus extracting useful signals from them is not trivial.',\n",
       " 'Evaluations on the data from the year 2015 and 2016 demonstrate the effectiveness of our model. In addition, our approach is able to automatically determine the importance of each data source and identify the crucial input information that is considered to drive the movements, making the predictions interpretable. However, the indicators obtained in previous studies are usually based on only one data source and thus may not fully cover the factors that can affect the stock market movements. The estimation of the stock market index is of clear interest to various stakeholders in the market. INTRODUCTION Stock markets play important roles in the economic operations of modern society. To effectively capture the news events, we successfully apply a novel event extraction and representation method. In this paper, to improve the prediction for stock market composite index movements, we exploit the consistencies among different data sources, and develop a multi-source multiple instance model that can effectively combine events, sentiments, as well as the quantitative data into a comprehensive framework. According to the Efﬁcient Market Hypothesis (EMH) [1], the stock market prices reﬂect all available information, and thus the prediction naturally relies on information from multiple sources, which can be roughly categorized into (1) quantitative data, e.g., historical prices, turnover rate, and (2) qualitative descriptions, such as the annual reports, announcements, news and social media posts. As the Web information grows, researchers begin to extract effective indicators (e.g., the events and sentiments) from the Web to facilitate the prediction. INDEX TERMS Stock prediction, multiple instance, event extraction, sentiment analysis. Along with the growing Web information and the advance of Natural Language Processing (NLP) techniques, recent works begin to explore Web news for market prediction.',\n",
       " 'It is challenging to deal with qualitative data as they are usually unstructured and thus extracting useful signals from them is not trivial. In addition, our approach is able to automatically determine the importance of each data source and identify the crucial input information that is considered to drive the movements, making the predictions interpretable. Evaluations on the data from the year 2015 and 2016 demonstrate the effectiveness of our model. To effectively capture the news events, we successfully apply a novel event extraction and representation method. In this paper, to improve the prediction for stock market composite index movements, we exploit the consistencies among different data sources, and develop a multi-source multiple instance model that can effectively combine events, sentiments, as well as the quantitative data into a comprehensive framework. INDEX TERMS Stock prediction, multiple instance, event extraction, sentiment analysis. The estimation of the stock market index is of clear interest to various stakeholders in the market. According to the Efﬁcient Market Hypothesis (EMH) [1], the stock market prices reﬂect all available information, and thus the prediction naturally relies on information from multiple sources, which can be roughly categorized into (1) quantitative data, e.g., historical prices, turnover rate, and (2) qualitative descriptions, such as the annual reports, announcements, news and social media posts. INTRODUCTION Stock markets play important roles in the economic operations of modern society. Along with the growing Web information and the advance of Natural Language Processing (NLP) techniques, recent works begin to explore Web news for market prediction. A number of existing studies have shown that the events reported in news are important signals that can drive market ﬂuctuations [2]–[4]. However, most of the previous works represent news documents using simple features (e.g., bagwords, noun phrases, named entities) [5], [6], which may discard syntax information.',\n",
       " 'However, most of the previous works represent news documents using simple features (e.g., bagwords, noun phrases, named entities) [5], [6], which may discard syntax information. It is challenging to deal with qualitative data as they are usually unstructured and thus extracting useful signals from them is not trivial. In addition, our approach is able to automatically determine the importance of each data source and identify the crucial input information that is considered to drive the movements, making the predictions interpretable. The estimation of the stock market index is of clear interest to various stakeholders in the market. Along with the growing Web information and the advance of Natural Language Processing (NLP) techniques, recent works begin to explore Web news for market prediction. INTRODUCTION Stock markets play important roles in the economic operations of modern society. To effectively capture the news events, we successfully apply a novel event extraction and representation method. According to the Efﬁcient Market Hypothesis (EMH) [1], the stock market prices reﬂect all available information, and thus the prediction naturally relies on information from multiple sources, which can be roughly categorized into (1) quantitative data, e.g., historical prices, turnover rate, and (2) qualitative descriptions, such as the annual reports, announcements, news and social media posts. INDEX TERMS Stock prediction, multiple instance, event extraction, sentiment analysis. A number of existing studies have shown that the events reported in news are important signals that can drive market ﬂuctuations [2]–[4]. Due to the large volume and diverse expressions of the events, how to represent them as useful features, and how to identify the crucial events that have signiﬁcant impacts on the stock market are not trivial problems. In addition to events, a line of studies has shown that the investors’ opinions can also largely inﬂuence the market volatility [7], [8].',\n",
       " 'The estimation of the stock market index is of clear interest to various stakeholders in the market. In addition, our approach is able to automatically determine the importance of each data source and identify the crucial input information that is considered to drive the movements, making the predictions interpretable. Due to the large volume and diverse expressions of the events, how to represent them as useful features, and how to identify the crucial events that have signiﬁcant impacts on the stock market are not trivial problems. However, most of the previous works represent news documents using simple features (e.g., bagwords, noun phrases, named entities) [5], [6], which may discard syntax information. It is challenging to deal with qualitative data as they are usually unstructured and thus extracting useful signals from them is not trivial. A number of existing studies have shown that the events reported in news are important signals that can drive market ﬂuctuations [2]–[4]. In addition to events, a line of studies has shown that the investors’ opinions can also largely inﬂuence the market volatility [7], [8]. According to the Efﬁcient Market Hypothesis (EMH) [1], the stock market prices reﬂect all available information, and thus the prediction naturally relies on information from multiple sources, which can be roughly categorized into (1) quantitative data, e.g., historical prices, turnover rate, and (2) qualitative descriptions, such as the annual reports, announcements, news and social media posts. INDEX TERMS Stock prediction, multiple instance, event extraction, sentiment analysis. Along with the growing Web information and the advance of Natural Language Processing (NLP) techniques, recent works begin to explore Web news for market prediction. With the prosperity of Web 2.0, the sentiments extracted from social media can be beneﬁcial to predictions.',\n",
       " 'It is challenging to deal with qualitative data as they are usually unstructured and thus extracting useful signals from them is not trivial. However, most of the previous works represent news documents using simple features (e.g., bagwords, noun phrases, named entities) [5], [6], which may discard syntax information. The estimation of the stock market index is of clear interest to various stakeholders in the market. INTRODUCTION Stock markets play important roles in the economic operations of modern society. According to the Efﬁcient Market Hypothesis (EMH) [1], the stock market prices reﬂect all available information, and thus the prediction naturally relies on information from multiple sources, which can be roughly categorized into (1) quantitative data, e.g., historical prices, turnover rate, and (2) qualitative descriptions, such as the annual reports, announcements, news and social media posts. A number of existing studies have shown that the events reported in news are important signals that can drive market ﬂuctuations [2]–[4]. With the prosperity of Web 2.0, the sentiments extracted from social media can be beneﬁcial to predictions. Due to the large volume and diverse expressions of the events, how to represent them as useful features, and how to identify the crucial events that have signiﬁcant impacts on the stock market are not trivial problems. Along with the growing Web information and the advance of Natural Language Processing (NLP) techniques, recent works begin to explore Web news for market prediction. In addition to events, a line of studies has shown that the investors’ opinions can also largely inﬂuence the market volatility [7], [8]. Since both events and sentiments can drive the ﬂuctuations of the market, it is natural to investigate how to effectively fuse them together to make a better prediction.',\n",
       " 'It is challenging to deal with qualitative data as they are usually unstructured and thus extracting useful signals from them is not trivial. However, most of the previous works represent news documents using simple features (e.g., bagwords, noun phrases, named entities) [5], [6], which may discard syntax information. The estimation of the stock market index is of clear interest to various stakeholders in the market. Due to the large volume and diverse expressions of the events, how to represent them as useful features, and how to identify the crucial events that have signiﬁcant impacts on the stock market are not trivial problems. A number of existing studies have shown that the events reported in news are important signals that can drive market ﬂuctuations [2]–[4]. In addition to events, a line of studies has shown that the investors’ opinions can also largely inﬂuence the market volatility [7], [8]. Along with the growing Web information and the advance of Natural Language Processing (NLP) techniques, recent works begin to explore Web news for market prediction. With the prosperity of Web 2.0, the sentiments extracted from social media can be beneﬁcial to predictions. Since both events and sentiments can drive the ﬂuctuations of the market, it is natural to investigate how to effectively fuse them together to make a better prediction. According to the Efﬁcient Market Hypothesis (EMH) [1], the stock market prices reﬂect all available information, and thus the prediction naturally relies on information from multiple sources, which can be roughly categorized into (1) quantitative data, e.g., historical prices, turnover rate, and (2) qualitative descriptions, such as the annual reports, announcements, news and social media posts. The improvement may come from the correlations among different sources, and the consensus prediction with multisource information can potentially outperform each prediction relying on a single source.',\n",
       " 'A number of existing studies have shown that the events reported in news are important signals that can drive market ﬂuctuations [2]–[4]. However, most of the previous works represent news documents using simple features (e.g., bagwords, noun phrases, named entities) [5], [6], which may discard syntax information. In addition to events, a line of studies has shown that the investors’ opinions can also largely inﬂuence the market volatility [7], [8]. Due to the large volume and diverse expressions of the events, how to represent them as useful features, and how to identify the crucial events that have signiﬁcant impacts on the stock market are not trivial problems. According to the Efﬁcient Market Hypothesis (EMH) [1], the stock market prices reﬂect all available information, and thus the prediction naturally relies on information from multiple sources, which can be roughly categorized into (1) quantitative data, e.g., historical prices, turnover rate, and (2) qualitative descriptions, such as the annual reports, announcements, news and social media posts. The estimation of the stock market index is of clear interest to various stakeholders in the market. Along with the growing Web information and the advance of Natural Language Processing (NLP) techniques, recent works begin to explore Web news for market prediction. With the prosperity of Web 2.0, the sentiments extracted from social media can be beneﬁcial to predictions. Since both events and sentiments can drive the ﬂuctuations of the market, it is natural to investigate how to effectively fuse them together to make a better prediction. The improvement may come from the correlations among different sources, and the consensus prediction with multisource information can potentially outperform each prediction relying on a single source. This problem is analogous to the multi-labeler learning problem in crowdsourcing [9], [10], 50720 2169-3536   2018 IEEE.',\n",
       " 'This problem is analogous to the multi-labeler learning problem in crowdsourcing [9], [10], 50720 2169-3536   2018 IEEE. According to the Efﬁcient Market Hypothesis (EMH) [1], the stock market prices reﬂect all available information, and thus the prediction naturally relies on information from multiple sources, which can be roughly categorized into (1) quantitative data, e.g., historical prices, turnover rate, and (2) qualitative descriptions, such as the annual reports, announcements, news and social media posts. The improvement may come from the correlations among different sources, and the consensus prediction with multisource information can potentially outperform each prediction relying on a single source. A number of existing studies have shown that the events reported in news are important signals that can drive market ﬂuctuations [2]–[4]. Due to the large volume and diverse expressions of the events, how to represent them as useful features, and how to identify the crucial events that have signiﬁcant impacts on the stock market are not trivial problems. Since both events and sentiments can drive the ﬂuctuations of the market, it is natural to investigate how to effectively fuse them together to make a better prediction. It is challenging to deal with qualitative data as they are usually unstructured and thus extracting useful signals from them is not trivial. Along with the growing Web information and the advance of Natural Language Processing (NLP) techniques, recent works begin to explore Web news for market prediction. However, most of the previous works represent news documents using simple features (e.g., bagwords, noun phrases, named entities) [5], [6], which may discard syntax information. With the prosperity of Web 2.0, the sentiments extracted from social media can be beneﬁcial to predictions. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution requires IEEE permission.',\n",
       " 'With the prosperity of Web 2.0, the sentiments extracted from social media can be beneﬁcial to predictions. A number of existing studies have shown that the events reported in news are important signals that can drive market ﬂuctuations [2]–[4]. Due to the large volume and diverse expressions of the events, how to represent them as useful features, and how to identify the crucial events that have signiﬁcant impacts on the stock market are not trivial problems. The improvement may come from the correlations among different sources, and the consensus prediction with multisource information can potentially outperform each prediction relying on a single source. It is challenging to deal with qualitative data as they are usually unstructured and thus extracting useful signals from them is not trivial. Along with the growing Web information and the advance of Natural Language Processing (NLP) techniques, recent works begin to explore Web news for market prediction. This problem is analogous to the multi-labeler learning problem in crowdsourcing [9], [10], 50720 2169-3536   2018 IEEE. Translations and content mining are permitted for academic research only. However, most of the previous works represent news documents using simple features (e.g., bagwords, noun phrases, named entities) [5], [6], which may discard syntax information. Personal use is also permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. VOLUME 6, 2018 X. Zhang et al. : Stock Market Prediction via Multi-Source Multiple Instance Learning but different from those studies that usually assume a labeler conducts classiﬁcation with full information, each ‘‘labeler’’ (i.e., classiﬁer) in this study is source-speciﬁc and only provided with limited information from its own source, making the consensus among labelers even more challenging.',\n",
       " 'This problem is analogous to the multi-labeler learning problem in crowdsourcing [9], [10], 50720 2169-3536   2018 IEEE. Translations and content mining are permitted for academic research only. However, most of the previous works represent news documents using simple features (e.g., bagwords, noun phrases, named entities) [5], [6], which may discard syntax information. The improvement may come from the correlations among different sources, and the consensus prediction with multisource information can potentially outperform each prediction relying on a single source. In addition to events, a line of studies has shown that the investors’ opinions can also largely inﬂuence the market volatility [7], [8]. : Stock Market Prediction via Multi-Source Multiple Instance Learning but different from those studies that usually assume a labeler conducts classiﬁcation with full information, each ‘‘labeler’’ (i.e., classiﬁer) in this study is source-speciﬁc and only provided with limited information from its own source, making the consensus among labelers even more challenging. With the prosperity of Web 2.0, the sentiments extracted from social media can be beneﬁcial to predictions. A number of existing studies have shown that the events reported in news are important signals that can drive market ﬂuctuations [2]–[4]. Due to the large volume and diverse expressions of the events, how to represent them as useful features, and how to identify the crucial events that have signiﬁcant impacts on the stock market are not trivial problems. Since both events and sentiments can drive the ﬂuctuations of the market, it is natural to investigate how to effectively fuse them together to make a better prediction. In this work, we aim to learn a predictive model for describing the ﬂuctuations in the stock market index by utilizing various sources of data, involving the historical quantitative data, the social media and Web news.',\n",
       " 'See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. VOLUME 6, 2018 X. Zhang et al. This problem is analogous to the multi-labeler learning problem in crowdsourcing [9], [10], 50720 2169-3536   2018 IEEE. : Stock Market Prediction via Multi-Source Multiple Instance Learning but different from those studies that usually assume a labeler conducts classiﬁcation with full information, each ‘‘labeler’’ (i.e., classiﬁer) in this study is source-speciﬁc and only provided with limited information from its own source, making the consensus among labelers even more challenging. The improvement may come from the correlations among different sources, and the consensus prediction with multisource information can potentially outperform each prediction relying on a single source. In addition to events, a line of studies has shown that the investors’ opinions can also largely inﬂuence the market volatility [7], [8]. Translations and content mining are permitted for academic research only. In this work, we aim to learn a predictive model for describing the ﬂuctuations in the stock market index by utilizing various sources of data, involving the historical quantitative data, the social media and Web news. With the prosperity of Web 2.0, the sentiments extracted from social media can be beneﬁcial to predictions. Since both events and sentiments can drive the ﬂuctuations of the market, it is natural to investigate how to effectively fuse them together to make a better prediction. The essential features we extract include the event representations from news articles and the sentiments from social media. Firstly, we propose a novel method to capture the event information. Speciﬁcally, structured events are extracted from news texts and then used as the inputs for Restricted Boltzmann Machines (RBMs) to do the pre-training. After that, the output vectors from RBMs are used as the inputs to a recently proposed sentence2vec framework [11], in order to achieve effective event embeddings.',\n",
       " 'VOLUME 6, 2018 X. Zhang et al. With the prosperity of Web 2.0, the sentiments extracted from social media can be beneﬁcial to predictions. The essential features we extract include the event representations from news articles and the sentiments from social media. In this work, we aim to learn a predictive model for describing the ﬂuctuations in the stock market index by utilizing various sources of data, involving the historical quantitative data, the social media and Web news. After that, the output vectors from RBMs are used as the inputs to a recently proposed sentence2vec framework [11], in order to achieve effective event embeddings. Speciﬁcally, structured events are extracted from news texts and then used as the inputs for Restricted Boltzmann Machines (RBMs) to do the pre-training. Since both events and sentiments can drive the ﬂuctuations of the market, it is natural to investigate how to effectively fuse them together to make a better prediction. This problem is analogous to the multi-labeler learning problem in crowdsourcing [9], [10], 50720 2169-3536   2018 IEEE. The improvement may come from the correlations among different sources, and the consensus prediction with multisource information can potentially outperform each prediction relying on a single source. : Stock Market Prediction via Multi-Source Multiple Instance Learning but different from those studies that usually assume a labeler conducts classiﬁcation with full information, each ‘‘labeler’’ (i.e., classiﬁer) in this study is source-speciﬁc and only provided with limited information from its own source, making the consensus among labelers even more challenging. Secondly, we exploit the latent relationships among different data sources with carefully designed loss terms, and propose an extension of the Multiple Instance Learning (MIL) model that can effectively integrate the features from multiple sources to make more accurate predictions.',\n",
       " 'Personal use is also permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. Firstly, we propose a novel method to capture the event information. Secondly, we exploit the latent relationships among different data sources with carefully designed loss terms, and propose an extension of the Multiple Instance Learning (MIL) model that can effectively integrate the features from multiple sources to make more accurate predictions. Speciﬁcally, structured events are extracted from news texts and then used as the inputs for Restricted Boltzmann Machines (RBMs) to do the pre-training. : Stock Market Prediction via Multi-Source Multiple Instance Learning but different from those studies that usually assume a labeler conducts classiﬁcation with full information, each ‘‘labeler’’ (i.e., classiﬁer) in this study is source-speciﬁc and only provided with limited information from its own source, making the consensus among labelers even more challenging. The essential features we extract include the event representations from news articles and the sentiments from social media. In this work, we aim to learn a predictive model for describing the ﬂuctuations in the stock market index by utilizing various sources of data, involving the historical quantitative data, the social media and Web news. VOLUME 6, 2018 X. Zhang et al. Translations and content mining are permitted for academic research only. One beneﬁt of our method is that we can determine source-speciﬁc weights and identify the speciﬁc factors that incur the changes in the composite index. Figure 1 shows an example of the news precursors identiﬁed by our model, and the dots with numbers denote the probabilistic estimates for the events leading to the index change on Jan. 26, 2016. FIGURE 1. An example of the news events that are responsible for the Shanghai Composite Index change on Jan. 26, 2016. The x-axis is the timeline.',\n",
       " 'See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. Speciﬁcally, structured events are extracted from news texts and then used as the inputs for Restricted Boltzmann Machines (RBMs) to do the pre-training. VOLUME 6, 2018 X. Zhang et al. Firstly, we propose a novel method to capture the event information. FIGURE 1. In this work, we aim to learn a predictive model for describing the ﬂuctuations in the stock market index by utilizing various sources of data, involving the historical quantitative data, the social media and Web news. One beneﬁt of our method is that we can determine source-speciﬁc weights and identify the speciﬁc factors that incur the changes in the composite index. An example of the news events that are responsible for the Shanghai Composite Index change on Jan. 26, 2016. Figure 1 shows an example of the news precursors identiﬁed by our model, and the dots with numbers denote the probabilistic estimates for the events leading to the index change on Jan. 26, 2016. The x-axis is the timeline. The left y-axis is the probability of each event leading to the index change. The right y-axis is the composite index in Shanghai Stock Exchange. The summary of the contributions is as follows: 1) To provide robust and accurate predictions for stock market movements, we extend the Multiple Instance Learning model to integrate the heterogeneous information including Web news, social media posts, and quantitative data. 2) The latent consistencies among different data sources are modeled in our framework by sharing the common estimated true label among the hinge losses of different data sources at the instance level. 3) A novel event representation model is proposed by ﬁrst extracting structured events from news text, and then training them with deep learning methods involving RBM and sentence2vec to obtain dense vectors.',\n",
       " '3) A novel event representation model is proposed by ﬁrst extracting structured events from news text, and then training them with deep learning methods involving RBM and sentence2vec to obtain dense vectors. The essential features we extract include the event representations from news articles and the sentiments from social media. After that, the output vectors from RBMs are used as the inputs to a recently proposed sentence2vec framework [11], in order to achieve effective event embeddings. Firstly, we propose a novel method to capture the event information. One beneﬁt of our method is that we can determine source-speciﬁc weights and identify the speciﬁc factors that incur the changes in the composite index. Speciﬁcally, structured events are extracted from news texts and then used as the inputs for Restricted Boltzmann Machines (RBMs) to do the pre-training. Secondly, we exploit the latent relationships among different data sources with carefully designed loss terms, and propose an extension of the Multiple Instance Learning (MIL) model that can effectively integrate the features from multiple sources to make more accurate predictions. Figure 1 shows an example of the news precursors identiﬁed by our model, and the dots with numbers denote the probabilistic estimates for the events leading to the index change on Jan. 26, 2016. The summary of the contributions is as follows: 1) To provide robust and accurate predictions for stock market movements, we extend the Multiple Instance Learning model to integrate the heterogeneous information including Web news, social media posts, and quantitative data. 2) The latent consistencies among different data sources are modeled in our framework by sharing the common estimated true label among the hinge losses of different data sources at the instance level. 4) Evaluation results on two-year datasets show that our proposal can outperform the state-of-art baselines.',\n",
       " '4) Evaluation results on two-year datasets show that our proposal can outperform the state-of-art baselines. 2) The latent consistencies among different data sources are modeled in our framework by sharing the common estimated true label among the hinge losses of different data sources at the instance level. 3) A novel event representation model is proposed by ﬁrst extracting structured events from news text, and then training them with deep learning methods involving RBM and sentence2vec to obtain dense vectors. The summary of the contributions is as follows: 1) To provide robust and accurate predictions for stock market movements, we extend the Multiple Instance Learning model to integrate the heterogeneous information including Web news, social media posts, and quantitative data. The right y-axis is the composite index in Shanghai Stock Exchange. The x-axis is the timeline. The left y-axis is the probability of each event leading to the index change. An example of the news events that are responsible for the Shanghai Composite Index change on Jan. 26, 2016. Figure 1 shows an example of the news precursors identiﬁed by our model, and the dots with numbers denote the probabilistic estimates for the events leading to the index change on Jan. 26, 2016. One beneﬁt of our method is that we can determine source-speciﬁc weights and identify the speciﬁc factors that incur the changes in the composite index. Moreover, the impacts of different sources and the key factors that drive the movements can be obtained. II. RELATED WORK A. STOCK MARKET PREDICTION There is a line of research works using event-driven stock prediction models. Hogenboom et al. [12] give an overview of event extraction methods. Akita et al.',\n",
       " 'Moreover, the impacts of different sources and the key factors that drive the movements can be obtained. The right y-axis is the composite index in Shanghai Stock Exchange. 4) Evaluation results on two-year datasets show that our proposal can outperform the state-of-art baselines. 2) The latent consistencies among different data sources are modeled in our framework by sharing the common estimated true label among the hinge losses of different data sources at the instance level. [12] give an overview of event extraction methods. STOCK MARKET PREDICTION There is a line of research works using event-driven stock prediction models. An example of the news events that are responsible for the Shanghai Composite Index change on Jan. 26, 2016. Figure 1 shows an example of the news precursors identiﬁed by our model, and the dots with numbers denote the probabilistic estimates for the events leading to the index change on Jan. 26, 2016. The summary of the contributions is as follows: 1) To provide robust and accurate predictions for stock market movements, we extend the Multiple Instance Learning model to integrate the heterogeneous information including Web news, social media posts, and quantitative data. 3) A novel event representation model is proposed by ﬁrst extracting structured events from news text, and then training them with deep learning methods involving RBM and sentence2vec to obtain dense vectors. [13] convert newspaper articles into distributed representations via Paragraph Vector and model the temporal effects of past events with LSTM on opening prices of stocks in Tokyo Stock Exchange. Nguyen et al. [14] formulated a temporal sentiment index function, which is used to extract signiﬁcant events. Then the corresponding blog posts are analyzed using topic modeling to understand the contents. Ding et al.',\n",
       " 'II. 2) The latent consistencies among different data sources are modeled in our framework by sharing the common estimated true label among the hinge losses of different data sources at the instance level. Moreover, the impacts of different sources and the key factors that drive the movements can be obtained. 4) Evaluation results on two-year datasets show that our proposal can outperform the state-of-art baselines. STOCK MARKET PREDICTION There is a line of research works using event-driven stock prediction models. Then the corresponding blog posts are analyzed using topic modeling to understand the contents. [13] convert newspaper articles into distributed representations via Paragraph Vector and model the temporal effects of past events with LSTM on opening prices of stocks in Tokyo Stock Exchange. [14] formulated a temporal sentiment index function, which is used to extract signiﬁcant events. 3) A novel event representation model is proposed by ﬁrst extracting structured events from news text, and then training them with deep learning methods involving RBM and sentence2vec to obtain dense vectors. [12] give an overview of event extraction methods. [15] applied the Open IE tool to extract structured events from texts, and this event extraction method is also implemented as a baseline and compared with our proposal. Ding et al. [16] then trained event embeddings with a neural tensor network and then used the deep convolutional neural network to model inﬂuences of events. In addition to events, investors’ emotions also have great impacts on the stock market index. Bollen et al. [17] revealed that the public moods derived from Twitter have impacts on stock indicators. Si et al. [3] proposed a technique to leverage topic based sentiments from Twitter to predict the stock market. Makrehchi et al. [18] assigned a positive or negative label for each tweet according to stock movements.',\n",
       " 'Ding et al. [16] then trained event embeddings with a neural tensor network and then used the deep convolutional neural network to model inﬂuences of events. Then the corresponding blog posts are analyzed using topic modeling to understand the contents. STOCK MARKET PREDICTION There is a line of research works using event-driven stock prediction models. [13] convert newspaper articles into distributed representations via Paragraph Vector and model the temporal effects of past events with LSTM on opening prices of stocks in Tokyo Stock Exchange. In addition to events, investors’ emotions also have great impacts on the stock market index. [18] assigned a positive or negative label for each tweet according to stock movements. [14] formulated a temporal sentiment index function, which is used to extract signiﬁcant events. [17] revealed that the public moods derived from Twitter have impacts on stock indicators. [3] proposed a technique to leverage topic based sentiments from Twitter to predict the stock market. The aggregate sentiment per day shows predictive power for stock market prediction. Topic-speciﬁc sentiments are learned in [19] to facilitate the stock prediction. However, this method is not suitable to short texts in social media. The common limitation of the aforementioned methods is that they rely only on a single data source and thus may limit the predictive power. In [20], events and sentiments are integrated into a tensor framework together with ﬁrm-speciﬁc features (e.g., P/B, P/E), to model the joint impacts on the stock volatility. We also implement it as a baseline. However, it uses a simple event extraction method which may not fully capture sufﬁcient event information. B. MULTIPLE INSTANCE LEARNING The multiple instance learning (MIL) paradigm is a form of weakly supervised learning. Training instances arranged in sets are called bags or groups.',\n",
       " 'Makrehchi et al. However, this method is not suitable to short texts in social media. Si et al. Bollen et al. Ding et al. We also implement it as a baseline. However, it uses a simple event extraction method which may not fully capture sufﬁcient event information. MULTIPLE INSTANCE LEARNING The multiple instance learning (MIL) paradigm is a form of weakly supervised learning. [18] assigned a positive or negative label for each tweet according to stock movements. Training instances arranged in sets are called bags or groups. A label is provided for entire groups instead of individual instances. Negative groups don’t contain any positive instances, while positive groups contain at least one positive instance [21]. Various applications and the comparisons of different methods in MIL VOLUME 6, 2018 50721 X. Zhang et al. : Stock Market Prediction via Multi-Source Multiple Instance Learning were given in [22]. The common MIL approach is used to predict the group-level label, Liu et al. [23], however, proposed an approach to identify the instance-level labels, especially the labels of key instances in groups based on K nearest neighbors (K-NN). Kotzias et al. [24] predicted the labels for sentences given labels for reviews, which can be used to detect sentiments. A multiple-instance multiple-label learning framework with deep neutral network formation is proposed in [25]. An event forecasting framework via the nested multiple instance learning is proposed in [26]. However, it only uses one data source and simple event features, which may not be sufﬁcient in the stock market application. We have implemented this algorithm as a baseline for comparison. III. MULTI-SOURCE MULTIPLE INSTANCE MODEL In this section, we ﬁrst state and formulate the problem, and then propose the multi-source multiple instance (M-MI) framework.',\n",
       " 'Various applications and the comparisons of different methods in MIL VOLUME 6, 2018 50721 X. Zhang et al. However, it uses a simple event extraction method which may not fully capture sufﬁcient event information. III. In [20], events and sentiments are integrated into a tensor framework together with ﬁrm-speciﬁc features (e.g., P/B, P/E), to model the joint impacts on the stock volatility. A label is provided for entire groups instead of individual instances. The common MIL approach is used to predict the group-level label, Liu et al. [23], however, proposed an approach to identify the instance-level labels, especially the labels of key instances in groups based on K nearest neighbors (K-NN). We also implement it as a baseline. MULTI-SOURCE MULTIPLE INSTANCE MODEL In this section, we ﬁrst state and formulate the problem, and then propose the multi-source multiple instance (M-MI) framework. We have implemented this algorithm as a baseline for comparison. Before going into details of our framework, we deﬁne some important notations as shown in Table 1. TABLE 1. Notations in our model. A. PROBLEM STATEMENT Stock markets are impacted by various factors, such as the trading volume, news events and the investors’ emotions. Thus, relying on a single data source may not be sufﬁcient to make accurate predictions. The object of our study is to develop a multi-source data integration approach to predict the stock market trends. Speciﬁcally, given a collection of economic news, social network posts and historical trading data, we aim to forecast the stock market index movements. Moreover, we also try to obtain the impacts of each data source and identify the key factors that have decisive inﬂuences, which may be inﬂuential news, collective sentiments or some important quantitative index in the trading data.',\n",
       " '[24] predicted the labels for sentences given labels for reviews, which can be used to detect sentiments. However, it only uses one data source and simple event features, which may not be sufﬁcient in the stock market application. Before going into details of our framework, we deﬁne some important notations as shown in Table 1. Notations in our model. The object of our study is to develop a multi-source data integration approach to predict the stock market trends. Speciﬁcally, given a collection of economic news, social network posts and historical trading data, we aim to forecast the stock market index movements. The common MIL approach is used to predict the group-level label, Liu et al. PROBLEM STATEMENT Stock markets are impacted by various factors, such as the trading volume, news events and the investors’ emotions. Thus, relying on a single data source may not be sufﬁcient to make accurate predictions. Moreover, we also try to obtain the impacts of each data source and identify the key factors that have decisive inﬂuences, which may be inﬂuential news, collective sentiments or some important quantitative index in the trading data. These key factors are supporting evidence for further analysis and can make our prediction interpretable. Formally, according to Table 1, a news article j on day i is denoted as a V-dimensional vector xij ∈RV×1 (please note that the process of representing a news article as a vector will be illustrated in the next section). In order to predict the stock market movement on day t + k, we assume that there are a group of news articles for each day i (i < t), which is denoted as Xi, and thus Xi = {xij}, j ∈{1, · · · , ni}.',\n",
       " 'Thus, relying on a single data source may not be sufﬁcient to make accurate predictions. Before going into details of our framework, we deﬁne some important notations as shown in Table 1. Notations in our model. The object of our study is to develop a multi-source data integration approach to predict the stock market trends. These key factors are supporting evidence for further analysis and can make our prediction interpretable. PROBLEM STATEMENT Stock markets are impacted by various factors, such as the trading volume, news events and the investors’ emotions. Speciﬁcally, given a collection of economic news, social network posts and historical trading data, we aim to forecast the stock market index movements. In order to predict the stock market movement on day t + k, we assume that there are a group of news articles for each day i (i < t), which is denoted as Xi, and thus Xi = {xij}, j ∈{1, · · · , ni}. Moreover, we also try to obtain the impacts of each data source and identify the key factors that have decisive inﬂuences, which may be inﬂuential news, collective sentiments or some important quantitative index in the trading data. Formally, according to Table 1, a news article j on day i is denoted as a V-dimensional vector xij ∈RV×1 (please note that the process of representing a news article as a vector will be illustrated in the next section). In addition to the news articles, the sentiment and quantitative indices on day i (denoted as si and di respectively) are also taken into account.',\n",
       " 'Notations in our model. These key factors are supporting evidence for further analysis and can make our prediction interpretable. Thus, relying on a single data source may not be sufﬁcient to make accurate predictions. PROBLEM STATEMENT Stock markets are impacted by various factors, such as the trading volume, news events and the investors’ emotions. The object of our study is to develop a multi-source data integration approach to predict the stock market trends. Moreover, we also try to obtain the impacts of each data source and identify the key factors that have decisive inﬂuences, which may be inﬂuential news, collective sentiments or some important quantitative index in the trading data. Speciﬁcally, given a collection of economic news, social network posts and historical trading data, we aim to forecast the stock market index movements. In addition to the news articles, the sentiment and quantitative indices on day i (denoted as si and di respectively) are also taken into account. In order to predict the stock market movement on day t + k, we assume that there are a group of news articles for each day i (i < t), which is denoted as Xi, and thus Xi = {xij}, j ∈{1, · · · , ni}. Formally, according to Table 1, a news article j on day i is denoted as a V-dimensional vector xij ∈RV×1 (please note that the process of representing a news article as a vector will be illustrated in the next section). Then the temporal ordered collection of news articles, sentiments and quantitative indices across t days can be represented as a multi-source super group, that is, G = {Ci}, i ∈{1, · · · , t}, where Ci = {Xi, di, si}.',\n",
       " 'Thus, relying on a single data source may not be sufﬁcient to make accurate predictions. These key factors are supporting evidence for further analysis and can make our prediction interpretable. The object of our study is to develop a multi-source data integration approach to predict the stock market trends. Then the temporal ordered collection of news articles, sentiments and quantitative indices across t days can be represented as a multi-source super group, that is, G = {Ci}, i ∈{1, · · · , t}, where Ci = {Xi, di, si}. Formally, according to Table 1, a news article j on day i is denoted as a V-dimensional vector xij ∈RV×1 (please note that the process of representing a news article as a vector will be illustrated in the next section). In addition to the news articles, the sentiment and quantitative indices on day i (denoted as si and di respectively) are also taken into account. Moreover, we also try to obtain the impacts of each data source and identify the key factors that have decisive inﬂuences, which may be inﬂuential news, collective sentiments or some important quantitative index in the trading data. PROBLEM STATEMENT Stock markets are impacted by various factors, such as the trading volume, news events and the investors’ emotions. Speciﬁcally, given a collection of economic news, social network posts and historical trading data, we aim to forecast the stock market index movements. In order to predict the stock market movement on day t + k, we assume that there are a group of news articles for each day i (i < t), which is denoted as Xi, and thus Xi = {xij}, j ∈{1, · · · , ni}. The change in the stock market movement on day t + k can be denoted as Yt+k ∈{+1, −1}, where +1 denotes the index rise and -1 denotes the index decline.',\n",
       " 'These key factors are supporting evidence for further analysis and can make our prediction interpretable. Thus, relying on a single data source may not be sufﬁcient to make accurate predictions. In addition to the news articles, the sentiment and quantitative indices on day i (denoted as si and di respectively) are also taken into account. Then the temporal ordered collection of news articles, sentiments and quantitative indices across t days can be represented as a multi-source super group, that is, G = {Ci}, i ∈{1, · · · , t}, where Ci = {Xi, di, si}. Moreover, we also try to obtain the impacts of each data source and identify the key factors that have decisive inﬂuences, which may be inﬂuential news, collective sentiments or some important quantitative index in the trading data. Formally, according to Table 1, a news article j on day i is denoted as a V-dimensional vector xij ∈RV×1 (please note that the process of representing a news article as a vector will be illustrated in the next section). The change in the stock market movement on day t + k can be denoted as Yt+k ∈{+1, −1}, where +1 denotes the index rise and -1 denotes the index decline. The object of our study is to develop a multi-source data integration approach to predict the stock market trends. Speciﬁcally, given a collection of economic news, social network posts and historical trading data, we aim to forecast the stock market index movements. In order to predict the stock market movement on day t + k, we assume that there are a group of news articles for each day i (i < t), which is denoted as Xi, and thus Xi = {xij}, j ∈{1, · · · , ni}. Then the forecasting problem can be modeled as a mathematical function f (G) → Yt+k, indicating that we map the multi-source information to an indicator (i.e., label) k days in the future from the day t, where k is number of the lead days that we aim to forecast.',\n",
       " 'The object of our study is to develop a multi-source data integration approach to predict the stock market trends. Speciﬁcally, given a collection of economic news, social network posts and historical trading data, we aim to forecast the stock market index movements. Thus, relying on a single data source may not be sufﬁcient to make accurate predictions. Then the temporal ordered collection of news articles, sentiments and quantitative indices across t days can be represented as a multi-source super group, that is, G = {Ci}, i ∈{1, · · · , t}, where Ci = {Xi, di, si}. The change in the stock market movement on day t + k can be denoted as Yt+k ∈{+1, −1}, where +1 denotes the index rise and -1 denotes the index decline. Moreover, we also try to obtain the impacts of each data source and identify the key factors that have decisive inﬂuences, which may be inﬂuential news, collective sentiments or some important quantitative index in the trading data. These key factors are supporting evidence for further analysis and can make our prediction interpretable. In addition to the news articles, the sentiment and quantitative indices on day i (denoted as si and di respectively) are also taken into account. Then the forecasting problem can be modeled as a mathematical function f (G) → Yt+k, indicating that we map the multi-source information to an indicator (i.e., label) k days in the future from the day t, where k is number of the lead days that we aim to forecast. Formally, according to Table 1, a news article j on day i is denoted as a V-dimensional vector xij ∈RV×1 (please note that the process of representing a news article as a vector will be illustrated in the next section). FIGURE 2.',\n",
       " 'The object of our study is to develop a multi-source data integration approach to predict the stock market trends. The change in the stock market movement on day t + k can be denoted as Yt+k ∈{+1, −1}, where +1 denotes the index rise and -1 denotes the index decline. In addition to the news articles, the sentiment and quantitative indices on day i (denoted as si and di respectively) are also taken into account. Speciﬁcally, given a collection of economic news, social network posts and historical trading data, we aim to forecast the stock market index movements. Then the temporal ordered collection of news articles, sentiments and quantitative indices across t days can be represented as a multi-source super group, that is, G = {Ci}, i ∈{1, · · · , t}, where Ci = {Xi, di, si}. FIGURE 2. These key factors are supporting evidence for further analysis and can make our prediction interpretable. Formally, according to Table 1, a news article j on day i is denoted as a V-dimensional vector xij ∈RV×1 (please note that the process of representing a news article as a vector will be illustrated in the next section). Moreover, we also try to obtain the impacts of each data source and identify the key factors that have decisive inﬂuences, which may be inﬂuential news, collective sentiments or some important quantitative index in the trading data. Then the forecasting problem can be modeled as a mathematical function f (G) → Yt+k, indicating that we map the multi-source information to an indicator (i.e., label) k days in the future from the day t, where k is number of the lead days that we aim to forecast. The system framework of our proposed model. B.',\n",
       " 'These key factors are supporting evidence for further analysis and can make our prediction interpretable. Speciﬁcally, given a collection of economic news, social network posts and historical trading data, we aim to forecast the stock market index movements. Moreover, we also try to obtain the impacts of each data source and identify the key factors that have decisive inﬂuences, which may be inﬂuential news, collective sentiments or some important quantitative index in the trading data. In order to predict the stock market movement on day t + k, we assume that there are a group of news articles for each day i (i < t), which is denoted as Xi, and thus Xi = {xij}, j ∈{1, · · · , ni}. Then the temporal ordered collection of news articles, sentiments and quantitative indices across t days can be represented as a multi-source super group, that is, G = {Ci}, i ∈{1, · · · , t}, where Ci = {Xi, di, si}. Formally, according to Table 1, a news article j on day i is denoted as a V-dimensional vector xij ∈RV×1 (please note that the process of representing a news article as a vector will be illustrated in the next section). The change in the stock market movement on day t + k can be denoted as Yt+k ∈{+1, −1}, where +1 denotes the index rise and -1 denotes the index decline. FIGURE 2. Then the forecasting problem can be modeled as a mathematical function f (G) → Yt+k, indicating that we map the multi-source information to an indicator (i.e., label) k days in the future from the day t, where k is number of the lead days that we aim to forecast. The system framework of our proposed model. THE PROPOSED APPROACH The framework of our proposal is shown in Fig.',\n",
       " 'In order to predict the stock market movement on day t + k, we assume that there are a group of news articles for each day i (i < t), which is denoted as Xi, and thus Xi = {xij}, j ∈{1, · · · , ni}. Then the forecasting problem can be modeled as a mathematical function f (G) → Yt+k, indicating that we map the multi-source information to an indicator (i.e., label) k days in the future from the day t, where k is number of the lead days that we aim to forecast. THE PROPOSED APPROACH The framework of our proposal is shown in Fig. The change in the stock market movement on day t + k can be denoted as Yt+k ∈{+1, −1}, where +1 denotes the index rise and -1 denotes the index decline. Then the temporal ordered collection of news articles, sentiments and quantitative indices across t days can be represented as a multi-source super group, that is, G = {Ci}, i ∈{1, · · · , t}, where Ci = {Xi, di, si}. In addition to the news articles, the sentiment and quantitative indices on day i (denoted as si and di respectively) are also taken into account. Formally, according to Table 1, a news article j on day i is denoted as a V-dimensional vector xij ∈RV×1 (please note that the process of representing a news article as a vector will be illustrated in the next section). The system framework of our proposed model. B. FIGURE 2. 2. The inputs of the framework are the stock quantitative data, the social media and Web news. We ﬁrst use the sentiment analyzer to obtain the collective sentiments from social media, and extract effective event representations from the Web news.',\n",
       " 'THE PROPOSED APPROACH The framework of our proposal is shown in Fig. The system framework of our proposed model. We ﬁrst use the sentiment analyzer to obtain the collective sentiments from social media, and extract effective event representations from the Web news. Formally, according to Table 1, a news article j on day i is denoted as a V-dimensional vector xij ∈RV×1 (please note that the process of representing a news article as a vector will be illustrated in the next section). The change in the stock market movement on day t + k can be denoted as Yt+k ∈{+1, −1}, where +1 denotes the index rise and -1 denotes the index decline. Then the forecasting problem can be modeled as a mathematical function f (G) → Yt+k, indicating that we map the multi-source information to an indicator (i.e., label) k days in the future from the day t, where k is number of the lead days that we aim to forecast. In order to predict the stock market movement on day t + k, we assume that there are a group of news articles for each day i (i < t), which is denoted as Xi, and thus Xi = {xij}, j ∈{1, · · · , ni}. Then the temporal ordered collection of news articles, sentiments and quantitative indices across t days can be represented as a multi-source super group, that is, G = {Ci}, i ∈{1, · · · , t}, where Ci = {Xi, di, si}. The inputs of the framework are the stock quantitative data, the social media and Web news. In addition to the news articles, the sentiment and quantitative indices on day i (denoted as si and di respectively) are also taken into account. Then the extracted sentiments, events as well as the stock quantitative data are fed into the M-MI model.',\n",
       " 'B. FIGURE 2. The inputs of the framework are the stock quantitative data, the social media and Web news. The change in the stock market movement on day t + k can be denoted as Yt+k ∈{+1, −1}, where +1 denotes the index rise and -1 denotes the index decline. The system framework of our proposed model. THE PROPOSED APPROACH The framework of our proposal is shown in Fig. We ﬁrst use the sentiment analyzer to obtain the collective sentiments from social media, and extract effective event representations from the Web news. Then the forecasting problem can be modeled as a mathematical function f (G) → Yt+k, indicating that we map the multi-source information to an indicator (i.e., label) k days in the future from the day t, where k is number of the lead days that we aim to forecast. Then the temporal ordered collection of news articles, sentiments and quantitative indices across t days can be represented as a multi-source super group, that is, G = {Ci}, i ∈{1, · · · , t}, where Ci = {Xi, di, si}. Then the extracted sentiments, events as well as the stock quantitative data are fed into the M-MI model. The M-MI model is proposed based on the Multiple Instance Learning algorithm, that is, a group of instances are given group labels, which are assumed to be an association function (e.g., OR, average) of the instance-level labels. Our work further distinguishes the instance-level labels, multi-source group-level labels, and multi-source super group-level labels. The primary goal is to predict the label for the multi-source super group that indicates the rise or decline of the stock market index.',\n",
       " 'THE PROPOSED APPROACH The framework of our proposal is shown in Fig. The inputs of the framework are the stock quantitative data, the social media and Web news. FIGURE 2. The change in the stock market movement on day t + k can be denoted as Yt+k ∈{+1, −1}, where +1 denotes the index rise and -1 denotes the index decline. Then the forecasting problem can be modeled as a mathematical function f (G) → Yt+k, indicating that we map the multi-source information to an indicator (i.e., label) k days in the future from the day t, where k is number of the lead days that we aim to forecast. We ﬁrst use the sentiment analyzer to obtain the collective sentiments from social media, and extract effective event representations from the Web news. Then the extracted sentiments, events as well as the stock quantitative data are fed into the M-MI model. The primary goal is to predict the label for the multi-source super group that indicates the rise or decline of the stock market index. The M-MI model is proposed based on the Multiple Instance Learning algorithm, that is, a group of instances are given group labels, which are assumed to be an association function (e.g., OR, average) of the instance-level labels. Our work further distinguishes the instance-level labels, multi-source group-level labels, and multi-source super group-level labels. In addition, we also try to estimate the instancelevel probabilities indicating how related a speciﬁc instance is to the index movement (i.e., target label), as well as the 50722 VOLUME 6, 2018 X. Zhang et al. : Stock Market Prediction via Multi-Source Multiple Instance Learning source-speciﬁc weights that reveal how related a speciﬁc source is to the index movement.',\n",
       " 'The system framework of our proposed model. THE PROPOSED APPROACH The framework of our proposal is shown in Fig. The inputs of the framework are the stock quantitative data, the social media and Web news. Our work further distinguishes the instance-level labels, multi-source group-level labels, and multi-source super group-level labels. Then the extracted sentiments, events as well as the stock quantitative data are fed into the M-MI model. : Stock Market Prediction via Multi-Source Multiple Instance Learning source-speciﬁc weights that reveal how related a speciﬁc source is to the index movement. The primary goal is to predict the label for the multi-source super group that indicates the rise or decline of the stock market index. The M-MI model is proposed based on the Multiple Instance Learning algorithm, that is, a group of instances are given group labels, which are assumed to be an association function (e.g., OR, average) of the instance-level labels. We ﬁrst use the sentiment analyzer to obtain the collective sentiments from social media, and extract effective event representations from the Web news. In addition, we also try to estimate the instancelevel probabilities indicating how related a speciﬁc instance is to the index movement (i.e., target label), as well as the 50722 VOLUME 6, 2018 X. Zhang et al. To this end, for a given day, we ﬁrst model the instancelevel probability pij for a news article j on day i to the target label with a logistic function, that is pij = σ(wmT xij) = 1 1 + e−wmT xij (1) where wm denotes the weight vector of the news articles. The higher the probability pij, the more related the article j is to the target label.',\n",
       " 'THE PROPOSED APPROACH The framework of our proposal is shown in Fig. The M-MI model is proposed based on the Multiple Instance Learning algorithm, that is, a group of instances are given group labels, which are assumed to be an association function (e.g., OR, average) of the instance-level labels. In addition, we also try to estimate the instancelevel probabilities indicating how related a speciﬁc instance is to the index movement (i.e., target label), as well as the 50722 VOLUME 6, 2018 X. Zhang et al. The higher the probability pij, the more related the article j is to the target label. The primary goal is to predict the label for the multi-source super group that indicates the rise or decline of the stock market index. : Stock Market Prediction via Multi-Source Multiple Instance Learning source-speciﬁc weights that reveal how related a speciﬁc source is to the index movement. The inputs of the framework are the stock quantitative data, the social media and Web news. Then the extracted sentiments, events as well as the stock quantitative data are fed into the M-MI model. We ﬁrst use the sentiment analyzer to obtain the collective sentiments from social media, and extract effective event representations from the Web news. To this end, for a given day, we ﬁrst model the instancelevel probability pij for a news article j on day i to the target label with a logistic function, that is pij = σ(wmT xij) = 1 1 + e−wmT xij (1) where wm denotes the weight vector of the news articles. The probability of all the news articles for a given day i can be computed as the average of probabilities of each news article, that is pm−i = 1 ni ni X j=1 pij (2) In addition to news articles, we also model the probability pd−i for stock quantitative data and ps−i for sentiments on day i, that is pd−i = σ(wT d di) = 1 1 + e−wT d di (3) ps−i = σ(wT s si) = 1 1 + e−wTs si (4) where wd and ws denote the weight vector of di and si respectively.',\n",
       " 'Our work further distinguishes the instance-level labels, multi-source group-level labels, and multi-source super group-level labels. The inputs of the framework are the stock quantitative data, the social media and Web news. The M-MI model is proposed based on the Multiple Instance Learning algorithm, that is, a group of instances are given group labels, which are assumed to be an association function (e.g., OR, average) of the instance-level labels. The primary goal is to predict the label for the multi-source super group that indicates the rise or decline of the stock market index. : Stock Market Prediction via Multi-Source Multiple Instance Learning source-speciﬁc weights that reveal how related a speciﬁc source is to the index movement. Then the extracted sentiments, events as well as the stock quantitative data are fed into the M-MI model. In addition, we also try to estimate the instancelevel probabilities indicating how related a speciﬁc instance is to the index movement (i.e., target label), as well as the 50722 VOLUME 6, 2018 X. Zhang et al. The higher the probability pij, the more related the article j is to the target label. To this end, for a given day, we ﬁrst model the instancelevel probability pij for a news article j on day i to the target label with a logistic function, that is pij = σ(wmT xij) = 1 1 + e−wmT xij (1) where wm denotes the weight vector of the news articles. The probability of all the news articles for a given day i can be computed as the average of probabilities of each news article, that is pm−i = 1 ni ni X j=1 pij (2) In addition to news articles, we also model the probability pd−i for stock quantitative data and ps−i for sentiments on day i, that is pd−i = σ(wT d di) = 1 1 + e−wT d di (3) ps−i = σ(wT s si) = 1 1 + e−wTs si (4) where wd and ws denote the weight vector of di and si respectively. We then model the probability Pi for multisource information on day i as Pi = θ0pm−i + θ1pd−i + θ2ps−i = θ(pm−i, pd−i, ps−i)T (5) where θ0, θ1 and θ2 denote the source-speciﬁc weights of pm−i, pd−i and ps−i respectively, and θ0 + θ1 + θ2 = 1.',\n",
       " 'In addition, we also try to estimate the instancelevel probabilities indicating how related a speciﬁc instance is to the index movement (i.e., target label), as well as the 50722 VOLUME 6, 2018 X. Zhang et al. The M-MI model is proposed based on the Multiple Instance Learning algorithm, that is, a group of instances are given group labels, which are assumed to be an association function (e.g., OR, average) of the instance-level labels. We ﬁrst use the sentiment analyzer to obtain the collective sentiments from social media, and extract effective event representations from the Web news. Then the extracted sentiments, events as well as the stock quantitative data are fed into the M-MI model. Our work further distinguishes the instance-level labels, multi-source group-level labels, and multi-source super group-level labels. The probability of all the news articles for a given day i can be computed as the average of probabilities of each news article, that is pm−i = 1 ni ni X j=1 pij (2) In addition to news articles, we also model the probability pd−i for stock quantitative data and ps−i for sentiments on day i, that is pd−i = σ(wT d di) = 1 1 + e−wT d di (3) ps−i = σ(wT s si) = 1 1 + e−wTs si (4) where wd and ws denote the weight vector of di and si respectively. The primary goal is to predict the label for the multi-source super group that indicates the rise or decline of the stock market index. To this end, for a given day, we ﬁrst model the instancelevel probability pij for a news article j on day i to the target label with a logistic function, that is pij = σ(wmT xij) = 1 1 + e−wmT xij (1) where wm denotes the weight vector of the news articles. We then model the probability Pi for multisource information on day i as Pi = θ0pm−i + θ1pd−i + θ2ps−i = θ(pm−i, pd−i, ps−i)T (5) where θ0, θ1 and θ2 denote the source-speciﬁc weights of pm−i, pd−i and ps−i respectively, and θ0 + θ1 + θ2 = 1. The higher the probability pij, the more related the article j is to the target label. It is obvious that Pi ∈[0, 1].',\n",
       " 'Then the extracted sentiments, events as well as the stock quantitative data are fed into the M-MI model. The higher the probability pij, the more related the article j is to the target label. The probability of all the news articles for a given day i can be computed as the average of probabilities of each news article, that is pm−i = 1 ni ni X j=1 pij (2) In addition to news articles, we also model the probability pd−i for stock quantitative data and ps−i for sentiments on day i, that is pd−i = σ(wT d di) = 1 1 + e−wT d di (3) ps−i = σ(wT s si) = 1 1 + e−wTs si (4) where wd and ws denote the weight vector of di and si respectively. : Stock Market Prediction via Multi-Source Multiple Instance Learning source-speciﬁc weights that reveal how related a speciﬁc source is to the index movement. In addition, we also try to estimate the instancelevel probabilities indicating how related a speciﬁc instance is to the index movement (i.e., target label), as well as the 50722 VOLUME 6, 2018 X. Zhang et al. The M-MI model is proposed based on the Multiple Instance Learning algorithm, that is, a group of instances are given group labels, which are assumed to be an association function (e.g., OR, average) of the instance-level labels. To this end, for a given day, we ﬁrst model the instancelevel probability pij for a news article j on day i to the target label with a logistic function, that is pij = σ(wmT xij) = 1 1 + e−wmT xij (1) where wm denotes the weight vector of the news articles. The primary goal is to predict the label for the multi-source super group that indicates the rise or decline of the stock market index. Our work further distinguishes the instance-level labels, multi-source group-level labels, and multi-source super group-level labels. We then model the probability Pi for multisource information on day i as Pi = θ0pm−i + θ1pd−i + θ2ps−i = θ(pm−i, pd−i, ps−i)T (5) where θ0, θ1 and θ2 denote the source-speciﬁc weights of pm−i, pd−i and ps−i respectively, and θ0 + θ1 + θ2 = 1. We use θ = (θ0, θ1, θ2) to denote the source weight vector, and then model the probability of the multi-source super group as the average of the probabilities in t days, that is P = 1 t tX i=1 Pi (6) Then, we start with a log-likelihood loss function: min wm,wd,ws,θ 1 n X G∈S f (G, Y, wm, wd, ws, θ) = 1 n X G∈S (−I(Y = 1)logP −I(Y = −1)log(1 −P)) (7) where G is a multi-source super group, n is the number of multi-source super groups, and Y denotes the set of true labels.',\n",
       " ': Stock Market Prediction via Multi-Source Multiple Instance Learning source-speciﬁc weights that reveal how related a speciﬁc source is to the index movement. It is obvious that Pi ∈[0, 1]. Our work further distinguishes the instance-level labels, multi-source group-level labels, and multi-source super group-level labels. The probability of all the news articles for a given day i can be computed as the average of probabilities of each news article, that is pm−i = 1 ni ni X j=1 pij (2) In addition to news articles, we also model the probability pd−i for stock quantitative data and ps−i for sentiments on day i, that is pd−i = σ(wT d di) = 1 1 + e−wT d di (3) ps−i = σ(wT s si) = 1 1 + e−wTs si (4) where wd and ws denote the weight vector of di and si respectively. The M-MI model is proposed based on the Multiple Instance Learning algorithm, that is, a group of instances are given group labels, which are assumed to be an association function (e.g., OR, average) of the instance-level labels. To this end, for a given day, we ﬁrst model the instancelevel probability pij for a news article j on day i to the target label with a logistic function, that is pij = σ(wmT xij) = 1 1 + e−wmT xij (1) where wm denotes the weight vector of the news articles. In addition, we also try to estimate the instancelevel probabilities indicating how related a speciﬁc instance is to the index movement (i.e., target label), as well as the 50722 VOLUME 6, 2018 X. Zhang et al. We then model the probability Pi for multisource information on day i as Pi = θ0pm−i + θ1pd−i + θ2ps−i = θ(pm−i, pd−i, ps−i)T (5) where θ0, θ1 and θ2 denote the source-speciﬁc weights of pm−i, pd−i and ps−i respectively, and θ0 + θ1 + θ2 = 1. The primary goal is to predict the label for the multi-source super group that indicates the rise or decline of the stock market index. The higher the probability pij, the more related the article j is to the target label. I(·) is the indicator function.',\n",
       " 'I(·) is the indicator function. Our work further distinguishes the instance-level labels, multi-source group-level labels, and multi-source super group-level labels. In addition, we also try to estimate the instancelevel probabilities indicating how related a speciﬁc instance is to the index movement (i.e., target label), as well as the 50722 VOLUME 6, 2018 X. Zhang et al. The higher the probability pij, the more related the article j is to the target label. The primary goal is to predict the label for the multi-source super group that indicates the rise or decline of the stock market index. : Stock Market Prediction via Multi-Source Multiple Instance Learning source-speciﬁc weights that reveal how related a speciﬁc source is to the index movement. The probability of all the news articles for a given day i can be computed as the average of probabilities of each news article, that is pm−i = 1 ni ni X j=1 pij (2) In addition to news articles, we also model the probability pd−i for stock quantitative data and ps−i for sentiments on day i, that is pd−i = σ(wT d di) = 1 1 + e−wT d di (3) ps−i = σ(wT s si) = 1 1 + e−wTs si (4) where wd and ws denote the weight vector of di and si respectively. To this end, for a given day, we ﬁrst model the instancelevel probability pij for a news article j on day i to the target label with a logistic function, that is pij = σ(wmT xij) = 1 1 + e−wmT xij (1) where wm denotes the weight vector of the news articles. We use θ = (θ0, θ1, θ2) to denote the source weight vector, and then model the probability of the multi-source super group as the average of the probabilities in t days, that is P = 1 t tX i=1 Pi (6) Then, we start with a log-likelihood loss function: min wm,wd,ws,θ 1 n X G∈S f (G, Y, wm, wd, ws, θ) = 1 n X G∈S (−I(Y = 1)logP −I(Y = −1)log(1 −P)) (7) where G is a multi-source super group, n is the number of multi-source super groups, and Y denotes the set of true labels. We then model the probability Pi for multisource information on day i as Pi = θ0pm−i + θ1pd−i + θ2ps−i = θ(pm−i, pd−i, ps−i)T (5) where θ0, θ1 and θ2 denote the source-speciﬁc weights of pm−i, pd−i and ps−i respectively, and θ0 + θ1 + θ2 = 1. As the inﬂuences of the multi-source information usually last for a number of days, we assume the probabilities on two consecutive days are essentially similar, which can be represented by minimizing the cost g(Ci, Ci−1) = (Pi −Pi−1)2 (8) where Ci denotes the multi-source group for the day i.',\n",
       " 'The higher the probability pij, the more related the article j is to the target label. : Stock Market Prediction via Multi-Source Multiple Instance Learning source-speciﬁc weights that reveal how related a speciﬁc source is to the index movement. The probability of all the news articles for a given day i can be computed as the average of probabilities of each news article, that is pm−i = 1 ni ni X j=1 pij (2) In addition to news articles, we also model the probability pd−i for stock quantitative data and ps−i for sentiments on day i, that is pd−i = σ(wT d di) = 1 1 + e−wT d di (3) ps−i = σ(wT s si) = 1 1 + e−wTs si (4) where wd and ws denote the weight vector of di and si respectively. In addition, we also try to estimate the instancelevel probabilities indicating how related a speciﬁc instance is to the index movement (i.e., target label), as well as the 50722 VOLUME 6, 2018 X. Zhang et al. The primary goal is to predict the label for the multi-source super group that indicates the rise or decline of the stock market index. To this end, for a given day, we ﬁrst model the instancelevel probability pij for a news article j on day i to the target label with a logistic function, that is pij = σ(wmT xij) = 1 1 + e−wmT xij (1) where wm denotes the weight vector of the news articles. As the inﬂuences of the multi-source information usually last for a number of days, we assume the probabilities on two consecutive days are essentially similar, which can be represented by minimizing the cost g(Ci, Ci−1) = (Pi −Pi−1)2 (8) where Ci denotes the multi-source group for the day i. We then model the probability Pi for multisource information on day i as Pi = θ0pm−i + θ1pd−i + θ2ps−i = θ(pm−i, pd−i, ps−i)T (5) where θ0, θ1 and θ2 denote the source-speciﬁc weights of pm−i, pd−i and ps−i respectively, and θ0 + θ1 + θ2 = 1. I(·) is the indicator function. We use θ = (θ0, θ1, θ2) to denote the source weight vector, and then model the probability of the multi-source super group as the average of the probabilities in t days, that is P = 1 t tX i=1 Pi (6) Then, we start with a log-likelihood loss function: min wm,wd,ws,θ 1 n X G∈S f (G, Y, wm, wd, ws, θ) = 1 n X G∈S (−I(Y = 1)logP −I(Y = −1)log(1 −P)) (7) where G is a multi-source super group, n is the number of multi-source super groups, and Y denotes the set of true labels. By introducing this loss term, Eq.',\n",
       " ': Stock Market Prediction via Multi-Source Multiple Instance Learning source-speciﬁc weights that reveal how related a speciﬁc source is to the index movement. The probability of all the news articles for a given day i can be computed as the average of probabilities of each news article, that is pm−i = 1 ni ni X j=1 pij (2) In addition to news articles, we also model the probability pd−i for stock quantitative data and ps−i for sentiments on day i, that is pd−i = σ(wT d di) = 1 1 + e−wT d di (3) ps−i = σ(wT s si) = 1 1 + e−wTs si (4) where wd and ws denote the weight vector of di and si respectively. To this end, for a given day, we ﬁrst model the instancelevel probability pij for a news article j on day i to the target label with a logistic function, that is pij = σ(wmT xij) = 1 1 + e−wmT xij (1) where wm denotes the weight vector of the news articles. The higher the probability pij, the more related the article j is to the target label. In addition, we also try to estimate the instancelevel probabilities indicating how related a speciﬁc instance is to the index movement (i.e., target label), as well as the 50722 VOLUME 6, 2018 X. Zhang et al. I(·) is the indicator function. We then model the probability Pi for multisource information on day i as Pi = θ0pm−i + θ1pd−i + θ2ps−i = θ(pm−i, pd−i, ps−i)T (5) where θ0, θ1 and θ2 denote the source-speciﬁc weights of pm−i, pd−i and ps−i respectively, and θ0 + θ1 + θ2 = 1. As the inﬂuences of the multi-source information usually last for a number of days, we assume the probabilities on two consecutive days are essentially similar, which can be represented by minimizing the cost g(Ci, Ci−1) = (Pi −Pi−1)2 (8) where Ci denotes the multi-source group for the day i. We use θ = (θ0, θ1, θ2) to denote the source weight vector, and then model the probability of the multi-source super group as the average of the probabilities in t days, that is P = 1 t tX i=1 Pi (6) Then, we start with a log-likelihood loss function: min wm,wd,ws,θ 1 n X G∈S f (G, Y, wm, wd, ws, θ) = 1 n X G∈S (−I(Y = 1)logP −I(Y = −1)log(1 −P)) (7) where G is a multi-source super group, n is the number of multi-source super groups, and Y denotes the set of true labels. By introducing this loss term, Eq. 7 can be rewritten as: min wm,wd,ws,θ β n X G∈S f (G, Y, wm, wd, ws, θ) + 1 n X Ci,Ci−1∈G;G∈S; 1 t tX i=1 g(Ci, Ci−1, wm, wd, ws, θ) (9) where β is a constant to control the contribution of the ﬁrst term.',\n",
       " 'We use θ = (θ0, θ1, θ2) to denote the source weight vector, and then model the probability of the multi-source super group as the average of the probabilities in t days, that is P = 1 t tX i=1 Pi (6) Then, we start with a log-likelihood loss function: min wm,wd,ws,θ 1 n X G∈S f (G, Y, wm, wd, ws, θ) = 1 n X G∈S (−I(Y = 1)logP −I(Y = −1)log(1 −P)) (7) where G is a multi-source super group, n is the number of multi-source super groups, and Y denotes the set of true labels. As the inﬂuences of the multi-source information usually last for a number of days, we assume the probabilities on two consecutive days are essentially similar, which can be represented by minimizing the cost g(Ci, Ci−1) = (Pi −Pi−1)2 (8) where Ci denotes the multi-source group for the day i. We then model the probability Pi for multisource information on day i as Pi = θ0pm−i + θ1pd−i + θ2ps−i = θ(pm−i, pd−i, ps−i)T (5) where θ0, θ1 and θ2 denote the source-speciﬁc weights of pm−i, pd−i and ps−i respectively, and θ0 + θ1 + θ2 = 1. The higher the probability pij, the more related the article j is to the target label. To this end, for a given day, we ﬁrst model the instancelevel probability pij for a news article j on day i to the target label with a logistic function, that is pij = σ(wmT xij) = 1 1 + e−wmT xij (1) where wm denotes the weight vector of the news articles. The probability of all the news articles for a given day i can be computed as the average of probabilities of each news article, that is pm−i = 1 ni ni X j=1 pij (2) In addition to news articles, we also model the probability pd−i for stock quantitative data and ps−i for sentiments on day i, that is pd−i = σ(wT d di) = 1 1 + e−wT d di (3) ps−i = σ(wT s si) = 1 1 + e−wTs si (4) where wd and ws denote the weight vector of di and si respectively. : Stock Market Prediction via Multi-Source Multiple Instance Learning source-speciﬁc weights that reveal how related a speciﬁc source is to the index movement. I(·) is the indicator function. 7 can be rewritten as: min wm,wd,ws,θ β n X G∈S f (G, Y, wm, wd, ws, θ) + 1 n X Ci,Ci−1∈G;G∈S; 1 t tX i=1 g(Ci, Ci−1, wm, wd, ws, θ) (9) where β is a constant to control the contribution of the ﬁrst term. By introducing this loss term, Eq. Eq.',\n",
       " 'The probability of all the news articles for a given day i can be computed as the average of probabilities of each news article, that is pm−i = 1 ni ni X j=1 pij (2) In addition to news articles, we also model the probability pd−i for stock quantitative data and ps−i for sentiments on day i, that is pd−i = σ(wT d di) = 1 1 + e−wT d di (3) ps−i = σ(wT s si) = 1 1 + e−wTs si (4) where wd and ws denote the weight vector of di and si respectively. To this end, for a given day, we ﬁrst model the instancelevel probability pij for a news article j on day i to the target label with a logistic function, that is pij = σ(wmT xij) = 1 1 + e−wmT xij (1) where wm denotes the weight vector of the news articles. I(·) is the indicator function. By introducing this loss term, Eq. Eq. We then model the probability Pi for multisource information on day i as Pi = θ0pm−i + θ1pd−i + θ2ps−i = θ(pm−i, pd−i, ps−i)T (5) where θ0, θ1 and θ2 denote the source-speciﬁc weights of pm−i, pd−i and ps−i respectively, and θ0 + θ1 + θ2 = 1. The higher the probability pij, the more related the article j is to the target label. As the inﬂuences of the multi-source information usually last for a number of days, we assume the probabilities on two consecutive days are essentially similar, which can be represented by minimizing the cost g(Ci, Ci−1) = (Pi −Pi−1)2 (8) where Ci denotes the multi-source group for the day i. 7 can be rewritten as: min wm,wd,ws,θ β n X G∈S f (G, Y, wm, wd, ws, θ) + 1 n X Ci,Ci−1∈G;G∈S; 1 t tX i=1 g(Ci, Ci−1, wm, wd, ws, θ) (9) where β is a constant to control the contribution of the ﬁrst term. We use θ = (θ0, θ1, θ2) to denote the source weight vector, and then model the probability of the multi-source super group as the average of the probabilities in t days, that is P = 1 t tX i=1 Pi (6) Then, we start with a log-likelihood loss function: min wm,wd,ws,θ 1 n X G∈S f (G, Y, wm, wd, ws, θ) = 1 n X G∈S (−I(Y = 1)logP −I(Y = −1)log(1 −P)) (7) where G is a multi-source super group, n is the number of multi-source super groups, and Y denotes the set of true labels. 9 aggregates the costs at the super group level and the group level.',\n",
       " 'I(·) is the indicator function. 7 can be rewritten as: min wm,wd,ws,θ β n X G∈S f (G, Y, wm, wd, ws, θ) + 1 n X Ci,Ci−1∈G;G∈S; 1 t tX i=1 g(Ci, Ci−1, wm, wd, ws, θ) (9) where β is a constant to control the contribution of the ﬁrst term. Eq. The probability of all the news articles for a given day i can be computed as the average of probabilities of each news article, that is pm−i = 1 ni ni X j=1 pij (2) In addition to news articles, we also model the probability pd−i for stock quantitative data and ps−i for sentiments on day i, that is pd−i = σ(wT d di) = 1 1 + e−wT d di (3) ps−i = σ(wT s si) = 1 1 + e−wTs si (4) where wd and ws denote the weight vector of di and si respectively. 9 aggregates the costs at the super group level and the group level. We then model the probability Pi for multisource information on day i as Pi = θ0pm−i + θ1pd−i + θ2ps−i = θ(pm−i, pd−i, ps−i)T (5) where θ0, θ1 and θ2 denote the source-speciﬁc weights of pm−i, pd−i and ps−i respectively, and θ0 + θ1 + θ2 = 1. By introducing this loss term, Eq. The higher the probability pij, the more related the article j is to the target label. As the inﬂuences of the multi-source information usually last for a number of days, we assume the probabilities on two consecutive days are essentially similar, which can be represented by minimizing the cost g(Ci, Ci−1) = (Pi −Pi−1)2 (8) where Ci denotes the multi-source group for the day i. We use θ = (θ0, θ1, θ2) to denote the source weight vector, and then model the probability of the multi-source super group as the average of the probabilities in t days, that is P = 1 t tX i=1 Pi (6) Then, we start with a log-likelihood loss function: min wm,wd,ws,θ 1 n X G∈S f (G, Y, wm, wd, ws, θ) = 1 n X G∈S (−I(Y = 1)logP −I(Y = −1)log(1 −P)) (7) where G is a multi-source super group, n is the number of multi-source super groups, and Y denotes the set of true labels. However, the instance-level loss has not been considered yet, which is challenging to be designed due to two reasons: (1) it lacks of true labels at the instance level; (2) the instances from different sources are heterogeneous but intrinsically correlated.',\n",
       " 'Eq. I(·) is the indicator function. 7 can be rewritten as: min wm,wd,ws,θ β n X G∈S f (G, Y, wm, wd, ws, θ) + 1 n X Ci,Ci−1∈G;G∈S; 1 t tX i=1 g(Ci, Ci−1, wm, wd, ws, θ) (9) where β is a constant to control the contribution of the ﬁrst term. The probability of all the news articles for a given day i can be computed as the average of probabilities of each news article, that is pm−i = 1 ni ni X j=1 pij (2) In addition to news articles, we also model the probability pd−i for stock quantitative data and ps−i for sentiments on day i, that is pd−i = σ(wT d di) = 1 1 + e−wT d di (3) ps−i = σ(wT s si) = 1 1 + e−wTs si (4) where wd and ws denote the weight vector of di and si respectively. We then model the probability Pi for multisource information on day i as Pi = θ0pm−i + θ1pd−i + θ2ps−i = θ(pm−i, pd−i, ps−i)T (5) where θ0, θ1 and θ2 denote the source-speciﬁc weights of pm−i, pd−i and ps−i respectively, and θ0 + θ1 + θ2 = 1. 9 aggregates the costs at the super group level and the group level. By introducing this loss term, Eq. As the inﬂuences of the multi-source information usually last for a number of days, we assume the probabilities on two consecutive days are essentially similar, which can be represented by minimizing the cost g(Ci, Ci−1) = (Pi −Pi−1)2 (8) where Ci denotes the multi-source group for the day i. We use θ = (θ0, θ1, θ2) to denote the source weight vector, and then model the probability of the multi-source super group as the average of the probabilities in t days, that is P = 1 t tX i=1 Pi (6) Then, we start with a log-likelihood loss function: min wm,wd,ws,θ 1 n X G∈S f (G, Y, wm, wd, ws, θ) = 1 n X G∈S (−I(Y = 1)logP −I(Y = −1)log(1 −P)) (7) where G is a multi-source super group, n is the number of multi-source super groups, and Y denotes the set of true labels. However, the instance-level loss has not been considered yet, which is challenging to be designed due to two reasons: (1) it lacks of true labels at the instance level; (2) the instances from different sources are heterogeneous but intrinsically correlated. The instances can be categorized into three types according to their sources, and each type leads to a distinct loss term.',\n",
       " 'Eq. 9 aggregates the costs at the super group level and the group level. 7 can be rewritten as: min wm,wd,ws,θ β n X G∈S f (G, Y, wm, wd, ws, θ) + 1 n X Ci,Ci−1∈G;G∈S; 1 t tX i=1 g(Ci, Ci−1, wm, wd, ws, θ) (9) where β is a constant to control the contribution of the ﬁrst term. I(·) is the indicator function. We then model the probability Pi for multisource information on day i as Pi = θ0pm−i + θ1pd−i + θ2ps−i = θ(pm−i, pd−i, ps−i)T (5) where θ0, θ1 and θ2 denote the source-speciﬁc weights of pm−i, pd−i and ps−i respectively, and θ0 + θ1 + θ2 = 1. By introducing this loss term, Eq. As the inﬂuences of the multi-source information usually last for a number of days, we assume the probabilities on two consecutive days are essentially similar, which can be represented by minimizing the cost g(Ci, Ci−1) = (Pi −Pi−1)2 (8) where Ci denotes the multi-source group for the day i. We use θ = (θ0, θ1, θ2) to denote the source weight vector, and then model the probability of the multi-source super group as the average of the probabilities in t days, that is P = 1 t tX i=1 Pi (6) Then, we start with a log-likelihood loss function: min wm,wd,ws,θ 1 n X G∈S f (G, Y, wm, wd, ws, θ) = 1 n X G∈S (−I(Y = 1)logP −I(Y = −1)log(1 −P)) (7) where G is a multi-source super group, n is the number of multi-source super groups, and Y denotes the set of true labels. The instances can be categorized into three types according to their sources, and each type leads to a distinct loss term. However, the instance-level loss has not been considered yet, which is challenging to be designed due to two reasons: (1) it lacks of true labels at the instance level; (2) the instances from different sources are heterogeneous but intrinsically correlated. Inspired by the hinge loss used in Support Vector Machines (SVMs), the classiﬁcation loss term for the instances of news article instance xij is h1(xij, wm) = max(0, m0 −sgn(Pi −P0)wT mxij) (10) Here, sgn(·) is the sign function, m0 is a margin parameter used to separate the positive and negative instances from the hyperplane in the feature space.',\n",
       " '9 aggregates the costs at the super group level and the group level. By introducing this loss term, Eq. Eq. We use θ = (θ0, θ1, θ2) to denote the source weight vector, and then model the probability of the multi-source super group as the average of the probabilities in t days, that is P = 1 t tX i=1 Pi (6) Then, we start with a log-likelihood loss function: min wm,wd,ws,θ 1 n X G∈S f (G, Y, wm, wd, ws, θ) = 1 n X G∈S (−I(Y = 1)logP −I(Y = −1)log(1 −P)) (7) where G is a multi-source super group, n is the number of multi-source super groups, and Y denotes the set of true labels. However, the instance-level loss has not been considered yet, which is challenging to be designed due to two reasons: (1) it lacks of true labels at the instance level; (2) the instances from different sources are heterogeneous but intrinsically correlated. As the inﬂuences of the multi-source information usually last for a number of days, we assume the probabilities on two consecutive days are essentially similar, which can be represented by minimizing the cost g(Ci, Ci−1) = (Pi −Pi−1)2 (8) where Ci denotes the multi-source group for the day i. 7 can be rewritten as: min wm,wd,ws,θ β n X G∈S f (G, Y, wm, wd, ws, θ) + 1 n X Ci,Ci−1∈G;G∈S; 1 t tX i=1 g(Ci, Ci−1, wm, wd, ws, θ) (9) where β is a constant to control the contribution of the ﬁrst term. The instances can be categorized into three types according to their sources, and each type leads to a distinct loss term. I(·) is the indicator function. Inspired by the hinge loss used in Support Vector Machines (SVMs), the classiﬁcation loss term for the instances of news article instance xij is h1(xij, wm) = max(0, m0 −sgn(Pi −P0)wT mxij) (10) Here, sgn(·) is the sign function, m0 is a margin parameter used to separate the positive and negative instances from the hyperplane in the feature space. wT mxij denotes the prediction with article xij.',\n",
       " 'Eq. 7 can be rewritten as: min wm,wd,ws,θ β n X G∈S f (G, Y, wm, wd, ws, θ) + 1 n X Ci,Ci−1∈G;G∈S; 1 t tX i=1 g(Ci, Ci−1, wm, wd, ws, θ) (9) where β is a constant to control the contribution of the ﬁrst term. I(·) is the indicator function. By introducing this loss term, Eq. wT mxij denotes the prediction with article xij. As the inﬂuences of the multi-source information usually last for a number of days, we assume the probabilities on two consecutive days are essentially similar, which can be represented by minimizing the cost g(Ci, Ci−1) = (Pi −Pi−1)2 (8) where Ci denotes the multi-source group for the day i. The instances can be categorized into three types according to their sources, and each type leads to a distinct loss term. We use θ = (θ0, θ1, θ2) to denote the source weight vector, and then model the probability of the multi-source super group as the average of the probabilities in t days, that is P = 1 t tX i=1 Pi (6) Then, we start with a log-likelihood loss function: min wm,wd,ws,θ 1 n X G∈S f (G, Y, wm, wd, ws, θ) = 1 n X G∈S (−I(Y = 1)logP −I(Y = −1)log(1 −P)) (7) where G is a multi-source super group, n is the number of multi-source super groups, and Y denotes the set of true labels. However, the instance-level loss has not been considered yet, which is challenging to be designed due to two reasons: (1) it lacks of true labels at the instance level; (2) the instances from different sources are heterogeneous but intrinsically correlated. Inspired by the hinge loss used in Support Vector Machines (SVMs), the classiﬁcation loss term for the instances of news article instance xij is h1(xij, wm) = max(0, m0 −sgn(Pi −P0)wT mxij) (10) Here, sgn(·) is the sign function, m0 is a margin parameter used to separate the positive and negative instances from the hyperplane in the feature space. As the true label for each instance is unknown during the classiﬁer training, we replace it with the estimated true label sgn(Pi −P0), where P0 is a threshold parameter to determine the positiveness of the prediction.',\n",
       " '9 aggregates the costs at the super group level and the group level. By introducing this loss term, Eq. 7 can be rewritten as: min wm,wd,ws,θ β n X G∈S f (G, Y, wm, wd, ws, θ) + 1 n X Ci,Ci−1∈G;G∈S; 1 t tX i=1 g(Ci, Ci−1, wm, wd, ws, θ) (9) where β is a constant to control the contribution of the ﬁrst term. However, the instance-level loss has not been considered yet, which is challenging to be designed due to two reasons: (1) it lacks of true labels at the instance level; (2) the instances from different sources are heterogeneous but intrinsically correlated. I(·) is the indicator function. Inspired by the hinge loss used in Support Vector Machines (SVMs), the classiﬁcation loss term for the instances of news article instance xij is h1(xij, wm) = max(0, m0 −sgn(Pi −P0)wT mxij) (10) Here, sgn(·) is the sign function, m0 is a margin parameter used to separate the positive and negative instances from the hyperplane in the feature space. The instances can be categorized into three types according to their sources, and each type leads to a distinct loss term. wT mxij denotes the prediction with article xij. As the true label for each instance is unknown during the classiﬁer training, we replace it with the estimated true label sgn(Pi −P0), where P0 is a threshold parameter to determine the positiveness of the prediction. As the inﬂuences of the multi-source information usually last for a number of days, we assume the probabilities on two consecutive days are essentially similar, which can be represented by minimizing the cost g(Ci, Ci−1) = (Pi −Pi−1)2 (8) where Ci denotes the multi-source group for the day i. If (Pi−P0)>0, the prediction with multiple-source information on day i would be positive.',\n",
       " 'As the inﬂuences of the multi-source information usually last for a number of days, we assume the probabilities on two consecutive days are essentially similar, which can be represented by minimizing the cost g(Ci, Ci−1) = (Pi −Pi−1)2 (8) where Ci denotes the multi-source group for the day i. Eq. 7 can be rewritten as: min wm,wd,ws,θ β n X G∈S f (G, Y, wm, wd, ws, θ) + 1 n X Ci,Ci−1∈G;G∈S; 1 t tX i=1 g(Ci, Ci−1, wm, wd, ws, θ) (9) where β is a constant to control the contribution of the ﬁrst term. The instances can be categorized into three types according to their sources, and each type leads to a distinct loss term. 9 aggregates the costs at the super group level and the group level. Inspired by the hinge loss used in Support Vector Machines (SVMs), the classiﬁcation loss term for the instances of news article instance xij is h1(xij, wm) = max(0, m0 −sgn(Pi −P0)wT mxij) (10) Here, sgn(·) is the sign function, m0 is a margin parameter used to separate the positive and negative instances from the hyperplane in the feature space. By introducing this loss term, Eq. wT mxij denotes the prediction with article xij. As the true label for each instance is unknown during the classiﬁer training, we replace it with the estimated true label sgn(Pi −P0), where P0 is a threshold parameter to determine the positiveness of the prediction. If (Pi−P0)>0, the prediction with multiple-source information on day i would be positive. Otherwise, it would be negative. Similarly, we can derive the instance-level loss terms for quantitative data and sentiments respectively, h2(di, wd) = max(0, m1 −sgn(Pi −P0)wT d di) (11) h3(si, ws) = max(0, m2 −sgn(Pi −P0)wT s si) (12) Based on Eq.',\n",
       " '7 can be rewritten as: min wm,wd,ws,θ β n X G∈S f (G, Y, wm, wd, ws, θ) + 1 n X Ci,Ci−1∈G;G∈S; 1 t tX i=1 g(Ci, Ci−1, wm, wd, ws, θ) (9) where β is a constant to control the contribution of the ﬁrst term. 9 aggregates the costs at the super group level and the group level. If (Pi−P0)>0, the prediction with multiple-source information on day i would be positive. By introducing this loss term, Eq. wT mxij denotes the prediction with article xij. As the true label for each instance is unknown during the classiﬁer training, we replace it with the estimated true label sgn(Pi −P0), where P0 is a threshold parameter to determine the positiveness of the prediction. Inspired by the hinge loss used in Support Vector Machines (SVMs), the classiﬁcation loss term for the instances of news article instance xij is h1(xij, wm) = max(0, m0 −sgn(Pi −P0)wT mxij) (10) Here, sgn(·) is the sign function, m0 is a margin parameter used to separate the positive and negative instances from the hyperplane in the feature space. Similarly, we can derive the instance-level loss terms for quantitative data and sentiments respectively, h2(di, wd) = max(0, m1 −sgn(Pi −P0)wT d di) (11) h3(si, ws) = max(0, m2 −sgn(Pi −P0)wT s si) (12) Based on Eq. The instances can be categorized into three types according to their sources, and each type leads to a distinct loss term. However, the instance-level loss has not been considered yet, which is challenging to be designed due to two reasons: (1) it lacks of true labels at the instance level; (2) the instances from different sources are heterogeneous but intrinsically correlated. 10, 11 and 12, the classiﬁcation loss at the instance level for each data source has been obtained.',\n",
       " '9 aggregates the costs at the super group level and the group level. Eq. wT mxij denotes the prediction with article xij. Inspired by the hinge loss used in Support Vector Machines (SVMs), the classiﬁcation loss term for the instances of news article instance xij is h1(xij, wm) = max(0, m0 −sgn(Pi −P0)wT mxij) (10) Here, sgn(·) is the sign function, m0 is a margin parameter used to separate the positive and negative instances from the hyperplane in the feature space. The instances can be categorized into three types according to their sources, and each type leads to a distinct loss term. However, the instance-level loss has not been considered yet, which is challenging to be designed due to two reasons: (1) it lacks of true labels at the instance level; (2) the instances from different sources are heterogeneous but intrinsically correlated. If (Pi−P0)>0, the prediction with multiple-source information on day i would be positive. 10, 11 and 12, the classiﬁcation loss at the instance level for each data source has been obtained. Similarly, we can derive the instance-level loss terms for quantitative data and sentiments respectively, h2(di, wd) = max(0, m1 −sgn(Pi −P0)wT d di) (11) h3(si, ws) = max(0, m2 −sgn(Pi −P0)wT s si) (12) Based on Eq. As the true label for each instance is unknown during the classiﬁer training, we replace it with the estimated true label sgn(Pi −P0), where P0 is a threshold parameter to determine the positiveness of the prediction. We then explain why they share a common estimated true label (i.e., sgn(Pi −P0)). As predictions from different sources are commonly correlated with each other, instead of treating the loss of each source independently, we need to consider their intrinsic consistencies.',\n",
       " 'As the true label for each instance is unknown during the classiﬁer training, we replace it with the estimated true label sgn(Pi −P0), where P0 is a threshold parameter to determine the positiveness of the prediction. 10, 11 and 12, the classiﬁcation loss at the instance level for each data source has been obtained. Inspired by the hinge loss used in Support Vector Machines (SVMs), the classiﬁcation loss term for the instances of news article instance xij is h1(xij, wm) = max(0, m0 −sgn(Pi −P0)wT mxij) (10) Here, sgn(·) is the sign function, m0 is a margin parameter used to separate the positive and negative instances from the hyperplane in the feature space. Otherwise, it would be negative. The instances can be categorized into three types according to their sources, and each type leads to a distinct loss term. Similarly, we can derive the instance-level loss terms for quantitative data and sentiments respectively, h2(di, wd) = max(0, m1 −sgn(Pi −P0)wT d di) (11) h3(si, ws) = max(0, m2 −sgn(Pi −P0)wT s si) (12) Based on Eq. We then explain why they share a common estimated true label (i.e., sgn(Pi −P0)). As predictions from different sources are commonly correlated with each other, instead of treating the loss of each source independently, we need to consider their intrinsic consistencies. If (Pi−P0)>0, the prediction with multiple-source information on day i would be positive. wT mxij denotes the prediction with article xij. The intuition behind is that according to Efﬁcient Market Hypothesis, different data sources would keep up to date with the latest stock market information, and they commonly indicate the same sign (index rise or fall). Thus, through sharing the same estimated true label, we are able to combine the indications from different data sources to learn a consensus label.',\n",
       " 'The intuition behind is that according to Efﬁcient Market Hypothesis, different data sources would keep up to date with the latest stock market information, and they commonly indicate the same sign (index rise or fall). The instances can be categorized into three types according to their sources, and each type leads to a distinct loss term. wT mxij denotes the prediction with article xij. Similarly, we can derive the instance-level loss terms for quantitative data and sentiments respectively, h2(di, wd) = max(0, m1 −sgn(Pi −P0)wT d di) (11) h3(si, ws) = max(0, m2 −sgn(Pi −P0)wT s si) (12) Based on Eq. 10, 11 and 12, the classiﬁcation loss at the instance level for each data source has been obtained. As the true label for each instance is unknown during the classiﬁer training, we replace it with the estimated true label sgn(Pi −P0), where P0 is a threshold parameter to determine the positiveness of the prediction. We then explain why they share a common estimated true label (i.e., sgn(Pi −P0)). If (Pi−P0)>0, the prediction with multiple-source information on day i would be positive. As predictions from different sources are commonly correlated with each other, instead of treating the loss of each source independently, we need to consider their intrinsic consistencies. Thus, through sharing the same estimated true label, we are able to combine the indications from different data sources to learn a consensus label. This can potentially provide more robust and conﬁdent predictions. We then give several cases to illustrate the consensus among sources. The three source-speciﬁc predictions are denoted as l0, l1 and l2 respectively.',\n",
       " '10, 11 and 12, the classiﬁcation loss at the instance level for each data source has been obtained. wT mxij denotes the prediction with article xij. Similarly, we can derive the instance-level loss terms for quantitative data and sentiments respectively, h2(di, wd) = max(0, m1 −sgn(Pi −P0)wT d di) (11) h3(si, ws) = max(0, m2 −sgn(Pi −P0)wT s si) (12) Based on Eq. As the true label for each instance is unknown during the classiﬁer training, we replace it with the estimated true label sgn(Pi −P0), where P0 is a threshold parameter to determine the positiveness of the prediction. Otherwise, it would be negative. As predictions from different sources are commonly correlated with each other, instead of treating the loss of each source independently, we need to consider their intrinsic consistencies. This can potentially provide more robust and conﬁdent predictions. We then explain why they share a common estimated true label (i.e., sgn(Pi −P0)). The three source-speciﬁc predictions are denoted as l0, l1 and l2 respectively. If (Pi−P0)>0, the prediction with multiple-source information on day i would be positive. Firstly, if l0, l1 and l2 all make very positive predictions, i.e., large values of pm−i, pd−i and ps−i, it would be conﬁdent to make a positive group-level prediction due to Pi > P0 and l0, l1 and l2 VOLUME 6, 2018 50723 X. Zhang et al. : Stock Market Prediction via Multi-Source Multiple Instance Learning all agree with the label without costs. Secondly, if only l0 disagrees with the estimated true label, l0 will be penalized as l1 and l2 agree with this label and make Pi approach their predictions.',\n",
       " 'We then give several cases to illustrate the consensus among sources. This can potentially provide more robust and conﬁdent predictions. The three source-speciﬁc predictions are denoted as l0, l1 and l2 respectively. As predictions from different sources are commonly correlated with each other, instead of treating the loss of each source independently, we need to consider their intrinsic consistencies. Thus, through sharing the same estimated true label, we are able to combine the indications from different data sources to learn a consensus label. 10, 11 and 12, the classiﬁcation loss at the instance level for each data source has been obtained. Similarly, we can derive the instance-level loss terms for quantitative data and sentiments respectively, h2(di, wd) = max(0, m1 −sgn(Pi −P0)wT d di) (11) h3(si, ws) = max(0, m2 −sgn(Pi −P0)wT s si) (12) Based on Eq. Firstly, if l0, l1 and l2 all make very positive predictions, i.e., large values of pm−i, pd−i and ps−i, it would be conﬁdent to make a positive group-level prediction due to Pi > P0 and l0, l1 and l2 VOLUME 6, 2018 50723 X. Zhang et al. We then explain why they share a common estimated true label (i.e., sgn(Pi −P0)). Secondly, if only l0 disagrees with the estimated true label, l0 will be penalized as l1 and l2 agree with this label and make Pi approach their predictions. Thirdly, if l0 disagrees with l1 and l2, but l0 is very conﬁdent (and thus far from hyperplane) while l1 and l2 are not conﬁdent enough (and thus close to hyperplane), this may make Pi approach l0, resulting in that l0 agrees with the estimated true label while l1 and l2 disagree with it and thus are penalized. Our proposed instance-level loss terms are consistent with these cases and thus make sense.',\n",
       " 'Firstly, if l0, l1 and l2 all make very positive predictions, i.e., large values of pm−i, pd−i and ps−i, it would be conﬁdent to make a positive group-level prediction due to Pi > P0 and l0, l1 and l2 VOLUME 6, 2018 50723 X. Zhang et al. We then explain why they share a common estimated true label (i.e., sgn(Pi −P0)). : Stock Market Prediction via Multi-Source Multiple Instance Learning all agree with the label without costs. This can potentially provide more robust and conﬁdent predictions. Thus, through sharing the same estimated true label, we are able to combine the indications from different data sources to learn a consensus label. The three source-speciﬁc predictions are denoted as l0, l1 and l2 respectively. Thirdly, if l0 disagrees with l1 and l2, but l0 is very conﬁdent (and thus far from hyperplane) while l1 and l2 are not conﬁdent enough (and thus close to hyperplane), this may make Pi approach l0, resulting in that l0 agrees with the estimated true label while l1 and l2 disagree with it and thus are penalized. Secondly, if only l0 disagrees with the estimated true label, l0 will be penalized as l1 and l2 agree with this label and make Pi approach their predictions. As predictions from different sources are commonly correlated with each other, instead of treating the loss of each source independently, we need to consider their intrinsic consistencies. Our proposed instance-level loss terms are consistent with these cases and thus make sense. Then we try to minimize the overall instance-level loss, that is, h1(xij, wm) + h2(di, wd) + h3(si, ws). By introducing this summation and other regularization terms, the objective function Eq.',\n",
       " 'This can potentially provide more robust and conﬁdent predictions. Thus, through sharing the same estimated true label, we are able to combine the indications from different data sources to learn a consensus label. Our proposed instance-level loss terms are consistent with these cases and thus make sense. Thirdly, if l0 disagrees with l1 and l2, but l0 is very conﬁdent (and thus far from hyperplane) while l1 and l2 are not conﬁdent enough (and thus close to hyperplane), this may make Pi approach l0, resulting in that l0 agrees with the estimated true label while l1 and l2 disagree with it and thus are penalized. Firstly, if l0, l1 and l2 all make very positive predictions, i.e., large values of pm−i, pd−i and ps−i, it would be conﬁdent to make a positive group-level prediction due to Pi > P0 and l0, l1 and l2 VOLUME 6, 2018 50723 X. Zhang et al. Secondly, if only l0 disagrees with the estimated true label, l0 will be penalized as l1 and l2 agree with this label and make Pi approach their predictions. By introducing this summation and other regularization terms, the objective function Eq. The three source-speciﬁc predictions are denoted as l0, l1 and l2 respectively. Then we try to minimize the overall instance-level loss, that is, h1(xij, wm) + h2(di, wd) + h3(si, ws). We then give several cases to illustrate the consensus among sources. 9 can be reformulated as L(wm, wd, ws, θ) = β n X G∈S f (G, Y, wm, wd, ws, θ) + 1 n X Ci,Ci−1∈G;G∈S; 1 t tX i=1 g(Ci, Ci−1, wm, wd, ws, θ) + 1 n X xij∈Xi;Xi∈G;G∈S 1 t tX i=1 1 ni ni X j=1 h1(xij, wm) + 1 n X di,si∈Ci;Ci∈G;G∈S 1 t tX i=1 1 ni (h2(di, wd) + h3(si, ws)) + λmR(wm) + λdR(wd) + λsR(ws) + λθR(θ) (13) Eq.',\n",
       " 'We then give several cases to illustrate the consensus among sources. Firstly, if l0, l1 and l2 all make very positive predictions, i.e., large values of pm−i, pd−i and ps−i, it would be conﬁdent to make a positive group-level prediction due to Pi > P0 and l0, l1 and l2 VOLUME 6, 2018 50723 X. Zhang et al. : Stock Market Prediction via Multi-Source Multiple Instance Learning all agree with the label without costs. Our proposed instance-level loss terms are consistent with these cases and thus make sense. Secondly, if only l0 disagrees with the estimated true label, l0 will be penalized as l1 and l2 agree with this label and make Pi approach their predictions. This can potentially provide more robust and conﬁdent predictions. 9 can be reformulated as L(wm, wd, ws, θ) = β n X G∈S f (G, Y, wm, wd, ws, θ) + 1 n X Ci,Ci−1∈G;G∈S; 1 t tX i=1 g(Ci, Ci−1, wm, wd, ws, θ) + 1 n X xij∈Xi;Xi∈G;G∈S 1 t tX i=1 1 ni ni X j=1 h1(xij, wm) + 1 n X di,si∈Ci;Ci∈G;G∈S 1 t tX i=1 1 ni (h2(di, wd) + h3(si, ws)) + λmR(wm) + λdR(wd) + λsR(ws) + λθR(θ) (13) Eq. The three source-speciﬁc predictions are denoted as l0, l1 and l2 respectively. Then we try to minimize the overall instance-level loss, that is, h1(xij, wm) + h2(di, wd) + h3(si, ws). By introducing this summation and other regularization terms, the objective function Eq. 13 is the ultimate objective function to optimize. To summarize, it consists of losses at three levels: the super group level, the group level and the instance level.',\n",
       " 'To summarize, it consists of losses at three levels: the super group level, the group level and the instance level. We then give several cases to illustrate the consensus among sources. Secondly, if only l0 disagrees with the estimated true label, l0 will be penalized as l1 and l2 agree with this label and make Pi approach their predictions. Our proposed instance-level loss terms are consistent with these cases and thus make sense. 13 is the ultimate objective function to optimize. Thirdly, if l0 disagrees with l1 and l2, but l0 is very conﬁdent (and thus far from hyperplane) while l1 and l2 are not conﬁdent enough (and thus close to hyperplane), this may make Pi approach l0, resulting in that l0 agrees with the estimated true label while l1 and l2 disagree with it and thus are penalized. Then we try to minimize the overall instance-level loss, that is, h1(xij, wm) + h2(di, wd) + h3(si, ws). 9 can be reformulated as L(wm, wd, ws, θ) = β n X G∈S f (G, Y, wm, wd, ws, θ) + 1 n X Ci,Ci−1∈G;G∈S; 1 t tX i=1 g(Ci, Ci−1, wm, wd, ws, θ) + 1 n X xij∈Xi;Xi∈G;G∈S 1 t tX i=1 1 ni ni X j=1 h1(xij, wm) + 1 n X di,si∈Ci;Ci∈G;G∈S 1 t tX i=1 1 ni (h2(di, wd) + h3(si, ws)) + λmR(wm) + λdR(wd) + λsR(ws) + λθR(θ) (13) Eq. The three source-speciﬁc predictions are denoted as l0, l1 and l2 respectively. By introducing this summation and other regularization terms, the objective function Eq. In addition, it includes the regularization terms, that is, R(wm), R(wd), R(ws) and R(θ), and β, λm, λd, λs and λθ are constants to control the trade-offs among multiple terms.',\n",
       " ': Stock Market Prediction via Multi-Source Multiple Instance Learning all agree with the label without costs. 9 can be reformulated as L(wm, wd, ws, θ) = β n X G∈S f (G, Y, wm, wd, ws, θ) + 1 n X Ci,Ci−1∈G;G∈S; 1 t tX i=1 g(Ci, Ci−1, wm, wd, ws, θ) + 1 n X xij∈Xi;Xi∈G;G∈S 1 t tX i=1 1 ni ni X j=1 h1(xij, wm) + 1 n X di,si∈Ci;Ci∈G;G∈S 1 t tX i=1 1 ni (h2(di, wd) + h3(si, ws)) + λmR(wm) + λdR(wd) + λsR(ws) + λθR(θ) (13) Eq. To summarize, it consists of losses at three levels: the super group level, the group level and the instance level. Thirdly, if l0 disagrees with l1 and l2, but l0 is very conﬁdent (and thus far from hyperplane) while l1 and l2 are not conﬁdent enough (and thus close to hyperplane), this may make Pi approach l0, resulting in that l0 agrees with the estimated true label while l1 and l2 disagree with it and thus are penalized. 13 is the ultimate objective function to optimize. Our proposed instance-level loss terms are consistent with these cases and thus make sense. Secondly, if only l0 disagrees with the estimated true label, l0 will be penalized as l1 and l2 agree with this label and make Pi approach their predictions. By introducing this summation and other regularization terms, the objective function Eq. In addition, it includes the regularization terms, that is, R(wm), R(wd), R(ws) and R(θ), and β, λm, λd, λs and λθ are constants to control the trade-offs among multiple terms. Then we try to minimize the overall instance-level loss, that is, h1(xij, wm) + h2(di, wd) + h3(si, ws). The model learning goal is to estimate the parameters wm, wd, ws and θ to minimize L(wm, wd, ws, θ).',\n",
       " '9 can be reformulated as L(wm, wd, ws, θ) = β n X G∈S f (G, Y, wm, wd, ws, θ) + 1 n X Ci,Ci−1∈G;G∈S; 1 t tX i=1 g(Ci, Ci−1, wm, wd, ws, θ) + 1 n X xij∈Xi;Xi∈G;G∈S 1 t tX i=1 1 ni ni X j=1 h1(xij, wm) + 1 n X di,si∈Ci;Ci∈G;G∈S 1 t tX i=1 1 ni (h2(di, wd) + h3(si, ws)) + λmR(wm) + λdR(wd) + λsR(ws) + λθR(θ) (13) Eq. To summarize, it consists of losses at three levels: the super group level, the group level and the instance level. 13 is the ultimate objective function to optimize. In addition, it includes the regularization terms, that is, R(wm), R(wd), R(ws) and R(θ), and β, λm, λd, λs and λθ are constants to control the trade-offs among multiple terms. Secondly, if only l0 disagrees with the estimated true label, l0 will be penalized as l1 and l2 agree with this label and make Pi approach their predictions. : Stock Market Prediction via Multi-Source Multiple Instance Learning all agree with the label without costs. Then we try to minimize the overall instance-level loss, that is, h1(xij, wm) + h2(di, wd) + h3(si, ws). Our proposed instance-level loss terms are consistent with these cases and thus make sense. By introducing this summation and other regularization terms, the objective function Eq. The model learning goal is to estimate the parameters wm, wd, ws and θ to minimize L(wm, wd, ws, θ). We randomly choose a set (G, Y) from S, and the online stochastic gradient descent optimization is adopted to ﬁt the model. C. IDENTIFYING THE KEY FACTORS After the learning process, the source weight vector θ is obtained, representing the impacts of different data sources on the market movements.',\n",
       " 'To summarize, it consists of losses at three levels: the super group level, the group level and the instance level. Our proposed instance-level loss terms are consistent with these cases and thus make sense. 13 is the ultimate objective function to optimize. In addition, it includes the regularization terms, that is, R(wm), R(wd), R(ws) and R(θ), and β, λm, λd, λs and λθ are constants to control the trade-offs among multiple terms. We randomly choose a set (G, Y) from S, and the online stochastic gradient descent optimization is adopted to ﬁt the model. Then we try to minimize the overall instance-level loss, that is, h1(xij, wm) + h2(di, wd) + h3(si, ws). By introducing this summation and other regularization terms, the objective function Eq. The model learning goal is to estimate the parameters wm, wd, ws and θ to minimize L(wm, wd, ws, θ). Secondly, if only l0 disagrees with the estimated true label, l0 will be penalized as l1 and l2 agree with this label and make Pi approach their predictions. C. IDENTIFYING THE KEY FACTORS After the learning process, the source weight vector θ is obtained, representing the impacts of different data sources on the market movements. In addition, a probability for each piece of input information can also be obtained through Eq. 1, 3 or 4, which reveals the probability of that information signifying the rise of the market index on the target day. Note that if the probability signifying the index rise is pr, the probability indicating index decline would be 1 −pr. We can identify the key input information that triggers the market index movement, if the product of its probability value and its source-speciﬁc weight is above a given threshold τ. IV.',\n",
       " 'We randomly choose a set (G, Y) from S, and the online stochastic gradient descent optimization is adopted to ﬁt the model. We can identify the key input information that triggers the market index movement, if the product of its probability value and its source-speciﬁc weight is above a given threshold τ. IV. 13 is the ultimate objective function to optimize. By introducing this summation and other regularization terms, the objective function Eq. In addition, it includes the regularization terms, that is, R(wm), R(wd), R(ws) and R(θ), and β, λm, λd, λs and λθ are constants to control the trade-offs among multiple terms. Then we try to minimize the overall instance-level loss, that is, h1(xij, wm) + h2(di, wd) + h3(si, ws). 1, 3 or 4, which reveals the probability of that information signifying the rise of the market index on the target day. The model learning goal is to estimate the parameters wm, wd, ws and θ to minimize L(wm, wd, ws, θ). In addition, a probability for each piece of input information can also be obtained through Eq. C. IDENTIFYING THE KEY FACTORS After the learning process, the source weight vector θ is obtained, representing the impacts of different data sources on the market movements. FEATURE EXTRACTION The quantitative features are quite simple to extract, we just collect three indices and normalize each index to form di ∈R3×1. Here we introduce how to extract event representations from news articles and extract the sentiments from posts in social media, which are used as the inputs to M-MI framework. FIGURE 3. Structured event extraction from texts. A. EVENT FEATURE EXTRACTION Conventional methods commonly represent events using simple features such as TF-IDF, noun phrases or named entities. Recent advances in NLP techniques enable more accurate event models with structures.',\n",
       " 'We randomly choose a set (G, Y) from S, and the online stochastic gradient descent optimization is adopted to ﬁt the model. We can identify the key input information that triggers the market index movement, if the product of its probability value and its source-speciﬁc weight is above a given threshold τ. IV. The model learning goal is to estimate the parameters wm, wd, ws and θ to minimize L(wm, wd, ws, θ). In addition, a probability for each piece of input information can also be obtained through Eq. C. IDENTIFYING THE KEY FACTORS After the learning process, the source weight vector θ is obtained, representing the impacts of different data sources on the market movements. FEATURE EXTRACTION The quantitative features are quite simple to extract, we just collect three indices and normalize each index to form di ∈R3×1. EVENT FEATURE EXTRACTION Conventional methods commonly represent events using simple features such as TF-IDF, noun phrases or named entities. Here we introduce how to extract event representations from news articles and extract the sentiments from posts in social media, which are used as the inputs to M-MI framework. Structured event extraction from texts. Recent advances in NLP techniques enable more accurate event models with structures. In this study, we ﬁrst use the syntactic analysis method to extract the main structure information of the sentences, and then use it as the input to an RBM. The output of an RBM would be a pre-trained vector used as the input to sentence2vec, and then the event representations are obtained. The process is shown in Figure 3 and described in detail as follows. Note that though we use the Chinese dataset in this study, this process can also be applied to other languages. 1) Structured event extraction.',\n",
       " 'The process is shown in Figure 3 and described in detail as follows. FEATURE EXTRACTION The quantitative features are quite simple to extract, we just collect three indices and normalize each index to form di ∈R3×1. The output of an RBM would be a pre-trained vector used as the input to sentence2vec, and then the event representations are obtained. Here we introduce how to extract event representations from news articles and extract the sentiments from posts in social media, which are used as the inputs to M-MI framework. EVENT FEATURE EXTRACTION Conventional methods commonly represent events using simple features such as TF-IDF, noun phrases or named entities. Note that though we use the Chinese dataset in this study, this process can also be applied to other languages. Recent advances in NLP techniques enable more accurate event models with structures. Structured event extraction from texts. 1) Structured event extraction. In this study, we ﬁrst use the syntactic analysis method to extract the main structure information of the sentences, and then use it as the input to an RBM. With a commonly used text parser HanLP,1 we can capture the syntactic structure of a sentence, which is depicted as a three-level tree at the top of Fig. 3. The root node denotes the core verb, and the nodes of the second layer are the subject of the verb and the object of the verb respectively. The child of the subject is the modiﬁer who is the nearest to the subject in the sentence, and so is the child of the object. Then we connect these core words together as the structure information to represent the event information. 2) Training with RBM. We then map the structured event into a vector.',\n",
       " 'Structured event extraction from texts. Note that though we use the Chinese dataset in this study, this process can also be applied to other languages. Then we connect these core words together as the structure information to represent the event information. EVENT FEATURE EXTRACTION Conventional methods commonly represent events using simple features such as TF-IDF, noun phrases or named entities. 1) Structured event extraction. Recent advances in NLP techniques enable more accurate event models with structures. In this study, we ﬁrst use the syntactic analysis method to extract the main structure information of the sentences, and then use it as the input to an RBM. We then map the structured event into a vector. 2) Training with RBM. The output of an RBM would be a pre-trained vector used as the input to sentence2vec, and then the event representations are obtained. To make the vectors better reconstruct the original events, we use RBM as a pre-training module. The Restricted Boltzmann Machine (RBM) is a generative stochastic artiﬁcial neural network, and has been applied in various applications such as dimensionality reduction [27]. RBM contains two-layer neural nets, one is the visible layer or input layer, and the 1https://github.com/hankcs/HanLP 50724 VOLUME 6, 2018 X. Zhang et al. : Stock Market Prediction via Multi-Source Multiple Instance Learning other is the hidden layer. In our model, each event is represented as an m-dimensional vector with one-hot encoding, which is the visible layer. Our target is to estimate the n-dimensional hidden layer to approximate the input layer as much as possible. Then the hidden layer will be set as the initial vector in sentence2vec. The reason is that directly training the representations using sentence2vec without RBM may fall into the local minimum. 3) Training with sentence2vec.',\n",
       " 'Note that though we use the Chinese dataset in this study, this process can also be applied to other languages. With a commonly used text parser HanLP,1 we can capture the syntactic structure of a sentence, which is depicted as a three-level tree at the top of Fig. The reason is that directly training the representations using sentence2vec without RBM may fall into the local minimum. Then we connect these core words together as the structure information to represent the event information. To make the vectors better reconstruct the original events, we use RBM as a pre-training module. Then the hidden layer will be set as the initial vector in sentence2vec. In our model, each event is represented as an m-dimensional vector with one-hot encoding, which is the visible layer. We then map the structured event into a vector. 1) Structured event extraction. 3) Training with sentence2vec. Finally, we use sentence2vec, the neural probabilistic language model to obtain the event representations. Different from the word2vec with CBOW model, the sentence id will be added during the training process of sentence2vec, and will also be mapped into a vector, called sentence vector, which would be the ﬁnal vector that we want. In the training process, the sentence vector and the word vectors of context will be concatenated as the input to softmax. After training, the sentence vector will be obtained and used as the features for the proposed model. Here is an example of extracting structured events from the news. The news text is that it is expected that the Renminbi speculators will face huge losses. After the dependency parsing analysis, the core words (Renminbi, speculators, face, huge, losses) are obtained, and after one hot coding, each word is encoded into zero or one vector.',\n",
       " 'The reason is that directly training the representations using sentence2vec without RBM may fall into the local minimum. We then map the structured event into a vector. Different from the word2vec with CBOW model, the sentence id will be added during the training process of sentence2vec, and will also be mapped into a vector, called sentence vector, which would be the ﬁnal vector that we want. In our model, each event is represented as an m-dimensional vector with one-hot encoding, which is the visible layer. 3) Training with sentence2vec. In the training process, the sentence vector and the word vectors of context will be concatenated as the input to softmax. After the dependency parsing analysis, the core words (Renminbi, speculators, face, huge, losses) are obtained, and after one hot coding, each word is encoded into zero or one vector. To make the vectors better reconstruct the original events, we use RBM as a pre-training module. Finally, we use sentence2vec, the neural probabilistic language model to obtain the event representations. After training, the sentence vector will be obtained and used as the features for the proposed model. Then the vector preprocesses by RBM into a 100-dimensional vector, and ﬁnally processes by the sentence2vec became the news event feature vector. Through the above steps, the news event is obtained as a feature of the M-MI model, a 100-dimensional vector. B. SENTIMENT EXTRACTION To extract the sentiments from the posts in the social network, we use the LDA-S method [28], an extension of Latent Dirichlet Allocation (LDA) model that proposed to obtain the topic-speciﬁc sentiments for short texts. The intuition behind is that extracting sentiments discarding topics may be not sufﬁcient as sentiment polarities usually depend on topics or domains [29].',\n",
       " 'Through the above steps, the news event is obtained as a feature of the M-MI model, a 100-dimensional vector. Different from the word2vec with CBOW model, the sentence id will be added during the training process of sentence2vec, and will also be mapped into a vector, called sentence vector, which would be the ﬁnal vector that we want. In the training process, the sentence vector and the word vectors of context will be concatenated as the input to softmax. Finally, we use sentence2vec, the neural probabilistic language model to obtain the event representations. After training, the sentence vector will be obtained and used as the features for the proposed model. After the dependency parsing analysis, the core words (Renminbi, speculators, face, huge, losses) are obtained, and after one hot coding, each word is encoded into zero or one vector. 3) Training with sentence2vec. Then the vector preprocesses by RBM into a 100-dimensional vector, and ﬁnally processes by the sentence2vec became the news event feature vector. SENTIMENT EXTRACTION To extract the sentiments from the posts in the social network, we use the LDA-S method [28], an extension of Latent Dirichlet Allocation (LDA) model that proposed to obtain the topic-speciﬁc sentiments for short texts. The intuition behind is that extracting sentiments discarding topics may be not sufﬁcient as sentiment polarities usually depend on topics or domains [29]. In other words, the exact same word may express different sentiment polarities for different topics, e.g., the opinion word ‘‘low’’ in the phrase ‘‘low speed’’ in a trafﬁc-related topic and ‘‘low fat’’ in a food-related topic. Therefore, extracting the sentiments corresponding to different topics can potentially improve the sentiment classiﬁcation accuracy. The LDA-S model can infer sentiment distribution and topic distribution simultaneously for short texts. It consists of two steps.',\n",
       " 'It consists of two steps. In the training process, the sentence vector and the word vectors of context will be concatenated as the input to softmax. After training, the sentence vector will be obtained and used as the features for the proposed model. Therefore, extracting the sentiments corresponding to different topics can potentially improve the sentiment classiﬁcation accuracy. After the dependency parsing analysis, the core words (Renminbi, speculators, face, huge, losses) are obtained, and after one hot coding, each word is encoded into zero or one vector. The intuition behind is that extracting sentiments discarding topics may be not sufﬁcient as sentiment polarities usually depend on topics or domains [29]. SENTIMENT EXTRACTION To extract the sentiments from the posts in the social network, we use the LDA-S method [28], an extension of Latent Dirichlet Allocation (LDA) model that proposed to obtain the topic-speciﬁc sentiments for short texts. Then the vector preprocesses by RBM into a 100-dimensional vector, and ﬁnally processes by the sentence2vec became the news event feature vector. The LDA-S model can infer sentiment distribution and topic distribution simultaneously for short texts. Through the above steps, the news event is obtained as a feature of the M-MI model, a 100-dimensional vector. The ﬁrst step aims to obtain the topic distribution of each post, and then set the topic as the one with the largest probability. The second step gets the sentiment distribution of each post. In this work, a sentiment word list called NTUSD [30] is adopted, which contains 4370 negative words and 4566 positive words. If a word is an adjective but not in the sentiment word list, the sentiment label of this word is set as neutral. If a word is a noun, it is considered as a topic word.',\n",
       " 'B. The LDA-S model can infer sentiment distribution and topic distribution simultaneously for short texts. The intuition behind is that extracting sentiments discarding topics may be not sufﬁcient as sentiment polarities usually depend on topics or domains [29]. Therefore, extracting the sentiments corresponding to different topics can potentially improve the sentiment classiﬁcation accuracy. In this work, a sentiment word list called NTUSD [30] is adopted, which contains 4370 negative words and 4566 positive words. Then the vector preprocesses by RBM into a 100-dimensional vector, and ﬁnally processes by the sentence2vec became the news event feature vector. After the dependency parsing analysis, the core words (Renminbi, speculators, face, huge, losses) are obtained, and after one hot coding, each word is encoded into zero or one vector. In other words, the exact same word may express different sentiment polarities for different topics, e.g., the opinion word ‘‘low’’ in the phrase ‘‘low speed’’ in a trafﬁc-related topic and ‘‘low fat’’ in a food-related topic. If a word is an adjective but not in the sentiment word list, the sentiment label of this word is set as neutral. If a word is a noun, it is considered as a topic word. Otherwise, it is considered as a background word. For each topic, opinion word distributions are distinguished from two polarities, that is, positive or negative. V. EXPERIMENTS A. DATA COLLECTION AND DESCRIPTION We collected stock market-related information from Jan. 1, 2015 to Dec. 31, 2016, and separate the information into two data sets, one for the year 2015 and the other for 2016. The data consist of three parts, the historical quantitative data, the news articles and the posts on the social network, which are introduced in detail as follows.',\n",
       " 'The ﬁrst step aims to obtain the topic distribution of each post, and then set the topic as the one with the largest probability. Therefore, extracting the sentiments corresponding to different topics can potentially improve the sentiment classiﬁcation accuracy. The intuition behind is that extracting sentiments discarding topics may be not sufﬁcient as sentiment polarities usually depend on topics or domains [29]. In this work, a sentiment word list called NTUSD [30] is adopted, which contains 4370 negative words and 4566 positive words. In other words, the exact same word may express different sentiment polarities for different topics, e.g., the opinion word ‘‘low’’ in the phrase ‘‘low speed’’ in a trafﬁc-related topic and ‘‘low fat’’ in a food-related topic. V. EXPERIMENTS A. For each topic, opinion word distributions are distinguished from two polarities, that is, positive or negative. The second step gets the sentiment distribution of each post. DATA COLLECTION AND DESCRIPTION We collected stock market-related information from Jan. 1, 2015 to Dec. 31, 2016, and separate the information into two data sets, one for the year 2015 and the other for 2016. The data consist of three parts, the historical quantitative data, the news articles and the posts on the social network, which are introduced in detail as follows. • Quantitative data: the source of quantitative data is Wind,2 a widely used ﬁnancial information service provider in China. The data we collect are the average prices, market index change and turnover rate of the Shanghai Composite Index in each trading day. • News data: we collect the news articles on the macro economy through Wind, and get 38,727 and 39,465 news articles in 2015 and 2016 respectively. The news articles are aggregated by Wind from major ﬁnancial news websites in China, such as http://ﬁnance.sina.com.cn and http://www.hexun.com.',\n",
       " 'In this work, a sentiment word list called NTUSD [30] is adopted, which contains 4370 negative words and 4566 positive words. DATA COLLECTION AND DESCRIPTION We collected stock market-related information from Jan. 1, 2015 to Dec. 31, 2016, and separate the information into two data sets, one for the year 2015 and the other for 2016. The data we collect are the average prices, market index change and turnover rate of the Shanghai Composite Index in each trading day. For each topic, opinion word distributions are distinguished from two polarities, that is, positive or negative. The second step gets the sentiment distribution of each post. If a word is a noun, it is considered as a topic word. The data consist of three parts, the historical quantitative data, the news articles and the posts on the social network, which are introduced in detail as follows. • News data: we collect the news articles on the macro economy through Wind, and get 38,727 and 39,465 news articles in 2015 and 2016 respectively. The ﬁrst step aims to obtain the topic distribution of each post, and then set the topic as the one with the largest probability. The news articles are aggregated by Wind from major ﬁnancial news websites in China, such as http://ﬁnance.sina.com.cn and http://www.hexun.com. We process the news titles rather than the whole articles to extract the events, as the main topic of a news article is often summed up in the title. • Social media data: the sentiments are extracted from the posts crawled from a popular investor social network in China named Xueqiu.3 Totally 6,163,056 postings are collected for 2015 and 2016. For each post, we get the posting time stamp and the content.',\n",
       " 'Otherwise, it is considered as a background word. We process the news titles rather than the whole articles to extract the events, as the main topic of a news article is often summed up in the title. • Quantitative data: the source of quantitative data is Wind,2 a widely used ﬁnancial information service provider in China. The data consist of three parts, the historical quantitative data, the news articles and the posts on the social network, which are introduced in detail as follows. For each post, we get the posting time stamp and the content. • News data: we collect the news articles on the macro economy through Wind, and get 38,727 and 39,465 news articles in 2015 and 2016 respectively. • Social media data: the sentiments are extracted from the posts crawled from a popular investor social network in China named Xueqiu.3 Totally 6,163,056 postings are collected for 2015 and 2016. For each topic, opinion word distributions are distinguished from two polarities, that is, positive or negative. The data we collect are the average prices, market index change and turnover rate of the Shanghai Composite Index in each trading day. DATA COLLECTION AND DESCRIPTION We collected stock market-related information from Jan. 1, 2015 to Dec. 31, 2016, and separate the information into two data sets, one for the year 2015 and the other for 2016. For each trading day, if the stock market index rises, it would be a positive instance, otherwise it is a negative instance. For each year, we use the data from the ﬁrst 10 months as the training set and the last 2 months as the testing set. We evaluate the performance of our model with varying lead days and varying historical days.',\n",
       " 'The news articles are aggregated by Wind from major ﬁnancial news websites in China, such as http://ﬁnance.sina.com.cn and http://www.hexun.com. We process the news titles rather than the whole articles to extract the events, as the main topic of a news article is often summed up in the title. For each post, we get the posting time stamp and the content. • Quantitative data: the source of quantitative data is Wind,2 a widely used ﬁnancial information service provider in China. For each year, we use the data from the ﬁrst 10 months as the training set and the last 2 months as the testing set. DATA COLLECTION AND DESCRIPTION We collected stock market-related information from Jan. 1, 2015 to Dec. 31, 2016, and separate the information into two data sets, one for the year 2015 and the other for 2016. For each trading day, if the stock market index rises, it would be a positive instance, otherwise it is a negative instance. The data consist of three parts, the historical quantitative data, the news articles and the posts on the social network, which are introduced in detail as follows. The data we collect are the average prices, market index change and turnover rate of the Shanghai Composite Index in each trading day. We evaluate the performance of our model with varying lead days and varying historical days. Lead days refers to the number of days in advance the model makes predictions and the historical days indicates the number of days over which the multi-source information is utilized. The evaluation metrics we use are F1-score and accuracy (ACC). B. COMPARISON METHODS The following baselines and variations of our proposed model are implemented for comparisons. The full implementation of our framework is named as Multi-source Multiple Instance (M-MI) model.',\n",
       " 'For each trading day, if the stock market index rises, it would be a positive instance, otherwise it is a negative instance. For each post, we get the posting time stamp and the content. Lead days refers to the number of days in advance the model makes predictions and the historical days indicates the number of days over which the multi-source information is utilized. The data we collect are the average prices, market index change and turnover rate of the Shanghai Composite Index in each trading day. • Social media data: the sentiments are extracted from the posts crawled from a popular investor social network in China named Xueqiu.3 Totally 6,163,056 postings are collected for 2015 and 2016. COMPARISON METHODS The following baselines and variations of our proposed model are implemented for comparisons. The evaluation metrics we use are F1-score and accuracy (ACC). We evaluate the performance of our model with varying lead days and varying historical days. The full implementation of our framework is named as Multi-source Multiple Instance (M-MI) model. For each year, we use the data from the ﬁrst 10 months as the training set and the last 2 months as the testing set. • SVM: the standard support vector machine is used as a basic prediction method. During the training process, the label assigned to each instance and each group is the same as its multi-source super group label. During the 2http://www.wind.com.cn/ 3https://xueqiu.com/ VOLUME 6, 2018 50725 X. Zhang et al. : Stock Market Prediction via Multi-Source Multiple Instance Learning prediction phase, we obtain the predicted label for each of the instance, and then average the labels as the ﬁnal label of the super group. • TeSIA: the tensor-based learning approach [20] utilizes multi-source information for stock prediction.',\n",
       " 'During the 2http://www.wind.com.cn/ 3https://xueqiu.com/ VOLUME 6, 2018 50725 X. Zhang et al. For each trading day, if the stock market index rises, it would be a positive instance, otherwise it is a negative instance. For each year, we use the data from the ﬁrst 10 months as the training set and the last 2 months as the testing set. The evaluation metrics we use are F1-score and accuracy (ACC). During the training process, the label assigned to each instance and each group is the same as its multi-source super group label. The full implementation of our framework is named as Multi-source Multiple Instance (M-MI) model. For each post, we get the posting time stamp and the content. • SVM: the standard support vector machine is used as a basic prediction method. • TeSIA: the tensor-based learning approach [20] utilizes multi-source information for stock prediction. • Social media data: the sentiments are extracted from the posts crawled from a popular investor social network in China named Xueqiu.3 Totally 6,163,056 postings are collected for 2015 and 2016. Speciﬁcally, it uses a third-order tensor to model the ﬁrm-mode, event-mode, and sentiment-mode data. • nMIL: nested Multi-Instance Learning (nMIL) model [26] is the state-of-art baseline. In this model, only one data source, i.e., the news, is used to extract simple event features. It ignores the impacts of the sentiments and the historical quantitative indices. • O-MI: Open IE Multiple Instance (O-MI) Learning model differs from M-MI in the event extraction module. It adopts a previously proposed event extraction method [15], and uses Open IE [31] to extract event tuples from sentences. The structured event tuples are then processed by sentence2vec to obtain event representations. Please note that the sentiment data and quantitative data are also used in this model.',\n",
       " 'Speciﬁcally, it uses a third-order tensor to model the ﬁrm-mode, event-mode, and sentiment-mode data. Please note that the sentiment data and quantitative data are also used in this model. • TeSIA: the tensor-based learning approach [20] utilizes multi-source information for stock prediction. In this model, only one data source, i.e., the news, is used to extract simple event features. During the training process, the label assigned to each instance and each group is the same as its multi-source super group label. : Stock Market Prediction via Multi-Source Multiple Instance Learning prediction phase, we obtain the predicted label for each of the instance, and then average the labels as the ﬁnal label of the super group. • O-MI: Open IE Multiple Instance (O-MI) Learning model differs from M-MI in the event extraction module. • SVM: the standard support vector machine is used as a basic prediction method. The full implementation of our framework is named as Multi-source Multiple Instance (M-MI) model. • nMIL: nested Multi-Instance Learning (nMIL) model [26] is the state-of-art baseline. • WoR-MI: Without RBM Multiple Instance (WoR-MI) Learning model is also a part of the M-MI framework. It differs M-MI in that it works without the RBM module, and therefore the sentence2vec module is fed with original structured events instead of pre-trained vectors. • WoH-MI: Compare to M-MI, Without Hinge loss Multiple Instance (WoH-MI) Learning model lacks the instance-level hinge loss terms (i.e., Eq. 10, 11 and 12). To make a fair comparison, we use the same set of instances and the same setting of parameters to evaluate different methods. In our proposal and the baselines, we set the predicted label to −1 if the estimated probability for a multisource super group is less than 0.5; otherwise, we set the predicted label to +1. TABLE 2.',\n",
       " 'The structured event tuples are then processed by sentence2vec to obtain event representations. Speciﬁcally, it uses a third-order tensor to model the ﬁrm-mode, event-mode, and sentiment-mode data. 10, 11 and 12). It adopts a previously proposed event extraction method [15], and uses Open IE [31] to extract event tuples from sentences. : Stock Market Prediction via Multi-Source Multiple Instance Learning prediction phase, we obtain the predicted label for each of the instance, and then average the labels as the ﬁnal label of the super group. It ignores the impacts of the sentiments and the historical quantitative indices. • TeSIA: the tensor-based learning approach [20] utilizes multi-source information for stock prediction. In this model, only one data source, i.e., the news, is used to extract simple event features. Please note that the sentiment data and quantitative data are also used in this model. In our proposal and the baselines, we set the predicted label to −1 if the estimated probability for a multisource super group is less than 0.5; otherwise, we set the predicted label to +1. Prediction Results (history day=1, lead day=1). C. PREDICTION RESULTS We set both the number of history days and the number of lead days to 1. We empirically set m0 = 0.6, and set m1, m2 and P0 all as 0.5, i.e., the default setting in hinge loss. β is set as 3.0, and λm, λd, λs and λθ are set as 0.05 by sensitivity analysis. The dimension of event representations is set as 100. Table 2 shows the performance of M-MI and the baselines. We can observe that M-MI outperforms all the baselines in both of the metrics, while SVM method shows the worst performance, indicating that simply tagging each news article with the label of its super-group is not effective.',\n",
       " 'In our proposal and the baselines, we set the predicted label to −1 if the estimated probability for a multisource super group is less than 0.5; otherwise, we set the predicted label to +1. 10, 11 and 12). C. PREDICTION RESULTS We set both the number of history days and the number of lead days to 1. To make a fair comparison, we use the same set of instances and the same setting of parameters to evaluate different methods. We can observe that M-MI outperforms all the baselines in both of the metrics, while SVM method shows the worst performance, indicating that simply tagging each news article with the label of its super-group is not effective. We empirically set m0 = 0.6, and set m1, m2 and P0 all as 0.5, i.e., the default setting in hinge loss. It differs M-MI in that it works without the RBM module, and therefore the sentence2vec module is fed with original structured events instead of pre-trained vectors. • WoR-MI: Without RBM Multiple Instance (WoR-MI) Learning model is also a part of the M-MI framework. • WoH-MI: Compare to M-MI, Without Hinge loss Multiple Instance (WoH-MI) Learning model lacks the instance-level hinge loss terms (i.e., Eq. Table 2 shows the performance of M-MI and the baselines. It can also be observed that M-MI and its variations (i.e., O-MI, WoH-MI and WoR-MI) all outperform nMIL. Compared to nMIL, M-MI improves F-1 by 6.9% in 2015 and 9.2% in 2016, while it improves accuracy by 6.7% and 9.4% in 2015 and 2016 respectively. Such gains mainly come from (1) utilizing multi-source information instead of only news articles, and (2) the advanced event representations rather than simple event features.',\n",
       " 'Prediction Results (history day=1, lead day=1). We empirically set m0 = 0.6, and set m1, m2 and P0 all as 0.5, i.e., the default setting in hinge loss. C. PREDICTION RESULTS We set both the number of history days and the number of lead days to 1. To make a fair comparison, we use the same set of instances and the same setting of parameters to evaluate different methods. In our proposal and the baselines, we set the predicted label to −1 if the estimated probability for a multisource super group is less than 0.5; otherwise, we set the predicted label to +1. Compared to nMIL, M-MI improves F-1 by 6.9% in 2015 and 9.2% in 2016, while it improves accuracy by 6.7% and 9.4% in 2015 and 2016 respectively. Such gains mainly come from (1) utilizing multi-source information instead of only news articles, and (2) the advanced event representations rather than simple event features. It can also be observed that M-MI and its variations (i.e., O-MI, WoH-MI and WoR-MI) all outperform nMIL. Table 2 shows the performance of M-MI and the baselines. We can observe that M-MI outperforms all the baselines in both of the metrics, while SVM method shows the worst performance, indicating that simply tagging each news article with the label of its super-group is not effective. Though all using multi-source information, TeSIA performs worse than M-MI and its variations, showing the effectiveness of our proposed models and feature extraction methods. Both M-MI and WoR-MI perform better than O-MI, indicating that both the structured event extraction module and the RBM pre-training module in our framework are effective. WoH-MI performs worse than M-MI, showing the proposed instance-level hinge losses across multiple data sources are useful for accurate predictions. FIGURE 4. F-1 scores with varying history days. (a) 2015.',\n",
       " 'It can also be observed that M-MI and its variations (i.e., O-MI, WoH-MI and WoR-MI) all outperform nMIL. Both M-MI and WoR-MI perform better than O-MI, indicating that both the structured event extraction module and the RBM pre-training module in our framework are effective. Though all using multi-source information, TeSIA performs worse than M-MI and its variations, showing the effectiveness of our proposed models and feature extraction methods. Table 2 shows the performance of M-MI and the baselines. The dimension of event representations is set as 100. F-1 scores with varying history days. Prediction Results (history day=1, lead day=1). C. PREDICTION RESULTS We set both the number of history days and the number of lead days to 1. FIGURE 4. (a) 2015. (b) 2016. Figure 4 (a) and (b) show the F-1 scores of all the comparative models with varying history days in training for 2015 and 2016 respectively (where lead day remains 1). The number of history days (i.e., t in Eq. 13) is varied from 1 to 5 and the results show that M-MI consistently performs better than the others. We can also observe that as the number of history days keeps increasing, the F-1 scores generally ﬁrst go up and then go down. The possible reason is that the impacts of the news, sentiments and quantitative indices released on some day will quickly decay after a period of time (2 or 3 days). Thus, out-of-date information should be assigned with small weights or even discarded. Fortunately, our learning process can automatically assign small weights for information with weak impacts, alleviating the impact decaying problem.',\n",
       " 'Though all using multi-source information, TeSIA performs worse than M-MI and its variations, showing the effectiveness of our proposed models and feature extraction methods. WoH-MI performs worse than M-MI, showing the proposed instance-level hinge losses across multiple data sources are useful for accurate predictions. The number of history days (i.e., t in Eq. It can also be observed that M-MI and its variations (i.e., O-MI, WoH-MI and WoR-MI) all outperform nMIL. Compared to nMIL, M-MI improves F-1 by 6.9% in 2015 and 9.2% in 2016, while it improves accuracy by 6.7% and 9.4% in 2015 and 2016 respectively. 13) is varied from 1 to 5 and the results show that M-MI consistently performs better than the others. The possible reason is that the impacts of the news, sentiments and quantitative indices released on some day will quickly decay after a period of time (2 or 3 days). We can also observe that as the number of history days keeps increasing, the F-1 scores generally ﬁrst go up and then go down. F-1 scores with varying history days. Figure 4 (a) and (b) show the F-1 scores of all the comparative models with varying history days in training for 2015 and 2016 respectively (where lead day remains 1). In order to know how early our model can predict the index movement, we show the F-1 scores of WoR-MI and M-MI with varied lead days from 1 to 3 and history days from 1 to 5 in Table 3. We observe that as the number of lead days increases, the predictive capabilities of our models decrease. This makes sense since the stock market commonly reﬂects the available information in a timely manner.',\n",
       " 'F-1 scores with varying history days. The number of history days (i.e., t in Eq. Figure 4 (a) and (b) show the F-1 scores of all the comparative models with varying history days in training for 2015 and 2016 respectively (where lead day remains 1). We can also observe that as the number of history days keeps increasing, the F-1 scores generally ﬁrst go up and then go down. This makes sense since the stock market commonly reﬂects the available information in a timely manner. Fortunately, our learning process can automatically assign small weights for information with weak impacts, alleviating the impact decaying problem. The possible reason is that the impacts of the news, sentiments and quantitative indices released on some day will quickly decay after a period of time (2 or 3 days). Thus, out-of-date information should be assigned with small weights or even discarded. In order to know how early our model can predict the index movement, we show the F-1 scores of WoR-MI and M-MI with varied lead days from 1 to 3 and history days from 1 to 5 in Table 3. We observe that as the number of lead days increases, the predictive capabilities of our models decrease. In other words, the up-to-date information will immediately be reﬂected in the index change and the impacts will decay as time goes, making it difﬁcult for long-term predictions. Figure 5 shows the weights of different data sources, that is, θ1, θ2 and θ3. It can be observed that among the 50726 VOLUME 6, 2018 X. Zhang et al. : Stock Market Prediction via Multi-Source Multiple Instance Learning TABLE 3. F-1 scores for M-MI and WoR-MI in 2015 and 2016 with varying lead days. FIGURE 5. The weights of different data sources.',\n",
       " 'Fortunately, our learning process can automatically assign small weights for information with weak impacts, alleviating the impact decaying problem. 13) is varied from 1 to 5 and the results show that M-MI consistently performs better than the others. In other words, the up-to-date information will immediately be reﬂected in the index change and the impacts will decay as time goes, making it difﬁcult for long-term predictions. We observe that as the number of lead days increases, the predictive capabilities of our models decrease. The possible reason is that the impacts of the news, sentiments and quantitative indices released on some day will quickly decay after a period of time (2 or 3 days). In order to know how early our model can predict the index movement, we show the F-1 scores of WoR-MI and M-MI with varied lead days from 1 to 3 and history days from 1 to 5 in Table 3. Figure 5 shows the weights of different data sources, that is, θ1, θ2 and θ3. : Stock Market Prediction via Multi-Source Multiple Instance Learning TABLE 3. This makes sense since the stock market commonly reﬂects the available information in a timely manner. The weights of different data sources. three sources, news events contribute most to the overall prediction, while the quantitative data takes the second place. It indicates that both news events and quantitative data have larger impacts to drive stock ﬂuctuations than sentiments. VI. CONCLUSIONS In this paper, a Multi-source Multiple Instance model is proposed which can predict the stock market movement and identify the importance of the information simultaneously.',\n",
       " 'F-1 scores for M-MI and WoR-MI in 2015 and 2016 with varying lead days. In other words, the up-to-date information will immediately be reﬂected in the index change and the impacts will decay as time goes, making it difﬁcult for long-term predictions. We observe that as the number of lead days increases, the predictive capabilities of our models decrease. The weights of different data sources. This makes sense since the stock market commonly reﬂects the available information in a timely manner. In order to know how early our model can predict the index movement, we show the F-1 scores of WoR-MI and M-MI with varied lead days from 1 to 3 and history days from 1 to 5 in Table 3. : Stock Market Prediction via Multi-Source Multiple Instance Learning TABLE 3. CONCLUSIONS In this paper, a Multi-source Multiple Instance model is proposed which can predict the stock market movement and identify the importance of the information simultaneously. It indicates that both news events and quantitative data have larger impacts to drive stock ﬂuctuations than sentiments. three sources, news events contribute most to the overall prediction, while the quantitative data takes the second place. Different from previous studies that commonly exploit only one data source, our model effectively integrates heterogeneous information, that is, the events, sentiments and historical quantitative features into a comprehensive framework, and considers the consistencies among different data sources to make a better prediction. We also propose a novel event representation learning process that can effectively capture the event information. Extensive evaluations on the two-year data conﬁrm the effectiveness of our model. REFERENCES [1] E. F. Fama, ‘‘The behavior of stock-market prices,’’ J. Bus., vol. 38, no. 1, pp. 34–105, 1965. [2] S. R. Das and M. Y. Chen, ‘‘Yahoo!',\n",
       " 'Extensive evaluations on the two-year data conﬁrm the effectiveness of our model. Bus., vol. REFERENCES [1] E. F. Fama, ‘‘The behavior of stock-market prices,’’ J. [2] S. R. Das and M. Y. Chen, ‘‘Yahoo! CONCLUSIONS In this paper, a Multi-source Multiple Instance model is proposed which can predict the stock market movement and identify the importance of the information simultaneously. We also propose a novel event representation learning process that can effectively capture the event information. : Stock Market Prediction via Multi-Source Multiple Instance Learning TABLE 3. three sources, news events contribute most to the overall prediction, while the quantitative data takes the second place. It indicates that both news events and quantitative data have larger impacts to drive stock ﬂuctuations than sentiments. Different from previous studies that commonly exploit only one data source, our model effectively integrates heterogeneous information, that is, the events, sentiments and historical quantitative features into a comprehensive framework, and considers the consistencies among different data sources to make a better prediction. for Amazon: Sentiment extraction from small talk on the Web,’’ Manage. Sci., vol. 53, no. 9, pp. 1375–1388, 2007. [3] J. Si, A. Mukherjee, B. Liu, Q. Li, H. Li, and X. Deng, ‘‘Exploiting topic based twitter sentiment for stock prediction,’’ in Proc. 51st Annu. Meeting Assoc. Comput. Linguistics (ACL), 2013, pp. 24–29. [4] W. Y. Wang and Z. Hua, ‘‘A semiparametric Gaussian copula regression model for predicting ﬁnancial risks from earnings calls,’’ in Proc. 52nd Annu. Meeting Assoc. Comput. Linguistics (ACL), Jun. 2014, pp. 1155–1165. [5] S. Kogan, D. Levin, B. R. Routledge, J. S. Sagi, and N. A. Smith, ‘‘Predicting risk from ﬁnancial reports with regression,’’ in Proc. Annu. Conf. North Amer. Chapter Assoc. Comput. Linguistics Hum. Lang. Technol., 2009, pp. 272–280.',\n",
       " 'We also propose a novel event representation learning process that can effectively capture the event information. [2] S. R. Das and M. Y. Chen, ‘‘Yahoo! Extensive evaluations on the two-year data conﬁrm the effectiveness of our model. for Amazon: Sentiment extraction from small talk on the Web,’’ Manage. REFERENCES [1] E. F. Fama, ‘‘The behavior of stock-market prices,’’ J. [4] W. Y. Wang and Z. Hua, ‘‘A semiparametric Gaussian copula regression model for predicting ﬁnancial risks from earnings calls,’’ in Proc. [5] S. Kogan, D. Levin, B. R. Routledge, J. S. Sagi, and N. A. Smith, ‘‘Predicting risk from ﬁnancial reports with regression,’’ in Proc. Different from previous studies that commonly exploit only one data source, our model effectively integrates heterogeneous information, that is, the events, sentiments and historical quantitative features into a comprehensive framework, and considers the consistencies among different data sources to make a better prediction. CONCLUSIONS In this paper, a Multi-source Multiple Instance model is proposed which can predict the stock market movement and identify the importance of the information simultaneously. [3] J. Si, A. Mukherjee, B. Liu, Q. Li, H. Li, and X. Deng, ‘‘Exploiting topic based twitter sentiment for stock prediction,’’ in Proc. [6] R. Luss and A. D’Aspremont, ‘‘Predicting abnormal returns from news using text classiﬁcation,’’ Quant. Finance, vol. 15, no. 6, pp. 999–1012, 2015. [7] R. R. Prechter, The Wave Principle of Human Social Behavior and the New Science of Socionomics, vol. 1. Gainesville, GA, USA: New Classics Library, 1999. [8] J. R. Nofsinger, ‘‘Social mood and ﬁnancial economics,’’ J. Behav. Finance, vol. 6, no. 3, pp. 144–160, 2005. [9] J. Bi and X. Wang, ‘‘Learning classiﬁers from dual annotation ambiguity via a min–max framework,’’ Neurocomputing, vol. 151, pp. 891–904, Mar. 2015.',\n",
       " '3, pp. 151, pp. 1. 6, pp. Conf. Comput. [7] R. R. Prechter, The Wave Principle of Human Social Behavior and the New Science of Socionomics, vol. [5] S. Kogan, D. Levin, B. R. Routledge, J. S. Sagi, and N. A. Smith, ‘‘Predicting risk from ﬁnancial reports with regression,’’ in Proc. [6] R. Luss and A. D’Aspremont, ‘‘Predicting abnormal returns from news using text classiﬁcation,’’ Quant. [9] J. Bi and X. Wang, ‘‘Learning classiﬁers from dual annotation ambiguity via a min–max framework,’’ Neurocomputing, vol. [10] S. Xie, W. Fan, and P. S. Yu, ‘‘An iterative and re-weighting framework for rejection and uncertainty resolution in crowdsourcing,’’ in Proc. SIAM Int. Conf. Data Mining, 2012, pp. 1107–1118. [11] Q. Le and T. Mikolov, ‘‘Distributed representations of sentences and documents,’’ in Proc. 31st Int. Conf. Mach. Learn. (ICML), 2014, pp. 1188–1196. [12] F. Hogenboom, F. Frasincar, U. Kaymak, and F. De Jong, ‘‘An overview of event extraction from text,’’ in Proc. Workshop Detection, Represent., Exploitation Events Semantic Web (DeRiVE), 10th Int. Semantic Web Conf. (ISWC), vol. 779, 2011, pp. 48–57. [13] R. Akita, A. Yoshihara, T. Matsubara, and K. Uehara, ‘‘Deep learning for stock prediction using numerical and textual information,’’ in Proc. IEEE/ACIS 15th Int. Conf. Comput. Inf. Sci. (ICIS), Jun. 2016, pp. 1–6. [14] T. Nguyen, D. Phung, B. Adams, and S. Venkatesh, ‘‘Event extraction using behaviors of sentiment signals and burst structure in social media,’’ Knowl. Inf. Syst., vol. 37, no. 2, pp. 279–304, 2013. [15] X. Ding, Y. Zhang, T. Liu, and J. Duan, ‘‘Using structured events to predict stock price movement: An empirical investigation,’’ in Proc. Conf. Empirical Methods Natural Lang. Process. (EMNLP), 2014, pp. 1415–1425. [16] X. Ding, Y. Zhang, T. Liu, and J. Duan, ‘‘Deep learning for event-driven stock prediction,’’ in Proc. 24th Int. Joint Conf. Artif.',\n",
       " '(ISWC), vol. Learn. Comput. 31st Int. 24th Int. Artif. SIAM Int. Inf. Inf. (ICIS), Jun. Intell. (IJCAI), 2015, pp. 2327–2333. [17] J. Bollen, H. Mao, and X. Zeng, ‘‘Twitter mood predicts the stock market,’’ J. Comput. Sci., vol. 2, no. 1, pp. 1–8, Mar. 2011. [18] M. Makrehchi, S. Shah, and W. Liao, ‘‘Stock prediction using eventbased sentiment analysis,’’ in Proc. IEEE/WIC/ACM Int. Joint Conf. Web Intell. (WI) Intell. Agent Technol. (IAT), vol. 1, Nov. 2013, pp. 337–342. [19] T. H. Nguyen and K. Shirai, ‘‘Topic modeling based sentiment analysis on social media for stock market prediction,’’ in Proc. 53rd Annu. Meeting Assoc. Comput. Linguistics (ACL), 2015, pp. 1354–1364. [20] Q. Li, L. Jiang, P. Li, and H. Chen, ‘‘Tensor-based learning for predicting stock movements,’’ in Proc. 29th AAAI Conf. Artif. Intell. (AAAI), 2015, pp. 1784–1790. [21] T. G. Dietterich, R. H. Lathrop, and T. Lozano-Pérez, ‘‘Solving the multiple instance problem with axis-parallel rectangles,’’ Artif. Intell., vol. 89, nos. 1–2, pp. 31–71, 1997. [22] J. Amores, ‘‘Multiple instance classiﬁcation: Review, taxonomy and comparative study,’’ Artif. Intell., vol. 201, pp. 81–105, Aug. 2013. [23] G. Liu, J. Wu, and Z.-H. Zhou, ‘‘Key instance detection in multiinstance learning,’’ in Proc. Asian Conf. Mach. Learn., 2012, pp. 253–268. [24] D. Kotzias, M. Denil, N. de Freitas, and P. Smyth, ‘‘From group to individual labels using deep features,’’ in Proc. 21st ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD), 2015, pp. 597–606. [25] J. Feng and Z.-H. Zhou, ‘‘Deep MIML network,’’ in Proc. 21st AAAI Conf. Artif. Intell. (AAAI), 2017, pp. 1884–1890. [26] Y. Ning, S. Muthiah, H. Rangwala, and N. Ramakrishnan, ‘‘Modeling precursors for event forecasting via nested multi-instance learning,’’ in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD), 2016, pp. 1095–1104.',\n",
       " 'Intell., vol. Intell., vol. Discovery Data Mining (KDD), 2015, pp. Discovery Data Mining (KDD), 2016, pp. [16] X. Ding, Y. Zhang, T. Liu, and J. Duan, ‘‘Deep learning for event-driven stock prediction,’’ in Proc. [20] Q. Li, L. Jiang, P. Li, and H. Chen, ‘‘Tensor-based learning for predicting stock movements,’’ in Proc. [21] T. G. Dietterich, R. H. Lathrop, and T. Lozano-Pérez, ‘‘Solving the multiple instance problem with axis-parallel rectangles,’’ Artif. [23] G. Liu, J. Wu, and Z.-H. Zhou, ‘‘Key instance detection in multiinstance learning,’’ in Proc. [24] D. Kotzias, M. Denil, N. de Freitas, and P. Smyth, ‘‘From group to individual labels using deep features,’’ in Proc. [25] J. Feng and Z.-H. Zhou, ‘‘Deep MIML network,’’ in Proc. [27] G. E. Hinton and R. R. Salakhutdinov, ‘‘Reducing the dimensionality of data with neural networks,’’ Science, vol. 313, no. 5786, pp. 504–507, 2006. [28] X. Zhang et al., ‘‘IAD: Interaction-aware diffusion framework in social networks,’’ IEEE Trans. Knowl. Data Eng., to be published. [29] W. X. Zhao, J. Jiang, H. Yan, and X. Li, ‘‘Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid,’’ in Proc. Conf. Empirical Methods Natural Lang. Process., 2010, pp. 56–65. [30] L.-W. Ku and H.-H. Chen, ‘‘Mining opinions from the Web: Beyond relevance retrieval,’’ J. Amer. Soc. Inf. Sci. Technol., vol. 58, no. 12, pp. 1838–1850, 2007. [31] A. Fader, S. Soderland, and O. Etzioni, ‘‘Identifying relations for open information extraction,’’ in Proc. Conf. Empirical Methods Natural Lang. Process., 2011, pp. 1535–1545. VOLUME 6, 2018 50727 X. Zhang et al. : Stock Market Prediction via Multi-Source Multiple Instance Learning XI ZHANG (M’17) received the Ph.D. degree in computer science from Tsinghua University. He was a Visiting Scholar at The University of Illinois at Chicago.',\n",
       " 'Intell. [31] A. Fader, S. Soderland, and O. Etzioni, ‘‘Identifying relations for open information extraction,’’ in Proc. VOLUME 6, 2018 50727 X. Zhang et al. 22nd ACM SIGKDD Int. [30] L.-W. Ku and H.-H. Chen, ‘‘Mining opinions from the Web: Beyond relevance retrieval,’’ J. Amer. Soc. Discovery Data Mining (KDD), 2016, pp. [28] X. Zhang et al., ‘‘IAD: Interaction-aware diffusion framework in social networks,’’ IEEE Trans. He was a Visiting Scholar at The University of Illinois at Chicago. Technol., vol. He is currently an Associate Professor with the Beijing University of Posts and Telecommunications and is also the Vice Director of the Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, China. His research interests include data mining and computer architecture. SIYU QU received the bachelor’s degree in computer science from Xidian University in 2012. She is currently pursuing the master’s degree with the Key Laboratory of Trustworthy Distributed Computing and Service, Beijing University of Posts and Telecommunications, Ministry of Education, China. Her research interests include data mining and machine learning. JIEYUN HUANG received the bachelor’s degree in information security from the Beijing University of Posts and Telecommunications in 2017, where she is currently pursuing the master’s degree with the Key Laboratory of Trustworthy Distributed Computing and Service. Her research interests are in data mining and machine learning. BINXING FANG received the Ph.D. degree from the Harbin Institute of Technology, China, in 1989. He was the Chief Scientist of the State Key Development Program of Basic Research of China. He is currently a member of the Chinese Academy of Engineering and is also a Professor with the School of Cyberspace Security, Beijing University of Posts and Telecommunications. His current research interests include big data and cybersecurity.',\n",
       " 'She is currently pursuing the master’s degree with the Key Laboratory of Trustworthy Distributed Computing and Service, Beijing University of Posts and Telecommunications, Ministry of Education, China. He was a Visiting Scholar at The University of Illinois at Chicago. His current research interests include big data and cybersecurity. JIEYUN HUANG received the bachelor’s degree in information security from the Beijing University of Posts and Telecommunications in 2017, where she is currently pursuing the master’s degree with the Key Laboratory of Trustworthy Distributed Computing and Service. He was the Chief Scientist of the State Key Development Program of Basic Research of China. His research interests include data mining and computer architecture. He is currently an Associate Professor with the Beijing University of Posts and Telecommunications and is also the Vice Director of the Key Laboratory of Trustworthy Distributed Computing and Service, Ministry of Education, China. SIYU QU received the bachelor’s degree in computer science from Xidian University in 2012. He is currently a member of the Chinese Academy of Engineering and is also a Professor with the School of Cyberspace Security, Beijing University of Posts and Telecommunications. BINXING FANG received the Ph.D. degree from the Harbin Institute of Technology, China, in 1989. PHILIP YU (F’93) received the Ph.D. degree in electrical engineering from Stanford University. He is currently a Distinguished Professor in computer science at The University of Illinois at Chicago and is also the Wexler Chair in information technology. His research interests include big data, data mining, data stream, database, and privacy. He is a fellow of ACM. He received the Research Contributions Award from the IEEE International Conference on Data Mining in 2003, the Technical Achievement Award from the IEEE Computer Society in 2013, and the ACM SIGKDD 2016 Innovation Award.',\n",
       " 'Her research interests include data mining and machine learning. Her research interests are in data mining and machine learning. He was the Chief Scientist of the State Key Development Program of Basic Research of China. He is currently a member of the Chinese Academy of Engineering and is also a Professor with the School of Cyberspace Security, Beijing University of Posts and Telecommunications. He is a fellow of ACM. He is currently a Distinguished Professor in computer science at The University of Illinois at Chicago and is also the Wexler Chair in information technology. His current research interests include big data and cybersecurity. His research interests include big data, data mining, data stream, database, and privacy. His research interests include data mining and computer architecture. He received the Research Contributions Award from the IEEE International Conference on Data Mining in 2003, the Technical Achievement Award from the IEEE Computer Society in 2013, and the ACM SIGKDD 2016 Innovation Award. He was the Editor-in-Chief of the IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING and the ACM Transactions on Knowledge Discovery from Data. 50728 VOLUME 6, 2018']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_text_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d39c8e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'similarity': np.float32(0.33650464),\n",
       "  'chunk': 'The process is shown in Figure 3 and described in detail as follows. FEATURE EXTRACTION The quantitative features are quite simple to extract, we just collect three indices and normalize each index to form di ∈R3×1. The output of an RBM would be a pre-trained vector used as the input to sentence2vec, and then the event representations are obtained. Here we introduce how to extract event representations from news articles and extract the sentiments from posts in social media, which are used as the inputs to M-MI framework. EVENT FEATURE EXTRACTION Conventional methods commonly represent events using simple features such as TF-IDF, noun phrases or named entities. Note that though we use the Chinese dataset in this study, this process can also be applied to other languages. Recent advances in NLP techniques enable more accurate event models with structures. Structured event extraction from texts. 1) Structured event extraction. In this study, we ﬁrst use the syntactic analysis method to extract the main structure information of the sentences, and then use it as the input to an RBM. With a commonly used text parser HanLP,1 we can capture the syntactic structure of a sentence, which is depicted as a three-level tree at the top of Fig. 3. The root node denotes the core verb, and the nodes of the second layer are the subject of the verb and the object of the verb respectively. The child of the subject is the modiﬁer who is the nearest to the subject in the sentence, and so is the child of the object. Then we connect these core words together as the structure information to represent the event information. 2) Training with RBM. We then map the structured event into a vector.'},\n",
       " {'similarity': np.float32(0.29783845),\n",
       "  'chunk': 'Structured event extraction from texts. Note that though we use the Chinese dataset in this study, this process can also be applied to other languages. Then we connect these core words together as the structure information to represent the event information. EVENT FEATURE EXTRACTION Conventional methods commonly represent events using simple features such as TF-IDF, noun phrases or named entities. 1) Structured event extraction. Recent advances in NLP techniques enable more accurate event models with structures. In this study, we ﬁrst use the syntactic analysis method to extract the main structure information of the sentences, and then use it as the input to an RBM. We then map the structured event into a vector. 2) Training with RBM. The output of an RBM would be a pre-trained vector used as the input to sentence2vec, and then the event representations are obtained. To make the vectors better reconstruct the original events, we use RBM as a pre-training module. The Restricted Boltzmann Machine (RBM) is a generative stochastic artiﬁcial neural network, and has been applied in various applications such as dimensionality reduction [27]. RBM contains two-layer neural nets, one is the visible layer or input layer, and the 1https://github.com/hankcs/HanLP 50724 VOLUME 6, 2018 X. Zhang et al. : Stock Market Prediction via Multi-Source Multiple Instance Learning other is the hidden layer. In our model, each event is represented as an m-dimensional vector with one-hot encoding, which is the visible layer. Our target is to estimate the n-dimensional hidden layer to approximate the input layer as much as possible. Then the hidden layer will be set as the initial vector in sentence2vec. The reason is that directly training the representations using sentence2vec without RBM may fall into the local minimum. 3) Training with sentence2vec.'},\n",
       " {'similarity': np.float32(0.28102458),\n",
       "  'chunk': 'Note that though we use the Chinese dataset in this study, this process can also be applied to other languages. With a commonly used text parser HanLP,1 we can capture the syntactic structure of a sentence, which is depicted as a three-level tree at the top of Fig. The reason is that directly training the representations using sentence2vec without RBM may fall into the local minimum. Then we connect these core words together as the structure information to represent the event information. To make the vectors better reconstruct the original events, we use RBM as a pre-training module. Then the hidden layer will be set as the initial vector in sentence2vec. In our model, each event is represented as an m-dimensional vector with one-hot encoding, which is the visible layer. We then map the structured event into a vector. 1) Structured event extraction. 3) Training with sentence2vec. Finally, we use sentence2vec, the neural probabilistic language model to obtain the event representations. Different from the word2vec with CBOW model, the sentence id will be added during the training process of sentence2vec, and will also be mapped into a vector, called sentence vector, which would be the ﬁnal vector that we want. In the training process, the sentence vector and the word vectors of context will be concatenated as the input to softmax. After training, the sentence vector will be obtained and used as the features for the proposed model. Here is an example of extracting structured events from the news. The news text is that it is expected that the Renminbi speculators will face huge losses. After the dependency parsing analysis, the core words (Renminbi, speculators, face, huge, losses) are obtained, and after one hot coding, each word is encoded into zero or one vector.'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = model.encode(\"How did you extract features?\", normalize_embeddings=True)\n",
    "similarities = []\n",
    "for chunk in extracted_text_chunked:\n",
    "    chunk_embedding = model.encode(chunk, normalize_embeddings=True)\n",
    "    similarity = cosine_similarity([question], [chunk_embedding])[0][0]\n",
    "    similarities.append({'similarity':similarity, 'chunk': chunk})\n",
    "\n",
    "similarities = sorted(similarities, key=lambda x: (-x['similarity']))\n",
    "similarities[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb0e098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantwise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
