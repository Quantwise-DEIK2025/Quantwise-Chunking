{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c40da70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/projects/Quantwise/quantwise-venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling.chunking import HybridChunker\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "embedding_model_name = \"nomic-ai/nomic-embed-text-v1.5\"\n",
    "MAX_TOKENS = 2000\n",
    "\n",
    "source = \"Stock_Market_Prediction_via_Multi-Source_Multiple_Instance_Learning.pdf\"  # document per local path or URL\n",
    "converter = DocumentConverter()\n",
    "doc = converter.convert(source).document\n",
    "\n",
    "tokenizer = HuggingFaceTokenizer(\n",
    "    tokenizer=AutoTokenizer.from_pretrained(embedding_model_name),\n",
    "    max_tokens=MAX_TOKENS # Optional, uses the max token number of the HF tokenizer by default\n",
    ")\n",
    "\n",
    "\n",
    "chunker = HybridChunker(\n",
    "    tokenizer=tokenizer,\n",
    "    merge_peers=True #Optional, defaults to true\n",
    ")\n",
    "chunks = list(chunker.chunk(dl_doc=doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589fd9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SentenceTransformer(embedding_model_name, trust_remote_code=True)\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df465975",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_with_metadata = []\n",
    "for chunk in chunks:\n",
    "    pages = set(\n",
    "                prov.page_no\n",
    "                for doc_item in chunk.meta.doc_items\n",
    "                for prov in doc_item.prov\n",
    "            )\n",
    "    chunks_with_metadata.append({'text': chunk.text, 'document':source, 'pages':list(pages)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7aa3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/projects/Quantwise/quantwise-venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "from lancedb.embeddings import get_registry\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "import torch\n",
    "\n",
    "registry = get_registry()\n",
    "hf = registry.get(\"huggingface\").create(name=\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)#, device=\"cuda\" if torch.cuda.is_available() else \"cpu\") #TODO: Test if there's a point to running this on GPU. LanceDB seems indifferent to the device.\n",
    "\n",
    "\n",
    "# Define model\n",
    "class MyDocument(LanceModel):\n",
    "    text: str = hf.SourceField()\n",
    "    vector: Vector(hf.ndims()) = hf.VectorField()\n",
    "    document: str\n",
    "    pages: list[int]  # Any additional metadata\n",
    "\n",
    "\n",
    "db = lancedb.connect(\"./db\")\n",
    "# db.create_table(\"my_table\", schema=MyDocument, mode=\"overwrite\") # Uncommend this line when running this cell for the first time\n",
    "table = db.open_table(\"my_table\")\n",
    "# table.add(chunks_with_metadata) # Uncomment this line when running this cell for the first time to add data to the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e42b9cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "results = table.search(\"How was the stock-market related information collected?\") \\\n",
    "               .limit(3) \\\n",
    "               .select([\"text\", \"document\", \"pages\"]) \\\n",
    "               .to_pandas()\n",
    "\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b757444f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: We collected stock market-related information from Jan. 1, 2015 to Dec. 31, 2016, and separate the information into two data sets, one for the year 2015 and the other for 2016. The data consist of three parts, the historical quantitative data, the news articles and the posts on the social network, which are introduced in detail as follows.\n",
      "- GLYPH<15> Quantitative data : the source of quantitative data is Wind, 2 a widely used GLYPH<28>nancial information service provider in China. The data we collect are the average prices, market index change and turnover rate of the Shanghai Composite Index in each trading day.\n",
      "- GLYPH<15> News data : we collect the news articles on the macro economy through Wind, and get 38,727 and 39,465 news articles in 2015 and 2016 respectively. The news articles are aggregated by Wind from major GLYPH<28>nancial news websites in China, such as http://GLYPH<28>nance.sina.com.cn and http://www.hexun.com. We process the news titles rather than the whole articles to extract the events, as the main topic of a news article is often summed up in the title.\n",
      "- GLYPH<15> Social media data : the sentiments are extracted from the posts crawled from a popular investor social network in China named Xueqiu. 3 Totally 6,163,056 postings are collected for 2015 and 2016. For each post, we get the posting time stamp and the content.\n",
      "For each trading day, if the stock market index rises, it would be a positive instance, otherwise it is a negative instance. For each year, we use the data from the GLYPH<28>rst 10 months as the training set and the last 2 months as the testing set. We evaluate the performance of our model with varying lead days and varying historical days. Lead days refers to the number of days in advance the model makes predictions and the historical days indicates the number of days over which the multi-source information is utilized. The evaluation metrics we use are F1-score and accuracy (ACC).\n",
      "-------------------------\n",
      "Text: Stock markets play important roles in the economic operations of modern society. The estimation of the stock market index is of clear interest to various stakeholders in the market. According to the EfGLYPH<28>cient Market Hypothesis (EMH) [1], the stock market prices reGLYPH<29>ect all available information, and thus the prediction naturally relies on information from multiple sources, which can be roughly categorized into (1) quantitative data, e.g., historical prices, turnover rate, and (2) qualitative descriptions, such as the annual reports, announcements, news and social media posts. It is challenging to deal with qualitative data as they are usually unstructured and thus extracting useful signals from them is not trivial.\n",
      "Along with the growing Web information and the advance of Natural Language Processing (NLP) techniques, recent works begin to explore Web news for market prediction. A number of existing studies have shown that the events reported in news are important signals that can drive market\n",
      "GLYPH<29>uctuations [2]GLYPH<21>[4]. However, most of the previous works represent news documents using simple features (e.g., bagwords, noun phrases, named entities) [5], [6], which may discard syntax information. Due to the large volume and diverse expressions of the events, how to represent them as useful features, and how to identify the crucial events that have signiGLYPH<28>cant impacts on the stock market are not trivial problems. In addition to events, a line of studies has shown that the investors' opinions can also largely inGLYPH<29>uence the market volatility [7], [8]. With the prosperity of Web 2.0, the sentiments extracted from social media can be beneGLYPH<28>cial to predictions. Since both events and sentiments can drive the GLYPH<29>uctuations of the market, it is natural to investigate how to effectively fuse them together to make a better prediction. The improvement may come from the correlations among different sources, and the consensus prediction with multisource information can potentially outperform each prediction relying on a single source. This problem is analogous to the multi-labeler learning problem in crowdsourcing [9], [10],\n",
      "VOLUME 6, 2018\n",
      "but different from those studies that usually assume a labeler conducts classiGLYPH<28>cation with full information, each ''labeler'' (i.e., classiGLYPH<28>er) in this study is source-speciGLYPH<28>c and only provided with limited information from its own source, making the consensus among labelers even more challenging.\n",
      "In this work, we aim to learn a predictive model for describing the GLYPH<29>uctuations in the stock market index by utilizing various sources of data, involving the historical quantitative data, the social media and Web news. The essential features we extract include the event representations from news articles and the sentiments from social media. Firstly, we propose a novel method to capture the event information. SpeciGLYPH<28>cally, structured events are extracted from news texts and then used as the inputs for Restricted Boltzmann Machines (RBMs) to do the pre-training. After that, the output vectors from RBMs are used as the inputs to a recently proposed sentence2vec framework [11], in order to achieve effective event embeddings. Secondly, we exploit the latent relationships among different data sources with carefully designed loss terms, and propose an extension of the Multiple Instance Learning (MIL) model that can effectively integrate the features from multiple sources to make more accurate predictions. One beneGLYPH<28>t of our method is that we can determine source-speciGLYPH<28>c weights and identify the speciGLYPH<28>c factors that incur the changes in the composite index. Figure 1 shows an example of the news precursors identiGLYPH<28>ed by our model, and the dots with numbers denote the probabilistic estimates for the events leading to the index change on Jan. 26, 2016.\n",
      "FIGURE 1. An example of the news events that are responsible for the Shanghai Composite Index change on Jan. 26, 2016. The x-axis is the timeline. The left y-axis is the probability of each event leading to the index change. The right y-axis is the composite index in Shanghai Stock Exchange.\n",
      "The summary of the contributions is as follows:\n",
      "- 1) To provide robust and accurate predictions for stock market movements, we extend the Multiple Instance Learning model to integrate the heterogeneous information including Web news, social media posts, and quantitative data.\n",
      "- 2) The latent consistencies among different data sources are modeled in our framework by sharing the common estimated true label among the hinge losses of different data sources at the instance level.\n",
      "- 3) A novel event representation model is proposed by GLYPH<28>rst extracting structured events from news text, and then training them with deep learning methods involving RBM and sentence2vec to obtain dense vectors.\n",
      "- 4) Evaluation results on two-year datasets show that our proposal can outperform the state-of-art baselines. Moreover, the impacts of different sources and the key factors that drive the movements can be obtained.\n",
      "-------------------------\n",
      "Text: In this paper, a Multi-source Multiple Instance model is proposed which can predict the stock market movement and identify the importance of the information simultaneously. Different from previous studies that commonly exploit only one data source, our model effectively integrates heterogeneous information, that is, the events, sentiments and historical quantitative features into a comprehensive framework, and considers the consistencies among different data sources to make a better prediction. We also propose a novel event representation learning process that can effectively capture the event information. Extensive evaluations on the two-year data conGLYPH<28>rm the effectiveness of our model.\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for result in results.itertuples():\n",
    "    print(f\"Text: {result.text}\")\n",
    "    print(\"-\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4922d4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = model.encode(\"What was the frequency of each label in the DocLayNet dataset?\", normalize_embeddings=True)\n",
    "# similarities = []\n",
    "# for chunk in contextualized_chunks:\n",
    "#     chunk_embedding = model.encode(chunk, normalize_embeddings=True)\n",
    "#     similarity = cosine_similarity([question], [chunk_embedding])[0][0]\n",
    "#     similarities.append({'similarity':similarity, 'chunk': chunk})\n",
    "\n",
    "# similarities = sorted(similarities, key=lambda x: (-x['similarity']))\n",
    "# similarities[:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantwise-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
