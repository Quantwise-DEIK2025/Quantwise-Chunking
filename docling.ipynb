{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6bc954c",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05a79355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25 studies which are already processed.\n",
      "Studies which STILL need to be processed: 0:\n",
      "[]...\n"
     ]
    }
   ],
   "source": [
    "from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling.chunking import HybridChunker\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import hashlib\n",
    "import lancedb\n",
    "from lancedb.embeddings import get_registry\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.rerankers import ColbertReranker\n",
    "import ollama\n",
    "import os\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "embedding_model_name = \"nomic-ai/nomic-embed-text-v1.5\"\n",
    "MAX_TOKENS = 2000\n",
    "\n",
    "converter = DocumentConverter()\n",
    "tokenizer = HuggingFaceTokenizer(\n",
    "    tokenizer=AutoTokenizer.from_pretrained(embedding_model_name),\n",
    "    max_tokens=MAX_TOKENS # Optional, uses the max token number of the HF tokenizer by default\n",
    ")\n",
    "chunker = HybridChunker(\n",
    "    tokenizer=tokenizer,\n",
    "    merge_peers=True #Optional, defaults to true\n",
    ")\n",
    "\n",
    "study_names = [f for f in os.listdir(\"input\") if f.endswith('.pdf')]\n",
    "processed_chunks=[]\n",
    "try:\n",
    "    with open(\"sliding_chunks_with_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        processed_chunks = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"No existing chunks_with_metadata.json file found, starting fresh.\")\n",
    "    \n",
    "\n",
    "chunks_with_metadata = processed_chunks.copy()\n",
    "processed_studies = set(chunk[\"document\"] for chunk in processed_chunks)\n",
    "\n",
    "study_names = [f for f in study_names if f not in processed_studies]\n",
    "print(f\"Found {len(processed_studies)} studies which are already processed.\\nStudies which STILL need to be processed: {len(study_names)}:\\n{study_names}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508b299",
   "metadata": {},
   "source": [
    "# Creating chunks and adding Metadata\n",
    "\n",
    "As well as semantic context with ollama (Anthropic style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df465975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b74889e4d84837a75d9e2e7dee857a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunking documents...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b920a11270746c18371b4e78ff8e1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding context for chunks of srep04487.pdf...:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1676b884844175ba7ae1bc7329e7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding context for chunks of srep03578.pdf...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88cbb97bdb941b989e2569c9adc292b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding context for chunks of srep01684.pdf...:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a395ede3a3d343459b719c504dcb4479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding context for chunks of s41586-019-1138-y.pd...:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee3fae3c1b0454cb84069386079f0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding context for chunks of s41598-020-77823-3.p...:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7759c9998d34a13a37b0a1cca4e10ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding context for chunks of Electron_Paramagneti...:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d3df0f2dba48edb153dd9493efd5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding context for chunks of Thermal_Imagery_for_...:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ed72e627e44544a0d3f338006e7a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding context for chunks of The_Graph_Database_J...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5e6d2b4a774d65bb8f5926b7162a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding context for chunks of The_Application_of_t...:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a601256e5f440938b9c5072ad96a6b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding context for chunks of Realization_of_a_Rub...:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for source in tqdm(study_names, desc=\"Chunking documents...\"):\n",
    "    # Free up CUDA memory between documents\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    entire_doc = \"\"\n",
    "    doc = converter.convert(f\"input/{source}\").document\n",
    "    chunks = list(chunker.chunk(dl_doc=doc))\n",
    "    # for chunk in chunks:                  # This is leftover from before we implemented sliding window for context provision\n",
    "    #     entire_doc += \" \" +chunk.text\n",
    "\n",
    "    entire_doc = \"FULL DOCUMENT:\\n\" + entire_doc\n",
    "\n",
    "    \n",
    "\n",
    "    for chunk in tqdm(chunks, desc=f\"Adding context for chunks of {source[:20]}...\", leave=False):    \n",
    "        entire_doc = \"\"\n",
    "        chunk_index = chunks.index(chunk)\n",
    "\n",
    "        context_length = 16_000 # Reduce window to save memory\n",
    "        context_length = context_length - 2 * MAX_TOKENS # We need to reserve space for the chunk itself (twice, the context contains the chunk itself)\n",
    "        total_context_chunk_number = context_length // (MAX_TOKENS*2) # 2x, cuz before and after the chunk\n",
    "\n",
    "        start_index_original = chunk_index - total_context_chunk_number\n",
    "        start_index_truncated = max(0, start_index_original) # Avoid index out of bounds\n",
    "\n",
    "        end_index_original = chunk_index + total_context_chunk_number\n",
    "        end_index_truncated = min(len(chunks)-1, end_index_original)\n",
    "\n",
    "        if start_index_original < 0: # We are at the start of the document, so we need to add more chunks at the end\n",
    "            end_index_truncated = min(len(chunks)-1, end_index_truncated + abs(start_index_original))\n",
    "        if end_index_original > len(chunks)-1: # We are at the end of the document, so we need to add more chunks at the start\n",
    "            start_index_truncated = max(0, start_index_truncated + abs(end_index_original - end_index_truncated))\n",
    "\n",
    "        for i in range(start_index_truncated, end_index_truncated + 1):\n",
    "            entire_doc += \" \" + chunks[i].text\n",
    "\n",
    "        entire_doc = \"FULL DOCUMENT:\\n\" + entire_doc\n",
    "\n",
    "        ollama_prompt = f\"CHUNK:\\n{chunk.text}\"\n",
    "        history =  [{'role': 'user', 'content': entire_doc}, {'role': 'user', 'content': ollama_prompt}]\n",
    "\n",
    "        response = ollama.chat(\n",
    "            model=\"chunker_full_doc\",\n",
    "            messages=history\n",
    "        )\n",
    "        context = response['message']['content']\n",
    "        # print(f\"Context for chunk: {context}\")\n",
    "        text_to_embed = chunk.text + \"\\n\\n\" + context # We put the context AFTER the chunk to not mess up cosine similarity but still\n",
    "\n",
    "        pages = set(\n",
    "                prov.page_no\n",
    "                for doc_item in chunk.meta.doc_items\n",
    "                for prov in doc_item.prov\n",
    "            )\n",
    "        id = hashlib.sha256(chunk.text.encode()).hexdigest()\n",
    "        chunks_with_metadata.append({'text': text_to_embed, 'original_text':chunk.text, 'context':context, 'document':source, 'pages':list(pages), 'id': id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e7b3232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to existing sliding_chunks_with_metadata.json file.\n",
      "Results saved to sliding_chunks_with_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Save the the processed chunks in case VectorDB upload goes wrong.\n",
    "# Luckily since this is a notebook, if the chunking is interrupted, we can still save the partial results here.\n",
    "# Append new chunks to the existing file if it exists, otherwise create it\n",
    "if os.path.exists(\"sliding_chunks_with_metadata.json\"):\n",
    "    print(\"Appending to existing sliding_chunks_with_metadata.json file.\")\n",
    "    with open(\"sliding_chunks_with_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        existing_data = json.load(f)\n",
    "    # Avoid duplicate entries by id\n",
    "    existing_ids = {chunk['id'] for chunk in existing_data}\n",
    "    new_chunks = [chunk for chunk in chunks_with_metadata if chunk['id'] not in existing_ids]\n",
    "    chunks_with_metadata = existing_data + new_chunks\n",
    "\n",
    "with open(\"sliding_chunks_with_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunks_with_metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Results saved to sliding_chunks_with_metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9490cd2",
   "metadata": {},
   "source": [
    "# Creating Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e7aa3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1b9c01f98b45369c61f30e01060dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading chunks to VectorDB:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n",
      "WARNING:root:Error occurred: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.62 GiB of which 16.75 MiB is free. Process 133293 has 2.76 GiB memory in use. Including non-PyTorch memory, this process has 4.81 GiB memory in use. Of the allocated memory 4.17 GiB is allocated by PyTorch, and 543.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) \n",
      " Retrying in 2.9652224071656823 seconds (retry 1 of 7) \n",
      "\n",
      "WARNING:transformers_modules.nomic-ai.nomic-bert-2048.7710840340a098cfb869c4f65e87cf2b1b70caca.modeling_hf_nomic_bert:<All keys matched successfully>\n",
      "WARNING:transformers_modules.nomic-ai.nomic-bert-2048.7710840340a098cfb869c4f65e87cf2b1b70caca.modeling_hf_nomic_bert:<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ColBERTRanker model colbert-ir/colbertv2.0 (this message can be suppressed by setting verbose=0)\n",
      "No device set\n",
      "Using device cuda\n",
      "No dtype set\n",
      "Using dtype torch.float32\n",
      "Loading model colbert-ir/colbertv2.0, this might take a while...\n",
      "Linear Dim set to: 128 for downcasting\n"
     ]
    }
   ],
   "source": [
    "registry = get_registry()\n",
    "hf = registry.get(\"huggingface\").create(name=embedding_model_name, trust_remote_code=True, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Define model\n",
    "class MyDocument(LanceModel):\n",
    "    text: str = hf.SourceField()\n",
    "    vector: Vector(hf.ndims()) = hf.VectorField()\n",
    "    original_text: str\n",
    "    context: str\n",
    "    document: str\n",
    "    pages: list[int]  # Any additional metadata\n",
    "    id: str  # Unique identifier for the chunk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "db = lancedb.connect(\"./db\")\n",
    "db.create_table(\"my_sliding_table\", schema=MyDocument, mode=\"overwrite\") # Uncomment this line when running this cell for the first time\n",
    "table = db.open_table(\"my_sliding_table\")\n",
    "\n",
    "# Upload in batches with progress bar\n",
    "batch_size = 100\n",
    "for i in tqdm(range(0, len(chunks_with_metadata), batch_size), desc=\"Uploading chunks to VectorDB\"):\n",
    "    batch = chunks_with_metadata[i:i+batch_size]\n",
    "    table.add(batch)\n",
    "\n",
    "table.create_scalar_index(\"id\", replace=True) # Index based on the chunk's id, used to manually prevent duplicates\n",
    "\n",
    "reranker = ColbertReranker()\n",
    "table.create_fts_index(\"text\", replace=True) # Used by the reranker as well as the hybrid search's BM25 index\n",
    "table.wait_for_index([\"text_idx\"])  # Wait for the indexing to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3357c7",
   "metadata": {},
   "source": [
    "# Example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e42b9cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers_modules.nomic-ai.nomic-bert-2048.7710840340a098cfb869c4f65e87cf2b1b70caca.modeling_hf_nomic_bert:<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>vector</th>\n",
       "      <th>original_text</th>\n",
       "      <th>context</th>\n",
       "      <th>document</th>\n",
       "      <th>pages</th>\n",
       "      <th>id</th>\n",
       "      <th>_relevance_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We collected stock market-related information ...</td>\n",
       "      <td>[0.5996058, 1.1112232, -3.3586943, -0.36808917...</td>\n",
       "      <td>We collected stock market-related information ...</td>\n",
       "      <td>Details the data collection process, specifica...</td>\n",
       "      <td>Stock_Market_Prediction_via_Multi-Source_Multi...</td>\n",
       "      <td>[6]</td>\n",
       "      <td>08aa975b95d5c9c782b71fcb335e2ab61751bb4c4f6bb1...</td>\n",
       "      <td>0.878606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Experimental design. Our paper relates to rese...</td>\n",
       "      <td>[0.62975305, 1.1866554, -2.867439, -0.6627391,...</td>\n",
       "      <td>Experimental design. Our paper relates to rese...</td>\n",
       "      <td>This section details the experimental design, ...</td>\n",
       "      <td>s41598-020-77823-3.pdf</td>\n",
       "      <td>[4, 5]</td>\n",
       "      <td>a3a29e665eb3215584b2cfab75bcaa99692dd7f3757763...</td>\n",
       "      <td>0.766360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XI ZHANG 1 , (Member, IEEE), SIYU QU 1 , JIEYU...</td>\n",
       "      <td>[-0.1864265, 1.2888607, -3.9965787, -0.2823099...</td>\n",
       "      <td>XI ZHANG 1 , (Member, IEEE), SIYU QU 1 , JIEYU...</td>\n",
       "      <td>Introduces the core problem and proposed solut...</td>\n",
       "      <td>Stock_Market_Prediction_via_Multi-Source_Multi...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>9ed6907515207f4a5e5cd48f826d4d5e8cc95ce11d0377...</td>\n",
       "      <td>0.600578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  We collected stock market-related information ...   \n",
       "1  Experimental design. Our paper relates to rese...   \n",
       "2  XI ZHANG 1 , (Member, IEEE), SIYU QU 1 , JIEYU...   \n",
       "\n",
       "                                              vector  \\\n",
       "0  [0.5996058, 1.1112232, -3.3586943, -0.36808917...   \n",
       "1  [0.62975305, 1.1866554, -2.867439, -0.6627391,...   \n",
       "2  [-0.1864265, 1.2888607, -3.9965787, -0.2823099...   \n",
       "\n",
       "                                       original_text  \\\n",
       "0  We collected stock market-related information ...   \n",
       "1  Experimental design. Our paper relates to rese...   \n",
       "2  XI ZHANG 1 , (Member, IEEE), SIYU QU 1 , JIEYU...   \n",
       "\n",
       "                                             context  \\\n",
       "0  Details the data collection process, specifica...   \n",
       "1  This section details the experimental design, ...   \n",
       "2  Introduces the core problem and proposed solut...   \n",
       "\n",
       "                                            document   pages  \\\n",
       "0  Stock_Market_Prediction_via_Multi-Source_Multi...     [6]   \n",
       "1                             s41598-020-77823-3.pdf  [4, 5]   \n",
       "2  Stock_Market_Prediction_via_Multi-Source_Multi...     [1]   \n",
       "\n",
       "                                                  id  _relevance_score  \n",
       "0  08aa975b95d5c9c782b71fcb335e2ab61751bb4c4f6bb1...          0.878606  \n",
       "1  a3a29e665eb3215584b2cfab75bcaa99692dd7f3757763...          0.766360  \n",
       "2  9ed6907515207f4a5e5cd48f826d4d5e8cc95ce11d0377...          0.600578  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = db.open_table(\"my_table\")\n",
    "prompt = \"How was the stock-market related information collected?\"\n",
    "results = table.search(prompt, query_type=\"hybrid\", vector_column_name=\"vector\", fts_columns=\"text\") \\\n",
    "            .rerank(reranker=reranker) \\\n",
    "            .limit(3) \\\n",
    "            .to_pandas()\n",
    "\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "190f09d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We collected stock market-related information from Jan. 1, 2015 to Dec. 31, 2016, and separate the information into two data sets, one for the year 2015 and the other for 2016. The data consist of three parts, the historical quantitative data, the news articles and the posts on the social network, which are introduced in detail as follows.\\n- GLYPH<15> Quantitative data : the source of quantitative data is Wind, 2 a widely used GLYPH<28>nancial information service provider in China. The data we collect are the average prices, market index change and turnover rate of the Shanghai Composite Index in each trading day.\\n- GLYPH<15> News data : we collect the news articles on the macro economy through Wind, and get 38,727 and 39,465 news articles in 2015 and 2016 respectively. The news articles are aggregated by Wind from major GLYPH<28>nancial news websites in China, such as http://GLYPH<28>nance.sina.com.cn and http://www.hexun.com. We process the news titles rather than the whole articles to extract the events, as the main topic of a news article is often summed up in the title.\\n- GLYPH<15> Social media data : the sentiments are extracted from the posts crawled from a popular investor social network in China named Xueqiu. 3 Totally 6,163,056 postings are collected for 2015 and 2016. For each post, we get the posting time stamp and the content.\\nFor each trading day, if the stock market index rises, it would be a positive instance, otherwise it is a negative instance. For each year, we use the data from the GLYPH<28>rst 10 months as the training set and the last 2 months as the testing set. We evaluate the performance of our model with varying lead days and varying historical days. Lead days refers to the number of days in advance the model makes predictions and the historical days indicates the number of days over which the multi-source information is utilized. The evaluation metrics we use are F1-score and accuracy (ACC).\\n\\nDetails the data collection process, specifically outlining the sources and types of data used for the studyâ€™s evaluation.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25663f04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantwise-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
